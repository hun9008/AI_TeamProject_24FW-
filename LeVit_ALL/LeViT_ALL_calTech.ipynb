{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gOt_dilyzGPf",
    "outputId": "4ac2f53f-5039-44e0-ce65-166719f3e257"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchinfo in /Users/jeong-yonghun/.pyenv/versions/3.10.0/lib/python3.10/site-packages (1.8.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gdown\n",
      "  Using cached gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting beautifulsoup4 (from gdown)\n",
      "  Using cached beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: filelock in /Users/jeong-yonghun/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from gdown) (3.16.1)\n",
      "Requirement already satisfied: requests[socks] in /Users/jeong-yonghun/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from gdown) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /Users/jeong-yonghun/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from gdown) (4.66.5)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4->gdown)\n",
      "  Using cached soupsieve-2.6-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/jeong-yonghun/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from requests[socks]->gdown) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jeong-yonghun/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from requests[socks]->gdown) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/jeong-yonghun/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from requests[socks]->gdown) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jeong-yonghun/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from requests[socks]->gdown) (2024.7.4)\n",
      "Collecting PySocks!=1.5.7,>=1.5.6 (from requests[socks]->gdown)\n",
      "  Using cached PySocks-1.7.1-py3-none-any.whl.metadata (13 kB)\n",
      "Using cached gdown-5.2.0-py3-none-any.whl (18 kB)\n",
      "Using cached beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
      "Using cached PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Using cached soupsieve-2.6-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: soupsieve, PySocks, beautifulsoup4, gdown\n",
      "Successfully installed PySocks-1.7.1 beautifulsoup4-4.12.3 gdown-5.2.0 soupsieve-2.6\n"
     ]
    }
   ],
   "source": [
    "!pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "tZSEFQzWzBPy"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeong-yonghun/.pyenv/versions/3.10.0/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split, ConcatDataset\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import itertools\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "5N0s6e-mzBPz"
   },
   "outputs": [],
   "source": [
    "class ConvNorm(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=2, padding=1):\n",
    "        super(ConvNorm, self).__init__()\n",
    "        self.linear = nn.Conv2d(\n",
    "            in_channels, out_channels, kernel_size=kernel_size,\n",
    "            stride=stride, padding=padding, bias=False\n",
    "        )\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = self.bn(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "fX0CnXd9zBP0"
   },
   "outputs": [],
   "source": [
    "class Stem16(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Stem16, self).__init__()\n",
    "        self.conv1 = ConvNorm(3, 32)\n",
    "        self.act1 = nn.Hardswish()\n",
    "        self.conv2 = ConvNorm(32, 64)\n",
    "        self.act2 = nn.Hardswish()\n",
    "        self.conv3 = ConvNorm(64, 128)\n",
    "        self.act3 = nn.Hardswish()\n",
    "        self.conv4 = ConvNorm(128, 256)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act1(self.conv1(x))\n",
    "        x = self.act2(self.conv2(x))\n",
    "        x = self.act3(self.conv3(x))\n",
    "        x = self.conv4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "7tw3fRfnzBP0"
   },
   "outputs": [],
   "source": [
    "class LinearNorm(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(LinearNorm, self).__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features, bias=False)\n",
    "        self.bn = nn.BatchNorm1d(out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        if x.dim() == 3:\n",
    "            B, N, C = x.shape\n",
    "            x = x.reshape(B * N, C)\n",
    "            x = self.bn(self.linear(x))\n",
    "            x = x.reshape(B, N, -1)\n",
    "        else:\n",
    "            x = self.bn(self.linear(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "rW7vhH2YzBP1"
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads, attn_ratio=2):\n",
    "        super(Attention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "        inner_dim = head_dim * num_heads * 3\n",
    "        self.qkv = LinearNorm(dim, inner_dim)\n",
    "\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Hardswish(),\n",
    "            LinearNorm(dim, dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x)\n",
    "        qkv = qkv.view(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        return self.proj(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ewOcRA2dzBP1"
   },
   "outputs": [],
   "source": [
    "class LevitMlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features):\n",
    "        super(LevitMlp, self).__init__()\n",
    "        self.ln1 = LinearNorm(in_features, hidden_features)\n",
    "        self.act = nn.Hardswish()\n",
    "        self.drop = nn.Dropout(p=0.5, inplace=False)#dropout 적용\n",
    "        self.ln2 = LinearNorm(hidden_features, out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.ln1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.ln2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "1m-F4YvfzBP2"
   },
   "outputs": [],
   "source": [
    "class LevitBlock(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=2):\n",
    "        super(LevitBlock, self).__init__()\n",
    "        self.attn = Attention(dim, num_heads)\n",
    "        self.drop_path1 = nn.Identity()\n",
    "        self.mlp = LevitMlp(dim, dim * mlp_ratio, dim)\n",
    "        self.drop_path2 = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.drop_path1(self.attn(x))\n",
    "        x = x + self.drop_path2(self.mlp(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "pDfX1xE1zBP2"
   },
   "outputs": [],
   "source": [
    "# class AttentionDownsample(nn.Module):\n",
    "#     def __init__(self, dim, out_dim, num_heads, attn_ratio=2):\n",
    "#         super(AttentionDownsample, self).__init__()\n",
    "#         self.num_heads = num_heads\n",
    "#         self.scale = (dim // num_heads) ** -0.5\n",
    "#         inner_dim = dim * attn_ratio * num_heads\n",
    "#         self.kv = LinearNorm(dim, inner_dim)\n",
    "\n",
    "#         self.q = nn.Sequential(\n",
    "#             nn.Conv2d(dim, dim, kernel_size=2, stride=2),\n",
    "#             nn.Flatten(start_dim=1)\n",
    "#         )\n",
    "\n",
    "#         self.proj = nn.Sequential(\n",
    "#             nn.Hardswish(),\n",
    "#             LinearNorm(dim, out_dim)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         B, N, C = x.shape\n",
    "#         H = W = int(N ** 0.5)\n",
    "#         x = x.reshape(B, C, H, W)\n",
    "\n",
    "#         kv = self.kv(x.flatten(2).transpose(1, 2))\n",
    "#         q = self.q(x)\n",
    "\n",
    "#         q = q.reshape(B, -1, C)\n",
    "#         x = self.proj(q)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Z26uegOwzBP2"
   },
   "outputs": [],
   "source": [
    "# class LevitDownsample(nn.Module):\n",
    "#     def __init__(self, dim, out_dim, num_heads, attn_ratio=2):\n",
    "#         super(LevitDownsample, self).__init__()\n",
    "#         self.attn_downsample = AttentionDownsample(dim, out_dim, num_heads, attn_ratio)\n",
    "#         self.mlp = LevitMlp(out_dim, out_dim * attn_ratio, out_dim)\n",
    "#         self.drop_path = nn.Identity()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.attn_downsample(x)\n",
    "#         x = self.drop_path(self.mlp(x))\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "p2DJrMeyJ2RO"
   },
   "outputs": [],
   "source": [
    "#CNNDownSample 적용\n",
    "class CNNDownsample(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(CNNDownsample, self).__init__()\n",
    "        self.out_channels = out_channels\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1)\n",
    "        self.act = nn.Hardswish()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "        B, N, C = x.shape # (B, N, C)  N=H*W (16 * 16 = 196)\n",
    "        H = int(np.sqrt(N))\n",
    "        x = x.view(B, H, H, C).permute(0, 3, 1, 2)\n",
    "        x = self.conv(x)\n",
    "        x = self.act(x)\n",
    "        x = x.permute(0, 2, 3, 1).view(B, -1, self.out_channels)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "oGsAuLPfzBP3"
   },
   "outputs": [],
   "source": [
    "class LevitStage(nn.Module):\n",
    "    def __init__(self, dim, out_dim, num_heads, num_blocks, downsample=True):\n",
    "        super(LevitStage, self).__init__()\n",
    "        self.downsample = CNNDownsample(dim, out_dim) if downsample else nn.Identity()\n",
    "        self.blocks = nn.Sequential(*[LevitBlock(out_dim, num_heads) for _ in range(num_blocks)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.downsample(x)\n",
    "        x = self.blocks(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "2PBkdZUuhsDI"
   },
   "outputs": [],
   "source": [
    "class ConvLevitStage(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, num_blocks, kernel_size, stride, padding):\n",
    "        super(ConvLevitStage, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            *[nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size, stride, padding)\n",
    "              for i in range(num_blocks)],\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "bBPec4pbzBP3"
   },
   "outputs": [],
   "source": [
    "class NormLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, dropout_prob=0.5):#drop_out_0.5 적용\n",
    "        super(NormLinear, self).__init__()\n",
    "        self.bn = nn.BatchNorm1d(in_features)\n",
    "        self.drop = nn.Dropout(p=dropout_prob, inplace=False)\n",
    "        self.linear = nn.Linear(in_features, out_features, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bn(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.linear(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "mt5kkekezBP3"
   },
   "outputs": [],
   "source": [
    "class LevitDistilled(nn.Module):\n",
    "    def __init__(self, num_classes=257):\n",
    "        super(LevitDistilled, self).__init__()\n",
    "\n",
    "        self.stem = Stem16()\n",
    "\n",
    "        self.stage1 = LevitStage(dim=256, out_dim=256, num_heads=4, num_blocks=2, downsample=False) # block 수 적용\n",
    "        self.stage2 = LevitStage(dim=256, out_dim=384, num_heads=6, num_blocks=2, downsample=True)\n",
    "\n",
    "        self.conv1x1 = nn.Sequential(\n",
    "            nn.Conv2d(384, 512, kernel_size=1, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.head = NormLinear(in_features=512, out_features=num_classes, dropout_prob=0.0)\n",
    "        self.head_dist = NormLinear(in_features=512, out_features=num_classes, dropout_prob=0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "\n",
    "        B, C, H, W = x.shape\n",
    "        x = x.view(B, C, -1).transpose(1, 2)\n",
    "        x = self.stage1(x)\n",
    "        x = self.stage2(x)\n",
    "\n",
    "        H = W = int(x.shape[1]**0.5)\n",
    "        x = x.transpose(1, 2).view(B, 384, H, W)\n",
    "\n",
    "        x = self.conv1x1(x)\n",
    "\n",
    "        x = torch.mean(x, dim=(2, 3))\n",
    "        out = self.head(x)\n",
    "        out_dist = self.head_dist(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kz4Um3NmzBP3",
    "outputId": "6dcb4967-531d-42a9-b15f-f43671293027"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LevitDistilled(\n",
      "  (stem): Stem16(\n",
      "    (conv1): ConvNorm(\n",
      "      (linear): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (act1): Hardswish()\n",
      "    (conv2): ConvNorm(\n",
      "      (linear): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (act2): Hardswish()\n",
      "    (conv3): ConvNorm(\n",
      "      (linear): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (act3): Hardswish()\n",
      "    (conv4): ConvNorm(\n",
      "      (linear): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (stage1): LevitStage(\n",
      "    (downsample): Identity()\n",
      "    (blocks): Sequential(\n",
      "      (0): LevitBlock(\n",
      "        (attn): Attention(\n",
      "          (qkv): LinearNorm(\n",
      "            (linear): Linear(in_features=256, out_features=768, bias=False)\n",
      "            (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (proj): Sequential(\n",
      "            (0): Hardswish()\n",
      "            (1): LinearNorm(\n",
      "              (linear): Linear(in_features=256, out_features=256, bias=False)\n",
      "              (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (drop_path1): Identity()\n",
      "        (mlp): LevitMlp(\n",
      "          (ln1): LinearNorm(\n",
      "            (linear): Linear(in_features=256, out_features=512, bias=False)\n",
      "            (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (act): Hardswish()\n",
      "          (drop): Dropout(p=0.5, inplace=False)\n",
      "          (ln2): LinearNorm(\n",
      "            (linear): Linear(in_features=512, out_features=256, bias=False)\n",
      "            (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "      (1): LevitBlock(\n",
      "        (attn): Attention(\n",
      "          (qkv): LinearNorm(\n",
      "            (linear): Linear(in_features=256, out_features=768, bias=False)\n",
      "            (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (proj): Sequential(\n",
      "            (0): Hardswish()\n",
      "            (1): LinearNorm(\n",
      "              (linear): Linear(in_features=256, out_features=256, bias=False)\n",
      "              (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (drop_path1): Identity()\n",
      "        (mlp): LevitMlp(\n",
      "          (ln1): LinearNorm(\n",
      "            (linear): Linear(in_features=256, out_features=512, bias=False)\n",
      "            (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (act): Hardswish()\n",
      "          (drop): Dropout(p=0.5, inplace=False)\n",
      "          (ln2): LinearNorm(\n",
      "            (linear): Linear(in_features=512, out_features=256, bias=False)\n",
      "            (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (stage2): LevitStage(\n",
      "    (downsample): CNNDownsample(\n",
      "      (conv): Conv2d(256, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (act): Hardswish()\n",
      "    )\n",
      "    (blocks): Sequential(\n",
      "      (0): LevitBlock(\n",
      "        (attn): Attention(\n",
      "          (qkv): LinearNorm(\n",
      "            (linear): Linear(in_features=384, out_features=1152, bias=False)\n",
      "            (bn): BatchNorm1d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (proj): Sequential(\n",
      "            (0): Hardswish()\n",
      "            (1): LinearNorm(\n",
      "              (linear): Linear(in_features=384, out_features=384, bias=False)\n",
      "              (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (drop_path1): Identity()\n",
      "        (mlp): LevitMlp(\n",
      "          (ln1): LinearNorm(\n",
      "            (linear): Linear(in_features=384, out_features=768, bias=False)\n",
      "            (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (act): Hardswish()\n",
      "          (drop): Dropout(p=0.5, inplace=False)\n",
      "          (ln2): LinearNorm(\n",
      "            (linear): Linear(in_features=768, out_features=384, bias=False)\n",
      "            (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "      (1): LevitBlock(\n",
      "        (attn): Attention(\n",
      "          (qkv): LinearNorm(\n",
      "            (linear): Linear(in_features=384, out_features=1152, bias=False)\n",
      "            (bn): BatchNorm1d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (proj): Sequential(\n",
      "            (0): Hardswish()\n",
      "            (1): LinearNorm(\n",
      "              (linear): Linear(in_features=384, out_features=384, bias=False)\n",
      "              (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (drop_path1): Identity()\n",
      "        (mlp): LevitMlp(\n",
      "          (ln1): LinearNorm(\n",
      "            (linear): Linear(in_features=384, out_features=768, bias=False)\n",
      "            (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (act): Hardswish()\n",
      "          (drop): Dropout(p=0.5, inplace=False)\n",
      "          (ln2): LinearNorm(\n",
      "            (linear): Linear(in_features=768, out_features=384, bias=False)\n",
      "            (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (conv1x1): Sequential(\n",
      "    (0): Conv2d(384, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (head): NormLinear(\n",
      "    (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (linear): Linear(in_features=512, out_features=257, bias=True)\n",
      "  )\n",
      "  (head_dist): NormLinear(\n",
      "    (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (linear): Linear(in_features=512, out_features=257, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = LevitDistilled()\n",
    "# model = LauncherModel()\n",
    "print(model)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bmb6Hs2cO5_F",
    "outputId": "488b75fe-7e0a-44ad-b7f4-ce6e1c0ab662"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================================\n",
      "Layer (type:depth-idx)                                  Output Shape              Param #\n",
      "=========================================================================================================\n",
      "LevitDistilled                                          [32, 257]                 --\n",
      "├─Stem16: 1-1                                           [32, 256, 14, 14]         --\n",
      "│    └─ConvNorm: 2-1                                    [32, 32, 112, 112]        --\n",
      "│    │    └─Conv2d: 3-1                                 [32, 32, 112, 112]        864\n",
      "│    │    └─BatchNorm2d: 3-2                            [32, 32, 112, 112]        64\n",
      "│    └─Hardswish: 2-2                                   [32, 32, 112, 112]        --\n",
      "│    └─ConvNorm: 2-3                                    [32, 64, 56, 56]          --\n",
      "│    │    └─Conv2d: 3-3                                 [32, 64, 56, 56]          18,432\n",
      "│    │    └─BatchNorm2d: 3-4                            [32, 64, 56, 56]          128\n",
      "│    └─Hardswish: 2-4                                   [32, 64, 56, 56]          --\n",
      "│    └─ConvNorm: 2-5                                    [32, 128, 28, 28]         --\n",
      "│    │    └─Conv2d: 3-5                                 [32, 128, 28, 28]         73,728\n",
      "│    │    └─BatchNorm2d: 3-6                            [32, 128, 28, 28]         256\n",
      "│    └─Hardswish: 2-6                                   [32, 128, 28, 28]         --\n",
      "│    └─ConvNorm: 2-7                                    [32, 256, 14, 14]         --\n",
      "│    │    └─Conv2d: 3-7                                 [32, 256, 14, 14]         294,912\n",
      "│    │    └─BatchNorm2d: 3-8                            [32, 256, 14, 14]         512\n",
      "├─LevitStage: 1-2                                       [32, 196, 256]            --\n",
      "│    └─Identity: 2-8                                    [32, 196, 256]            --\n",
      "│    └─Sequential: 2-9                                  [32, 196, 256]            --\n",
      "│    │    └─LevitBlock: 3-9                             [32, 196, 256]            527,872\n",
      "│    │    └─LevitBlock: 3-10                            [32, 196, 256]            527,872\n",
      "├─LevitStage: 1-3                                       [32, 49, 384]             --\n",
      "│    └─CNNDownsample: 2-10                              [32, 49, 384]             --\n",
      "│    │    └─Conv2d: 3-11                                [32, 384, 7, 7]           885,120\n",
      "│    │    └─Hardswish: 3-12                             [32, 384, 7, 7]           --\n",
      "│    └─Sequential: 2-11                                 [32, 49, 384]             --\n",
      "│    │    └─LevitBlock: 3-13                            [32, 49, 384]             1,185,024\n",
      "│    │    └─LevitBlock: 3-14                            [32, 49, 384]             1,185,024\n",
      "├─Sequential: 1-4                                       [32, 512, 7, 7]           --\n",
      "│    └─Conv2d: 2-12                                     [32, 512, 7, 7]           197,120\n",
      "│    └─BatchNorm2d: 2-13                                [32, 512, 7, 7]           1,024\n",
      "│    └─ReLU: 2-14                                       [32, 512, 7, 7]           --\n",
      "├─NormLinear: 1-5                                       [32, 257]                 --\n",
      "│    └─BatchNorm1d: 2-15                                [32, 512]                 1,024\n",
      "│    └─Dropout: 2-16                                    [32, 512]                 --\n",
      "│    └─Linear: 2-17                                     [32, 257]                 131,841\n",
      "├─NormLinear: 1-6                                       [32, 257]                 --\n",
      "│    └─BatchNorm1d: 2-18                                [32, 512]                 1,024\n",
      "│    └─Dropout: 2-19                                    [32, 512]                 --\n",
      "│    └─Linear: 2-20                                     [32, 257]                 131,841\n",
      "=========================================================================================================\n",
      "Total params: 5,163,682\n",
      "Trainable params: 5,163,682\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 17.94\n",
      "=========================================================================================================\n",
      "Input size (MB): 19.27\n",
      "Forward/backward pass size (MB): 897.94\n",
      "Params size (MB): 20.65\n",
      "Estimated Total Size (MB): 937.86\n",
      "=========================================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(summary(model, input_size=(32, 3, 224, 224)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M7rBknzRzBP3",
    "outputId": "13f79afe-fbcd-4146-b9f9-558d0ff13ab4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================================\n",
      "Layer (type:depth-idx)                                  Output Shape              Param #\n",
      "=========================================================================================================\n",
      "LevitDistilled                                          [32, 257]                 --\n",
      "├─Stem16: 1-1                                           [32, 256, 14, 14]         --\n",
      "│    └─conv1.linear.weight                                                        ├─864\n",
      "│    └─conv1.bn.weight                                                            ├─32\n",
      "│    └─conv1.bn.bias                                                              ├─32\n",
      "│    └─conv2.linear.weight                                                        ├─18,432\n",
      "│    └─conv2.bn.weight                                                            ├─64\n",
      "│    └─conv2.bn.bias                                                              ├─64\n",
      "│    └─conv3.linear.weight                                                        ├─73,728\n",
      "│    └─conv3.bn.weight                                                            ├─128\n",
      "│    └─conv3.bn.bias                                                              ├─128\n",
      "│    └─conv4.linear.weight                                                        ├─294,912\n",
      "│    └─conv4.bn.weight                                                            ├─256\n",
      "│    └─conv4.bn.bias                                                              └─256\n",
      "│    └─ConvNorm: 2-1                                    [32, 32, 112, 112]        --\n",
      "│    │    └─linear.weight                                                         ├─864\n",
      "│    │    └─bn.weight                                                             ├─32\n",
      "│    │    └─bn.bias                                                               └─32\n",
      "│    │    └─Conv2d: 3-1                                 [32, 32, 112, 112]        864\n",
      "│    │    │    └─weight                                                           └─864\n",
      "│    │    └─BatchNorm2d: 3-2                            [32, 32, 112, 112]        64\n",
      "│    │    │    └─weight                                                           ├─32\n",
      "│    │    │    └─bias                                                             └─32\n",
      "│    └─Hardswish: 2-2                                   [32, 32, 112, 112]        --\n",
      "│    └─ConvNorm: 2-3                                    [32, 64, 56, 56]          --\n",
      "│    │    └─linear.weight                                                         ├─18,432\n",
      "│    │    └─bn.weight                                                             ├─64\n",
      "│    │    └─bn.bias                                                               └─64\n",
      "│    │    └─Conv2d: 3-3                                 [32, 64, 56, 56]          18,432\n",
      "│    │    │    └─weight                                                           └─18,432\n",
      "│    │    └─BatchNorm2d: 3-4                            [32, 64, 56, 56]          128\n",
      "│    │    │    └─weight                                                           ├─64\n",
      "│    │    │    └─bias                                                             └─64\n",
      "│    └─Hardswish: 2-4                                   [32, 64, 56, 56]          --\n",
      "│    └─ConvNorm: 2-5                                    [32, 128, 28, 28]         --\n",
      "│    │    └─linear.weight                                                         ├─73,728\n",
      "│    │    └─bn.weight                                                             ├─128\n",
      "│    │    └─bn.bias                                                               └─128\n",
      "│    │    └─Conv2d: 3-5                                 [32, 128, 28, 28]         73,728\n",
      "│    │    │    └─weight                                                           └─73,728\n",
      "│    │    └─BatchNorm2d: 3-6                            [32, 128, 28, 28]         256\n",
      "│    │    │    └─weight                                                           ├─128\n",
      "│    │    │    └─bias                                                             └─128\n",
      "│    └─Hardswish: 2-6                                   [32, 128, 28, 28]         --\n",
      "│    └─ConvNorm: 2-7                                    [32, 256, 14, 14]         --\n",
      "│    │    └─linear.weight                                                         ├─294,912\n",
      "│    │    └─bn.weight                                                             ├─256\n",
      "│    │    └─bn.bias                                                               └─256\n",
      "│    │    └─Conv2d: 3-7                                 [32, 256, 14, 14]         294,912\n",
      "│    │    │    └─weight                                                           └─294,912\n",
      "│    │    └─BatchNorm2d: 3-8                            [32, 256, 14, 14]         512\n",
      "│    │    │    └─weight                                                           ├─256\n",
      "│    │    │    └─bias                                                             └─256\n",
      "├─LevitStage: 1-2                                       [32, 196, 256]            --\n",
      "│    └─blocks.0.attn.qkv.linear.weight                                            ├─196,608\n",
      "│    └─blocks.0.attn.qkv.bn.weight                                                ├─768\n",
      "│    └─blocks.0.attn.qkv.bn.bias                                                  ├─768\n",
      "│    └─blocks.0.attn.proj.1.linear.weight                                         ├─65,536\n",
      "│    └─blocks.0.attn.proj.1.bn.weight                                             ├─256\n",
      "│    └─blocks.0.attn.proj.1.bn.bias                                               ├─256\n",
      "│    └─blocks.0.mlp.ln1.linear.weight                                             ├─131,072\n",
      "│    └─blocks.0.mlp.ln1.bn.weight                                                 ├─512\n",
      "│    └─blocks.0.mlp.ln1.bn.bias                                                   ├─512\n",
      "│    └─blocks.0.mlp.ln2.linear.weight                                             ├─131,072\n",
      "│    └─blocks.0.mlp.ln2.bn.weight                                                 ├─256\n",
      "│    └─blocks.0.mlp.ln2.bn.bias                                                   ├─256\n",
      "│    └─blocks.1.attn.qkv.linear.weight                                            ├─196,608\n",
      "│    └─blocks.1.attn.qkv.bn.weight                                                ├─768\n",
      "│    └─blocks.1.attn.qkv.bn.bias                                                  ├─768\n",
      "│    └─blocks.1.attn.proj.1.linear.weight                                         ├─65,536\n",
      "│    └─blocks.1.attn.proj.1.bn.weight                                             ├─256\n",
      "│    └─blocks.1.attn.proj.1.bn.bias                                               ├─256\n",
      "│    └─blocks.1.mlp.ln1.linear.weight                                             ├─131,072\n",
      "│    └─blocks.1.mlp.ln1.bn.weight                                                 ├─512\n",
      "│    └─blocks.1.mlp.ln1.bn.bias                                                   ├─512\n",
      "│    └─blocks.1.mlp.ln2.linear.weight                                             ├─131,072\n",
      "│    └─blocks.1.mlp.ln2.bn.weight                                                 ├─256\n",
      "│    └─blocks.1.mlp.ln2.bn.bias                                                   └─256\n",
      "│    └─Identity: 2-8                                    [32, 196, 256]            --\n",
      "│    └─Sequential: 2-9                                  [32, 196, 256]            --\n",
      "│    │    └─0.attn.qkv.linear.weight                                              ├─196,608\n",
      "│    │    └─0.attn.qkv.bn.weight                                                  ├─768\n",
      "│    │    └─0.attn.qkv.bn.bias                                                    ├─768\n",
      "│    │    └─0.attn.proj.1.linear.weight                                           ├─65,536\n",
      "│    │    └─0.attn.proj.1.bn.weight                                               ├─256\n",
      "│    │    └─0.attn.proj.1.bn.bias                                                 ├─256\n",
      "│    │    └─0.mlp.ln1.linear.weight                                               ├─131,072\n",
      "│    │    └─0.mlp.ln1.bn.weight                                                   ├─512\n",
      "│    │    └─0.mlp.ln1.bn.bias                                                     ├─512\n",
      "│    │    └─0.mlp.ln2.linear.weight                                               ├─131,072\n",
      "│    │    └─0.mlp.ln2.bn.weight                                                   ├─256\n",
      "│    │    └─0.mlp.ln2.bn.bias                                                     ├─256\n",
      "│    │    └─1.attn.qkv.linear.weight                                              ├─196,608\n",
      "│    │    └─1.attn.qkv.bn.weight                                                  ├─768\n",
      "│    │    └─1.attn.qkv.bn.bias                                                    ├─768\n",
      "│    │    └─1.attn.proj.1.linear.weight                                           ├─65,536\n",
      "│    │    └─1.attn.proj.1.bn.weight                                               ├─256\n",
      "│    │    └─1.attn.proj.1.bn.bias                                                 ├─256\n",
      "│    │    └─1.mlp.ln1.linear.weight                                               ├─131,072\n",
      "│    │    └─1.mlp.ln1.bn.weight                                                   ├─512\n",
      "│    │    └─1.mlp.ln1.bn.bias                                                     ├─512\n",
      "│    │    └─1.mlp.ln2.linear.weight                                               ├─131,072\n",
      "│    │    └─1.mlp.ln2.bn.weight                                                   ├─256\n",
      "│    │    └─1.mlp.ln2.bn.bias                                                     └─256\n",
      "│    │    └─LevitBlock: 3-9                             [32, 196, 256]            527,872\n",
      "│    │    │    └─attn.qkv.linear.weight                                           ├─196,608\n",
      "│    │    │    └─attn.qkv.bn.weight                                               ├─768\n",
      "│    │    │    └─attn.qkv.bn.bias                                                 ├─768\n",
      "│    │    │    └─attn.proj.1.linear.weight                                        ├─65,536\n",
      "│    │    │    └─attn.proj.1.bn.weight                                            ├─256\n",
      "│    │    │    └─attn.proj.1.bn.bias                                              ├─256\n",
      "│    │    │    └─mlp.ln1.linear.weight                                            ├─131,072\n",
      "│    │    │    └─mlp.ln1.bn.weight                                                ├─512\n",
      "│    │    │    └─mlp.ln1.bn.bias                                                  ├─512\n",
      "│    │    │    └─mlp.ln2.linear.weight                                            ├─131,072\n",
      "│    │    │    └─mlp.ln2.bn.weight                                                ├─256\n",
      "│    │    │    └─mlp.ln2.bn.bias                                                  └─256\n",
      "│    │    └─LevitBlock: 3-10                            [32, 196, 256]            527,872\n",
      "│    │    │    └─attn.qkv.linear.weight                                           ├─196,608\n",
      "│    │    │    └─attn.qkv.bn.weight                                               ├─768\n",
      "│    │    │    └─attn.qkv.bn.bias                                                 ├─768\n",
      "│    │    │    └─attn.proj.1.linear.weight                                        ├─65,536\n",
      "│    │    │    └─attn.proj.1.bn.weight                                            ├─256\n",
      "│    │    │    └─attn.proj.1.bn.bias                                              ├─256\n",
      "│    │    │    └─mlp.ln1.linear.weight                                            ├─131,072\n",
      "│    │    │    └─mlp.ln1.bn.weight                                                ├─512\n",
      "│    │    │    └─mlp.ln1.bn.bias                                                  ├─512\n",
      "│    │    │    └─mlp.ln2.linear.weight                                            ├─131,072\n",
      "│    │    │    └─mlp.ln2.bn.weight                                                ├─256\n",
      "│    │    │    └─mlp.ln2.bn.bias                                                  └─256\n",
      "├─LevitStage: 1-3                                       [32, 49, 384]             --\n",
      "│    └─downsample.conv.weight                                                     ├─884,736\n",
      "│    └─downsample.conv.bias                                                       ├─384\n",
      "│    └─blocks.0.attn.qkv.linear.weight                                            ├─442,368\n",
      "│    └─blocks.0.attn.qkv.bn.weight                                                ├─1,152\n",
      "│    └─blocks.0.attn.qkv.bn.bias                                                  ├─1,152\n",
      "│    └─blocks.0.attn.proj.1.linear.weight                                         ├─147,456\n",
      "│    └─blocks.0.attn.proj.1.bn.weight                                             ├─384\n",
      "│    └─blocks.0.attn.proj.1.bn.bias                                               ├─384\n",
      "│    └─blocks.0.mlp.ln1.linear.weight                                             ├─294,912\n",
      "│    └─blocks.0.mlp.ln1.bn.weight                                                 ├─768\n",
      "│    └─blocks.0.mlp.ln1.bn.bias                                                   ├─768\n",
      "│    └─blocks.0.mlp.ln2.linear.weight                                             ├─294,912\n",
      "│    └─blocks.0.mlp.ln2.bn.weight                                                 ├─384\n",
      "│    └─blocks.0.mlp.ln2.bn.bias                                                   ├─384\n",
      "│    └─blocks.1.attn.qkv.linear.weight                                            ├─442,368\n",
      "│    └─blocks.1.attn.qkv.bn.weight                                                ├─1,152\n",
      "│    └─blocks.1.attn.qkv.bn.bias                                                  ├─1,152\n",
      "│    └─blocks.1.attn.proj.1.linear.weight                                         ├─147,456\n",
      "│    └─blocks.1.attn.proj.1.bn.weight                                             ├─384\n",
      "│    └─blocks.1.attn.proj.1.bn.bias                                               ├─384\n",
      "│    └─blocks.1.mlp.ln1.linear.weight                                             ├─294,912\n",
      "│    └─blocks.1.mlp.ln1.bn.weight                                                 ├─768\n",
      "│    └─blocks.1.mlp.ln1.bn.bias                                                   ├─768\n",
      "│    └─blocks.1.mlp.ln2.linear.weight                                             ├─294,912\n",
      "│    └─blocks.1.mlp.ln2.bn.weight                                                 ├─384\n",
      "│    └─blocks.1.mlp.ln2.bn.bias                                                   └─384\n",
      "│    └─CNNDownsample: 2-10                              [32, 49, 384]             --\n",
      "│    │    └─conv.weight                                                           ├─884,736\n",
      "│    │    └─conv.bias                                                             └─384\n",
      "│    │    └─Conv2d: 3-11                                [32, 384, 7, 7]           885,120\n",
      "│    │    │    └─weight                                                           ├─884,736\n",
      "│    │    │    └─bias                                                             └─384\n",
      "│    │    └─Hardswish: 3-12                             [32, 384, 7, 7]           --\n",
      "│    └─Sequential: 2-11                                 [32, 49, 384]             --\n",
      "│    │    └─0.attn.qkv.linear.weight                                              ├─442,368\n",
      "│    │    └─0.attn.qkv.bn.weight                                                  ├─1,152\n",
      "│    │    └─0.attn.qkv.bn.bias                                                    ├─1,152\n",
      "│    │    └─0.attn.proj.1.linear.weight                                           ├─147,456\n",
      "│    │    └─0.attn.proj.1.bn.weight                                               ├─384\n",
      "│    │    └─0.attn.proj.1.bn.bias                                                 ├─384\n",
      "│    │    └─0.mlp.ln1.linear.weight                                               ├─294,912\n",
      "│    │    └─0.mlp.ln1.bn.weight                                                   ├─768\n",
      "│    │    └─0.mlp.ln1.bn.bias                                                     ├─768\n",
      "│    │    └─0.mlp.ln2.linear.weight                                               ├─294,912\n",
      "│    │    └─0.mlp.ln2.bn.weight                                                   ├─384\n",
      "│    │    └─0.mlp.ln2.bn.bias                                                     ├─384\n",
      "│    │    └─1.attn.qkv.linear.weight                                              ├─442,368\n",
      "│    │    └─1.attn.qkv.bn.weight                                                  ├─1,152\n",
      "│    │    └─1.attn.qkv.bn.bias                                                    ├─1,152\n",
      "│    │    └─1.attn.proj.1.linear.weight                                           ├─147,456\n",
      "│    │    └─1.attn.proj.1.bn.weight                                               ├─384\n",
      "│    │    └─1.attn.proj.1.bn.bias                                                 ├─384\n",
      "│    │    └─1.mlp.ln1.linear.weight                                               ├─294,912\n",
      "│    │    └─1.mlp.ln1.bn.weight                                                   ├─768\n",
      "│    │    └─1.mlp.ln1.bn.bias                                                     ├─768\n",
      "│    │    └─1.mlp.ln2.linear.weight                                               ├─294,912\n",
      "│    │    └─1.mlp.ln2.bn.weight                                                   ├─384\n",
      "│    │    └─1.mlp.ln2.bn.bias                                                     └─384\n",
      "│    │    └─LevitBlock: 3-13                            [32, 49, 384]             1,185,024\n",
      "│    │    │    └─attn.qkv.linear.weight                                           ├─442,368\n",
      "│    │    │    └─attn.qkv.bn.weight                                               ├─1,152\n",
      "│    │    │    └─attn.qkv.bn.bias                                                 ├─1,152\n",
      "│    │    │    └─attn.proj.1.linear.weight                                        ├─147,456\n",
      "│    │    │    └─attn.proj.1.bn.weight                                            ├─384\n",
      "│    │    │    └─attn.proj.1.bn.bias                                              ├─384\n",
      "│    │    │    └─mlp.ln1.linear.weight                                            ├─294,912\n",
      "│    │    │    └─mlp.ln1.bn.weight                                                ├─768\n",
      "│    │    │    └─mlp.ln1.bn.bias                                                  ├─768\n",
      "│    │    │    └─mlp.ln2.linear.weight                                            ├─294,912\n",
      "│    │    │    └─mlp.ln2.bn.weight                                                ├─384\n",
      "│    │    │    └─mlp.ln2.bn.bias                                                  └─384\n",
      "│    │    └─LevitBlock: 3-14                            [32, 49, 384]             1,185,024\n",
      "│    │    │    └─attn.qkv.linear.weight                                           ├─442,368\n",
      "│    │    │    └─attn.qkv.bn.weight                                               ├─1,152\n",
      "│    │    │    └─attn.qkv.bn.bias                                                 ├─1,152\n",
      "│    │    │    └─attn.proj.1.linear.weight                                        ├─147,456\n",
      "│    │    │    └─attn.proj.1.bn.weight                                            ├─384\n",
      "│    │    │    └─attn.proj.1.bn.bias                                              ├─384\n",
      "│    │    │    └─mlp.ln1.linear.weight                                            ├─294,912\n",
      "│    │    │    └─mlp.ln1.bn.weight                                                ├─768\n",
      "│    │    │    └─mlp.ln1.bn.bias                                                  ├─768\n",
      "│    │    │    └─mlp.ln2.linear.weight                                            ├─294,912\n",
      "│    │    │    └─mlp.ln2.bn.weight                                                ├─384\n",
      "│    │    │    └─mlp.ln2.bn.bias                                                  └─384\n",
      "├─Sequential: 1-4                                       [32, 512, 7, 7]           --\n",
      "│    └─0.weight                                                                   ├─196,608\n",
      "│    └─0.bias                                                                     ├─512\n",
      "│    └─1.weight                                                                   ├─512\n",
      "│    └─1.bias                                                                     └─512\n",
      "│    └─Conv2d: 2-12                                     [32, 512, 7, 7]           197,120\n",
      "│    │    └─weight                                                                ├─196,608\n",
      "│    │    └─bias                                                                  └─512\n",
      "│    └─BatchNorm2d: 2-13                                [32, 512, 7, 7]           1,024\n",
      "│    │    └─weight                                                                ├─512\n",
      "│    │    └─bias                                                                  └─512\n",
      "│    └─ReLU: 2-14                                       [32, 512, 7, 7]           --\n",
      "├─NormLinear: 1-5                                       [32, 257]                 --\n",
      "│    └─bn.weight                                                                  ├─512\n",
      "│    └─bn.bias                                                                    ├─512\n",
      "│    └─linear.weight                                                              ├─131,584\n",
      "│    └─linear.bias                                                                └─257\n",
      "│    └─BatchNorm1d: 2-15                                [32, 512]                 1,024\n",
      "│    │    └─weight                                                                ├─512\n",
      "│    │    └─bias                                                                  └─512\n",
      "│    └─Dropout: 2-16                                    [32, 512]                 --\n",
      "│    └─Linear: 2-17                                     [32, 257]                 131,841\n",
      "│    │    └─weight                                                                ├─131,584\n",
      "│    │    └─bias                                                                  └─257\n",
      "├─NormLinear: 1-6                                       [32, 257]                 --\n",
      "│    └─bn.weight                                                                  ├─512\n",
      "│    └─bn.bias                                                                    ├─512\n",
      "│    └─linear.weight                                                              ├─131,584\n",
      "│    └─linear.bias                                                                └─257\n",
      "│    └─BatchNorm1d: 2-18                                [32, 512]                 1,024\n",
      "│    │    └─weight                                                                ├─512\n",
      "│    │    └─bias                                                                  └─512\n",
      "│    └─Dropout: 2-19                                    [32, 512]                 --\n",
      "│    └─Linear: 2-20                                     [32, 257]                 131,841\n",
      "│    │    └─weight                                                                ├─131,584\n",
      "│    │    └─bias                                                                  └─257\n",
      "=========================================================================================================\n",
      "Total params: 5,163,682\n",
      "Trainable params: 5,163,682\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 17.94\n",
      "=========================================================================================================\n",
      "Input size (MB): 19.27\n",
      "Forward/backward pass size (MB): 897.94\n",
      "Params size (MB): 20.65\n",
      "Estimated Total Size (MB): 937.86\n",
      "=========================================================================================================\n",
      "=========================================================================================================\n",
      "Layer (type:depth-idx)                                  Output Shape              Param #\n",
      "=========================================================================================================\n",
      "LevitDistilled                                          [32, 257]                 --\n",
      "├─Stem16: 1-1                                           [32, 256, 14, 14]         --\n",
      "│    └─conv1.linear.weight                                                        ├─864\n",
      "│    └─conv1.bn.weight                                                            ├─32\n",
      "│    └─conv1.bn.bias                                                              ├─32\n",
      "│    └─conv2.linear.weight                                                        ├─18,432\n",
      "│    └─conv2.bn.weight                                                            ├─64\n",
      "│    └─conv2.bn.bias                                                              ├─64\n",
      "│    └─conv3.linear.weight                                                        ├─73,728\n",
      "│    └─conv3.bn.weight                                                            ├─128\n",
      "│    └─conv3.bn.bias                                                              ├─128\n",
      "│    └─conv4.linear.weight                                                        ├─294,912\n",
      "│    └─conv4.bn.weight                                                            ├─256\n",
      "│    └─conv4.bn.bias                                                              └─256\n",
      "│    └─ConvNorm: 2-1                                    [32, 32, 112, 112]        --\n",
      "│    │    └─linear.weight                                                         ├─864\n",
      "│    │    └─bn.weight                                                             ├─32\n",
      "│    │    └─bn.bias                                                               └─32\n",
      "│    │    └─Conv2d: 3-1                                 [32, 32, 112, 112]        864\n",
      "│    │    │    └─weight                                                           └─864\n",
      "│    │    └─BatchNorm2d: 3-2                            [32, 32, 112, 112]        64\n",
      "│    │    │    └─weight                                                           ├─32\n",
      "│    │    │    └─bias                                                             └─32\n",
      "│    └─Hardswish: 2-2                                   [32, 32, 112, 112]        --\n",
      "│    └─ConvNorm: 2-3                                    [32, 64, 56, 56]          --\n",
      "│    │    └─linear.weight                                                         ├─18,432\n",
      "│    │    └─bn.weight                                                             ├─64\n",
      "│    │    └─bn.bias                                                               └─64\n",
      "│    │    └─Conv2d: 3-3                                 [32, 64, 56, 56]          18,432\n",
      "│    │    │    └─weight                                                           └─18,432\n",
      "│    │    └─BatchNorm2d: 3-4                            [32, 64, 56, 56]          128\n",
      "│    │    │    └─weight                                                           ├─64\n",
      "│    │    │    └─bias                                                             └─64\n",
      "│    └─Hardswish: 2-4                                   [32, 64, 56, 56]          --\n",
      "│    └─ConvNorm: 2-5                                    [32, 128, 28, 28]         --\n",
      "│    │    └─linear.weight                                                         ├─73,728\n",
      "│    │    └─bn.weight                                                             ├─128\n",
      "│    │    └─bn.bias                                                               └─128\n",
      "│    │    └─Conv2d: 3-5                                 [32, 128, 28, 28]         73,728\n",
      "│    │    │    └─weight                                                           └─73,728\n",
      "│    │    └─BatchNorm2d: 3-6                            [32, 128, 28, 28]         256\n",
      "│    │    │    └─weight                                                           ├─128\n",
      "│    │    │    └─bias                                                             └─128\n",
      "│    └─Hardswish: 2-6                                   [32, 128, 28, 28]         --\n",
      "│    └─ConvNorm: 2-7                                    [32, 256, 14, 14]         --\n",
      "│    │    └─linear.weight                                                         ├─294,912\n",
      "│    │    └─bn.weight                                                             ├─256\n",
      "│    │    └─bn.bias                                                               └─256\n",
      "│    │    └─Conv2d: 3-7                                 [32, 256, 14, 14]         294,912\n",
      "│    │    │    └─weight                                                           └─294,912\n",
      "│    │    └─BatchNorm2d: 3-8                            [32, 256, 14, 14]         512\n",
      "│    │    │    └─weight                                                           ├─256\n",
      "│    │    │    └─bias                                                             └─256\n",
      "├─LevitStage: 1-2                                       [32, 196, 256]            --\n",
      "│    └─blocks.0.attn.qkv.linear.weight                                            ├─196,608\n",
      "│    └─blocks.0.attn.qkv.bn.weight                                                ├─768\n",
      "│    └─blocks.0.attn.qkv.bn.bias                                                  ├─768\n",
      "│    └─blocks.0.attn.proj.1.linear.weight                                         ├─65,536\n",
      "│    └─blocks.0.attn.proj.1.bn.weight                                             ├─256\n",
      "│    └─blocks.0.attn.proj.1.bn.bias                                               ├─256\n",
      "│    └─blocks.0.mlp.ln1.linear.weight                                             ├─131,072\n",
      "│    └─blocks.0.mlp.ln1.bn.weight                                                 ├─512\n",
      "│    └─blocks.0.mlp.ln1.bn.bias                                                   ├─512\n",
      "│    └─blocks.0.mlp.ln2.linear.weight                                             ├─131,072\n",
      "│    └─blocks.0.mlp.ln2.bn.weight                                                 ├─256\n",
      "│    └─blocks.0.mlp.ln2.bn.bias                                                   ├─256\n",
      "│    └─blocks.1.attn.qkv.linear.weight                                            ├─196,608\n",
      "│    └─blocks.1.attn.qkv.bn.weight                                                ├─768\n",
      "│    └─blocks.1.attn.qkv.bn.bias                                                  ├─768\n",
      "│    └─blocks.1.attn.proj.1.linear.weight                                         ├─65,536\n",
      "│    └─blocks.1.attn.proj.1.bn.weight                                             ├─256\n",
      "│    └─blocks.1.attn.proj.1.bn.bias                                               ├─256\n",
      "│    └─blocks.1.mlp.ln1.linear.weight                                             ├─131,072\n",
      "│    └─blocks.1.mlp.ln1.bn.weight                                                 ├─512\n",
      "│    └─blocks.1.mlp.ln1.bn.bias                                                   ├─512\n",
      "│    └─blocks.1.mlp.ln2.linear.weight                                             ├─131,072\n",
      "│    └─blocks.1.mlp.ln2.bn.weight                                                 ├─256\n",
      "│    └─blocks.1.mlp.ln2.bn.bias                                                   └─256\n",
      "│    └─Identity: 2-8                                    [32, 196, 256]            --\n",
      "│    └─Sequential: 2-9                                  [32, 196, 256]            --\n",
      "│    │    └─0.attn.qkv.linear.weight                                              ├─196,608\n",
      "│    │    └─0.attn.qkv.bn.weight                                                  ├─768\n",
      "│    │    └─0.attn.qkv.bn.bias                                                    ├─768\n",
      "│    │    └─0.attn.proj.1.linear.weight                                           ├─65,536\n",
      "│    │    └─0.attn.proj.1.bn.weight                                               ├─256\n",
      "│    │    └─0.attn.proj.1.bn.bias                                                 ├─256\n",
      "│    │    └─0.mlp.ln1.linear.weight                                               ├─131,072\n",
      "│    │    └─0.mlp.ln1.bn.weight                                                   ├─512\n",
      "│    │    └─0.mlp.ln1.bn.bias                                                     ├─512\n",
      "│    │    └─0.mlp.ln2.linear.weight                                               ├─131,072\n",
      "│    │    └─0.mlp.ln2.bn.weight                                                   ├─256\n",
      "│    │    └─0.mlp.ln2.bn.bias                                                     ├─256\n",
      "│    │    └─1.attn.qkv.linear.weight                                              ├─196,608\n",
      "│    │    └─1.attn.qkv.bn.weight                                                  ├─768\n",
      "│    │    └─1.attn.qkv.bn.bias                                                    ├─768\n",
      "│    │    └─1.attn.proj.1.linear.weight                                           ├─65,536\n",
      "│    │    └─1.attn.proj.1.bn.weight                                               ├─256\n",
      "│    │    └─1.attn.proj.1.bn.bias                                                 ├─256\n",
      "│    │    └─1.mlp.ln1.linear.weight                                               ├─131,072\n",
      "│    │    └─1.mlp.ln1.bn.weight                                                   ├─512\n",
      "│    │    └─1.mlp.ln1.bn.bias                                                     ├─512\n",
      "│    │    └─1.mlp.ln2.linear.weight                                               ├─131,072\n",
      "│    │    └─1.mlp.ln2.bn.weight                                                   ├─256\n",
      "│    │    └─1.mlp.ln2.bn.bias                                                     └─256\n",
      "│    │    └─LevitBlock: 3-9                             [32, 196, 256]            527,872\n",
      "│    │    │    └─attn.qkv.linear.weight                                           ├─196,608\n",
      "│    │    │    └─attn.qkv.bn.weight                                               ├─768\n",
      "│    │    │    └─attn.qkv.bn.bias                                                 ├─768\n",
      "│    │    │    └─attn.proj.1.linear.weight                                        ├─65,536\n",
      "│    │    │    └─attn.proj.1.bn.weight                                            ├─256\n",
      "│    │    │    └─attn.proj.1.bn.bias                                              ├─256\n",
      "│    │    │    └─mlp.ln1.linear.weight                                            ├─131,072\n",
      "│    │    │    └─mlp.ln1.bn.weight                                                ├─512\n",
      "│    │    │    └─mlp.ln1.bn.bias                                                  ├─512\n",
      "│    │    │    └─mlp.ln2.linear.weight                                            ├─131,072\n",
      "│    │    │    └─mlp.ln2.bn.weight                                                ├─256\n",
      "│    │    │    └─mlp.ln2.bn.bias                                                  └─256\n",
      "│    │    └─LevitBlock: 3-10                            [32, 196, 256]            527,872\n",
      "│    │    │    └─attn.qkv.linear.weight                                           ├─196,608\n",
      "│    │    │    └─attn.qkv.bn.weight                                               ├─768\n",
      "│    │    │    └─attn.qkv.bn.bias                                                 ├─768\n",
      "│    │    │    └─attn.proj.1.linear.weight                                        ├─65,536\n",
      "│    │    │    └─attn.proj.1.bn.weight                                            ├─256\n",
      "│    │    │    └─attn.proj.1.bn.bias                                              ├─256\n",
      "│    │    │    └─mlp.ln1.linear.weight                                            ├─131,072\n",
      "│    │    │    └─mlp.ln1.bn.weight                                                ├─512\n",
      "│    │    │    └─mlp.ln1.bn.bias                                                  ├─512\n",
      "│    │    │    └─mlp.ln2.linear.weight                                            ├─131,072\n",
      "│    │    │    └─mlp.ln2.bn.weight                                                ├─256\n",
      "│    │    │    └─mlp.ln2.bn.bias                                                  └─256\n",
      "├─LevitStage: 1-3                                       [32, 49, 384]             --\n",
      "│    └─downsample.conv.weight                                                     ├─884,736\n",
      "│    └─downsample.conv.bias                                                       ├─384\n",
      "│    └─blocks.0.attn.qkv.linear.weight                                            ├─442,368\n",
      "│    └─blocks.0.attn.qkv.bn.weight                                                ├─1,152\n",
      "│    └─blocks.0.attn.qkv.bn.bias                                                  ├─1,152\n",
      "│    └─blocks.0.attn.proj.1.linear.weight                                         ├─147,456\n",
      "│    └─blocks.0.attn.proj.1.bn.weight                                             ├─384\n",
      "│    └─blocks.0.attn.proj.1.bn.bias                                               ├─384\n",
      "│    └─blocks.0.mlp.ln1.linear.weight                                             ├─294,912\n",
      "│    └─blocks.0.mlp.ln1.bn.weight                                                 ├─768\n",
      "│    └─blocks.0.mlp.ln1.bn.bias                                                   ├─768\n",
      "│    └─blocks.0.mlp.ln2.linear.weight                                             ├─294,912\n",
      "│    └─blocks.0.mlp.ln2.bn.weight                                                 ├─384\n",
      "│    └─blocks.0.mlp.ln2.bn.bias                                                   ├─384\n",
      "│    └─blocks.1.attn.qkv.linear.weight                                            ├─442,368\n",
      "│    └─blocks.1.attn.qkv.bn.weight                                                ├─1,152\n",
      "│    └─blocks.1.attn.qkv.bn.bias                                                  ├─1,152\n",
      "│    └─blocks.1.attn.proj.1.linear.weight                                         ├─147,456\n",
      "│    └─blocks.1.attn.proj.1.bn.weight                                             ├─384\n",
      "│    └─blocks.1.attn.proj.1.bn.bias                                               ├─384\n",
      "│    └─blocks.1.mlp.ln1.linear.weight                                             ├─294,912\n",
      "│    └─blocks.1.mlp.ln1.bn.weight                                                 ├─768\n",
      "│    └─blocks.1.mlp.ln1.bn.bias                                                   ├─768\n",
      "│    └─blocks.1.mlp.ln2.linear.weight                                             ├─294,912\n",
      "│    └─blocks.1.mlp.ln2.bn.weight                                                 ├─384\n",
      "│    └─blocks.1.mlp.ln2.bn.bias                                                   └─384\n",
      "│    └─CNNDownsample: 2-10                              [32, 49, 384]             --\n",
      "│    │    └─conv.weight                                                           ├─884,736\n",
      "│    │    └─conv.bias                                                             └─384\n",
      "│    │    └─Conv2d: 3-11                                [32, 384, 7, 7]           885,120\n",
      "│    │    │    └─weight                                                           ├─884,736\n",
      "│    │    │    └─bias                                                             └─384\n",
      "│    │    └─Hardswish: 3-12                             [32, 384, 7, 7]           --\n",
      "│    └─Sequential: 2-11                                 [32, 49, 384]             --\n",
      "│    │    └─0.attn.qkv.linear.weight                                              ├─442,368\n",
      "│    │    └─0.attn.qkv.bn.weight                                                  ├─1,152\n",
      "│    │    └─0.attn.qkv.bn.bias                                                    ├─1,152\n",
      "│    │    └─0.attn.proj.1.linear.weight                                           ├─147,456\n",
      "│    │    └─0.attn.proj.1.bn.weight                                               ├─384\n",
      "│    │    └─0.attn.proj.1.bn.bias                                                 ├─384\n",
      "│    │    └─0.mlp.ln1.linear.weight                                               ├─294,912\n",
      "│    │    └─0.mlp.ln1.bn.weight                                                   ├─768\n",
      "│    │    └─0.mlp.ln1.bn.bias                                                     ├─768\n",
      "│    │    └─0.mlp.ln2.linear.weight                                               ├─294,912\n",
      "│    │    └─0.mlp.ln2.bn.weight                                                   ├─384\n",
      "│    │    └─0.mlp.ln2.bn.bias                                                     ├─384\n",
      "│    │    └─1.attn.qkv.linear.weight                                              ├─442,368\n",
      "│    │    └─1.attn.qkv.bn.weight                                                  ├─1,152\n",
      "│    │    └─1.attn.qkv.bn.bias                                                    ├─1,152\n",
      "│    │    └─1.attn.proj.1.linear.weight                                           ├─147,456\n",
      "│    │    └─1.attn.proj.1.bn.weight                                               ├─384\n",
      "│    │    └─1.attn.proj.1.bn.bias                                                 ├─384\n",
      "│    │    └─1.mlp.ln1.linear.weight                                               ├─294,912\n",
      "│    │    └─1.mlp.ln1.bn.weight                                                   ├─768\n",
      "│    │    └─1.mlp.ln1.bn.bias                                                     ├─768\n",
      "│    │    └─1.mlp.ln2.linear.weight                                               ├─294,912\n",
      "│    │    └─1.mlp.ln2.bn.weight                                                   ├─384\n",
      "│    │    └─1.mlp.ln2.bn.bias                                                     └─384\n",
      "│    │    └─LevitBlock: 3-13                            [32, 49, 384]             1,185,024\n",
      "│    │    │    └─attn.qkv.linear.weight                                           ├─442,368\n",
      "│    │    │    └─attn.qkv.bn.weight                                               ├─1,152\n",
      "│    │    │    └─attn.qkv.bn.bias                                                 ├─1,152\n",
      "│    │    │    └─attn.proj.1.linear.weight                                        ├─147,456\n",
      "│    │    │    └─attn.proj.1.bn.weight                                            ├─384\n",
      "│    │    │    └─attn.proj.1.bn.bias                                              ├─384\n",
      "│    │    │    └─mlp.ln1.linear.weight                                            ├─294,912\n",
      "│    │    │    └─mlp.ln1.bn.weight                                                ├─768\n",
      "│    │    │    └─mlp.ln1.bn.bias                                                  ├─768\n",
      "│    │    │    └─mlp.ln2.linear.weight                                            ├─294,912\n",
      "│    │    │    └─mlp.ln2.bn.weight                                                ├─384\n",
      "│    │    │    └─mlp.ln2.bn.bias                                                  └─384\n",
      "│    │    └─LevitBlock: 3-14                            [32, 49, 384]             1,185,024\n",
      "│    │    │    └─attn.qkv.linear.weight                                           ├─442,368\n",
      "│    │    │    └─attn.qkv.bn.weight                                               ├─1,152\n",
      "│    │    │    └─attn.qkv.bn.bias                                                 ├─1,152\n",
      "│    │    │    └─attn.proj.1.linear.weight                                        ├─147,456\n",
      "│    │    │    └─attn.proj.1.bn.weight                                            ├─384\n",
      "│    │    │    └─attn.proj.1.bn.bias                                              ├─384\n",
      "│    │    │    └─mlp.ln1.linear.weight                                            ├─294,912\n",
      "│    │    │    └─mlp.ln1.bn.weight                                                ├─768\n",
      "│    │    │    └─mlp.ln1.bn.bias                                                  ├─768\n",
      "│    │    │    └─mlp.ln2.linear.weight                                            ├─294,912\n",
      "│    │    │    └─mlp.ln2.bn.weight                                                ├─384\n",
      "│    │    │    └─mlp.ln2.bn.bias                                                  └─384\n",
      "├─Sequential: 1-4                                       [32, 512, 7, 7]           --\n",
      "│    └─0.weight                                                                   ├─196,608\n",
      "│    └─0.bias                                                                     ├─512\n",
      "│    └─1.weight                                                                   ├─512\n",
      "│    └─1.bias                                                                     └─512\n",
      "│    └─Conv2d: 2-12                                     [32, 512, 7, 7]           197,120\n",
      "│    │    └─weight                                                                ├─196,608\n",
      "│    │    └─bias                                                                  └─512\n",
      "│    └─BatchNorm2d: 2-13                                [32, 512, 7, 7]           1,024\n",
      "│    │    └─weight                                                                ├─512\n",
      "│    │    └─bias                                                                  └─512\n",
      "│    └─ReLU: 2-14                                       [32, 512, 7, 7]           --\n",
      "├─NormLinear: 1-5                                       [32, 257]                 --\n",
      "│    └─bn.weight                                                                  ├─512\n",
      "│    └─bn.bias                                                                    ├─512\n",
      "│    └─linear.weight                                                              ├─131,584\n",
      "│    └─linear.bias                                                                └─257\n",
      "│    └─BatchNorm1d: 2-15                                [32, 512]                 1,024\n",
      "│    │    └─weight                                                                ├─512\n",
      "│    │    └─bias                                                                  └─512\n",
      "│    └─Dropout: 2-16                                    [32, 512]                 --\n",
      "│    └─Linear: 2-17                                     [32, 257]                 131,841\n",
      "│    │    └─weight                                                                ├─131,584\n",
      "│    │    └─bias                                                                  └─257\n",
      "├─NormLinear: 1-6                                       [32, 257]                 --\n",
      "│    └─bn.weight                                                                  ├─512\n",
      "│    └─bn.bias                                                                    ├─512\n",
      "│    └─linear.weight                                                              ├─131,584\n",
      "│    └─linear.bias                                                                └─257\n",
      "│    └─BatchNorm1d: 2-18                                [32, 512]                 1,024\n",
      "│    │    └─weight                                                                ├─512\n",
      "│    │    └─bias                                                                  └─512\n",
      "│    └─Dropout: 2-19                                    [32, 512]                 --\n",
      "│    └─Linear: 2-20                                     [32, 257]                 131,841\n",
      "│    │    └─weight                                                                ├─131,584\n",
      "│    │    └─bias                                                                  └─257\n",
      "=========================================================================================================\n",
      "Total params: 5,163,682\n",
      "Trainable params: 5,163,682\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 17.94\n",
      "=========================================================================================================\n",
      "Input size (MB): 19.27\n",
      "Forward/backward pass size (MB): 897.94\n",
      "Params size (MB): 20.65\n",
      "Estimated Total Size (MB): 937.86\n",
      "=========================================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(summary(model, input_size=(32, 3, 224, 224), verbose=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "TOaIwHDSzBP3"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Lambda(lambda img: img.convert(\"RGB\")), \n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),         \n",
    "    transforms.RandomRotation(15),             \n",
    "    transforms.ToTensor(),                        \n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "caltech_data = datasets.Caltech256(\n",
    "    root=\"data\",\n",
    "    transform=transform,  \n",
    "    download=True\n",
    ")\n",
    "\n",
    "# for img, label in caltech_data:\n",
    "#     # find gray scale images\n",
    "#     if img.shape[0] != 3:\n",
    "#         print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 256\n"
     ]
    }
   ],
   "source": [
    "# targets = []\n",
    "# for _, label in caltech_data:\n",
    "#     targets.append(label)\n",
    "\n",
    "# print(min(targets), max(targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 98, 1: 97, 2: 151, 3: 127, 4: 148, 5: 90, 6: 106, 7: 232, 8: 102, 9: 94, 10: 278, 11: 216, 12: 98, 13: 86, 14: 122, 15: 91, 16: 104, 17: 101, 18: 124, 19: 83, 20: 142, 21: 97, 22: 110, 23: 112, 24: 114, 25: 106, 26: 100, 27: 110, 28: 103, 29: 104, 30: 90, 31: 101, 32: 102, 33: 100, 34: 87, 35: 106, 36: 120, 37: 110, 38: 85, 39: 124, 40: 87, 41: 87, 42: 124, 43: 121, 44: 85, 45: 133, 46: 94, 47: 103, 48: 106, 49: 97, 50: 114, 51: 85, 52: 82, 53: 118, 54: 98, 55: 102, 56: 106, 57: 93, 58: 83, 59: 87, 60: 102, 61: 83, 62: 122, 63: 131, 64: 101, 65: 83, 66: 83, 67: 110, 68: 99, 69: 84, 70: 99, 71: 118, 72: 100, 73: 115, 74: 83, 75: 84, 76: 92, 77: 90, 78: 99, 79: 116, 80: 95, 81: 81, 82: 95, 83: 84, 84: 112, 85: 80, 86: 93, 87: 98, 88: 110, 89: 212, 90: 95, 91: 201, 92: 112, 93: 104, 94: 86, 95: 285, 96: 89, 97: 100, 98: 80, 99: 93, 100: 138, 101: 88, 102: 111, 103: 97, 104: 270, 105: 87, 106: 89, 107: 85, 108: 156, 109: 85, 110: 84, 111: 84, 112: 116, 113: 120, 114: 88, 115: 107, 116: 121, 117: 108, 118: 87, 119: 130, 120: 82, 121: 103, 122: 111, 123: 91, 124: 101, 125: 242, 126: 128, 127: 105, 128: 190, 129: 91, 130: 92, 131: 190, 132: 136, 133: 119, 134: 93, 135: 93, 136: 156, 137: 192, 138: 86, 139: 89, 140: 117, 141: 107, 142: 130, 143: 82, 144: 798, 145: 82, 146: 202, 147: 174, 148: 103, 149: 111, 150: 109, 151: 120, 152: 93, 153: 103, 154: 92, 155: 96, 156: 105, 157: 149, 158: 209, 159: 83, 160: 103, 161: 91, 162: 90, 163: 101, 164: 88, 165: 92, 166: 86, 167: 140, 168: 92, 169: 102, 170: 84, 171: 99, 172: 106, 173: 84, 174: 83, 175: 110, 176: 96, 177: 98, 178: 80, 179: 102, 180: 100, 181: 120, 182: 100, 183: 84, 184: 103, 185: 81, 186: 95, 187: 88, 188: 119, 189: 112, 190: 111, 191: 112, 192: 174, 193: 112, 194: 87, 195: 104, 196: 100, 197: 108, 198: 105, 199: 100, 200: 81, 201: 97, 202: 91, 203: 80, 204: 87, 205: 98, 206: 115, 207: 109, 208: 102, 209: 111, 210: 95, 211: 136, 212: 101, 213: 139, 214: 84, 215: 98, 216: 105, 217: 81, 218: 84, 219: 94, 220: 103, 221: 91, 222: 80, 223: 110, 224: 90, 225: 99, 226: 147, 227: 95, 228: 95, 229: 94, 230: 112, 231: 358, 232: 100, 233: 122, 234: 114, 235: 97, 236: 90, 237: 97, 238: 84, 239: 201, 240: 95, 241: 93, 242: 90, 243: 91, 244: 91, 245: 101, 246: 92, 247: 84, 248: 100, 249: 96, 250: 800, 251: 116, 252: 435, 253: 95, 254: 103, 255: 108, 256: 827}\n"
     ]
    }
   ],
   "source": [
    "# class_counts = {}\n",
    "# for _, label in caltech_data:\n",
    "#     if label not in class_counts:\n",
    "#         class_counts[label] = 0\n",
    "#     class_counts[label] += 1\n",
    "\n",
    "# print(class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N0Ww6weVzBP4",
    "outputId": "2ce1c4f3-cd26-4b6f-dd62-c9444a4cffd9"
   },
   "outputs": [],
   "source": [
    "train_size = int(0.7 * len(caltech_data)) \n",
    "val_size = int(0.15 * len(caltech_data)) \n",
    "test_size = len(caltech_data) - train_size - val_size \n",
    "train_data, val_data, test_data = random_split(caltech_data, [train_size, val_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S4sK4r0FzBP4",
    "outputId": "9cd976d3-0dd7-4566-a1a1-72369b008cf9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 21424\n",
      "Validation set size: 4591\n",
      "Test set size: 4592\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train set size: {len(train_data)}\")\n",
    "print(f\"Validation set size: {len(val_data)}\")\n",
    "print(f\"Test set size: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "HCIgSLtBzBP4"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "qCcVDgIPzBP4"
   },
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in tqdm(train_loader, desc=\"Training\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Train Loss: {epoch_loss:.4f}, Train Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "pzZwWmVFzBP5"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, criterion, device, phase=\"Validation\"):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(data_loader, desc=f\"{phase}\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(data_loader)\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"{phase} Loss: {epoch_loss:.4f}, {phase} Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "ATaK9u_JDWLD"
   },
   "outputs": [],
   "source": [
    "def measure_inference_time(model, data_loader, device):\n",
    "    model.eval()\n",
    "    times = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in data_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            start_time = torch.cuda.Event(enable_timing=True)\n",
    "            end_time = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "            start_time.record()\n",
    "            _ = model(inputs)  # inference 수행\n",
    "            end_time.record()\n",
    "\n",
    "            # 시간 측정\n",
    "            torch.cuda.synchronize()  # CUDA에서 모든 커널이 완료될 때까지 대기\n",
    "            elapsed_time = start_time.elapsed_time(end_time)  # 밀리초 단위로 반환\n",
    "            times.append(elapsed_time)\n",
    "\n",
    "    # 통계량 계산\n",
    "    times_np = np.array(times)\n",
    "    total_inferences = len(times_np)\n",
    "    avg_time = np.mean(times_np)\n",
    "    std_dev = np.std(times_np)\n",
    "    max_time = np.max(times_np)\n",
    "    min_time = np.min(times_np)\n",
    "\n",
    "    # 결과 출력\n",
    "    print(f\"Inference Time Measurement Results:\")\n",
    "    print(f\"Total Inferences: {total_inferences}\")\n",
    "    print(f\"Average Time: {avg_time:.2f} ms\")\n",
    "    print(f\"Standard Deviation: {std_dev:.2f} ms\")\n",
    "    print(f\"Maximum Time: {max_time:.2f} ms\")\n",
    "    print(f\"Minimum Time: {min_time:.2f} ms\")\n",
    "\n",
    "    return times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YdGy_hjpzBP5",
    "outputId": "df80b5c0-8ccb-4271-e313-eee074aaeba1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 670/670 [08:33<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5.1758, Train Accuracy: 7.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 144/144 [00:54<00:00,  2.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 5.1551, Validation Accuracy: 8.69%\n",
      "\n",
      "Epoch 2/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  77%|███████▋  | 517/670 [06:26<01:54,  1.34it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     evaluate(model, val_loader, criterion, device, phase\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[23], line 12\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m     10\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[1;32m     11\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m---> 12\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     15\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    train(model, train_loader, criterion, optimizer, device)\n",
    "    evaluate(model, val_loader, criterion, device, phase=\"Validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tAiyDwO0zMk1",
    "outputId": "0ae6ff9e-6571-46ea-e3c4-e900282c3715"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Test Evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100%|██████████| 35/35 [00:05<00:00,  6.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 5.5899, Test Accuracy: 27.47%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nFinal Test Evaluation\")\n",
    "evaluate(model, test_loader, criterion, device, phase=\"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HCv209A51QEb",
    "outputId": "8f66e16f-b8af-4894-fc48-f2b4bde9ad1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Time Measurement Results:\n",
      "Total Inferences: 35\n",
      "Average Time: 5.82 ms\n",
      "Standard Deviation: 0.18 ms\n",
      "Maximum Time: 6.77 ms\n",
      "Minimum Time: 5.61 ms\n"
     ]
    }
   ],
   "source": [
    "times = measure_inference_time(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "m_kVgdJKDZqe"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
