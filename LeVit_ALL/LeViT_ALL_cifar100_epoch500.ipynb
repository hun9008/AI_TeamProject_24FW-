{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOt_dilyzGPf",
        "outputId": "09408619-3657-4608-a86f-e69e6c2d466f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torchinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdqbv9JYes6c",
        "outputId": "797e028c-59ee-4cd3-fc03-9b7afc41fbb6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.16.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.6)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.8.30)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install gdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "tZSEFQzWzBPy"
      },
      "outputs": [],
      "source": [
        "import timm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split, ConcatDataset\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "import itertools\n",
        "from torchinfo import summary\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_losses = []\n",
        "train_accuracies = []\n",
        "val_losses = []\n",
        "val_accuracies = []\n",
        "\n",
        "# 모델 가중치 저장 경로 설정\n",
        "model_weight_path = \"./model_weights.pth\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "5N0s6e-mzBPz"
      },
      "outputs": [],
      "source": [
        "class ConvNorm(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=2, padding=1):\n",
        "        super(ConvNorm, self).__init__()\n",
        "        self.linear = nn.Conv2d(\n",
        "            in_channels, out_channels, kernel_size=kernel_size,\n",
        "            stride=stride, padding=padding, bias=False\n",
        "        )\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear(x)\n",
        "        x = self.bn(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "fX0CnXd9zBP0"
      },
      "outputs": [],
      "source": [
        "class Stem16(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Stem16, self).__init__()\n",
        "        self.conv1 = ConvNorm(3, 32)\n",
        "        self.act1 = nn.Hardswish()\n",
        "        self.conv2 = ConvNorm(32, 64)\n",
        "        self.act2 = nn.Hardswish()\n",
        "        self.conv3 = ConvNorm(64, 128)\n",
        "        self.act3 = nn.Hardswish()\n",
        "        self.conv4 = ConvNorm(128, 256)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.act1(self.conv1(x))\n",
        "        x = self.act2(self.conv2(x))\n",
        "        x = self.act3(self.conv3(x))\n",
        "        x = self.conv4(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "7tw3fRfnzBP0"
      },
      "outputs": [],
      "source": [
        "class LinearNorm(nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super(LinearNorm, self).__init__()\n",
        "        self.linear = nn.Linear(in_features, out_features, bias=False)\n",
        "        self.bn = nn.BatchNorm1d(out_features)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        if x.dim() == 3:\n",
        "            B, N, C = x.shape\n",
        "            x = x.reshape(B * N, C)\n",
        "            x = self.bn(self.linear(x))\n",
        "            x = x.reshape(B, N, -1)\n",
        "        else:\n",
        "            x = self.bn(self.linear(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "rW7vhH2YzBP1"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, num_heads, attn_ratio=2):\n",
        "        super(Attention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = head_dim ** -0.5\n",
        "        inner_dim = head_dim * num_heads * 3\n",
        "        self.qkv = LinearNorm(dim, inner_dim)\n",
        "\n",
        "        self.proj = nn.Sequential(\n",
        "            nn.Hardswish(),\n",
        "            LinearNorm(dim, dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x)\n",
        "        qkv = qkv.view(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        return self.proj(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "ewOcRA2dzBP1"
      },
      "outputs": [],
      "source": [
        "class LevitMlp(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features, out_features):\n",
        "        super(LevitMlp, self).__init__()\n",
        "        self.ln1 = LinearNorm(in_features, hidden_features)\n",
        "        self.act = nn.Hardswish()\n",
        "        self.drop = nn.Dropout(p=0.5, inplace=False)#dropout 적용\n",
        "        self.ln2 = LinearNorm(hidden_features, out_features)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.ln1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.ln2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "1m-F4YvfzBP2"
      },
      "outputs": [],
      "source": [
        "class LevitBlock(nn.Module):\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=2):\n",
        "        super(LevitBlock, self).__init__()\n",
        "        self.attn = Attention(dim, num_heads)\n",
        "        self.drop_path1 = nn.Identity()\n",
        "        self.mlp = LevitMlp(dim, dim * mlp_ratio, dim)\n",
        "        self.drop_path2 = nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.drop_path1(self.attn(x))\n",
        "        x = x + self.drop_path2(self.mlp(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "pDfX1xE1zBP2"
      },
      "outputs": [],
      "source": [
        "# class AttentionDownsample(nn.Module):\n",
        "#     def __init__(self, dim, out_dim, num_heads, attn_ratio=2):\n",
        "#         super(AttentionDownsample, self).__init__()\n",
        "#         self.num_heads = num_heads\n",
        "#         self.scale = (dim // num_heads) ** -0.5\n",
        "#         inner_dim = dim * attn_ratio * num_heads\n",
        "#         self.kv = LinearNorm(dim, inner_dim)\n",
        "\n",
        "#         self.q = nn.Sequential(\n",
        "#             nn.Conv2d(dim, dim, kernel_size=2, stride=2),\n",
        "#             nn.Flatten(start_dim=1)\n",
        "#         )\n",
        "\n",
        "#         self.proj = nn.Sequential(\n",
        "#             nn.Hardswish(),\n",
        "#             LinearNorm(dim, out_dim)\n",
        "#         )\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         B, N, C = x.shape\n",
        "#         H = W = int(N ** 0.5)\n",
        "#         x = x.reshape(B, C, H, W)\n",
        "\n",
        "#         kv = self.kv(x.flatten(2).transpose(1, 2))\n",
        "#         q = self.q(x)\n",
        "\n",
        "#         q = q.reshape(B, -1, C)\n",
        "#         x = self.proj(q)\n",
        "#         return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "Z26uegOwzBP2"
      },
      "outputs": [],
      "source": [
        "# class LevitDownsample(nn.Module):\n",
        "#     def __init__(self, dim, out_dim, num_heads, attn_ratio=2):\n",
        "#         super(LevitDownsample, self).__init__()\n",
        "#         self.attn_downsample = AttentionDownsample(dim, out_dim, num_heads, attn_ratio)\n",
        "#         self.mlp = LevitMlp(out_dim, out_dim * attn_ratio, out_dim)\n",
        "#         self.drop_path = nn.Identity()\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.attn_downsample(x)\n",
        "#         x = self.drop_path(self.mlp(x))\n",
        "#         return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "p2DJrMeyJ2RO"
      },
      "outputs": [],
      "source": [
        "#CNNDownSample 적용\n",
        "class CNNDownsample(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(CNNDownsample, self).__init__()\n",
        "        self.out_channels = out_channels\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1)\n",
        "        self.act = nn.Hardswish()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # print(x.shape)\n",
        "        B, N, C = x.shape # (B, N, C)  N=H*W (16 * 16 = 196)\n",
        "        H = int(np.sqrt(N))\n",
        "        x = x.view(B, H, H, C).permute(0, 3, 1, 2)\n",
        "        x = self.conv(x)\n",
        "        x = self.act(x)\n",
        "        x = x.permute(0, 2, 3, 1).view(B, -1, self.out_channels)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "oGsAuLPfzBP3"
      },
      "outputs": [],
      "source": [
        "class LevitStage(nn.Module):\n",
        "    def __init__(self, dim, out_dim, num_heads, num_blocks, downsample=True):\n",
        "        super(LevitStage, self).__init__()\n",
        "        self.downsample = CNNDownsample(dim, out_dim) if downsample else nn.Identity()\n",
        "        self.blocks = nn.Sequential(*[LevitBlock(out_dim, num_heads) for _ in range(num_blocks)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.downsample(x)\n",
        "        x = self.blocks(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "2PBkdZUuhsDI"
      },
      "outputs": [],
      "source": [
        "class ConvLevitStage(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, num_blocks, kernel_size, stride, padding):\n",
        "        super(ConvLevitStage, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            *[nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size, stride, padding)\n",
        "              for i in range(num_blocks)],\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "bBPec4pbzBP3"
      },
      "outputs": [],
      "source": [
        "class NormLinear(nn.Module):\n",
        "    def __init__(self, in_features, out_features, dropout_prob=0.5):#drop_out_0.5 적용\n",
        "        super(NormLinear, self).__init__()\n",
        "        self.bn = nn.BatchNorm1d(in_features)\n",
        "        self.drop = nn.Dropout(p=dropout_prob, inplace=False)\n",
        "        self.linear = nn.Linear(in_features, out_features, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.bn(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.linear(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "mt5kkekezBP3"
      },
      "outputs": [],
      "source": [
        "class LevitDistilled(nn.Module):\n",
        "    def __init__(self, num_classes=100):\n",
        "        super(LevitDistilled, self).__init__()\n",
        "\n",
        "        self.stem = Stem16()\n",
        "\n",
        "        self.stage1 = LevitStage(dim=256, out_dim=256, num_heads=4, num_blocks=2, downsample=False) # block 수 적용\n",
        "        self.stage2 = LevitStage(dim=256, out_dim=384, num_heads=6, num_blocks=2, downsample=True)\n",
        "\n",
        "        self.conv1x1 = nn.Sequential(\n",
        "            nn.Conv2d(384, 512, kernel_size=1, stride=1, padding=0),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        self.head = NormLinear(in_features=512, out_features=num_classes, dropout_prob=0.0)\n",
        "        self.head_dist = NormLinear(in_features=512, out_features=num_classes, dropout_prob=0.0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.stem(x)\n",
        "\n",
        "        B, C, H, W = x.shape\n",
        "        x = x.view(B, C, -1).transpose(1, 2)\n",
        "        x = self.stage1(x)\n",
        "        x = self.stage2(x)\n",
        "\n",
        "        H = W = int(x.shape[1]**0.5)\n",
        "        x = x.transpose(1, 2).view(B, 384, H, W)\n",
        "\n",
        "        x = self.conv1x1(x)\n",
        "\n",
        "        x = torch.mean(x, dim=(2, 3))\n",
        "        out = self.head(x)\n",
        "        out_dist = self.head_dist(x)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kz4Um3NmzBP3",
        "outputId": "45dad8f3-2bb4-4c3c-b80b-16864270e33d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LevitDistilled(\n",
            "  (stem): Stem16(\n",
            "    (conv1): ConvNorm(\n",
            "      (linear): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (act1): Hardswish()\n",
            "    (conv2): ConvNorm(\n",
            "      (linear): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (act2): Hardswish()\n",
            "    (conv3): ConvNorm(\n",
            "      (linear): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (act3): Hardswish()\n",
            "    (conv4): ConvNorm(\n",
            "      (linear): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (stage1): LevitStage(\n",
            "    (downsample): Identity()\n",
            "    (blocks): Sequential(\n",
            "      (0): LevitBlock(\n",
            "        (attn): Attention(\n",
            "          (qkv): LinearNorm(\n",
            "            (linear): Linear(in_features=256, out_features=768, bias=False)\n",
            "            (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (proj): Sequential(\n",
            "            (0): Hardswish()\n",
            "            (1): LinearNorm(\n",
            "              (linear): Linear(in_features=256, out_features=256, bias=False)\n",
            "              (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (drop_path1): Identity()\n",
            "        (mlp): LevitMlp(\n",
            "          (ln1): LinearNorm(\n",
            "            (linear): Linear(in_features=256, out_features=512, bias=False)\n",
            "            (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (act): Hardswish()\n",
            "          (drop): Dropout(p=0.5, inplace=False)\n",
            "          (ln2): LinearNorm(\n",
            "            (linear): Linear(in_features=512, out_features=256, bias=False)\n",
            "            (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (drop_path2): Identity()\n",
            "      )\n",
            "      (1): LevitBlock(\n",
            "        (attn): Attention(\n",
            "          (qkv): LinearNorm(\n",
            "            (linear): Linear(in_features=256, out_features=768, bias=False)\n",
            "            (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (proj): Sequential(\n",
            "            (0): Hardswish()\n",
            "            (1): LinearNorm(\n",
            "              (linear): Linear(in_features=256, out_features=256, bias=False)\n",
            "              (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (drop_path1): Identity()\n",
            "        (mlp): LevitMlp(\n",
            "          (ln1): LinearNorm(\n",
            "            (linear): Linear(in_features=256, out_features=512, bias=False)\n",
            "            (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (act): Hardswish()\n",
            "          (drop): Dropout(p=0.5, inplace=False)\n",
            "          (ln2): LinearNorm(\n",
            "            (linear): Linear(in_features=512, out_features=256, bias=False)\n",
            "            (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (drop_path2): Identity()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (stage2): LevitStage(\n",
            "    (downsample): CNNDownsample(\n",
            "      (conv): Conv2d(256, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "      (act): Hardswish()\n",
            "    )\n",
            "    (blocks): Sequential(\n",
            "      (0): LevitBlock(\n",
            "        (attn): Attention(\n",
            "          (qkv): LinearNorm(\n",
            "            (linear): Linear(in_features=384, out_features=1152, bias=False)\n",
            "            (bn): BatchNorm1d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (proj): Sequential(\n",
            "            (0): Hardswish()\n",
            "            (1): LinearNorm(\n",
            "              (linear): Linear(in_features=384, out_features=384, bias=False)\n",
            "              (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (drop_path1): Identity()\n",
            "        (mlp): LevitMlp(\n",
            "          (ln1): LinearNorm(\n",
            "            (linear): Linear(in_features=384, out_features=768, bias=False)\n",
            "            (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (act): Hardswish()\n",
            "          (drop): Dropout(p=0.5, inplace=False)\n",
            "          (ln2): LinearNorm(\n",
            "            (linear): Linear(in_features=768, out_features=384, bias=False)\n",
            "            (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (drop_path2): Identity()\n",
            "      )\n",
            "      (1): LevitBlock(\n",
            "        (attn): Attention(\n",
            "          (qkv): LinearNorm(\n",
            "            (linear): Linear(in_features=384, out_features=1152, bias=False)\n",
            "            (bn): BatchNorm1d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (proj): Sequential(\n",
            "            (0): Hardswish()\n",
            "            (1): LinearNorm(\n",
            "              (linear): Linear(in_features=384, out_features=384, bias=False)\n",
            "              (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (drop_path1): Identity()\n",
            "        (mlp): LevitMlp(\n",
            "          (ln1): LinearNorm(\n",
            "            (linear): Linear(in_features=384, out_features=768, bias=False)\n",
            "            (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (act): Hardswish()\n",
            "          (drop): Dropout(p=0.5, inplace=False)\n",
            "          (ln2): LinearNorm(\n",
            "            (linear): Linear(in_features=768, out_features=384, bias=False)\n",
            "            (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (drop_path2): Identity()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (conv1x1): Sequential(\n",
            "    (0): Conv2d(384, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "  )\n",
            "  (head): NormLinear(\n",
            "    (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (drop): Dropout(p=0.0, inplace=False)\n",
            "    (linear): Linear(in_features=512, out_features=100, bias=True)\n",
            "  )\n",
            "  (head_dist): NormLinear(\n",
            "    (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (drop): Dropout(p=0.0, inplace=False)\n",
            "    (linear): Linear(in_features=512, out_features=100, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model = LevitDistilled()\n",
        "# model = LauncherModel()\n",
        "print(model)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "batch_size = 32\n",
        "learning_rate = 0.001\n",
        "num_epochs = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmb6Hs2cO5_F",
        "outputId": "272aae96-4bf9-4d06-8a96-c6e9356c7f0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=========================================================================================================\n",
            "Layer (type:depth-idx)                                  Output Shape              Param #\n",
            "=========================================================================================================\n",
            "LevitDistilled                                          [32, 100]                 --\n",
            "├─Stem16: 1-1                                           [32, 256, 14, 14]         --\n",
            "│    └─ConvNorm: 2-1                                    [32, 32, 112, 112]        --\n",
            "│    │    └─Conv2d: 3-1                                 [32, 32, 112, 112]        864\n",
            "│    │    └─BatchNorm2d: 3-2                            [32, 32, 112, 112]        64\n",
            "│    └─Hardswish: 2-2                                   [32, 32, 112, 112]        --\n",
            "│    └─ConvNorm: 2-3                                    [32, 64, 56, 56]          --\n",
            "│    │    └─Conv2d: 3-3                                 [32, 64, 56, 56]          18,432\n",
            "│    │    └─BatchNorm2d: 3-4                            [32, 64, 56, 56]          128\n",
            "│    └─Hardswish: 2-4                                   [32, 64, 56, 56]          --\n",
            "│    └─ConvNorm: 2-5                                    [32, 128, 28, 28]         --\n",
            "│    │    └─Conv2d: 3-5                                 [32, 128, 28, 28]         73,728\n",
            "│    │    └─BatchNorm2d: 3-6                            [32, 128, 28, 28]         256\n",
            "│    └─Hardswish: 2-6                                   [32, 128, 28, 28]         --\n",
            "│    └─ConvNorm: 2-7                                    [32, 256, 14, 14]         --\n",
            "│    │    └─Conv2d: 3-7                                 [32, 256, 14, 14]         294,912\n",
            "│    │    └─BatchNorm2d: 3-8                            [32, 256, 14, 14]         512\n",
            "├─LevitStage: 1-2                                       [32, 196, 256]            --\n",
            "│    └─Identity: 2-8                                    [32, 196, 256]            --\n",
            "│    └─Sequential: 2-9                                  [32, 196, 256]            --\n",
            "│    │    └─LevitBlock: 3-9                             [32, 196, 256]            527,872\n",
            "│    │    └─LevitBlock: 3-10                            [32, 196, 256]            527,872\n",
            "├─LevitStage: 1-3                                       [32, 49, 384]             --\n",
            "│    └─CNNDownsample: 2-10                              [32, 49, 384]             --\n",
            "│    │    └─Conv2d: 3-11                                [32, 384, 7, 7]           885,120\n",
            "│    │    └─Hardswish: 3-12                             [32, 384, 7, 7]           --\n",
            "│    └─Sequential: 2-11                                 [32, 49, 384]             --\n",
            "│    │    └─LevitBlock: 3-13                            [32, 49, 384]             1,185,024\n",
            "│    │    └─LevitBlock: 3-14                            [32, 49, 384]             1,185,024\n",
            "├─Sequential: 1-4                                       [32, 512, 7, 7]           --\n",
            "│    └─Conv2d: 2-12                                     [32, 512, 7, 7]           197,120\n",
            "│    └─BatchNorm2d: 2-13                                [32, 512, 7, 7]           1,024\n",
            "│    └─ReLU: 2-14                                       [32, 512, 7, 7]           --\n",
            "├─NormLinear: 1-5                                       [32, 100]                 --\n",
            "│    └─BatchNorm1d: 2-15                                [32, 512]                 1,024\n",
            "│    └─Dropout: 2-16                                    [32, 512]                 --\n",
            "│    └─Linear: 2-17                                     [32, 100]                 51,300\n",
            "├─NormLinear: 1-6                                       [32, 100]                 --\n",
            "│    └─BatchNorm1d: 2-18                                [32, 512]                 1,024\n",
            "│    └─Dropout: 2-19                                    [32, 512]                 --\n",
            "│    └─Linear: 2-20                                     [32, 100]                 51,300\n",
            "=========================================================================================================\n",
            "Total params: 5,002,600\n",
            "Trainable params: 5,002,600\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (G): 17.93\n",
            "=========================================================================================================\n",
            "Input size (MB): 19.27\n",
            "Forward/backward pass size (MB): 897.86\n",
            "Params size (MB): 20.01\n",
            "Estimated Total Size (MB): 937.14\n",
            "=========================================================================================================\n"
          ]
        }
      ],
      "source": [
        "print(summary(model, input_size=(32, 3, 224, 224)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7rBknzRzBP3",
        "outputId": "b1fe171c-3655-4027-a1e4-c667a5f32ec2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=========================================================================================================\n",
            "Layer (type:depth-idx)                                  Output Shape              Param #\n",
            "=========================================================================================================\n",
            "LevitDistilled                                          [32, 100]                 --\n",
            "├─Stem16: 1-1                                           [32, 256, 14, 14]         --\n",
            "│    └─conv1.linear.weight                                                        ├─864\n",
            "│    └─conv1.bn.weight                                                            ├─32\n",
            "│    └─conv1.bn.bias                                                              ├─32\n",
            "│    └─conv2.linear.weight                                                        ├─18,432\n",
            "│    └─conv2.bn.weight                                                            ├─64\n",
            "│    └─conv2.bn.bias                                                              ├─64\n",
            "│    └─conv3.linear.weight                                                        ├─73,728\n",
            "│    └─conv3.bn.weight                                                            ├─128\n",
            "│    └─conv3.bn.bias                                                              ├─128\n",
            "│    └─conv4.linear.weight                                                        ├─294,912\n",
            "│    └─conv4.bn.weight                                                            ├─256\n",
            "│    └─conv4.bn.bias                                                              └─256\n",
            "│    └─ConvNorm: 2-1                                    [32, 32, 112, 112]        --\n",
            "│    │    └─linear.weight                                                         ├─864\n",
            "│    │    └─bn.weight                                                             ├─32\n",
            "│    │    └─bn.bias                                                               └─32\n",
            "│    │    └─Conv2d: 3-1                                 [32, 32, 112, 112]        864\n",
            "│    │    │    └─weight                                                           └─864\n",
            "│    │    └─BatchNorm2d: 3-2                            [32, 32, 112, 112]        64\n",
            "│    │    │    └─weight                                                           ├─32\n",
            "│    │    │    └─bias                                                             └─32\n",
            "│    └─Hardswish: 2-2                                   [32, 32, 112, 112]        --\n",
            "│    └─ConvNorm: 2-3                                    [32, 64, 56, 56]          --\n",
            "│    │    └─linear.weight                                                         ├─18,432\n",
            "│    │    └─bn.weight                                                             ├─64\n",
            "│    │    └─bn.bias                                                               └─64\n",
            "│    │    └─Conv2d: 3-3                                 [32, 64, 56, 56]          18,432\n",
            "│    │    │    └─weight                                                           └─18,432\n",
            "│    │    └─BatchNorm2d: 3-4                            [32, 64, 56, 56]          128\n",
            "│    │    │    └─weight                                                           ├─64\n",
            "│    │    │    └─bias                                                             └─64\n",
            "│    └─Hardswish: 2-4                                   [32, 64, 56, 56]          --\n",
            "│    └─ConvNorm: 2-5                                    [32, 128, 28, 28]         --\n",
            "│    │    └─linear.weight                                                         ├─73,728\n",
            "│    │    └─bn.weight                                                             ├─128\n",
            "│    │    └─bn.bias                                                               └─128\n",
            "│    │    └─Conv2d: 3-5                                 [32, 128, 28, 28]         73,728\n",
            "│    │    │    └─weight                                                           └─73,728\n",
            "│    │    └─BatchNorm2d: 3-6                            [32, 128, 28, 28]         256\n",
            "│    │    │    └─weight                                                           ├─128\n",
            "│    │    │    └─bias                                                             └─128\n",
            "│    └─Hardswish: 2-6                                   [32, 128, 28, 28]         --\n",
            "│    └─ConvNorm: 2-7                                    [32, 256, 14, 14]         --\n",
            "│    │    └─linear.weight                                                         ├─294,912\n",
            "│    │    └─bn.weight                                                             ├─256\n",
            "│    │    └─bn.bias                                                               └─256\n",
            "│    │    └─Conv2d: 3-7                                 [32, 256, 14, 14]         294,912\n",
            "│    │    │    └─weight                                                           └─294,912\n",
            "│    │    └─BatchNorm2d: 3-8                            [32, 256, 14, 14]         512\n",
            "│    │    │    └─weight                                                           ├─256\n",
            "│    │    │    └─bias                                                             └─256\n",
            "├─LevitStage: 1-2                                       [32, 196, 256]            --\n",
            "│    └─blocks.0.attn.qkv.linear.weight                                            ├─196,608\n",
            "│    └─blocks.0.attn.qkv.bn.weight                                                ├─768\n",
            "│    └─blocks.0.attn.qkv.bn.bias                                                  ├─768\n",
            "│    └─blocks.0.attn.proj.1.linear.weight                                         ├─65,536\n",
            "│    └─blocks.0.attn.proj.1.bn.weight                                             ├─256\n",
            "│    └─blocks.0.attn.proj.1.bn.bias                                               ├─256\n",
            "│    └─blocks.0.mlp.ln1.linear.weight                                             ├─131,072\n",
            "│    └─blocks.0.mlp.ln1.bn.weight                                                 ├─512\n",
            "│    └─blocks.0.mlp.ln1.bn.bias                                                   ├─512\n",
            "│    └─blocks.0.mlp.ln2.linear.weight                                             ├─131,072\n",
            "│    └─blocks.0.mlp.ln2.bn.weight                                                 ├─256\n",
            "│    └─blocks.0.mlp.ln2.bn.bias                                                   ├─256\n",
            "│    └─blocks.1.attn.qkv.linear.weight                                            ├─196,608\n",
            "│    └─blocks.1.attn.qkv.bn.weight                                                ├─768\n",
            "│    └─blocks.1.attn.qkv.bn.bias                                                  ├─768\n",
            "│    └─blocks.1.attn.proj.1.linear.weight                                         ├─65,536\n",
            "│    └─blocks.1.attn.proj.1.bn.weight                                             ├─256\n",
            "│    └─blocks.1.attn.proj.1.bn.bias                                               ├─256\n",
            "│    └─blocks.1.mlp.ln1.linear.weight                                             ├─131,072\n",
            "│    └─blocks.1.mlp.ln1.bn.weight                                                 ├─512\n",
            "│    └─blocks.1.mlp.ln1.bn.bias                                                   ├─512\n",
            "│    └─blocks.1.mlp.ln2.linear.weight                                             ├─131,072\n",
            "│    └─blocks.1.mlp.ln2.bn.weight                                                 ├─256\n",
            "│    └─blocks.1.mlp.ln2.bn.bias                                                   └─256\n",
            "│    └─Identity: 2-8                                    [32, 196, 256]            --\n",
            "│    └─Sequential: 2-9                                  [32, 196, 256]            --\n",
            "│    │    └─0.attn.qkv.linear.weight                                              ├─196,608\n",
            "│    │    └─0.attn.qkv.bn.weight                                                  ├─768\n",
            "│    │    └─0.attn.qkv.bn.bias                                                    ├─768\n",
            "│    │    └─0.attn.proj.1.linear.weight                                           ├─65,536\n",
            "│    │    └─0.attn.proj.1.bn.weight                                               ├─256\n",
            "│    │    └─0.attn.proj.1.bn.bias                                                 ├─256\n",
            "│    │    └─0.mlp.ln1.linear.weight                                               ├─131,072\n",
            "│    │    └─0.mlp.ln1.bn.weight                                                   ├─512\n",
            "│    │    └─0.mlp.ln1.bn.bias                                                     ├─512\n",
            "│    │    └─0.mlp.ln2.linear.weight                                               ├─131,072\n",
            "│    │    └─0.mlp.ln2.bn.weight                                                   ├─256\n",
            "│    │    └─0.mlp.ln2.bn.bias                                                     ├─256\n",
            "│    │    └─1.attn.qkv.linear.weight                                              ├─196,608\n",
            "│    │    └─1.attn.qkv.bn.weight                                                  ├─768\n",
            "│    │    └─1.attn.qkv.bn.bias                                                    ├─768\n",
            "│    │    └─1.attn.proj.1.linear.weight                                           ├─65,536\n",
            "│    │    └─1.attn.proj.1.bn.weight                                               ├─256\n",
            "│    │    └─1.attn.proj.1.bn.bias                                                 ├─256\n",
            "│    │    └─1.mlp.ln1.linear.weight                                               ├─131,072\n",
            "│    │    └─1.mlp.ln1.bn.weight                                                   ├─512\n",
            "│    │    └─1.mlp.ln1.bn.bias                                                     ├─512\n",
            "│    │    └─1.mlp.ln2.linear.weight                                               ├─131,072\n",
            "│    │    └─1.mlp.ln2.bn.weight                                                   ├─256\n",
            "│    │    └─1.mlp.ln2.bn.bias                                                     └─256\n",
            "│    │    └─LevitBlock: 3-9                             [32, 196, 256]            527,872\n",
            "│    │    │    └─attn.qkv.linear.weight                                           ├─196,608\n",
            "│    │    │    └─attn.qkv.bn.weight                                               ├─768\n",
            "│    │    │    └─attn.qkv.bn.bias                                                 ├─768\n",
            "│    │    │    └─attn.proj.1.linear.weight                                        ├─65,536\n",
            "│    │    │    └─attn.proj.1.bn.weight                                            ├─256\n",
            "│    │    │    └─attn.proj.1.bn.bias                                              ├─256\n",
            "│    │    │    └─mlp.ln1.linear.weight                                            ├─131,072\n",
            "│    │    │    └─mlp.ln1.bn.weight                                                ├─512\n",
            "│    │    │    └─mlp.ln1.bn.bias                                                  ├─512\n",
            "│    │    │    └─mlp.ln2.linear.weight                                            ├─131,072\n",
            "│    │    │    └─mlp.ln2.bn.weight                                                ├─256\n",
            "│    │    │    └─mlp.ln2.bn.bias                                                  └─256\n",
            "│    │    └─LevitBlock: 3-10                            [32, 196, 256]            527,872\n",
            "│    │    │    └─attn.qkv.linear.weight                                           ├─196,608\n",
            "│    │    │    └─attn.qkv.bn.weight                                               ├─768\n",
            "│    │    │    └─attn.qkv.bn.bias                                                 ├─768\n",
            "│    │    │    └─attn.proj.1.linear.weight                                        ├─65,536\n",
            "│    │    │    └─attn.proj.1.bn.weight                                            ├─256\n",
            "│    │    │    └─attn.proj.1.bn.bias                                              ├─256\n",
            "│    │    │    └─mlp.ln1.linear.weight                                            ├─131,072\n",
            "│    │    │    └─mlp.ln1.bn.weight                                                ├─512\n",
            "│    │    │    └─mlp.ln1.bn.bias                                                  ├─512\n",
            "│    │    │    └─mlp.ln2.linear.weight                                            ├─131,072\n",
            "│    │    │    └─mlp.ln2.bn.weight                                                ├─256\n",
            "│    │    │    └─mlp.ln2.bn.bias                                                  └─256\n",
            "├─LevitStage: 1-3                                       [32, 49, 384]             --\n",
            "│    └─downsample.conv.weight                                                     ├─884,736\n",
            "│    └─downsample.conv.bias                                                       ├─384\n",
            "│    └─blocks.0.attn.qkv.linear.weight                                            ├─442,368\n",
            "│    └─blocks.0.attn.qkv.bn.weight                                                ├─1,152\n",
            "│    └─blocks.0.attn.qkv.bn.bias                                                  ├─1,152\n",
            "│    └─blocks.0.attn.proj.1.linear.weight                                         ├─147,456\n",
            "│    └─blocks.0.attn.proj.1.bn.weight                                             ├─384\n",
            "│    └─blocks.0.attn.proj.1.bn.bias                                               ├─384\n",
            "│    └─blocks.0.mlp.ln1.linear.weight                                             ├─294,912\n",
            "│    └─blocks.0.mlp.ln1.bn.weight                                                 ├─768\n",
            "│    └─blocks.0.mlp.ln1.bn.bias                                                   ├─768\n",
            "│    └─blocks.0.mlp.ln2.linear.weight                                             ├─294,912\n",
            "│    └─blocks.0.mlp.ln2.bn.weight                                                 ├─384\n",
            "│    └─blocks.0.mlp.ln2.bn.bias                                                   ├─384\n",
            "│    └─blocks.1.attn.qkv.linear.weight                                            ├─442,368\n",
            "│    └─blocks.1.attn.qkv.bn.weight                                                ├─1,152\n",
            "│    └─blocks.1.attn.qkv.bn.bias                                                  ├─1,152\n",
            "│    └─blocks.1.attn.proj.1.linear.weight                                         ├─147,456\n",
            "│    └─blocks.1.attn.proj.1.bn.weight                                             ├─384\n",
            "│    └─blocks.1.attn.proj.1.bn.bias                                               ├─384\n",
            "│    └─blocks.1.mlp.ln1.linear.weight                                             ├─294,912\n",
            "│    └─blocks.1.mlp.ln1.bn.weight                                                 ├─768\n",
            "│    └─blocks.1.mlp.ln1.bn.bias                                                   ├─768\n",
            "│    └─blocks.1.mlp.ln2.linear.weight                                             ├─294,912\n",
            "│    └─blocks.1.mlp.ln2.bn.weight                                                 ├─384\n",
            "│    └─blocks.1.mlp.ln2.bn.bias                                                   └─384\n",
            "│    └─CNNDownsample: 2-10                              [32, 49, 384]             --\n",
            "│    │    └─conv.weight                                                           ├─884,736\n",
            "│    │    └─conv.bias                                                             └─384\n",
            "│    │    └─Conv2d: 3-11                                [32, 384, 7, 7]           885,120\n",
            "│    │    │    └─weight                                                           ├─884,736\n",
            "│    │    │    └─bias                                                             └─384\n",
            "│    │    └─Hardswish: 3-12                             [32, 384, 7, 7]           --\n",
            "│    └─Sequential: 2-11                                 [32, 49, 384]             --\n",
            "│    │    └─0.attn.qkv.linear.weight                                              ├─442,368\n",
            "│    │    └─0.attn.qkv.bn.weight                                                  ├─1,152\n",
            "│    │    └─0.attn.qkv.bn.bias                                                    ├─1,152\n",
            "│    │    └─0.attn.proj.1.linear.weight                                           ├─147,456\n",
            "│    │    └─0.attn.proj.1.bn.weight                                               ├─384\n",
            "│    │    └─0.attn.proj.1.bn.bias                                                 ├─384\n",
            "│    │    └─0.mlp.ln1.linear.weight                                               ├─294,912\n",
            "│    │    └─0.mlp.ln1.bn.weight                                                   ├─768\n",
            "│    │    └─0.mlp.ln1.bn.bias                                                     ├─768\n",
            "│    │    └─0.mlp.ln2.linear.weight                                               ├─294,912\n",
            "│    │    └─0.mlp.ln2.bn.weight                                                   ├─384\n",
            "│    │    └─0.mlp.ln2.bn.bias                                                     ├─384\n",
            "│    │    └─1.attn.qkv.linear.weight                                              ├─442,368\n",
            "│    │    └─1.attn.qkv.bn.weight                                                  ├─1,152\n",
            "│    │    └─1.attn.qkv.bn.bias                                                    ├─1,152\n",
            "│    │    └─1.attn.proj.1.linear.weight                                           ├─147,456\n",
            "│    │    └─1.attn.proj.1.bn.weight                                               ├─384\n",
            "│    │    └─1.attn.proj.1.bn.bias                                                 ├─384\n",
            "│    │    └─1.mlp.ln1.linear.weight                                               ├─294,912\n",
            "│    │    └─1.mlp.ln1.bn.weight                                                   ├─768\n",
            "│    │    └─1.mlp.ln1.bn.bias                                                     ├─768\n",
            "│    │    └─1.mlp.ln2.linear.weight                                               ├─294,912\n",
            "│    │    └─1.mlp.ln2.bn.weight                                                   ├─384\n",
            "│    │    └─1.mlp.ln2.bn.bias                                                     └─384\n",
            "│    │    └─LevitBlock: 3-13                            [32, 49, 384]             1,185,024\n",
            "│    │    │    └─attn.qkv.linear.weight                                           ├─442,368\n",
            "│    │    │    └─attn.qkv.bn.weight                                               ├─1,152\n",
            "│    │    │    └─attn.qkv.bn.bias                                                 ├─1,152\n",
            "│    │    │    └─attn.proj.1.linear.weight                                        ├─147,456\n",
            "│    │    │    └─attn.proj.1.bn.weight                                            ├─384\n",
            "│    │    │    └─attn.proj.1.bn.bias                                              ├─384\n",
            "│    │    │    └─mlp.ln1.linear.weight                                            ├─294,912\n",
            "│    │    │    └─mlp.ln1.bn.weight                                                ├─768\n",
            "│    │    │    └─mlp.ln1.bn.bias                                                  ├─768\n",
            "│    │    │    └─mlp.ln2.linear.weight                                            ├─294,912\n",
            "│    │    │    └─mlp.ln2.bn.weight                                                ├─384\n",
            "│    │    │    └─mlp.ln2.bn.bias                                                  └─384\n",
            "│    │    └─LevitBlock: 3-14                            [32, 49, 384]             1,185,024\n",
            "│    │    │    └─attn.qkv.linear.weight                                           ├─442,368\n",
            "│    │    │    └─attn.qkv.bn.weight                                               ├─1,152\n",
            "│    │    │    └─attn.qkv.bn.bias                                                 ├─1,152\n",
            "│    │    │    └─attn.proj.1.linear.weight                                        ├─147,456\n",
            "│    │    │    └─attn.proj.1.bn.weight                                            ├─384\n",
            "│    │    │    └─attn.proj.1.bn.bias                                              ├─384\n",
            "│    │    │    └─mlp.ln1.linear.weight                                            ├─294,912\n",
            "│    │    │    └─mlp.ln1.bn.weight                                                ├─768\n",
            "│    │    │    └─mlp.ln1.bn.bias                                                  ├─768\n",
            "│    │    │    └─mlp.ln2.linear.weight                                            ├─294,912\n",
            "│    │    │    └─mlp.ln2.bn.weight                                                ├─384\n",
            "│    │    │    └─mlp.ln2.bn.bias                                                  └─384\n",
            "├─Sequential: 1-4                                       [32, 512, 7, 7]           --\n",
            "│    └─0.weight                                                                   ├─196,608\n",
            "│    └─0.bias                                                                     ├─512\n",
            "│    └─1.weight                                                                   ├─512\n",
            "│    └─1.bias                                                                     └─512\n",
            "│    └─Conv2d: 2-12                                     [32, 512, 7, 7]           197,120\n",
            "│    │    └─weight                                                                ├─196,608\n",
            "│    │    └─bias                                                                  └─512\n",
            "│    └─BatchNorm2d: 2-13                                [32, 512, 7, 7]           1,024\n",
            "│    │    └─weight                                                                ├─512\n",
            "│    │    └─bias                                                                  └─512\n",
            "│    └─ReLU: 2-14                                       [32, 512, 7, 7]           --\n",
            "├─NormLinear: 1-5                                       [32, 100]                 --\n",
            "│    └─bn.weight                                                                  ├─512\n",
            "│    └─bn.bias                                                                    ├─512\n",
            "│    └─linear.weight                                                              ├─51,200\n",
            "│    └─linear.bias                                                                └─100\n",
            "│    └─BatchNorm1d: 2-15                                [32, 512]                 1,024\n",
            "│    │    └─weight                                                                ├─512\n",
            "│    │    └─bias                                                                  └─512\n",
            "│    └─Dropout: 2-16                                    [32, 512]                 --\n",
            "│    └─Linear: 2-17                                     [32, 100]                 51,300\n",
            "│    │    └─weight                                                                ├─51,200\n",
            "│    │    └─bias                                                                  └─100\n",
            "├─NormLinear: 1-6                                       [32, 100]                 --\n",
            "│    └─bn.weight                                                                  ├─512\n",
            "│    └─bn.bias                                                                    ├─512\n",
            "│    └─linear.weight                                                              ├─51,200\n",
            "│    └─linear.bias                                                                └─100\n",
            "│    └─BatchNorm1d: 2-18                                [32, 512]                 1,024\n",
            "│    │    └─weight                                                                ├─512\n",
            "│    │    └─bias                                                                  └─512\n",
            "│    └─Dropout: 2-19                                    [32, 512]                 --\n",
            "│    └─Linear: 2-20                                     [32, 100]                 51,300\n",
            "│    │    └─weight                                                                ├─51,200\n",
            "│    │    └─bias                                                                  └─100\n",
            "=========================================================================================================\n",
            "Total params: 5,002,600\n",
            "Trainable params: 5,002,600\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (G): 17.93\n",
            "=========================================================================================================\n",
            "Input size (MB): 19.27\n",
            "Forward/backward pass size (MB): 897.86\n",
            "Params size (MB): 20.01\n",
            "Estimated Total Size (MB): 937.14\n",
            "=========================================================================================================\n",
            "=========================================================================================================\n",
            "Layer (type:depth-idx)                                  Output Shape              Param #\n",
            "=========================================================================================================\n",
            "LevitDistilled                                          [32, 100]                 --\n",
            "├─Stem16: 1-1                                           [32, 256, 14, 14]         --\n",
            "│    └─conv1.linear.weight                                                        ├─864\n",
            "│    └─conv1.bn.weight                                                            ├─32\n",
            "│    └─conv1.bn.bias                                                              ├─32\n",
            "│    └─conv2.linear.weight                                                        ├─18,432\n",
            "│    └─conv2.bn.weight                                                            ├─64\n",
            "│    └─conv2.bn.bias                                                              ├─64\n",
            "│    └─conv3.linear.weight                                                        ├─73,728\n",
            "│    └─conv3.bn.weight                                                            ├─128\n",
            "│    └─conv3.bn.bias                                                              ├─128\n",
            "│    └─conv4.linear.weight                                                        ├─294,912\n",
            "│    └─conv4.bn.weight                                                            ├─256\n",
            "│    └─conv4.bn.bias                                                              └─256\n",
            "│    └─ConvNorm: 2-1                                    [32, 32, 112, 112]        --\n",
            "│    │    └─linear.weight                                                         ├─864\n",
            "│    │    └─bn.weight                                                             ├─32\n",
            "│    │    └─bn.bias                                                               └─32\n",
            "│    │    └─Conv2d: 3-1                                 [32, 32, 112, 112]        864\n",
            "│    │    │    └─weight                                                           └─864\n",
            "│    │    └─BatchNorm2d: 3-2                            [32, 32, 112, 112]        64\n",
            "│    │    │    └─weight                                                           ├─32\n",
            "│    │    │    └─bias                                                             └─32\n",
            "│    └─Hardswish: 2-2                                   [32, 32, 112, 112]        --\n",
            "│    └─ConvNorm: 2-3                                    [32, 64, 56, 56]          --\n",
            "│    │    └─linear.weight                                                         ├─18,432\n",
            "│    │    └─bn.weight                                                             ├─64\n",
            "│    │    └─bn.bias                                                               └─64\n",
            "│    │    └─Conv2d: 3-3                                 [32, 64, 56, 56]          18,432\n",
            "│    │    │    └─weight                                                           └─18,432\n",
            "│    │    └─BatchNorm2d: 3-4                            [32, 64, 56, 56]          128\n",
            "│    │    │    └─weight                                                           ├─64\n",
            "│    │    │    └─bias                                                             └─64\n",
            "│    └─Hardswish: 2-4                                   [32, 64, 56, 56]          --\n",
            "│    └─ConvNorm: 2-5                                    [32, 128, 28, 28]         --\n",
            "│    │    └─linear.weight                                                         ├─73,728\n",
            "│    │    └─bn.weight                                                             ├─128\n",
            "│    │    └─bn.bias                                                               └─128\n",
            "│    │    └─Conv2d: 3-5                                 [32, 128, 28, 28]         73,728\n",
            "│    │    │    └─weight                                                           └─73,728\n",
            "│    │    └─BatchNorm2d: 3-6                            [32, 128, 28, 28]         256\n",
            "│    │    │    └─weight                                                           ├─128\n",
            "│    │    │    └─bias                                                             └─128\n",
            "│    └─Hardswish: 2-6                                   [32, 128, 28, 28]         --\n",
            "│    └─ConvNorm: 2-7                                    [32, 256, 14, 14]         --\n",
            "│    │    └─linear.weight                                                         ├─294,912\n",
            "│    │    └─bn.weight                                                             ├─256\n",
            "│    │    └─bn.bias                                                               └─256\n",
            "│    │    └─Conv2d: 3-7                                 [32, 256, 14, 14]         294,912\n",
            "│    │    │    └─weight                                                           └─294,912\n",
            "│    │    └─BatchNorm2d: 3-8                            [32, 256, 14, 14]         512\n",
            "│    │    │    └─weight                                                           ├─256\n",
            "│    │    │    └─bias                                                             └─256\n",
            "├─LevitStage: 1-2                                       [32, 196, 256]            --\n",
            "│    └─blocks.0.attn.qkv.linear.weight                                            ├─196,608\n",
            "│    └─blocks.0.attn.qkv.bn.weight                                                ├─768\n",
            "│    └─blocks.0.attn.qkv.bn.bias                                                  ├─768\n",
            "│    └─blocks.0.attn.proj.1.linear.weight                                         ├─65,536\n",
            "│    └─blocks.0.attn.proj.1.bn.weight                                             ├─256\n",
            "│    └─blocks.0.attn.proj.1.bn.bias                                               ├─256\n",
            "│    └─blocks.0.mlp.ln1.linear.weight                                             ├─131,072\n",
            "│    └─blocks.0.mlp.ln1.bn.weight                                                 ├─512\n",
            "│    └─blocks.0.mlp.ln1.bn.bias                                                   ├─512\n",
            "│    └─blocks.0.mlp.ln2.linear.weight                                             ├─131,072\n",
            "│    └─blocks.0.mlp.ln2.bn.weight                                                 ├─256\n",
            "│    └─blocks.0.mlp.ln2.bn.bias                                                   ├─256\n",
            "│    └─blocks.1.attn.qkv.linear.weight                                            ├─196,608\n",
            "│    └─blocks.1.attn.qkv.bn.weight                                                ├─768\n",
            "│    └─blocks.1.attn.qkv.bn.bias                                                  ├─768\n",
            "│    └─blocks.1.attn.proj.1.linear.weight                                         ├─65,536\n",
            "│    └─blocks.1.attn.proj.1.bn.weight                                             ├─256\n",
            "│    └─blocks.1.attn.proj.1.bn.bias                                               ├─256\n",
            "│    └─blocks.1.mlp.ln1.linear.weight                                             ├─131,072\n",
            "│    └─blocks.1.mlp.ln1.bn.weight                                                 ├─512\n",
            "│    └─blocks.1.mlp.ln1.bn.bias                                                   ├─512\n",
            "│    └─blocks.1.mlp.ln2.linear.weight                                             ├─131,072\n",
            "│    └─blocks.1.mlp.ln2.bn.weight                                                 ├─256\n",
            "│    └─blocks.1.mlp.ln2.bn.bias                                                   └─256\n",
            "│    └─Identity: 2-8                                    [32, 196, 256]            --\n",
            "│    └─Sequential: 2-9                                  [32, 196, 256]            --\n",
            "│    │    └─0.attn.qkv.linear.weight                                              ├─196,608\n",
            "│    │    └─0.attn.qkv.bn.weight                                                  ├─768\n",
            "│    │    └─0.attn.qkv.bn.bias                                                    ├─768\n",
            "│    │    └─0.attn.proj.1.linear.weight                                           ├─65,536\n",
            "│    │    └─0.attn.proj.1.bn.weight                                               ├─256\n",
            "│    │    └─0.attn.proj.1.bn.bias                                                 ├─256\n",
            "│    │    └─0.mlp.ln1.linear.weight                                               ├─131,072\n",
            "│    │    └─0.mlp.ln1.bn.weight                                                   ├─512\n",
            "│    │    └─0.mlp.ln1.bn.bias                                                     ├─512\n",
            "│    │    └─0.mlp.ln2.linear.weight                                               ├─131,072\n",
            "│    │    └─0.mlp.ln2.bn.weight                                                   ├─256\n",
            "│    │    └─0.mlp.ln2.bn.bias                                                     ├─256\n",
            "│    │    └─1.attn.qkv.linear.weight                                              ├─196,608\n",
            "│    │    └─1.attn.qkv.bn.weight                                                  ├─768\n",
            "│    │    └─1.attn.qkv.bn.bias                                                    ├─768\n",
            "│    │    └─1.attn.proj.1.linear.weight                                           ├─65,536\n",
            "│    │    └─1.attn.proj.1.bn.weight                                               ├─256\n",
            "│    │    └─1.attn.proj.1.bn.bias                                                 ├─256\n",
            "│    │    └─1.mlp.ln1.linear.weight                                               ├─131,072\n",
            "│    │    └─1.mlp.ln1.bn.weight                                                   ├─512\n",
            "│    │    └─1.mlp.ln1.bn.bias                                                     ├─512\n",
            "│    │    └─1.mlp.ln2.linear.weight                                               ├─131,072\n",
            "│    │    └─1.mlp.ln2.bn.weight                                                   ├─256\n",
            "│    │    └─1.mlp.ln2.bn.bias                                                     └─256\n",
            "│    │    └─LevitBlock: 3-9                             [32, 196, 256]            527,872\n",
            "│    │    │    └─attn.qkv.linear.weight                                           ├─196,608\n",
            "│    │    │    └─attn.qkv.bn.weight                                               ├─768\n",
            "│    │    │    └─attn.qkv.bn.bias                                                 ├─768\n",
            "│    │    │    └─attn.proj.1.linear.weight                                        ├─65,536\n",
            "│    │    │    └─attn.proj.1.bn.weight                                            ├─256\n",
            "│    │    │    └─attn.proj.1.bn.bias                                              ├─256\n",
            "│    │    │    └─mlp.ln1.linear.weight                                            ├─131,072\n",
            "│    │    │    └─mlp.ln1.bn.weight                                                ├─512\n",
            "│    │    │    └─mlp.ln1.bn.bias                                                  ├─512\n",
            "│    │    │    └─mlp.ln2.linear.weight                                            ├─131,072\n",
            "│    │    │    └─mlp.ln2.bn.weight                                                ├─256\n",
            "│    │    │    └─mlp.ln2.bn.bias                                                  └─256\n",
            "│    │    └─LevitBlock: 3-10                            [32, 196, 256]            527,872\n",
            "│    │    │    └─attn.qkv.linear.weight                                           ├─196,608\n",
            "│    │    │    └─attn.qkv.bn.weight                                               ├─768\n",
            "│    │    │    └─attn.qkv.bn.bias                                                 ├─768\n",
            "│    │    │    └─attn.proj.1.linear.weight                                        ├─65,536\n",
            "│    │    │    └─attn.proj.1.bn.weight                                            ├─256\n",
            "│    │    │    └─attn.proj.1.bn.bias                                              ├─256\n",
            "│    │    │    └─mlp.ln1.linear.weight                                            ├─131,072\n",
            "│    │    │    └─mlp.ln1.bn.weight                                                ├─512\n",
            "│    │    │    └─mlp.ln1.bn.bias                                                  ├─512\n",
            "│    │    │    └─mlp.ln2.linear.weight                                            ├─131,072\n",
            "│    │    │    └─mlp.ln2.bn.weight                                                ├─256\n",
            "│    │    │    └─mlp.ln2.bn.bias                                                  └─256\n",
            "├─LevitStage: 1-3                                       [32, 49, 384]             --\n",
            "│    └─downsample.conv.weight                                                     ├─884,736\n",
            "│    └─downsample.conv.bias                                                       ├─384\n",
            "│    └─blocks.0.attn.qkv.linear.weight                                            ├─442,368\n",
            "│    └─blocks.0.attn.qkv.bn.weight                                                ├─1,152\n",
            "│    └─blocks.0.attn.qkv.bn.bias                                                  ├─1,152\n",
            "│    └─blocks.0.attn.proj.1.linear.weight                                         ├─147,456\n",
            "│    └─blocks.0.attn.proj.1.bn.weight                                             ├─384\n",
            "│    └─blocks.0.attn.proj.1.bn.bias                                               ├─384\n",
            "│    └─blocks.0.mlp.ln1.linear.weight                                             ├─294,912\n",
            "│    └─blocks.0.mlp.ln1.bn.weight                                                 ├─768\n",
            "│    └─blocks.0.mlp.ln1.bn.bias                                                   ├─768\n",
            "│    └─blocks.0.mlp.ln2.linear.weight                                             ├─294,912\n",
            "│    └─blocks.0.mlp.ln2.bn.weight                                                 ├─384\n",
            "│    └─blocks.0.mlp.ln2.bn.bias                                                   ├─384\n",
            "│    └─blocks.1.attn.qkv.linear.weight                                            ├─442,368\n",
            "│    └─blocks.1.attn.qkv.bn.weight                                                ├─1,152\n",
            "│    └─blocks.1.attn.qkv.bn.bias                                                  ├─1,152\n",
            "│    └─blocks.1.attn.proj.1.linear.weight                                         ├─147,456\n",
            "│    └─blocks.1.attn.proj.1.bn.weight                                             ├─384\n",
            "│    └─blocks.1.attn.proj.1.bn.bias                                               ├─384\n",
            "│    └─blocks.1.mlp.ln1.linear.weight                                             ├─294,912\n",
            "│    └─blocks.1.mlp.ln1.bn.weight                                                 ├─768\n",
            "│    └─blocks.1.mlp.ln1.bn.bias                                                   ├─768\n",
            "│    └─blocks.1.mlp.ln2.linear.weight                                             ├─294,912\n",
            "│    └─blocks.1.mlp.ln2.bn.weight                                                 ├─384\n",
            "│    └─blocks.1.mlp.ln2.bn.bias                                                   └─384\n",
            "│    └─CNNDownsample: 2-10                              [32, 49, 384]             --\n",
            "│    │    └─conv.weight                                                           ├─884,736\n",
            "│    │    └─conv.bias                                                             └─384\n",
            "│    │    └─Conv2d: 3-11                                [32, 384, 7, 7]           885,120\n",
            "│    │    │    └─weight                                                           ├─884,736\n",
            "│    │    │    └─bias                                                             └─384\n",
            "│    │    └─Hardswish: 3-12                             [32, 384, 7, 7]           --\n",
            "│    └─Sequential: 2-11                                 [32, 49, 384]             --\n",
            "│    │    └─0.attn.qkv.linear.weight                                              ├─442,368\n",
            "│    │    └─0.attn.qkv.bn.weight                                                  ├─1,152\n",
            "│    │    └─0.attn.qkv.bn.bias                                                    ├─1,152\n",
            "│    │    └─0.attn.proj.1.linear.weight                                           ├─147,456\n",
            "│    │    └─0.attn.proj.1.bn.weight                                               ├─384\n",
            "│    │    └─0.attn.proj.1.bn.bias                                                 ├─384\n",
            "│    │    └─0.mlp.ln1.linear.weight                                               ├─294,912\n",
            "│    │    └─0.mlp.ln1.bn.weight                                                   ├─768\n",
            "│    │    └─0.mlp.ln1.bn.bias                                                     ├─768\n",
            "│    │    └─0.mlp.ln2.linear.weight                                               ├─294,912\n",
            "│    │    └─0.mlp.ln2.bn.weight                                                   ├─384\n",
            "│    │    └─0.mlp.ln2.bn.bias                                                     ├─384\n",
            "│    │    └─1.attn.qkv.linear.weight                                              ├─442,368\n",
            "│    │    └─1.attn.qkv.bn.weight                                                  ├─1,152\n",
            "│    │    └─1.attn.qkv.bn.bias                                                    ├─1,152\n",
            "│    │    └─1.attn.proj.1.linear.weight                                           ├─147,456\n",
            "│    │    └─1.attn.proj.1.bn.weight                                               ├─384\n",
            "│    │    └─1.attn.proj.1.bn.bias                                                 ├─384\n",
            "│    │    └─1.mlp.ln1.linear.weight                                               ├─294,912\n",
            "│    │    └─1.mlp.ln1.bn.weight                                                   ├─768\n",
            "│    │    └─1.mlp.ln1.bn.bias                                                     ├─768\n",
            "│    │    └─1.mlp.ln2.linear.weight                                               ├─294,912\n",
            "│    │    └─1.mlp.ln2.bn.weight                                                   ├─384\n",
            "│    │    └─1.mlp.ln2.bn.bias                                                     └─384\n",
            "│    │    └─LevitBlock: 3-13                            [32, 49, 384]             1,185,024\n",
            "│    │    │    └─attn.qkv.linear.weight                                           ├─442,368\n",
            "│    │    │    └─attn.qkv.bn.weight                                               ├─1,152\n",
            "│    │    │    └─attn.qkv.bn.bias                                                 ├─1,152\n",
            "│    │    │    └─attn.proj.1.linear.weight                                        ├─147,456\n",
            "│    │    │    └─attn.proj.1.bn.weight                                            ├─384\n",
            "│    │    │    └─attn.proj.1.bn.bias                                              ├─384\n",
            "│    │    │    └─mlp.ln1.linear.weight                                            ├─294,912\n",
            "│    │    │    └─mlp.ln1.bn.weight                                                ├─768\n",
            "│    │    │    └─mlp.ln1.bn.bias                                                  ├─768\n",
            "│    │    │    └─mlp.ln2.linear.weight                                            ├─294,912\n",
            "│    │    │    └─mlp.ln2.bn.weight                                                ├─384\n",
            "│    │    │    └─mlp.ln2.bn.bias                                                  └─384\n",
            "│    │    └─LevitBlock: 3-14                            [32, 49, 384]             1,185,024\n",
            "│    │    │    └─attn.qkv.linear.weight                                           ├─442,368\n",
            "│    │    │    └─attn.qkv.bn.weight                                               ├─1,152\n",
            "│    │    │    └─attn.qkv.bn.bias                                                 ├─1,152\n",
            "│    │    │    └─attn.proj.1.linear.weight                                        ├─147,456\n",
            "│    │    │    └─attn.proj.1.bn.weight                                            ├─384\n",
            "│    │    │    └─attn.proj.1.bn.bias                                              ├─384\n",
            "│    │    │    └─mlp.ln1.linear.weight                                            ├─294,912\n",
            "│    │    │    └─mlp.ln1.bn.weight                                                ├─768\n",
            "│    │    │    └─mlp.ln1.bn.bias                                                  ├─768\n",
            "│    │    │    └─mlp.ln2.linear.weight                                            ├─294,912\n",
            "│    │    │    └─mlp.ln2.bn.weight                                                ├─384\n",
            "│    │    │    └─mlp.ln2.bn.bias                                                  └─384\n",
            "├─Sequential: 1-4                                       [32, 512, 7, 7]           --\n",
            "│    └─0.weight                                                                   ├─196,608\n",
            "│    └─0.bias                                                                     ├─512\n",
            "│    └─1.weight                                                                   ├─512\n",
            "│    └─1.bias                                                                     └─512\n",
            "│    └─Conv2d: 2-12                                     [32, 512, 7, 7]           197,120\n",
            "│    │    └─weight                                                                ├─196,608\n",
            "│    │    └─bias                                                                  └─512\n",
            "│    └─BatchNorm2d: 2-13                                [32, 512, 7, 7]           1,024\n",
            "│    │    └─weight                                                                ├─512\n",
            "│    │    └─bias                                                                  └─512\n",
            "│    └─ReLU: 2-14                                       [32, 512, 7, 7]           --\n",
            "├─NormLinear: 1-5                                       [32, 100]                 --\n",
            "│    └─bn.weight                                                                  ├─512\n",
            "│    └─bn.bias                                                                    ├─512\n",
            "│    └─linear.weight                                                              ├─51,200\n",
            "│    └─linear.bias                                                                └─100\n",
            "│    └─BatchNorm1d: 2-15                                [32, 512]                 1,024\n",
            "│    │    └─weight                                                                ├─512\n",
            "│    │    └─bias                                                                  └─512\n",
            "│    └─Dropout: 2-16                                    [32, 512]                 --\n",
            "│    └─Linear: 2-17                                     [32, 100]                 51,300\n",
            "│    │    └─weight                                                                ├─51,200\n",
            "│    │    └─bias                                                                  └─100\n",
            "├─NormLinear: 1-6                                       [32, 100]                 --\n",
            "│    └─bn.weight                                                                  ├─512\n",
            "│    └─bn.bias                                                                    ├─512\n",
            "│    └─linear.weight                                                              ├─51,200\n",
            "│    └─linear.bias                                                                └─100\n",
            "│    └─BatchNorm1d: 2-18                                [32, 512]                 1,024\n",
            "│    │    └─weight                                                                ├─512\n",
            "│    │    └─bias                                                                  └─512\n",
            "│    └─Dropout: 2-19                                    [32, 512]                 --\n",
            "│    └─Linear: 2-20                                     [32, 100]                 51,300\n",
            "│    │    └─weight                                                                ├─51,200\n",
            "│    │    └─bias                                                                  └─100\n",
            "=========================================================================================================\n",
            "Total params: 5,002,600\n",
            "Trainable params: 5,002,600\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (G): 17.93\n",
            "=========================================================================================================\n",
            "Input size (MB): 19.27\n",
            "Forward/backward pass size (MB): 897.86\n",
            "Params size (MB): 20.01\n",
            "Estimated Total Size (MB): 937.14\n",
            "=========================================================================================================\n"
          ]
        }
      ],
      "source": [
        "print(summary(model, input_size=(32, 3, 224, 224), verbose=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "TOaIwHDSzBP3"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0Ww6weVzBP4",
        "outputId": "3a9bdd99-000f-43a6-a151-6c6e8a600363"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "train_data = datasets.CIFAR100(root=\"data\", train=True, transform=transform, download=True)\n",
        "test_data = datasets.CIFAR100(root=\"data\", train=False, transform=transform, download=True)\n",
        "combined_data = ConcatDataset([train_data, test_data])\n",
        "\n",
        "train_size = int(0.7 * len(combined_data))\n",
        "val_size = int(0.15 * len(combined_data))\n",
        "test_size = len(combined_data) - train_size - val_size\n",
        "train_data, val_data, test_data = random_split(combined_data, [train_size, val_size, test_size])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4sK4r0FzBP4",
        "outputId": "4d5f0a9c-70aa-4aed-a5f9-ea4e9644c8a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train size: 42000\n",
            "Validation size: 9000\n",
            "Test size: 9000\n"
          ]
        }
      ],
      "source": [
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(f\"Train size: {len(train_data)}\")\n",
        "print(f\"Validation size: {len(val_data)}\")\n",
        "print(f\"Test size: {len(test_data)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "HCIgSLtBzBP4"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "qCcVDgIPzBP4"
      },
      "outputs": [],
      "source": [
        "def train(model, train_loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for inputs, labels in tqdm(train_loader, desc=\"Training\"):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    accuracy = 100 * correct / total\n",
        "    train_losses.append(epoch_loss)\n",
        "    train_accuracies.append(accuracy)\n",
        "    print(f\"Train Loss: {epoch_loss:.4f}, Train Accuracy: {accuracy:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "pzZwWmVFzBP5"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, data_loader, criterion, device, phase=\"Validation\"):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(data_loader, desc=f\"{phase}\"):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(data_loader)\n",
        "    accuracy = 100 * correct / total\n",
        "    if phase == \"Validation\":\n",
        "        val_losses.append(epoch_loss)\n",
        "        val_accuracies.append(accuracy)\n",
        "    print(f\"{phase} Loss: {epoch_loss:.4f}, {phase} Accuracy: {accuracy:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "ATaK9u_JDWLD"
      },
      "outputs": [],
      "source": [
        "def measure_inference_time(model, data_loader, device):\n",
        "    model.eval()\n",
        "    times = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, _ in data_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            start_time = torch.cuda.Event(enable_timing=True)\n",
        "            end_time = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "            start_time.record()\n",
        "            _ = model(inputs)  # inference 수행\n",
        "            end_time.record()\n",
        "\n",
        "            # 시간 측정\n",
        "            torch.cuda.synchronize()  # CUDA에서 모든 커널이 완료될 때까지 대기\n",
        "            elapsed_time = start_time.elapsed_time(end_time)  # 밀리초 단위로 반환\n",
        "            times.append(elapsed_time)\n",
        "\n",
        "    # 통계량 계산\n",
        "    times_np = np.array(times)\n",
        "    total_inferences = len(times_np)\n",
        "    avg_time = np.mean(times_np)\n",
        "    std_dev = np.std(times_np)\n",
        "    max_time = np.max(times_np)\n",
        "    min_time = np.min(times_np)\n",
        "\n",
        "    # 결과 출력\n",
        "    print(f\"Inference Time Measurement Results:\")\n",
        "    print(f\"Total Inferences: {total_inferences}\")\n",
        "    print(f\"Average Time: {avg_time:.2f} ms\")\n",
        "    print(f\"Standard Deviation: {std_dev:.2f} ms\")\n",
        "    print(f\"Maximum Time: {max_time:.2f} ms\")\n",
        "    print(f\"Minimum Time: {min_time:.2f} ms\")\n",
        "\n",
        "    return times"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdGy_hjpzBP5",
        "outputId": "13a27cca-0c6e-40b1-dfe4-b955f6974d8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 1313/1313 [15:34<00:00,  1.40it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 4.0289, Train Accuracy: 8.33%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation: 100%|██████████| 282/282 [01:33<00:00,  3.02it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Loss: 3.8024, Validation Accuracy: 12.53%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(num_epochs):\n",
        "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "    train(model, train_loader, criterion, optimizer, device)\n",
        "    evaluate(model, val_loader, criterion, device, phase=\"Validation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAiyDwO0zMk1",
        "outputId": "451b9fd0-e835-4c79-a383-a08caa162760"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Final Test Evaluation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Test:  10%|▉         | 28/282 [00:10<01:35,  2.65it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[67], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFinal Test Evaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mphase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[64], line 9\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(model, data_loader, criterion, device, phase)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m tqdm(data_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mphase\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      8\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 9\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     12\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[48], line 25\u001b[0m, in \u001b[0;36mLevitDistilled.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     23\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(B, C, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     24\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstage1(x)\n\u001b[0;32m---> 25\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstage2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m H \u001b[38;5;241m=\u001b[39m W \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m     28\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mview(B, \u001b[38;5;241m384\u001b[39m, H, W)\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[45], line 8\u001b[0m, in \u001b[0;36mLevitStage.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m----> 8\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownsample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks(x)\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[44], line 14\u001b[0m, in \u001b[0;36mCNNDownsample.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m H \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(np\u001b[38;5;241m.\u001b[39msqrt(N))\n\u001b[1;32m     13\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(B, H, H, C)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(x)\n\u001b[1;32m     16\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mview(B, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_channels)\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/torch/nn/modules/conv.py:458\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/torch/nn/modules/conv.py:454\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    452\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    453\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "print(\"\\nFinal Test Evaluation\")\n",
        "evaluate(model, test_loader, criterion, device, phase=\"Test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model weights saved to ./model_weights.pth\n"
          ]
        }
      ],
      "source": [
        "torch.save(model.state_dict(), model_weight_path)\n",
        "print(f\"Model weights saved to {model_weight_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HCv209A51QEb",
        "outputId": "27b14df6-1fa6-4be5-81bd-208761aa91d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inference Time Measurement Results:\n",
            "Total Inferences: 282\n",
            "Average Time: 23.60 ms\n",
            "Standard Deviation: 0.93 ms\n",
            "Maximum Time: 23.75 ms\n",
            "Minimum Time: 7.94 ms\n"
          ]
        }
      ],
      "source": [
        "times = measure_inference_time(model, test_loader, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_kVgdJKDZqe",
        "outputId": "4cfbbc34-cf41-4c74-d5ad-8dc0af5be06a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                           aten::matmul         0.79%     159.480us         8.55%       1.731ms      72.108us       0.000us         0.00%      11.075ms     461.450us            24  \n",
            "                                           aten::linear         0.35%      70.677us         5.36%       1.085ms      60.289us       0.000us         0.00%       8.409ms     467.184us            18  \n",
            "                                               aten::mm         2.64%     533.260us         3.51%     710.895us      44.431us       8.331ms        35.54%       8.331ms     520.662us            16  \n",
            "                                  volta_sgemm_128x64_tn         0.00%       0.000us         0.00%       0.000us       0.000us       7.074ms        30.18%       7.074ms     505.314us            14  \n",
            "                                           aten::conv2d         0.17%      34.278us         6.68%       1.353ms     225.433us       0.000us         0.00%       6.224ms       1.037ms             6  \n",
            "                                      aten::convolution         0.22%      44.891us         6.51%       1.318ms     219.720us       0.000us         0.00%       6.224ms       1.037ms             6  \n",
            "                                     aten::_convolution         0.42%      85.077us         6.29%       1.273ms     212.238us       0.000us         0.00%       6.224ms       1.037ms             6  \n",
            "                                aten::cudnn_convolution         4.73%     956.643us         5.64%       1.141ms     190.221us       6.166ms        26.31%       6.166ms       1.028ms             6  \n",
            "_5x_cudnn_volta_scudnn_128x64_relu_xregs_large_nn_v1...         0.00%       0.000us         0.00%       0.000us       0.000us       4.008ms        17.10%       4.008ms       1.336ms             3  \n",
            "                                       aten::batch_norm         0.55%     111.131us        11.56%       2.339ms     101.678us       0.000us         0.00%       2.268ms      98.619us            23  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 20.236ms\n",
            "Self CUDA time total: 23.442ms\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from torch import profiler\n",
        "\n",
        "dummy_input = torch.randn(32, 3, 224, 224).cuda()\n",
        "\n",
        "# Profiling inference\n",
        "with profiler.profile(\n",
        "    activities=[\n",
        "       profiler.ProfilerActivity.CPU,\n",
        "        profiler.ProfilerActivity.CUDA,  # Include if using GPU\n",
        "    ],\n",
        "    on_trace_ready=profiler.tensorboard_trace_handler(\"./logs\"),  # Optional logging\n",
        "    record_shapes=True,\n",
        "    with_stack=True\n",
        ") as prof:\n",
        "    with torch.no_grad():\n",
        "        model(dummy_input)\n",
        "\n",
        "\n",
        "# Print results\n",
        "print(prof.key_averages().table(sort_by=\"cuda_time_total\" if torch.cuda.is_available() else \"cpu_time_total\", row_limit=10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x600 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(1, num_epochs + 1), train_losses, label='Train Loss')\n",
        "plt.plot(range(1, num_epochs + 1), val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss Over Epochs')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(1, num_epochs + 1), train_accuracies, label='Train Accuracy')\n",
        "plt.plot(range(1, num_epochs + 1), val_accuracies, label='Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.title('Accuracy Over Epochs')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(log_dir, \"training_results.png\")) \n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
