{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!wget -O CRC-VAL-HE-7K.zip https://zenodo.org/records/1214456/files/CRC-VAL-HE-7K.zip\n",
        "!unzip -qq CRC-VAL-HE-7K.zip -d val"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QrjpYwInzmwA",
        "outputId": "2d8be20f-c921-4fbb-e79b-7a8f9f189396"
      },
      "execution_count": 261,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-27 14:31:58--  https://zenodo.org/records/1214456/files/CRC-VAL-HE-7K.zip\n",
            "Resolving zenodo.org (zenodo.org)... 188.185.43.25, 188.185.48.194, 188.185.45.92, ...\n",
            "Connecting to zenodo.org (zenodo.org)|188.185.43.25|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 800276929 (763M) [application/octet-stream]\n",
            "Saving to: ‘CRC-VAL-HE-7K.zip’\n",
            "\n",
            "CRC-VAL-HE-7K.zip   100%[===================>] 763.20M   562KB/s    in 24m 5s  \n",
            "\n",
            "2025-02-27 14:56:03 (541 KB/s) - ‘CRC-VAL-HE-7K.zip’ saved [800276929/800276929]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 262,
      "metadata": {
        "id": "Wa8mdzSwvzMe"
      },
      "outputs": [],
      "source": [
        "import timm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, ConcatDataset\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "import itertools\n",
        "from torchinfo import summary\n",
        "from sklearn.metrics import accuracy_score, balanced_accuracy_score, classification_report, confusion_matrix\n",
        "from PIL import Image\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import random\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Subset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 316,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "lbCq25bWzJSQ",
        "outputId": "d29d9dfc-471e-42db-b145-9080b2f269db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LevitDistilled(\n",
            "  (stem): Stem16(\n",
            "    (conv1): ConvNorm(\n",
            "      (linear): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (act1): Hardswish()\n",
            "    (conv2): ConvNorm(\n",
            "      (linear): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (act2): Hardswish()\n",
            "    (conv3): ConvNorm(\n",
            "      (linear): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (act3): Hardswish()\n",
            "    (conv4): ConvNorm(\n",
            "      (linear): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (stages): Sequential(\n",
            "    (0): LevitStage(\n",
            "      (downsample): Identity()\n",
            "      (blocks): Sequential(\n",
            "        (0): LevitBlock(\n",
            "          (attn): Attention(\n",
            "            (qkv): LinearNorm(\n",
            "              (linear): Linear(in_features=128, out_features=256, bias=False)\n",
            "              (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "            (proj): Sequential(\n",
            "              (act): Hardswish()\n",
            "              (ln): LinearNorm(\n",
            "                (linear): Linear(in_features=128, out_features=128, bias=False)\n",
            "                (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (drop_path1): Identity()\n",
            "          (mlp): LevitMlp(\n",
            "            (ln1): LinearNorm(\n",
            "              (linear): Linear(in_features=128, out_features=256, bias=False)\n",
            "              (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "            (act): Hardswish()\n",
            "            (drop): Dropout(p=0.0, inplace=False)\n",
            "            (ln2): LinearNorm(\n",
            "              (linear): Linear(in_features=256, out_features=128, bias=False)\n",
            "              (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "          )\n",
            "          (drop_path2): Identity()\n",
            "        )\n",
            "        (1): LevitBlock(\n",
            "          (attn): Attention(\n",
            "            (qkv): LinearNorm(\n",
            "              (linear): Linear(in_features=128, out_features=256, bias=False)\n",
            "              (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "            (proj): Sequential(\n",
            "              (act): Hardswish()\n",
            "              (ln): LinearNorm(\n",
            "                (linear): Linear(in_features=128, out_features=128, bias=False)\n",
            "                (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (drop_path1): Identity()\n",
            "          (mlp): LevitMlp(\n",
            "            (ln1): LinearNorm(\n",
            "              (linear): Linear(in_features=128, out_features=256, bias=False)\n",
            "              (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "            (act): Hardswish()\n",
            "            (drop): Dropout(p=0.0, inplace=False)\n",
            "            (ln2): LinearNorm(\n",
            "              (linear): Linear(in_features=256, out_features=128, bias=False)\n",
            "              (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "          )\n",
            "          (drop_path2): Identity()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (1): LevitStage(\n",
            "      (downsample): LevitDownsample(\n",
            "        (attn_downsample): AttentionDownsample(\n",
            "          (kv): LinearNorm(\n",
            "            (linear): Linear(in_features=128, out_features=640, bias=False)\n",
            "            (bn): BatchNorm1d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (q): Sequential(\n",
            "            (down): Downsample()\n",
            "            (ln): LinearNorm(\n",
            "              (linear): Linear(in_features=128, out_features=128, bias=False)\n",
            "              (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "          )\n",
            "          (proj): Sequential(\n",
            "            (act): Hardswish()\n",
            "            (ln): LinearNorm(\n",
            "              (linear): Linear(in_features=512, out_features=256, bias=False)\n",
            "              (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (mlp): LevitMlp(\n",
            "          (ln1): LinearNorm(\n",
            "            (linear): Linear(in_features=256, out_features=512, bias=False)\n",
            "            (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (act): Hardswish()\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "          (ln2): LinearNorm(\n",
            "            (linear): Linear(in_features=512, out_features=256, bias=False)\n",
            "            (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "      )\n",
            "      (blocks): Sequential(\n",
            "        (0): LevitBlock(\n",
            "          (attn): Attention(\n",
            "            (qkv): LinearNorm(\n",
            "              (linear): Linear(in_features=256, out_features=384, bias=False)\n",
            "              (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "            (proj): Sequential(\n",
            "              (act): Hardswish()\n",
            "              (ln): LinearNorm(\n",
            "                (linear): Linear(in_features=192, out_features=256, bias=False)\n",
            "                (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (drop_path1): Identity()\n",
            "          (mlp): LevitMlp(\n",
            "            (ln1): LinearNorm(\n",
            "              (linear): Linear(in_features=256, out_features=512, bias=False)\n",
            "              (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "            (act): Hardswish()\n",
            "            (drop): Dropout(p=0.0, inplace=False)\n",
            "            (ln2): LinearNorm(\n",
            "              (linear): Linear(in_features=512, out_features=256, bias=False)\n",
            "              (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "          )\n",
            "          (drop_path2): Identity()\n",
            "        )\n",
            "        (1): LevitBlock(\n",
            "          (attn): Attention(\n",
            "            (qkv): LinearNorm(\n",
            "              (linear): Linear(in_features=256, out_features=384, bias=False)\n",
            "              (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "            (proj): Sequential(\n",
            "              (act): Hardswish()\n",
            "              (ln): LinearNorm(\n",
            "                (linear): Linear(in_features=192, out_features=256, bias=False)\n",
            "                (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (drop_path1): Identity()\n",
            "          (mlp): LevitMlp(\n",
            "            (ln1): LinearNorm(\n",
            "              (linear): Linear(in_features=256, out_features=512, bias=False)\n",
            "              (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "            (act): Hardswish()\n",
            "            (drop): Dropout(p=0.0, inplace=False)\n",
            "            (ln2): LinearNorm(\n",
            "              (linear): Linear(in_features=512, out_features=256, bias=False)\n",
            "              (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "          )\n",
            "          (drop_path2): Identity()\n",
            "        )\n",
            "        (2): LevitBlock(\n",
            "          (attn): Attention(\n",
            "            (qkv): LinearNorm(\n",
            "              (linear): Linear(in_features=256, out_features=384, bias=False)\n",
            "              (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "            (proj): Sequential(\n",
            "              (act): Hardswish()\n",
            "              (ln): LinearNorm(\n",
            "                (linear): Linear(in_features=192, out_features=256, bias=False)\n",
            "                (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (drop_path1): Identity()\n",
            "          (mlp): LevitMlp(\n",
            "            (ln1): LinearNorm(\n",
            "              (linear): Linear(in_features=256, out_features=512, bias=False)\n",
            "              (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "            (act): Hardswish()\n",
            "            (drop): Dropout(p=0.0, inplace=False)\n",
            "            (ln2): LinearNorm(\n",
            "              (linear): Linear(in_features=512, out_features=256, bias=False)\n",
            "              (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "          )\n",
            "          (drop_path2): Identity()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (2): LevitStage(\n",
            "      (downsample): LevitDownsample(\n",
            "        (attn_downsample): AttentionDownsample(\n",
            "          (kv): LinearNorm(\n",
            "            (linear): Linear(in_features=256, out_features=1280, bias=False)\n",
            "            (bn): BatchNorm1d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (q): Sequential(\n",
            "            (down): Downsample()\n",
            "            (ln): LinearNorm(\n",
            "              (linear): Linear(in_features=256, out_features=256, bias=False)\n",
            "              (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "          )\n",
            "          (proj): Sequential(\n",
            "            (act): Hardswish()\n",
            "            (ln): LinearNorm(\n",
            "              (linear): Linear(in_features=1024, out_features=384, bias=False)\n",
            "              (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (mlp): LevitMlp(\n",
            "          (ln1): LinearNorm(\n",
            "            (linear): Linear(in_features=384, out_features=768, bias=False)\n",
            "            (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (act): Hardswish()\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "          (ln2): LinearNorm(\n",
            "            (linear): Linear(in_features=768, out_features=384, bias=False)\n",
            "            (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "      )\n",
            "      (blocks): Sequential(\n",
            "        (0): LevitBlock(\n",
            "          (attn): Attention(\n",
            "            (qkv): LinearNorm(\n",
            "              (linear): Linear(in_features=384, out_features=512, bias=False)\n",
            "              (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "            (proj): Sequential(\n",
            "              (act): Hardswish()\n",
            "              (ln): LinearNorm(\n",
            "                (linear): Linear(in_features=256, out_features=384, bias=False)\n",
            "                (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (drop_path1): Identity()\n",
            "          (mlp): LevitMlp(\n",
            "            (ln1): LinearNorm(\n",
            "              (linear): Linear(in_features=384, out_features=768, bias=False)\n",
            "              (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "            (act): Hardswish()\n",
            "            (drop): Dropout(p=0.0, inplace=False)\n",
            "            (ln2): LinearNorm(\n",
            "              (linear): Linear(in_features=768, out_features=384, bias=False)\n",
            "              (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "          )\n",
            "          (drop_path2): Identity()\n",
            "        )\n",
            "        (1): LevitBlock(\n",
            "          (attn): Attention(\n",
            "            (qkv): LinearNorm(\n",
            "              (linear): Linear(in_features=384, out_features=512, bias=False)\n",
            "              (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "            (proj): Sequential(\n",
            "              (act): Hardswish()\n",
            "              (ln): LinearNorm(\n",
            "                (linear): Linear(in_features=256, out_features=384, bias=False)\n",
            "                (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (drop_path1): Identity()\n",
            "          (mlp): LevitMlp(\n",
            "            (ln1): LinearNorm(\n",
            "              (linear): Linear(in_features=384, out_features=768, bias=False)\n",
            "              (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "            (act): Hardswish()\n",
            "            (drop): Dropout(p=0.0, inplace=False)\n",
            "            (ln2): LinearNorm(\n",
            "              (linear): Linear(in_features=768, out_features=384, bias=False)\n",
            "              (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "          )\n",
            "          (drop_path2): Identity()\n",
            "        )\n",
            "        (2): LevitBlock(\n",
            "          (attn): Attention(\n",
            "            (qkv): LinearNorm(\n",
            "              (linear): Linear(in_features=384, out_features=512, bias=False)\n",
            "              (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "            (proj): Sequential(\n",
            "              (act): Hardswish()\n",
            "              (ln): LinearNorm(\n",
            "                (linear): Linear(in_features=256, out_features=384, bias=False)\n",
            "                (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (drop_path1): Identity()\n",
            "          (mlp): LevitMlp(\n",
            "            (ln1): LinearNorm(\n",
            "              (linear): Linear(in_features=384, out_features=768, bias=False)\n",
            "              (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "            (act): Hardswish()\n",
            "            (drop): Dropout(p=0.0, inplace=False)\n",
            "            (ln2): LinearNorm(\n",
            "              (linear): Linear(in_features=768, out_features=384, bias=False)\n",
            "              (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "          )\n",
            "          (drop_path2): Identity()\n",
            "        )\n",
            "        (3): LevitBlock(\n",
            "          (attn): Attention(\n",
            "            (qkv): LinearNorm(\n",
            "              (linear): Linear(in_features=384, out_features=512, bias=False)\n",
            "              (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "            (proj): Sequential(\n",
            "              (act): Hardswish()\n",
            "              (ln): LinearNorm(\n",
            "                (linear): Linear(in_features=256, out_features=384, bias=False)\n",
            "                (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (drop_path1): Identity()\n",
            "          (mlp): LevitMlp(\n",
            "            (ln1): LinearNorm(\n",
            "              (linear): Linear(in_features=384, out_features=768, bias=False)\n",
            "              (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "            (act): Hardswish()\n",
            "            (drop): Dropout(p=0.0, inplace=False)\n",
            "            (ln2): LinearNorm(\n",
            "              (linear): Linear(in_features=768, out_features=384, bias=False)\n",
            "              (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "          )\n",
            "          (drop_path2): Identity()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (head): NormLinear(\n",
            "    (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (drop): Dropout(p=0.0, inplace=False)\n",
            "    (linear): Linear(in_features=384, out_features=9, bias=True)\n",
            "  )\n",
            "  (head_dist): NormLinear(\n",
            "    (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (drop): Dropout(p=0.0, inplace=False)\n",
            "    (linear): Linear(in_features=384, out_features=9, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model = timm.create_model('levit_128s', pretrained=False, num_classes=9)\n",
        "print(model)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "batch_size = 32\n",
        "learning_rate = 5e-4\n",
        "num_epochs = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 317,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNpMsNYAzLDF",
        "outputId": "662c987f-53e4-408c-df7e-748c11e5b9da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==============================================================================================================\n",
            "Layer (type:depth-idx)                                       Output Shape              Param #\n",
            "==============================================================================================================\n",
            "LevitDistilled                                               [32, 9]                   --\n",
            "├─Stem16: 1-1                                                [32, 128, 14, 14]         --\n",
            "│    └─ConvNorm: 2-1                                         [32, 16, 112, 112]        --\n",
            "│    │    └─Conv2d: 3-1                                      [32, 16, 112, 112]        432\n",
            "│    │    └─BatchNorm2d: 3-2                                 [32, 16, 112, 112]        32\n",
            "│    └─Hardswish: 2-2                                        [32, 16, 112, 112]        --\n",
            "│    └─ConvNorm: 2-3                                         [32, 32, 56, 56]          --\n",
            "│    │    └─Conv2d: 3-3                                      [32, 32, 56, 56]          4,608\n",
            "│    │    └─BatchNorm2d: 3-4                                 [32, 32, 56, 56]          64\n",
            "│    └─Hardswish: 2-4                                        [32, 32, 56, 56]          --\n",
            "│    └─ConvNorm: 2-5                                         [32, 64, 28, 28]          --\n",
            "│    │    └─Conv2d: 3-5                                      [32, 64, 28, 28]          18,432\n",
            "│    │    └─BatchNorm2d: 3-6                                 [32, 64, 28, 28]          128\n",
            "│    └─Hardswish: 2-6                                        [32, 64, 28, 28]          --\n",
            "│    └─ConvNorm: 2-7                                         [32, 128, 14, 14]         --\n",
            "│    │    └─Conv2d: 3-7                                      [32, 128, 14, 14]         73,728\n",
            "│    │    └─BatchNorm2d: 3-8                                 [32, 128, 14, 14]         256\n",
            "├─Sequential: 1-2                                            [32, 16, 384]             --\n",
            "│    └─LevitStage: 2-8                                       [32, 196, 128]            --\n",
            "│    │    └─Identity: 3-9                                    [32, 196, 128]            --\n",
            "│    │    └─Sequential: 3-10                                 [32, 196, 128]            234,016\n",
            "│    └─LevitStage: 2-9                                       [32, 49, 256]             --\n",
            "│    │    └─LevitDownsample: 3-11                            [32, 49, 256]             496,672\n",
            "│    │    └─Sequential: 3-12                                 [32, 49, 256]             1,238,130\n",
            "│    └─LevitStage: 2-10                                      [32, 16, 384]             --\n",
            "│    │    └─LevitDownsample: 3-13                            [32, 16, 384]             1,383,184\n",
            "│    │    └─Sequential: 3-14                                 [32, 16, 384]             3,555,840\n",
            "├─NormLinear: 1-3                                            [32, 9]                   --\n",
            "│    └─BatchNorm1d: 2-11                                     [32, 384]                 768\n",
            "│    └─Dropout: 2-12                                         [32, 384]                 --\n",
            "│    └─Linear: 2-13                                          [32, 9]                   3,465\n",
            "├─NormLinear: 1-4                                            [32, 9]                   --\n",
            "│    └─BatchNorm1d: 2-14                                     [32, 384]                 768\n",
            "│    └─Dropout: 2-15                                         [32, 384]                 --\n",
            "│    └─Linear: 2-16                                          [32, 9]                   3,465\n",
            "==============================================================================================================\n",
            "Total params: 7,013,988\n",
            "Trainable params: 7,013,988\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (Units.GIGABYTES): 1.84\n",
            "==============================================================================================================\n",
            "Input size (MB): 19.27\n",
            "Forward/backward pass size (MB): 660.02\n",
            "Params size (MB): 28.03\n",
            "Estimated Total Size (MB): 707.32\n",
            "==============================================================================================================\n"
          ]
        }
      ],
      "source": [
        "print(summary(model, input_size=(32, 3, 224, 224)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 318,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "qDELjYy2zP90",
        "outputId": "0500dbb9-32a7-409e-abf9-549a4e6a40ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==============================================================================================================\n",
            "Layer (type:depth-idx)                                       Output Shape              Param #\n",
            "==============================================================================================================\n",
            "LevitDistilled                                               [32, 9]                   --\n",
            "├─Stem16: 1-1                                                [32, 128, 14, 14]         --\n",
            "│    └─conv1.linear.weight                                                             ├─432\n",
            "│    └─conv1.bn.weight                                                                 ├─16\n",
            "│    └─conv1.bn.bias                                                                   ├─16\n",
            "│    └─conv2.linear.weight                                                             ├─4,608\n",
            "│    └─conv2.bn.weight                                                                 ├─32\n",
            "│    └─conv2.bn.bias                                                                   ├─32\n",
            "│    └─conv3.linear.weight                                                             ├─18,432\n",
            "│    └─conv3.bn.weight                                                                 ├─64\n",
            "│    └─conv3.bn.bias                                                                   ├─64\n",
            "│    └─conv4.linear.weight                                                             ├─73,728\n",
            "│    └─conv4.bn.weight                                                                 ├─128\n",
            "│    └─conv4.bn.bias                                                                   └─128\n",
            "│    └─ConvNorm: 2-1                                         [32, 16, 112, 112]        --\n",
            "│    │    └─linear.weight                                                              ├─432\n",
            "│    │    └─bn.weight                                                                  ├─16\n",
            "│    │    └─bn.bias                                                                    └─16\n",
            "│    │    └─Conv2d: 3-1                                      [32, 16, 112, 112]        432\n",
            "│    │    │    └─weight                                                                └─432\n",
            "│    │    └─BatchNorm2d: 3-2                                 [32, 16, 112, 112]        32\n",
            "│    │    │    └─weight                                                                ├─16\n",
            "│    │    │    └─bias                                                                  └─16\n",
            "│    └─Hardswish: 2-2                                        [32, 16, 112, 112]        --\n",
            "│    └─ConvNorm: 2-3                                         [32, 32, 56, 56]          --\n",
            "│    │    └─linear.weight                                                              ├─4,608\n",
            "│    │    └─bn.weight                                                                  ├─32\n",
            "│    │    └─bn.bias                                                                    └─32\n",
            "│    │    └─Conv2d: 3-3                                      [32, 32, 56, 56]          4,608\n",
            "│    │    │    └─weight                                                                └─4,608\n",
            "│    │    └─BatchNorm2d: 3-4                                 [32, 32, 56, 56]          64\n",
            "│    │    │    └─weight                                                                ├─32\n",
            "│    │    │    └─bias                                                                  └─32\n",
            "│    └─Hardswish: 2-4                                        [32, 32, 56, 56]          --\n",
            "│    └─ConvNorm: 2-5                                         [32, 64, 28, 28]          --\n",
            "│    │    └─linear.weight                                                              ├─18,432\n",
            "│    │    └─bn.weight                                                                  ├─64\n",
            "│    │    └─bn.bias                                                                    └─64\n",
            "│    │    └─Conv2d: 3-5                                      [32, 64, 28, 28]          18,432\n",
            "│    │    │    └─weight                                                                └─18,432\n",
            "│    │    └─BatchNorm2d: 3-6                                 [32, 64, 28, 28]          128\n",
            "│    │    │    └─weight                                                                ├─64\n",
            "│    │    │    └─bias                                                                  └─64\n",
            "│    └─Hardswish: 2-6                                        [32, 64, 28, 28]          --\n",
            "│    └─ConvNorm: 2-7                                         [32, 128, 14, 14]         --\n",
            "│    │    └─linear.weight                                                              ├─73,728\n",
            "│    │    └─bn.weight                                                                  ├─128\n",
            "│    │    └─bn.bias                                                                    └─128\n",
            "│    │    └─Conv2d: 3-7                                      [32, 128, 14, 14]         73,728\n",
            "│    │    │    └─weight                                                                └─73,728\n",
            "│    │    └─BatchNorm2d: 3-8                                 [32, 128, 14, 14]         256\n",
            "│    │    │    └─weight                                                                ├─128\n",
            "│    │    │    └─bias                                                                  └─128\n",
            "├─Sequential: 1-2                                            [32, 16, 384]             --\n",
            "│    └─0.blocks.0.attn.attention_biases                                                ├─784\n",
            "│    └─0.blocks.0.attn.qkv.linear.weight                                               ├─32,768\n",
            "│    └─0.blocks.0.attn.qkv.bn.weight                                                   ├─256\n",
            "│    └─0.blocks.0.attn.qkv.bn.bias                                                     ├─256\n",
            "│    └─0.blocks.0.attn.proj.ln.linear.weight                                           ├─16,384\n",
            "│    └─0.blocks.0.attn.proj.ln.bn.weight                                               ├─128\n",
            "│    └─0.blocks.0.attn.proj.ln.bn.bias                                                 ├─128\n",
            "│    └─0.blocks.0.mlp.ln1.linear.weight                                                ├─32,768\n",
            "│    └─0.blocks.0.mlp.ln1.bn.weight                                                    ├─256\n",
            "│    └─0.blocks.0.mlp.ln1.bn.bias                                                      ├─256\n",
            "│    └─0.blocks.0.mlp.ln2.linear.weight                                                ├─32,768\n",
            "│    └─0.blocks.0.mlp.ln2.bn.weight                                                    ├─128\n",
            "│    └─0.blocks.0.mlp.ln2.bn.bias                                                      ├─128\n",
            "│    └─0.blocks.1.attn.attention_biases                                                ├─784\n",
            "│    └─0.blocks.1.attn.qkv.linear.weight                                               ├─32,768\n",
            "│    └─0.blocks.1.attn.qkv.bn.weight                                                   ├─256\n",
            "│    └─0.blocks.1.attn.qkv.bn.bias                                                     ├─256\n",
            "│    └─0.blocks.1.attn.proj.ln.linear.weight                                           ├─16,384\n",
            "│    └─0.blocks.1.attn.proj.ln.bn.weight                                               ├─128\n",
            "│    └─0.blocks.1.attn.proj.ln.bn.bias                                                 ├─128\n",
            "│    └─0.blocks.1.mlp.ln1.linear.weight                                                ├─32,768\n",
            "│    └─0.blocks.1.mlp.ln1.bn.weight                                                    ├─256\n",
            "│    └─0.blocks.1.mlp.ln1.bn.bias                                                      ├─256\n",
            "│    └─0.blocks.1.mlp.ln2.linear.weight                                                ├─32,768\n",
            "│    └─0.blocks.1.mlp.ln2.bn.weight                                                    ├─128\n",
            "│    └─0.blocks.1.mlp.ln2.bn.bias                                                      ├─128\n",
            "│    └─1.downsample.attn_downsample.attention_biases                                   ├─1,568\n",
            "│    └─1.downsample.attn_downsample.kv.linear.weight                                   ├─81,920\n",
            "│    └─1.downsample.attn_downsample.kv.bn.weight                                       ├─640\n",
            "│    └─1.downsample.attn_downsample.kv.bn.bias                                         ├─640\n",
            "│    └─1.downsample.attn_downsample.q.ln.linear.weight                                 ├─16,384\n",
            "│    └─1.downsample.attn_downsample.q.ln.bn.weight                                     ├─128\n",
            "│    └─1.downsample.attn_downsample.q.ln.bn.bias                                       ├─128\n",
            "│    └─1.downsample.attn_downsample.proj.ln.linear.weight                              ├─131,072\n",
            "│    └─1.downsample.attn_downsample.proj.ln.bn.weight                                  ├─256\n",
            "│    └─1.downsample.attn_downsample.proj.ln.bn.bias                                    ├─256\n",
            "│    └─1.downsample.mlp.ln1.linear.weight                                              ├─131,072\n",
            "│    └─1.downsample.mlp.ln1.bn.weight                                                  ├─512\n",
            "│    └─1.downsample.mlp.ln1.bn.bias                                                    ├─512\n",
            "│    └─1.downsample.mlp.ln2.linear.weight                                              ├─131,072\n",
            "│    └─1.downsample.mlp.ln2.bn.weight                                                  ├─256\n",
            "│    └─1.downsample.mlp.ln2.bn.bias                                                    ├─256\n",
            "│    └─1.blocks.0.attn.attention_biases                                                ├─294\n",
            "│    └─1.blocks.0.attn.qkv.linear.weight                                               ├─98,304\n",
            "│    └─1.blocks.0.attn.qkv.bn.weight                                                   ├─384\n",
            "│    └─1.blocks.0.attn.qkv.bn.bias                                                     ├─384\n",
            "│    └─1.blocks.0.attn.proj.ln.linear.weight                                           ├─49,152\n",
            "│    └─1.blocks.0.attn.proj.ln.bn.weight                                               ├─256\n",
            "│    └─1.blocks.0.attn.proj.ln.bn.bias                                                 ├─256\n",
            "│    └─1.blocks.0.mlp.ln1.linear.weight                                                ├─131,072\n",
            "│    └─1.blocks.0.mlp.ln1.bn.weight                                                    ├─512\n",
            "│    └─1.blocks.0.mlp.ln1.bn.bias                                                      ├─512\n",
            "│    └─1.blocks.0.mlp.ln2.linear.weight                                                ├─131,072\n",
            "│    └─1.blocks.0.mlp.ln2.bn.weight                                                    ├─256\n",
            "│    └─1.blocks.0.mlp.ln2.bn.bias                                                      ├─256\n",
            "│    └─1.blocks.1.attn.attention_biases                                                ├─294\n",
            "│    └─1.blocks.1.attn.qkv.linear.weight                                               ├─98,304\n",
            "│    └─1.blocks.1.attn.qkv.bn.weight                                                   ├─384\n",
            "│    └─1.blocks.1.attn.qkv.bn.bias                                                     ├─384\n",
            "│    └─1.blocks.1.attn.proj.ln.linear.weight                                           ├─49,152\n",
            "│    └─1.blocks.1.attn.proj.ln.bn.weight                                               ├─256\n",
            "│    └─1.blocks.1.attn.proj.ln.bn.bias                                                 ├─256\n",
            "│    └─1.blocks.1.mlp.ln1.linear.weight                                                ├─131,072\n",
            "│    └─1.blocks.1.mlp.ln1.bn.weight                                                    ├─512\n",
            "│    └─1.blocks.1.mlp.ln1.bn.bias                                                      ├─512\n",
            "│    └─1.blocks.1.mlp.ln2.linear.weight                                                ├─131,072\n",
            "│    └─1.blocks.1.mlp.ln2.bn.weight                                                    ├─256\n",
            "│    └─1.blocks.1.mlp.ln2.bn.bias                                                      ├─256\n",
            "│    └─1.blocks.2.attn.attention_biases                                                ├─294\n",
            "│    └─1.blocks.2.attn.qkv.linear.weight                                               ├─98,304\n",
            "│    └─1.blocks.2.attn.qkv.bn.weight                                                   ├─384\n",
            "│    └─1.blocks.2.attn.qkv.bn.bias                                                     ├─384\n",
            "│    └─1.blocks.2.attn.proj.ln.linear.weight                                           ├─49,152\n",
            "│    └─1.blocks.2.attn.proj.ln.bn.weight                                               ├─256\n",
            "│    └─1.blocks.2.attn.proj.ln.bn.bias                                                 ├─256\n",
            "│    └─1.blocks.2.mlp.ln1.linear.weight                                                ├─131,072\n",
            "│    └─1.blocks.2.mlp.ln1.bn.weight                                                    ├─512\n",
            "│    └─1.blocks.2.mlp.ln1.bn.bias                                                      ├─512\n",
            "│    └─1.blocks.2.mlp.ln2.linear.weight                                                ├─131,072\n",
            "│    └─1.blocks.2.mlp.ln2.bn.weight                                                    ├─256\n",
            "│    └─1.blocks.2.mlp.ln2.bn.bias                                                      ├─256\n",
            "│    └─2.downsample.attn_downsample.attention_biases                                   ├─784\n",
            "│    └─2.downsample.attn_downsample.kv.linear.weight                                   ├─327,680\n",
            "│    └─2.downsample.attn_downsample.kv.bn.weight                                       ├─1,280\n",
            "│    └─2.downsample.attn_downsample.kv.bn.bias                                         ├─1,280\n",
            "│    └─2.downsample.attn_downsample.q.ln.linear.weight                                 ├─65,536\n",
            "│    └─2.downsample.attn_downsample.q.ln.bn.weight                                     ├─256\n",
            "│    └─2.downsample.attn_downsample.q.ln.bn.bias                                       ├─256\n",
            "│    └─2.downsample.attn_downsample.proj.ln.linear.weight                              ├─393,216\n",
            "│    └─2.downsample.attn_downsample.proj.ln.bn.weight                                  ├─384\n",
            "│    └─2.downsample.attn_downsample.proj.ln.bn.bias                                    ├─384\n",
            "│    └─2.downsample.mlp.ln1.linear.weight                                              ├─294,912\n",
            "│    └─2.downsample.mlp.ln1.bn.weight                                                  ├─768\n",
            "│    └─2.downsample.mlp.ln1.bn.bias                                                    ├─768\n",
            "│    └─2.downsample.mlp.ln2.linear.weight                                              ├─294,912\n",
            "│    └─2.downsample.mlp.ln2.bn.weight                                                  ├─384\n",
            "│    └─2.downsample.mlp.ln2.bn.bias                                                    ├─384\n",
            "│    └─2.blocks.0.attn.attention_biases                                                ├─128\n",
            "│    └─2.blocks.0.attn.qkv.linear.weight                                               ├─196,608\n",
            "│    └─2.blocks.0.attn.qkv.bn.weight                                                   ├─512\n",
            "│    └─2.blocks.0.attn.qkv.bn.bias                                                     ├─512\n",
            "│    └─2.blocks.0.attn.proj.ln.linear.weight                                           ├─98,304\n",
            "│    └─2.blocks.0.attn.proj.ln.bn.weight                                               ├─384\n",
            "│    └─2.blocks.0.attn.proj.ln.bn.bias                                                 ├─384\n",
            "│    └─2.blocks.0.mlp.ln1.linear.weight                                                ├─294,912\n",
            "│    └─2.blocks.0.mlp.ln1.bn.weight                                                    ├─768\n",
            "│    └─2.blocks.0.mlp.ln1.bn.bias                                                      ├─768\n",
            "│    └─2.blocks.0.mlp.ln2.linear.weight                                                ├─294,912\n",
            "│    └─2.blocks.0.mlp.ln2.bn.weight                                                    ├─384\n",
            "│    └─2.blocks.0.mlp.ln2.bn.bias                                                      ├─384\n",
            "│    └─2.blocks.1.attn.attention_biases                                                ├─128\n",
            "│    └─2.blocks.1.attn.qkv.linear.weight                                               ├─196,608\n",
            "│    └─2.blocks.1.attn.qkv.bn.weight                                                   ├─512\n",
            "│    └─2.blocks.1.attn.qkv.bn.bias                                                     ├─512\n",
            "│    └─2.blocks.1.attn.proj.ln.linear.weight                                           ├─98,304\n",
            "│    └─2.blocks.1.attn.proj.ln.bn.weight                                               ├─384\n",
            "│    └─2.blocks.1.attn.proj.ln.bn.bias                                                 ├─384\n",
            "│    └─2.blocks.1.mlp.ln1.linear.weight                                                ├─294,912\n",
            "│    └─2.blocks.1.mlp.ln1.bn.weight                                                    ├─768\n",
            "│    └─2.blocks.1.mlp.ln1.bn.bias                                                      ├─768\n",
            "│    └─2.blocks.1.mlp.ln2.linear.weight                                                ├─294,912\n",
            "│    └─2.blocks.1.mlp.ln2.bn.weight                                                    ├─384\n",
            "│    └─2.blocks.1.mlp.ln2.bn.bias                                                      ├─384\n",
            "│    └─2.blocks.2.attn.attention_biases                                                ├─128\n",
            "│    └─2.blocks.2.attn.qkv.linear.weight                                               ├─196,608\n",
            "│    └─2.blocks.2.attn.qkv.bn.weight                                                   ├─512\n",
            "│    └─2.blocks.2.attn.qkv.bn.bias                                                     ├─512\n",
            "│    └─2.blocks.2.attn.proj.ln.linear.weight                                           ├─98,304\n",
            "│    └─2.blocks.2.attn.proj.ln.bn.weight                                               ├─384\n",
            "│    └─2.blocks.2.attn.proj.ln.bn.bias                                                 ├─384\n",
            "│    └─2.blocks.2.mlp.ln1.linear.weight                                                ├─294,912\n",
            "│    └─2.blocks.2.mlp.ln1.bn.weight                                                    ├─768\n",
            "│    └─2.blocks.2.mlp.ln1.bn.bias                                                      ├─768\n",
            "│    └─2.blocks.2.mlp.ln2.linear.weight                                                ├─294,912\n",
            "│    └─2.blocks.2.mlp.ln2.bn.weight                                                    ├─384\n",
            "│    └─2.blocks.2.mlp.ln2.bn.bias                                                      ├─384\n",
            "│    └─2.blocks.3.attn.attention_biases                                                ├─128\n",
            "│    └─2.blocks.3.attn.qkv.linear.weight                                               ├─196,608\n",
            "│    └─2.blocks.3.attn.qkv.bn.weight                                                   ├─512\n",
            "│    └─2.blocks.3.attn.qkv.bn.bias                                                     ├─512\n",
            "│    └─2.blocks.3.attn.proj.ln.linear.weight                                           ├─98,304\n",
            "│    └─2.blocks.3.attn.proj.ln.bn.weight                                               ├─384\n",
            "│    └─2.blocks.3.attn.proj.ln.bn.bias                                                 ├─384\n",
            "│    └─2.blocks.3.mlp.ln1.linear.weight                                                ├─294,912\n",
            "│    └─2.blocks.3.mlp.ln1.bn.weight                                                    ├─768\n",
            "│    └─2.blocks.3.mlp.ln1.bn.bias                                                      ├─768\n",
            "│    └─2.blocks.3.mlp.ln2.linear.weight                                                ├─294,912\n",
            "│    └─2.blocks.3.mlp.ln2.bn.weight                                                    ├─384\n",
            "│    └─2.blocks.3.mlp.ln2.bn.bias                                                      └─384\n",
            "│    └─LevitStage: 2-8                                       [32, 196, 128]            --\n",
            "│    │    └─blocks.0.attn.attention_biases                                             ├─784\n",
            "│    │    └─blocks.0.attn.qkv.linear.weight                                            ├─32,768\n",
            "│    │    └─blocks.0.attn.qkv.bn.weight                                                ├─256\n",
            "│    │    └─blocks.0.attn.qkv.bn.bias                                                  ├─256\n",
            "│    │    └─blocks.0.attn.proj.ln.linear.weight                                        ├─16,384\n",
            "│    │    └─blocks.0.attn.proj.ln.bn.weight                                            ├─128\n",
            "│    │    └─blocks.0.attn.proj.ln.bn.bias                                              ├─128\n",
            "│    │    └─blocks.0.mlp.ln1.linear.weight                                             ├─32,768\n",
            "│    │    └─blocks.0.mlp.ln1.bn.weight                                                 ├─256\n",
            "│    │    └─blocks.0.mlp.ln1.bn.bias                                                   ├─256\n",
            "│    │    └─blocks.0.mlp.ln2.linear.weight                                             ├─32,768\n",
            "│    │    └─blocks.0.mlp.ln2.bn.weight                                                 ├─128\n",
            "│    │    └─blocks.0.mlp.ln2.bn.bias                                                   ├─128\n",
            "│    │    └─blocks.1.attn.attention_biases                                             ├─784\n",
            "│    │    └─blocks.1.attn.qkv.linear.weight                                            ├─32,768\n",
            "│    │    └─blocks.1.attn.qkv.bn.weight                                                ├─256\n",
            "│    │    └─blocks.1.attn.qkv.bn.bias                                                  ├─256\n",
            "│    │    └─blocks.1.attn.proj.ln.linear.weight                                        ├─16,384\n",
            "│    │    └─blocks.1.attn.proj.ln.bn.weight                                            ├─128\n",
            "│    │    └─blocks.1.attn.proj.ln.bn.bias                                              ├─128\n",
            "│    │    └─blocks.1.mlp.ln1.linear.weight                                             ├─32,768\n",
            "│    │    └─blocks.1.mlp.ln1.bn.weight                                                 ├─256\n",
            "│    │    └─blocks.1.mlp.ln1.bn.bias                                                   ├─256\n",
            "│    │    └─blocks.1.mlp.ln2.linear.weight                                             ├─32,768\n",
            "│    │    └─blocks.1.mlp.ln2.bn.weight                                                 ├─128\n",
            "│    │    └─blocks.1.mlp.ln2.bn.bias                                                   └─128\n",
            "│    │    └─Identity: 3-9                                    [32, 196, 128]            --\n",
            "│    │    └─Sequential: 3-10                                 [32, 196, 128]            234,016\n",
            "│    │    │    └─0.attn.attention_biases                                               ├─784\n",
            "│    │    │    └─0.attn.qkv.linear.weight                                              ├─32,768\n",
            "│    │    │    └─0.attn.qkv.bn.weight                                                  ├─256\n",
            "│    │    │    └─0.attn.qkv.bn.bias                                                    ├─256\n",
            "│    │    │    └─0.attn.proj.ln.linear.weight                                          ├─16,384\n",
            "│    │    │    └─0.attn.proj.ln.bn.weight                                              ├─128\n",
            "│    │    │    └─0.attn.proj.ln.bn.bias                                                ├─128\n",
            "│    │    │    └─0.mlp.ln1.linear.weight                                               ├─32,768\n",
            "│    │    │    └─0.mlp.ln1.bn.weight                                                   ├─256\n",
            "│    │    │    └─0.mlp.ln1.bn.bias                                                     ├─256\n",
            "│    │    │    └─0.mlp.ln2.linear.weight                                               ├─32,768\n",
            "│    │    │    └─0.mlp.ln2.bn.weight                                                   ├─128\n",
            "│    │    │    └─0.mlp.ln2.bn.bias                                                     ├─128\n",
            "│    │    │    └─1.attn.attention_biases                                               ├─784\n",
            "│    │    │    └─1.attn.qkv.linear.weight                                              ├─32,768\n",
            "│    │    │    └─1.attn.qkv.bn.weight                                                  ├─256\n",
            "│    │    │    └─1.attn.qkv.bn.bias                                                    ├─256\n",
            "│    │    │    └─1.attn.proj.ln.linear.weight                                          ├─16,384\n",
            "│    │    │    └─1.attn.proj.ln.bn.weight                                              ├─128\n",
            "│    │    │    └─1.attn.proj.ln.bn.bias                                                ├─128\n",
            "│    │    │    └─1.mlp.ln1.linear.weight                                               ├─32,768\n",
            "│    │    │    └─1.mlp.ln1.bn.weight                                                   ├─256\n",
            "│    │    │    └─1.mlp.ln1.bn.bias                                                     ├─256\n",
            "│    │    │    └─1.mlp.ln2.linear.weight                                               ├─32,768\n",
            "│    │    │    └─1.mlp.ln2.bn.weight                                                   ├─128\n",
            "│    │    │    └─1.mlp.ln2.bn.bias                                                     └─128\n",
            "│    └─LevitStage: 2-9                                       [32, 49, 256]             --\n",
            "│    │    └─downsample.attn_downsample.attention_biases                                ├─1,568\n",
            "│    │    └─downsample.attn_downsample.kv.linear.weight                                ├─81,920\n",
            "│    │    └─downsample.attn_downsample.kv.bn.weight                                    ├─640\n",
            "│    │    └─downsample.attn_downsample.kv.bn.bias                                      ├─640\n",
            "│    │    └─downsample.attn_downsample.q.ln.linear.weight                              ├─16,384\n",
            "│    │    └─downsample.attn_downsample.q.ln.bn.weight                                  ├─128\n",
            "│    │    └─downsample.attn_downsample.q.ln.bn.bias                                    ├─128\n",
            "│    │    └─downsample.attn_downsample.proj.ln.linear.weight                           ├─131,072\n",
            "│    │    └─downsample.attn_downsample.proj.ln.bn.weight                               ├─256\n",
            "│    │    └─downsample.attn_downsample.proj.ln.bn.bias                                 ├─256\n",
            "│    │    └─downsample.mlp.ln1.linear.weight                                           ├─131,072\n",
            "│    │    └─downsample.mlp.ln1.bn.weight                                               ├─512\n",
            "│    │    └─downsample.mlp.ln1.bn.bias                                                 ├─512\n",
            "│    │    └─downsample.mlp.ln2.linear.weight                                           ├─131,072\n",
            "│    │    └─downsample.mlp.ln2.bn.weight                                               ├─256\n",
            "│    │    └─downsample.mlp.ln2.bn.bias                                                 ├─256\n",
            "│    │    └─blocks.0.attn.attention_biases                                             ├─294\n",
            "│    │    └─blocks.0.attn.qkv.linear.weight                                            ├─98,304\n",
            "│    │    └─blocks.0.attn.qkv.bn.weight                                                ├─384\n",
            "│    │    └─blocks.0.attn.qkv.bn.bias                                                  ├─384\n",
            "│    │    └─blocks.0.attn.proj.ln.linear.weight                                        ├─49,152\n",
            "│    │    └─blocks.0.attn.proj.ln.bn.weight                                            ├─256\n",
            "│    │    └─blocks.0.attn.proj.ln.bn.bias                                              ├─256\n",
            "│    │    └─blocks.0.mlp.ln1.linear.weight                                             ├─131,072\n",
            "│    │    └─blocks.0.mlp.ln1.bn.weight                                                 ├─512\n",
            "│    │    └─blocks.0.mlp.ln1.bn.bias                                                   ├─512\n",
            "│    │    └─blocks.0.mlp.ln2.linear.weight                                             ├─131,072\n",
            "│    │    └─blocks.0.mlp.ln2.bn.weight                                                 ├─256\n",
            "│    │    └─blocks.0.mlp.ln2.bn.bias                                                   ├─256\n",
            "│    │    └─blocks.1.attn.attention_biases                                             ├─294\n",
            "│    │    └─blocks.1.attn.qkv.linear.weight                                            ├─98,304\n",
            "│    │    └─blocks.1.attn.qkv.bn.weight                                                ├─384\n",
            "│    │    └─blocks.1.attn.qkv.bn.bias                                                  ├─384\n",
            "│    │    └─blocks.1.attn.proj.ln.linear.weight                                        ├─49,152\n",
            "│    │    └─blocks.1.attn.proj.ln.bn.weight                                            ├─256\n",
            "│    │    └─blocks.1.attn.proj.ln.bn.bias                                              ├─256\n",
            "│    │    └─blocks.1.mlp.ln1.linear.weight                                             ├─131,072\n",
            "│    │    └─blocks.1.mlp.ln1.bn.weight                                                 ├─512\n",
            "│    │    └─blocks.1.mlp.ln1.bn.bias                                                   ├─512\n",
            "│    │    └─blocks.1.mlp.ln2.linear.weight                                             ├─131,072\n",
            "│    │    └─blocks.1.mlp.ln2.bn.weight                                                 ├─256\n",
            "│    │    └─blocks.1.mlp.ln2.bn.bias                                                   ├─256\n",
            "│    │    └─blocks.2.attn.attention_biases                                             ├─294\n",
            "│    │    └─blocks.2.attn.qkv.linear.weight                                            ├─98,304\n",
            "│    │    └─blocks.2.attn.qkv.bn.weight                                                ├─384\n",
            "│    │    └─blocks.2.attn.qkv.bn.bias                                                  ├─384\n",
            "│    │    └─blocks.2.attn.proj.ln.linear.weight                                        ├─49,152\n",
            "│    │    └─blocks.2.attn.proj.ln.bn.weight                                            ├─256\n",
            "│    │    └─blocks.2.attn.proj.ln.bn.bias                                              ├─256\n",
            "│    │    └─blocks.2.mlp.ln1.linear.weight                                             ├─131,072\n",
            "│    │    └─blocks.2.mlp.ln1.bn.weight                                                 ├─512\n",
            "│    │    └─blocks.2.mlp.ln1.bn.bias                                                   ├─512\n",
            "│    │    └─blocks.2.mlp.ln2.linear.weight                                             ├─131,072\n",
            "│    │    └─blocks.2.mlp.ln2.bn.weight                                                 ├─256\n",
            "│    │    └─blocks.2.mlp.ln2.bn.bias                                                   └─256\n",
            "│    │    └─LevitDownsample: 3-11                            [32, 49, 256]             496,672\n",
            "│    │    │    └─attn_downsample.attention_biases                                      ├─1,568\n",
            "│    │    │    └─attn_downsample.kv.linear.weight                                      ├─81,920\n",
            "│    │    │    └─attn_downsample.kv.bn.weight                                          ├─640\n",
            "│    │    │    └─attn_downsample.kv.bn.bias                                            ├─640\n",
            "│    │    │    └─attn_downsample.q.ln.linear.weight                                    ├─16,384\n",
            "│    │    │    └─attn_downsample.q.ln.bn.weight                                        ├─128\n",
            "│    │    │    └─attn_downsample.q.ln.bn.bias                                          ├─128\n",
            "│    │    │    └─attn_downsample.proj.ln.linear.weight                                 ├─131,072\n",
            "│    │    │    └─attn_downsample.proj.ln.bn.weight                                     ├─256\n",
            "│    │    │    └─attn_downsample.proj.ln.bn.bias                                       ├─256\n",
            "│    │    │    └─mlp.ln1.linear.weight                                                 ├─131,072\n",
            "│    │    │    └─mlp.ln1.bn.weight                                                     ├─512\n",
            "│    │    │    └─mlp.ln1.bn.bias                                                       ├─512\n",
            "│    │    │    └─mlp.ln2.linear.weight                                                 ├─131,072\n",
            "│    │    │    └─mlp.ln2.bn.weight                                                     ├─256\n",
            "│    │    │    └─mlp.ln2.bn.bias                                                       └─256\n",
            "│    │    └─Sequential: 3-12                                 [32, 49, 256]             1,238,130\n",
            "│    │    │    └─0.attn.attention_biases                                               ├─294\n",
            "│    │    │    └─0.attn.qkv.linear.weight                                              ├─98,304\n",
            "│    │    │    └─0.attn.qkv.bn.weight                                                  ├─384\n",
            "│    │    │    └─0.attn.qkv.bn.bias                                                    ├─384\n",
            "│    │    │    └─0.attn.proj.ln.linear.weight                                          ├─49,152\n",
            "│    │    │    └─0.attn.proj.ln.bn.weight                                              ├─256\n",
            "│    │    │    └─0.attn.proj.ln.bn.bias                                                ├─256\n",
            "│    │    │    └─0.mlp.ln1.linear.weight                                               ├─131,072\n",
            "│    │    │    └─0.mlp.ln1.bn.weight                                                   ├─512\n",
            "│    │    │    └─0.mlp.ln1.bn.bias                                                     ├─512\n",
            "│    │    │    └─0.mlp.ln2.linear.weight                                               ├─131,072\n",
            "│    │    │    └─0.mlp.ln2.bn.weight                                                   ├─256\n",
            "│    │    │    └─0.mlp.ln2.bn.bias                                                     ├─256\n",
            "│    │    │    └─1.attn.attention_biases                                               ├─294\n",
            "│    │    │    └─1.attn.qkv.linear.weight                                              ├─98,304\n",
            "│    │    │    └─1.attn.qkv.bn.weight                                                  ├─384\n",
            "│    │    │    └─1.attn.qkv.bn.bias                                                    ├─384\n",
            "│    │    │    └─1.attn.proj.ln.linear.weight                                          ├─49,152\n",
            "│    │    │    └─1.attn.proj.ln.bn.weight                                              ├─256\n",
            "│    │    │    └─1.attn.proj.ln.bn.bias                                                ├─256\n",
            "│    │    │    └─1.mlp.ln1.linear.weight                                               ├─131,072\n",
            "│    │    │    └─1.mlp.ln1.bn.weight                                                   ├─512\n",
            "│    │    │    └─1.mlp.ln1.bn.bias                                                     ├─512\n",
            "│    │    │    └─1.mlp.ln2.linear.weight                                               ├─131,072\n",
            "│    │    │    └─1.mlp.ln2.bn.weight                                                   ├─256\n",
            "│    │    │    └─1.mlp.ln2.bn.bias                                                     ├─256\n",
            "│    │    │    └─2.attn.attention_biases                                               ├─294\n",
            "│    │    │    └─2.attn.qkv.linear.weight                                              ├─98,304\n",
            "│    │    │    └─2.attn.qkv.bn.weight                                                  ├─384\n",
            "│    │    │    └─2.attn.qkv.bn.bias                                                    ├─384\n",
            "│    │    │    └─2.attn.proj.ln.linear.weight                                          ├─49,152\n",
            "│    │    │    └─2.attn.proj.ln.bn.weight                                              ├─256\n",
            "│    │    │    └─2.attn.proj.ln.bn.bias                                                ├─256\n",
            "│    │    │    └─2.mlp.ln1.linear.weight                                               ├─131,072\n",
            "│    │    │    └─2.mlp.ln1.bn.weight                                                   ├─512\n",
            "│    │    │    └─2.mlp.ln1.bn.bias                                                     ├─512\n",
            "│    │    │    └─2.mlp.ln2.linear.weight                                               ├─131,072\n",
            "│    │    │    └─2.mlp.ln2.bn.weight                                                   ├─256\n",
            "│    │    │    └─2.mlp.ln2.bn.bias                                                     └─256\n",
            "│    └─LevitStage: 2-10                                      [32, 16, 384]             --\n",
            "│    │    └─downsample.attn_downsample.attention_biases                                ├─784\n",
            "│    │    └─downsample.attn_downsample.kv.linear.weight                                ├─327,680\n",
            "│    │    └─downsample.attn_downsample.kv.bn.weight                                    ├─1,280\n",
            "│    │    └─downsample.attn_downsample.kv.bn.bias                                      ├─1,280\n",
            "│    │    └─downsample.attn_downsample.q.ln.linear.weight                              ├─65,536\n",
            "│    │    └─downsample.attn_downsample.q.ln.bn.weight                                  ├─256\n",
            "│    │    └─downsample.attn_downsample.q.ln.bn.bias                                    ├─256\n",
            "│    │    └─downsample.attn_downsample.proj.ln.linear.weight                           ├─393,216\n",
            "│    │    └─downsample.attn_downsample.proj.ln.bn.weight                               ├─384\n",
            "│    │    └─downsample.attn_downsample.proj.ln.bn.bias                                 ├─384\n",
            "│    │    └─downsample.mlp.ln1.linear.weight                                           ├─294,912\n",
            "│    │    └─downsample.mlp.ln1.bn.weight                                               ├─768\n",
            "│    │    └─downsample.mlp.ln1.bn.bias                                                 ├─768\n",
            "│    │    └─downsample.mlp.ln2.linear.weight                                           ├─294,912\n",
            "│    │    └─downsample.mlp.ln2.bn.weight                                               ├─384\n",
            "│    │    └─downsample.mlp.ln2.bn.bias                                                 ├─384\n",
            "│    │    └─blocks.0.attn.attention_biases                                             ├─128\n",
            "│    │    └─blocks.0.attn.qkv.linear.weight                                            ├─196,608\n",
            "│    │    └─blocks.0.attn.qkv.bn.weight                                                ├─512\n",
            "│    │    └─blocks.0.attn.qkv.bn.bias                                                  ├─512\n",
            "│    │    └─blocks.0.attn.proj.ln.linear.weight                                        ├─98,304\n",
            "│    │    └─blocks.0.attn.proj.ln.bn.weight                                            ├─384\n",
            "│    │    └─blocks.0.attn.proj.ln.bn.bias                                              ├─384\n",
            "│    │    └─blocks.0.mlp.ln1.linear.weight                                             ├─294,912\n",
            "│    │    └─blocks.0.mlp.ln1.bn.weight                                                 ├─768\n",
            "│    │    └─blocks.0.mlp.ln1.bn.bias                                                   ├─768\n",
            "│    │    └─blocks.0.mlp.ln2.linear.weight                                             ├─294,912\n",
            "│    │    └─blocks.0.mlp.ln2.bn.weight                                                 ├─384\n",
            "│    │    └─blocks.0.mlp.ln2.bn.bias                                                   ├─384\n",
            "│    │    └─blocks.1.attn.attention_biases                                             ├─128\n",
            "│    │    └─blocks.1.attn.qkv.linear.weight                                            ├─196,608\n",
            "│    │    └─blocks.1.attn.qkv.bn.weight                                                ├─512\n",
            "│    │    └─blocks.1.attn.qkv.bn.bias                                                  ├─512\n",
            "│    │    └─blocks.1.attn.proj.ln.linear.weight                                        ├─98,304\n",
            "│    │    └─blocks.1.attn.proj.ln.bn.weight                                            ├─384\n",
            "│    │    └─blocks.1.attn.proj.ln.bn.bias                                              ├─384\n",
            "│    │    └─blocks.1.mlp.ln1.linear.weight                                             ├─294,912\n",
            "│    │    └─blocks.1.mlp.ln1.bn.weight                                                 ├─768\n",
            "│    │    └─blocks.1.mlp.ln1.bn.bias                                                   ├─768\n",
            "│    │    └─blocks.1.mlp.ln2.linear.weight                                             ├─294,912\n",
            "│    │    └─blocks.1.mlp.ln2.bn.weight                                                 ├─384\n",
            "│    │    └─blocks.1.mlp.ln2.bn.bias                                                   ├─384\n",
            "│    │    └─blocks.2.attn.attention_biases                                             ├─128\n",
            "│    │    └─blocks.2.attn.qkv.linear.weight                                            ├─196,608\n",
            "│    │    └─blocks.2.attn.qkv.bn.weight                                                ├─512\n",
            "│    │    └─blocks.2.attn.qkv.bn.bias                                                  ├─512\n",
            "│    │    └─blocks.2.attn.proj.ln.linear.weight                                        ├─98,304\n",
            "│    │    └─blocks.2.attn.proj.ln.bn.weight                                            ├─384\n",
            "│    │    └─blocks.2.attn.proj.ln.bn.bias                                              ├─384\n",
            "│    │    └─blocks.2.mlp.ln1.linear.weight                                             ├─294,912\n",
            "│    │    └─blocks.2.mlp.ln1.bn.weight                                                 ├─768\n",
            "│    │    └─blocks.2.mlp.ln1.bn.bias                                                   ├─768\n",
            "│    │    └─blocks.2.mlp.ln2.linear.weight                                             ├─294,912\n",
            "│    │    └─blocks.2.mlp.ln2.bn.weight                                                 ├─384\n",
            "│    │    └─blocks.2.mlp.ln2.bn.bias                                                   ├─384\n",
            "│    │    └─blocks.3.attn.attention_biases                                             ├─128\n",
            "│    │    └─blocks.3.attn.qkv.linear.weight                                            ├─196,608\n",
            "│    │    └─blocks.3.attn.qkv.bn.weight                                                ├─512\n",
            "│    │    └─blocks.3.attn.qkv.bn.bias                                                  ├─512\n",
            "│    │    └─blocks.3.attn.proj.ln.linear.weight                                        ├─98,304\n",
            "│    │    └─blocks.3.attn.proj.ln.bn.weight                                            ├─384\n",
            "│    │    └─blocks.3.attn.proj.ln.bn.bias                                              ├─384\n",
            "│    │    └─blocks.3.mlp.ln1.linear.weight                                             ├─294,912\n",
            "│    │    └─blocks.3.mlp.ln1.bn.weight                                                 ├─768\n",
            "│    │    └─blocks.3.mlp.ln1.bn.bias                                                   ├─768\n",
            "│    │    └─blocks.3.mlp.ln2.linear.weight                                             ├─294,912\n",
            "│    │    └─blocks.3.mlp.ln2.bn.weight                                                 ├─384\n",
            "│    │    └─blocks.3.mlp.ln2.bn.bias                                                   └─384\n",
            "│    │    └─LevitDownsample: 3-13                            [32, 16, 384]             1,383,184\n",
            "│    │    │    └─attn_downsample.attention_biases                                      ├─784\n",
            "│    │    │    └─attn_downsample.kv.linear.weight                                      ├─327,680\n",
            "│    │    │    └─attn_downsample.kv.bn.weight                                          ├─1,280\n",
            "│    │    │    └─attn_downsample.kv.bn.bias                                            ├─1,280\n",
            "│    │    │    └─attn_downsample.q.ln.linear.weight                                    ├─65,536\n",
            "│    │    │    └─attn_downsample.q.ln.bn.weight                                        ├─256\n",
            "│    │    │    └─attn_downsample.q.ln.bn.bias                                          ├─256\n",
            "│    │    │    └─attn_downsample.proj.ln.linear.weight                                 ├─393,216\n",
            "│    │    │    └─attn_downsample.proj.ln.bn.weight                                     ├─384\n",
            "│    │    │    └─attn_downsample.proj.ln.bn.bias                                       ├─384\n",
            "│    │    │    └─mlp.ln1.linear.weight                                                 ├─294,912\n",
            "│    │    │    └─mlp.ln1.bn.weight                                                     ├─768\n",
            "│    │    │    └─mlp.ln1.bn.bias                                                       ├─768\n",
            "│    │    │    └─mlp.ln2.linear.weight                                                 ├─294,912\n",
            "│    │    │    └─mlp.ln2.bn.weight                                                     ├─384\n",
            "│    │    │    └─mlp.ln2.bn.bias                                                       └─384\n",
            "│    │    └─Sequential: 3-14                                 [32, 16, 384]             3,555,840\n",
            "│    │    │    └─0.attn.attention_biases                                               ├─128\n",
            "│    │    │    └─0.attn.qkv.linear.weight                                              ├─196,608\n",
            "│    │    │    └─0.attn.qkv.bn.weight                                                  ├─512\n",
            "│    │    │    └─0.attn.qkv.bn.bias                                                    ├─512\n",
            "│    │    │    └─0.attn.proj.ln.linear.weight                                          ├─98,304\n",
            "│    │    │    └─0.attn.proj.ln.bn.weight                                              ├─384\n",
            "│    │    │    └─0.attn.proj.ln.bn.bias                                                ├─384\n",
            "│    │    │    └─0.mlp.ln1.linear.weight                                               ├─294,912\n",
            "│    │    │    └─0.mlp.ln1.bn.weight                                                   ├─768\n",
            "│    │    │    └─0.mlp.ln1.bn.bias                                                     ├─768\n",
            "│    │    │    └─0.mlp.ln2.linear.weight                                               ├─294,912\n",
            "│    │    │    └─0.mlp.ln2.bn.weight                                                   ├─384\n",
            "│    │    │    └─0.mlp.ln2.bn.bias                                                     ├─384\n",
            "│    │    │    └─1.attn.attention_biases                                               ├─128\n",
            "│    │    │    └─1.attn.qkv.linear.weight                                              ├─196,608\n",
            "│    │    │    └─1.attn.qkv.bn.weight                                                  ├─512\n",
            "│    │    │    └─1.attn.qkv.bn.bias                                                    ├─512\n",
            "│    │    │    └─1.attn.proj.ln.linear.weight                                          ├─98,304\n",
            "│    │    │    └─1.attn.proj.ln.bn.weight                                              ├─384\n",
            "│    │    │    └─1.attn.proj.ln.bn.bias                                                ├─384\n",
            "│    │    │    └─1.mlp.ln1.linear.weight                                               ├─294,912\n",
            "│    │    │    └─1.mlp.ln1.bn.weight                                                   ├─768\n",
            "│    │    │    └─1.mlp.ln1.bn.bias                                                     ├─768\n",
            "│    │    │    └─1.mlp.ln2.linear.weight                                               ├─294,912\n",
            "│    │    │    └─1.mlp.ln2.bn.weight                                                   ├─384\n",
            "│    │    │    └─1.mlp.ln2.bn.bias                                                     ├─384\n",
            "│    │    │    └─2.attn.attention_biases                                               ├─128\n",
            "│    │    │    └─2.attn.qkv.linear.weight                                              ├─196,608\n",
            "│    │    │    └─2.attn.qkv.bn.weight                                                  ├─512\n",
            "│    │    │    └─2.attn.qkv.bn.bias                                                    ├─512\n",
            "│    │    │    └─2.attn.proj.ln.linear.weight                                          ├─98,304\n",
            "│    │    │    └─2.attn.proj.ln.bn.weight                                              ├─384\n",
            "│    │    │    └─2.attn.proj.ln.bn.bias                                                ├─384\n",
            "│    │    │    └─2.mlp.ln1.linear.weight                                               ├─294,912\n",
            "│    │    │    └─2.mlp.ln1.bn.weight                                                   ├─768\n",
            "│    │    │    └─2.mlp.ln1.bn.bias                                                     ├─768\n",
            "│    │    │    └─2.mlp.ln2.linear.weight                                               ├─294,912\n",
            "│    │    │    └─2.mlp.ln2.bn.weight                                                   ├─384\n",
            "│    │    │    └─2.mlp.ln2.bn.bias                                                     ├─384\n",
            "│    │    │    └─3.attn.attention_biases                                               ├─128\n",
            "│    │    │    └─3.attn.qkv.linear.weight                                              ├─196,608\n",
            "│    │    │    └─3.attn.qkv.bn.weight                                                  ├─512\n",
            "│    │    │    └─3.attn.qkv.bn.bias                                                    ├─512\n",
            "│    │    │    └─3.attn.proj.ln.linear.weight                                          ├─98,304\n",
            "│    │    │    └─3.attn.proj.ln.bn.weight                                              ├─384\n",
            "│    │    │    └─3.attn.proj.ln.bn.bias                                                ├─384\n",
            "│    │    │    └─3.mlp.ln1.linear.weight                                               ├─294,912\n",
            "│    │    │    └─3.mlp.ln1.bn.weight                                                   ├─768\n",
            "│    │    │    └─3.mlp.ln1.bn.bias                                                     ├─768\n",
            "│    │    │    └─3.mlp.ln2.linear.weight                                               ├─294,912\n",
            "│    │    │    └─3.mlp.ln2.bn.weight                                                   ├─384\n",
            "│    │    │    └─3.mlp.ln2.bn.bias                                                     └─384\n",
            "├─NormLinear: 1-3                                            [32, 9]                   --\n",
            "│    └─bn.weight                                                                       ├─384\n",
            "│    └─bn.bias                                                                         ├─384\n",
            "│    └─linear.weight                                                                   ├─3,456\n",
            "│    └─linear.bias                                                                     └─9\n",
            "│    └─BatchNorm1d: 2-11                                     [32, 384]                 768\n",
            "│    │    └─weight                                                                     ├─384\n",
            "│    │    └─bias                                                                       └─384\n",
            "│    └─Dropout: 2-12                                         [32, 384]                 --\n",
            "│    └─Linear: 2-13                                          [32, 9]                   3,465\n",
            "│    │    └─weight                                                                     ├─3,456\n",
            "│    │    └─bias                                                                       └─9\n",
            "├─NormLinear: 1-4                                            [32, 9]                   --\n",
            "│    └─bn.weight                                                                       ├─384\n",
            "│    └─bn.bias                                                                         ├─384\n",
            "│    └─linear.weight                                                                   ├─3,456\n",
            "│    └─linear.bias                                                                     └─9\n",
            "│    └─BatchNorm1d: 2-14                                     [32, 384]                 768\n",
            "│    │    └─weight                                                                     ├─384\n",
            "│    │    └─bias                                                                       └─384\n",
            "│    └─Dropout: 2-15                                         [32, 384]                 --\n",
            "│    └─Linear: 2-16                                          [32, 9]                   3,465\n",
            "│    │    └─weight                                                                     ├─3,456\n",
            "│    │    └─bias                                                                       └─9\n",
            "==============================================================================================================\n",
            "Total params: 7,013,988\n",
            "Trainable params: 7,013,988\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (Units.GIGABYTES): 1.84\n",
            "==============================================================================================================\n",
            "Input size (MB): 19.27\n",
            "Forward/backward pass size (MB): 660.02\n",
            "Params size (MB): 28.03\n",
            "Estimated Total Size (MB): 707.32\n",
            "==============================================================================================================\n",
            "==============================================================================================================\n",
            "Layer (type:depth-idx)                                       Output Shape              Param #\n",
            "==============================================================================================================\n",
            "LevitDistilled                                               [32, 9]                   --\n",
            "├─Stem16: 1-1                                                [32, 128, 14, 14]         --\n",
            "│    └─conv1.linear.weight                                                             ├─432\n",
            "│    └─conv1.bn.weight                                                                 ├─16\n",
            "│    └─conv1.bn.bias                                                                   ├─16\n",
            "│    └─conv2.linear.weight                                                             ├─4,608\n",
            "│    └─conv2.bn.weight                                                                 ├─32\n",
            "│    └─conv2.bn.bias                                                                   ├─32\n",
            "│    └─conv3.linear.weight                                                             ├─18,432\n",
            "│    └─conv3.bn.weight                                                                 ├─64\n",
            "│    └─conv3.bn.bias                                                                   ├─64\n",
            "│    └─conv4.linear.weight                                                             ├─73,728\n",
            "│    └─conv4.bn.weight                                                                 ├─128\n",
            "│    └─conv4.bn.bias                                                                   └─128\n",
            "│    └─ConvNorm: 2-1                                         [32, 16, 112, 112]        --\n",
            "│    │    └─linear.weight                                                              ├─432\n",
            "│    │    └─bn.weight                                                                  ├─16\n",
            "│    │    └─bn.bias                                                                    └─16\n",
            "│    │    └─Conv2d: 3-1                                      [32, 16, 112, 112]        432\n",
            "│    │    │    └─weight                                                                └─432\n",
            "│    │    └─BatchNorm2d: 3-2                                 [32, 16, 112, 112]        32\n",
            "│    │    │    └─weight                                                                ├─16\n",
            "│    │    │    └─bias                                                                  └─16\n",
            "│    └─Hardswish: 2-2                                        [32, 16, 112, 112]        --\n",
            "│    └─ConvNorm: 2-3                                         [32, 32, 56, 56]          --\n",
            "│    │    └─linear.weight                                                              ├─4,608\n",
            "│    │    └─bn.weight                                                                  ├─32\n",
            "│    │    └─bn.bias                                                                    └─32\n",
            "│    │    └─Conv2d: 3-3                                      [32, 32, 56, 56]          4,608\n",
            "│    │    │    └─weight                                                                └─4,608\n",
            "│    │    └─BatchNorm2d: 3-4                                 [32, 32, 56, 56]          64\n",
            "│    │    │    └─weight                                                                ├─32\n",
            "│    │    │    └─bias                                                                  └─32\n",
            "│    └─Hardswish: 2-4                                        [32, 32, 56, 56]          --\n",
            "│    └─ConvNorm: 2-5                                         [32, 64, 28, 28]          --\n",
            "│    │    └─linear.weight                                                              ├─18,432\n",
            "│    │    └─bn.weight                                                                  ├─64\n",
            "│    │    └─bn.bias                                                                    └─64\n",
            "│    │    └─Conv2d: 3-5                                      [32, 64, 28, 28]          18,432\n",
            "│    │    │    └─weight                                                                └─18,432\n",
            "│    │    └─BatchNorm2d: 3-6                                 [32, 64, 28, 28]          128\n",
            "│    │    │    └─weight                                                                ├─64\n",
            "│    │    │    └─bias                                                                  └─64\n",
            "│    └─Hardswish: 2-6                                        [32, 64, 28, 28]          --\n",
            "│    └─ConvNorm: 2-7                                         [32, 128, 14, 14]         --\n",
            "│    │    └─linear.weight                                                              ├─73,728\n",
            "│    │    └─bn.weight                                                                  ├─128\n",
            "│    │    └─bn.bias                                                                    └─128\n",
            "│    │    └─Conv2d: 3-7                                      [32, 128, 14, 14]         73,728\n",
            "│    │    │    └─weight                                                                └─73,728\n",
            "│    │    └─BatchNorm2d: 3-8                                 [32, 128, 14, 14]         256\n",
            "│    │    │    └─weight                                                                ├─128\n",
            "│    │    │    └─bias                                                                  └─128\n",
            "├─Sequential: 1-2                                            [32, 16, 384]             --\n",
            "│    └─0.blocks.0.attn.attention_biases                                                ├─784\n",
            "│    └─0.blocks.0.attn.qkv.linear.weight                                               ├─32,768\n",
            "│    └─0.blocks.0.attn.qkv.bn.weight                                                   ├─256\n",
            "│    └─0.blocks.0.attn.qkv.bn.bias                                                     ├─256\n",
            "│    └─0.blocks.0.attn.proj.ln.linear.weight                                           ├─16,384\n",
            "│    └─0.blocks.0.attn.proj.ln.bn.weight                                               ├─128\n",
            "│    └─0.blocks.0.attn.proj.ln.bn.bias                                                 ├─128\n",
            "│    └─0.blocks.0.mlp.ln1.linear.weight                                                ├─32,768\n",
            "│    └─0.blocks.0.mlp.ln1.bn.weight                                                    ├─256\n",
            "│    └─0.blocks.0.mlp.ln1.bn.bias                                                      ├─256\n",
            "│    └─0.blocks.0.mlp.ln2.linear.weight                                                ├─32,768\n",
            "│    └─0.blocks.0.mlp.ln2.bn.weight                                                    ├─128\n",
            "│    └─0.blocks.0.mlp.ln2.bn.bias                                                      ├─128\n",
            "│    └─0.blocks.1.attn.attention_biases                                                ├─784\n",
            "│    └─0.blocks.1.attn.qkv.linear.weight                                               ├─32,768\n",
            "│    └─0.blocks.1.attn.qkv.bn.weight                                                   ├─256\n",
            "│    └─0.blocks.1.attn.qkv.bn.bias                                                     ├─256\n",
            "│    └─0.blocks.1.attn.proj.ln.linear.weight                                           ├─16,384\n",
            "│    └─0.blocks.1.attn.proj.ln.bn.weight                                               ├─128\n",
            "│    └─0.blocks.1.attn.proj.ln.bn.bias                                                 ├─128\n",
            "│    └─0.blocks.1.mlp.ln1.linear.weight                                                ├─32,768\n",
            "│    └─0.blocks.1.mlp.ln1.bn.weight                                                    ├─256\n",
            "│    └─0.blocks.1.mlp.ln1.bn.bias                                                      ├─256\n",
            "│    └─0.blocks.1.mlp.ln2.linear.weight                                                ├─32,768\n",
            "│    └─0.blocks.1.mlp.ln2.bn.weight                                                    ├─128\n",
            "│    └─0.blocks.1.mlp.ln2.bn.bias                                                      ├─128\n",
            "│    └─1.downsample.attn_downsample.attention_biases                                   ├─1,568\n",
            "│    └─1.downsample.attn_downsample.kv.linear.weight                                   ├─81,920\n",
            "│    └─1.downsample.attn_downsample.kv.bn.weight                                       ├─640\n",
            "│    └─1.downsample.attn_downsample.kv.bn.bias                                         ├─640\n",
            "│    └─1.downsample.attn_downsample.q.ln.linear.weight                                 ├─16,384\n",
            "│    └─1.downsample.attn_downsample.q.ln.bn.weight                                     ├─128\n",
            "│    └─1.downsample.attn_downsample.q.ln.bn.bias                                       ├─128\n",
            "│    └─1.downsample.attn_downsample.proj.ln.linear.weight                              ├─131,072\n",
            "│    └─1.downsample.attn_downsample.proj.ln.bn.weight                                  ├─256\n",
            "│    └─1.downsample.attn_downsample.proj.ln.bn.bias                                    ├─256\n",
            "│    └─1.downsample.mlp.ln1.linear.weight                                              ├─131,072\n",
            "│    └─1.downsample.mlp.ln1.bn.weight                                                  ├─512\n",
            "│    └─1.downsample.mlp.ln1.bn.bias                                                    ├─512\n",
            "│    └─1.downsample.mlp.ln2.linear.weight                                              ├─131,072\n",
            "│    └─1.downsample.mlp.ln2.bn.weight                                                  ├─256\n",
            "│    └─1.downsample.mlp.ln2.bn.bias                                                    ├─256\n",
            "│    └─1.blocks.0.attn.attention_biases                                                ├─294\n",
            "│    └─1.blocks.0.attn.qkv.linear.weight                                               ├─98,304\n",
            "│    └─1.blocks.0.attn.qkv.bn.weight                                                   ├─384\n",
            "│    └─1.blocks.0.attn.qkv.bn.bias                                                     ├─384\n",
            "│    └─1.blocks.0.attn.proj.ln.linear.weight                                           ├─49,152\n",
            "│    └─1.blocks.0.attn.proj.ln.bn.weight                                               ├─256\n",
            "│    └─1.blocks.0.attn.proj.ln.bn.bias                                                 ├─256\n",
            "│    └─1.blocks.0.mlp.ln1.linear.weight                                                ├─131,072\n",
            "│    └─1.blocks.0.mlp.ln1.bn.weight                                                    ├─512\n",
            "│    └─1.blocks.0.mlp.ln1.bn.bias                                                      ├─512\n",
            "│    └─1.blocks.0.mlp.ln2.linear.weight                                                ├─131,072\n",
            "│    └─1.blocks.0.mlp.ln2.bn.weight                                                    ├─256\n",
            "│    └─1.blocks.0.mlp.ln2.bn.bias                                                      ├─256\n",
            "│    └─1.blocks.1.attn.attention_biases                                                ├─294\n",
            "│    └─1.blocks.1.attn.qkv.linear.weight                                               ├─98,304\n",
            "│    └─1.blocks.1.attn.qkv.bn.weight                                                   ├─384\n",
            "│    └─1.blocks.1.attn.qkv.bn.bias                                                     ├─384\n",
            "│    └─1.blocks.1.attn.proj.ln.linear.weight                                           ├─49,152\n",
            "│    └─1.blocks.1.attn.proj.ln.bn.weight                                               ├─256\n",
            "│    └─1.blocks.1.attn.proj.ln.bn.bias                                                 ├─256\n",
            "│    └─1.blocks.1.mlp.ln1.linear.weight                                                ├─131,072\n",
            "│    └─1.blocks.1.mlp.ln1.bn.weight                                                    ├─512\n",
            "│    └─1.blocks.1.mlp.ln1.bn.bias                                                      ├─512\n",
            "│    └─1.blocks.1.mlp.ln2.linear.weight                                                ├─131,072\n",
            "│    └─1.blocks.1.mlp.ln2.bn.weight                                                    ├─256\n",
            "│    └─1.blocks.1.mlp.ln2.bn.bias                                                      ├─256\n",
            "│    └─1.blocks.2.attn.attention_biases                                                ├─294\n",
            "│    └─1.blocks.2.attn.qkv.linear.weight                                               ├─98,304\n",
            "│    └─1.blocks.2.attn.qkv.bn.weight                                                   ├─384\n",
            "│    └─1.blocks.2.attn.qkv.bn.bias                                                     ├─384\n",
            "│    └─1.blocks.2.attn.proj.ln.linear.weight                                           ├─49,152\n",
            "│    └─1.blocks.2.attn.proj.ln.bn.weight                                               ├─256\n",
            "│    └─1.blocks.2.attn.proj.ln.bn.bias                                                 ├─256\n",
            "│    └─1.blocks.2.mlp.ln1.linear.weight                                                ├─131,072\n",
            "│    └─1.blocks.2.mlp.ln1.bn.weight                                                    ├─512\n",
            "│    └─1.blocks.2.mlp.ln1.bn.bias                                                      ├─512\n",
            "│    └─1.blocks.2.mlp.ln2.linear.weight                                                ├─131,072\n",
            "│    └─1.blocks.2.mlp.ln2.bn.weight                                                    ├─256\n",
            "│    └─1.blocks.2.mlp.ln2.bn.bias                                                      ├─256\n",
            "│    └─2.downsample.attn_downsample.attention_biases                                   ├─784\n",
            "│    └─2.downsample.attn_downsample.kv.linear.weight                                   ├─327,680\n",
            "│    └─2.downsample.attn_downsample.kv.bn.weight                                       ├─1,280\n",
            "│    └─2.downsample.attn_downsample.kv.bn.bias                                         ├─1,280\n",
            "│    └─2.downsample.attn_downsample.q.ln.linear.weight                                 ├─65,536\n",
            "│    └─2.downsample.attn_downsample.q.ln.bn.weight                                     ├─256\n",
            "│    └─2.downsample.attn_downsample.q.ln.bn.bias                                       ├─256\n",
            "│    └─2.downsample.attn_downsample.proj.ln.linear.weight                              ├─393,216\n",
            "│    └─2.downsample.attn_downsample.proj.ln.bn.weight                                  ├─384\n",
            "│    └─2.downsample.attn_downsample.proj.ln.bn.bias                                    ├─384\n",
            "│    └─2.downsample.mlp.ln1.linear.weight                                              ├─294,912\n",
            "│    └─2.downsample.mlp.ln1.bn.weight                                                  ├─768\n",
            "│    └─2.downsample.mlp.ln1.bn.bias                                                    ├─768\n",
            "│    └─2.downsample.mlp.ln2.linear.weight                                              ├─294,912\n",
            "│    └─2.downsample.mlp.ln2.bn.weight                                                  ├─384\n",
            "│    └─2.downsample.mlp.ln2.bn.bias                                                    ├─384\n",
            "│    └─2.blocks.0.attn.attention_biases                                                ├─128\n",
            "│    └─2.blocks.0.attn.qkv.linear.weight                                               ├─196,608\n",
            "│    └─2.blocks.0.attn.qkv.bn.weight                                                   ├─512\n",
            "│    └─2.blocks.0.attn.qkv.bn.bias                                                     ├─512\n",
            "│    └─2.blocks.0.attn.proj.ln.linear.weight                                           ├─98,304\n",
            "│    └─2.blocks.0.attn.proj.ln.bn.weight                                               ├─384\n",
            "│    └─2.blocks.0.attn.proj.ln.bn.bias                                                 ├─384\n",
            "│    └─2.blocks.0.mlp.ln1.linear.weight                                                ├─294,912\n",
            "│    └─2.blocks.0.mlp.ln1.bn.weight                                                    ├─768\n",
            "│    └─2.blocks.0.mlp.ln1.bn.bias                                                      ├─768\n",
            "│    └─2.blocks.0.mlp.ln2.linear.weight                                                ├─294,912\n",
            "│    └─2.blocks.0.mlp.ln2.bn.weight                                                    ├─384\n",
            "│    └─2.blocks.0.mlp.ln2.bn.bias                                                      ├─384\n",
            "│    └─2.blocks.1.attn.attention_biases                                                ├─128\n",
            "│    └─2.blocks.1.attn.qkv.linear.weight                                               ├─196,608\n",
            "│    └─2.blocks.1.attn.qkv.bn.weight                                                   ├─512\n",
            "│    └─2.blocks.1.attn.qkv.bn.bias                                                     ├─512\n",
            "│    └─2.blocks.1.attn.proj.ln.linear.weight                                           ├─98,304\n",
            "│    └─2.blocks.1.attn.proj.ln.bn.weight                                               ├─384\n",
            "│    └─2.blocks.1.attn.proj.ln.bn.bias                                                 ├─384\n",
            "│    └─2.blocks.1.mlp.ln1.linear.weight                                                ├─294,912\n",
            "│    └─2.blocks.1.mlp.ln1.bn.weight                                                    ├─768\n",
            "│    └─2.blocks.1.mlp.ln1.bn.bias                                                      ├─768\n",
            "│    └─2.blocks.1.mlp.ln2.linear.weight                                                ├─294,912\n",
            "│    └─2.blocks.1.mlp.ln2.bn.weight                                                    ├─384\n",
            "│    └─2.blocks.1.mlp.ln2.bn.bias                                                      ├─384\n",
            "│    └─2.blocks.2.attn.attention_biases                                                ├─128\n",
            "│    └─2.blocks.2.attn.qkv.linear.weight                                               ├─196,608\n",
            "│    └─2.blocks.2.attn.qkv.bn.weight                                                   ├─512\n",
            "│    └─2.blocks.2.attn.qkv.bn.bias                                                     ├─512\n",
            "│    └─2.blocks.2.attn.proj.ln.linear.weight                                           ├─98,304\n",
            "│    └─2.blocks.2.attn.proj.ln.bn.weight                                               ├─384\n",
            "│    └─2.blocks.2.attn.proj.ln.bn.bias                                                 ├─384\n",
            "│    └─2.blocks.2.mlp.ln1.linear.weight                                                ├─294,912\n",
            "│    └─2.blocks.2.mlp.ln1.bn.weight                                                    ├─768\n",
            "│    └─2.blocks.2.mlp.ln1.bn.bias                                                      ├─768\n",
            "│    └─2.blocks.2.mlp.ln2.linear.weight                                                ├─294,912\n",
            "│    └─2.blocks.2.mlp.ln2.bn.weight                                                    ├─384\n",
            "│    └─2.blocks.2.mlp.ln2.bn.bias                                                      ├─384\n",
            "│    └─2.blocks.3.attn.attention_biases                                                ├─128\n",
            "│    └─2.blocks.3.attn.qkv.linear.weight                                               ├─196,608\n",
            "│    └─2.blocks.3.attn.qkv.bn.weight                                                   ├─512\n",
            "│    └─2.blocks.3.attn.qkv.bn.bias                                                     ├─512\n",
            "│    └─2.blocks.3.attn.proj.ln.linear.weight                                           ├─98,304\n",
            "│    └─2.blocks.3.attn.proj.ln.bn.weight                                               ├─384\n",
            "│    └─2.blocks.3.attn.proj.ln.bn.bias                                                 ├─384\n",
            "│    └─2.blocks.3.mlp.ln1.linear.weight                                                ├─294,912\n",
            "│    └─2.blocks.3.mlp.ln1.bn.weight                                                    ├─768\n",
            "│    └─2.blocks.3.mlp.ln1.bn.bias                                                      ├─768\n",
            "│    └─2.blocks.3.mlp.ln2.linear.weight                                                ├─294,912\n",
            "│    └─2.blocks.3.mlp.ln2.bn.weight                                                    ├─384\n",
            "│    └─2.blocks.3.mlp.ln2.bn.bias                                                      └─384\n",
            "│    └─LevitStage: 2-8                                       [32, 196, 128]            --\n",
            "│    │    └─blocks.0.attn.attention_biases                                             ├─784\n",
            "│    │    └─blocks.0.attn.qkv.linear.weight                                            ├─32,768\n",
            "│    │    └─blocks.0.attn.qkv.bn.weight                                                ├─256\n",
            "│    │    └─blocks.0.attn.qkv.bn.bias                                                  ├─256\n",
            "│    │    └─blocks.0.attn.proj.ln.linear.weight                                        ├─16,384\n",
            "│    │    └─blocks.0.attn.proj.ln.bn.weight                                            ├─128\n",
            "│    │    └─blocks.0.attn.proj.ln.bn.bias                                              ├─128\n",
            "│    │    └─blocks.0.mlp.ln1.linear.weight                                             ├─32,768\n",
            "│    │    └─blocks.0.mlp.ln1.bn.weight                                                 ├─256\n",
            "│    │    └─blocks.0.mlp.ln1.bn.bias                                                   ├─256\n",
            "│    │    └─blocks.0.mlp.ln2.linear.weight                                             ├─32,768\n",
            "│    │    └─blocks.0.mlp.ln2.bn.weight                                                 ├─128\n",
            "│    │    └─blocks.0.mlp.ln2.bn.bias                                                   ├─128\n",
            "│    │    └─blocks.1.attn.attention_biases                                             ├─784\n",
            "│    │    └─blocks.1.attn.qkv.linear.weight                                            ├─32,768\n",
            "│    │    └─blocks.1.attn.qkv.bn.weight                                                ├─256\n",
            "│    │    └─blocks.1.attn.qkv.bn.bias                                                  ├─256\n",
            "│    │    └─blocks.1.attn.proj.ln.linear.weight                                        ├─16,384\n",
            "│    │    └─blocks.1.attn.proj.ln.bn.weight                                            ├─128\n",
            "│    │    └─blocks.1.attn.proj.ln.bn.bias                                              ├─128\n",
            "│    │    └─blocks.1.mlp.ln1.linear.weight                                             ├─32,768\n",
            "│    │    └─blocks.1.mlp.ln1.bn.weight                                                 ├─256\n",
            "│    │    └─blocks.1.mlp.ln1.bn.bias                                                   ├─256\n",
            "│    │    └─blocks.1.mlp.ln2.linear.weight                                             ├─32,768\n",
            "│    │    └─blocks.1.mlp.ln2.bn.weight                                                 ├─128\n",
            "│    │    └─blocks.1.mlp.ln2.bn.bias                                                   └─128\n",
            "│    │    └─Identity: 3-9                                    [32, 196, 128]            --\n",
            "│    │    └─Sequential: 3-10                                 [32, 196, 128]            234,016\n",
            "│    │    │    └─0.attn.attention_biases                                               ├─784\n",
            "│    │    │    └─0.attn.qkv.linear.weight                                              ├─32,768\n",
            "│    │    │    └─0.attn.qkv.bn.weight                                                  ├─256\n",
            "│    │    │    └─0.attn.qkv.bn.bias                                                    ├─256\n",
            "│    │    │    └─0.attn.proj.ln.linear.weight                                          ├─16,384\n",
            "│    │    │    └─0.attn.proj.ln.bn.weight                                              ├─128\n",
            "│    │    │    └─0.attn.proj.ln.bn.bias                                                ├─128\n",
            "│    │    │    └─0.mlp.ln1.linear.weight                                               ├─32,768\n",
            "│    │    │    └─0.mlp.ln1.bn.weight                                                   ├─256\n",
            "│    │    │    └─0.mlp.ln1.bn.bias                                                     ├─256\n",
            "│    │    │    └─0.mlp.ln2.linear.weight                                               ├─32,768\n",
            "│    │    │    └─0.mlp.ln2.bn.weight                                                   ├─128\n",
            "│    │    │    └─0.mlp.ln2.bn.bias                                                     ├─128\n",
            "│    │    │    └─1.attn.attention_biases                                               ├─784\n",
            "│    │    │    └─1.attn.qkv.linear.weight                                              ├─32,768\n",
            "│    │    │    └─1.attn.qkv.bn.weight                                                  ├─256\n",
            "│    │    │    └─1.attn.qkv.bn.bias                                                    ├─256\n",
            "│    │    │    └─1.attn.proj.ln.linear.weight                                          ├─16,384\n",
            "│    │    │    └─1.attn.proj.ln.bn.weight                                              ├─128\n",
            "│    │    │    └─1.attn.proj.ln.bn.bias                                                ├─128\n",
            "│    │    │    └─1.mlp.ln1.linear.weight                                               ├─32,768\n",
            "│    │    │    └─1.mlp.ln1.bn.weight                                                   ├─256\n",
            "│    │    │    └─1.mlp.ln1.bn.bias                                                     ├─256\n",
            "│    │    │    └─1.mlp.ln2.linear.weight                                               ├─32,768\n",
            "│    │    │    └─1.mlp.ln2.bn.weight                                                   ├─128\n",
            "│    │    │    └─1.mlp.ln2.bn.bias                                                     └─128\n",
            "│    └─LevitStage: 2-9                                       [32, 49, 256]             --\n",
            "│    │    └─downsample.attn_downsample.attention_biases                                ├─1,568\n",
            "│    │    └─downsample.attn_downsample.kv.linear.weight                                ├─81,920\n",
            "│    │    └─downsample.attn_downsample.kv.bn.weight                                    ├─640\n",
            "│    │    └─downsample.attn_downsample.kv.bn.bias                                      ├─640\n",
            "│    │    └─downsample.attn_downsample.q.ln.linear.weight                              ├─16,384\n",
            "│    │    └─downsample.attn_downsample.q.ln.bn.weight                                  ├─128\n",
            "│    │    └─downsample.attn_downsample.q.ln.bn.bias                                    ├─128\n",
            "│    │    └─downsample.attn_downsample.proj.ln.linear.weight                           ├─131,072\n",
            "│    │    └─downsample.attn_downsample.proj.ln.bn.weight                               ├─256\n",
            "│    │    └─downsample.attn_downsample.proj.ln.bn.bias                                 ├─256\n",
            "│    │    └─downsample.mlp.ln1.linear.weight                                           ├─131,072\n",
            "│    │    └─downsample.mlp.ln1.bn.weight                                               ├─512\n",
            "│    │    └─downsample.mlp.ln1.bn.bias                                                 ├─512\n",
            "│    │    └─downsample.mlp.ln2.linear.weight                                           ├─131,072\n",
            "│    │    └─downsample.mlp.ln2.bn.weight                                               ├─256\n",
            "│    │    └─downsample.mlp.ln2.bn.bias                                                 ├─256\n",
            "│    │    └─blocks.0.attn.attention_biases                                             ├─294\n",
            "│    │    └─blocks.0.attn.qkv.linear.weight                                            ├─98,304\n",
            "│    │    └─blocks.0.attn.qkv.bn.weight                                                ├─384\n",
            "│    │    └─blocks.0.attn.qkv.bn.bias                                                  ├─384\n",
            "│    │    └─blocks.0.attn.proj.ln.linear.weight                                        ├─49,152\n",
            "│    │    └─blocks.0.attn.proj.ln.bn.weight                                            ├─256\n",
            "│    │    └─blocks.0.attn.proj.ln.bn.bias                                              ├─256\n",
            "│    │    └─blocks.0.mlp.ln1.linear.weight                                             ├─131,072\n",
            "│    │    └─blocks.0.mlp.ln1.bn.weight                                                 ├─512\n",
            "│    │    └─blocks.0.mlp.ln1.bn.bias                                                   ├─512\n",
            "│    │    └─blocks.0.mlp.ln2.linear.weight                                             ├─131,072\n",
            "│    │    └─blocks.0.mlp.ln2.bn.weight                                                 ├─256\n",
            "│    │    └─blocks.0.mlp.ln2.bn.bias                                                   ├─256\n",
            "│    │    └─blocks.1.attn.attention_biases                                             ├─294\n",
            "│    │    └─blocks.1.attn.qkv.linear.weight                                            ├─98,304\n",
            "│    │    └─blocks.1.attn.qkv.bn.weight                                                ├─384\n",
            "│    │    └─blocks.1.attn.qkv.bn.bias                                                  ├─384\n",
            "│    │    └─blocks.1.attn.proj.ln.linear.weight                                        ├─49,152\n",
            "│    │    └─blocks.1.attn.proj.ln.bn.weight                                            ├─256\n",
            "│    │    └─blocks.1.attn.proj.ln.bn.bias                                              ├─256\n",
            "│    │    └─blocks.1.mlp.ln1.linear.weight                                             ├─131,072\n",
            "│    │    └─blocks.1.mlp.ln1.bn.weight                                                 ├─512\n",
            "│    │    └─blocks.1.mlp.ln1.bn.bias                                                   ├─512\n",
            "│    │    └─blocks.1.mlp.ln2.linear.weight                                             ├─131,072\n",
            "│    │    └─blocks.1.mlp.ln2.bn.weight                                                 ├─256\n",
            "│    │    └─blocks.1.mlp.ln2.bn.bias                                                   ├─256\n",
            "│    │    └─blocks.2.attn.attention_biases                                             ├─294\n",
            "│    │    └─blocks.2.attn.qkv.linear.weight                                            ├─98,304\n",
            "│    │    └─blocks.2.attn.qkv.bn.weight                                                ├─384\n",
            "│    │    └─blocks.2.attn.qkv.bn.bias                                                  ├─384\n",
            "│    │    └─blocks.2.attn.proj.ln.linear.weight                                        ├─49,152\n",
            "│    │    └─blocks.2.attn.proj.ln.bn.weight                                            ├─256\n",
            "│    │    └─blocks.2.attn.proj.ln.bn.bias                                              ├─256\n",
            "│    │    └─blocks.2.mlp.ln1.linear.weight                                             ├─131,072\n",
            "│    │    └─blocks.2.mlp.ln1.bn.weight                                                 ├─512\n",
            "│    │    └─blocks.2.mlp.ln1.bn.bias                                                   ├─512\n",
            "│    │    └─blocks.2.mlp.ln2.linear.weight                                             ├─131,072\n",
            "│    │    └─blocks.2.mlp.ln2.bn.weight                                                 ├─256\n",
            "│    │    └─blocks.2.mlp.ln2.bn.bias                                                   └─256\n",
            "│    │    └─LevitDownsample: 3-11                            [32, 49, 256]             496,672\n",
            "│    │    │    └─attn_downsample.attention_biases                                      ├─1,568\n",
            "│    │    │    └─attn_downsample.kv.linear.weight                                      ├─81,920\n",
            "│    │    │    └─attn_downsample.kv.bn.weight                                          ├─640\n",
            "│    │    │    └─attn_downsample.kv.bn.bias                                            ├─640\n",
            "│    │    │    └─attn_downsample.q.ln.linear.weight                                    ├─16,384\n",
            "│    │    │    └─attn_downsample.q.ln.bn.weight                                        ├─128\n",
            "│    │    │    └─attn_downsample.q.ln.bn.bias                                          ├─128\n",
            "│    │    │    └─attn_downsample.proj.ln.linear.weight                                 ├─131,072\n",
            "│    │    │    └─attn_downsample.proj.ln.bn.weight                                     ├─256\n",
            "│    │    │    └─attn_downsample.proj.ln.bn.bias                                       ├─256\n",
            "│    │    │    └─mlp.ln1.linear.weight                                                 ├─131,072\n",
            "│    │    │    └─mlp.ln1.bn.weight                                                     ├─512\n",
            "│    │    │    └─mlp.ln1.bn.bias                                                       ├─512\n",
            "│    │    │    └─mlp.ln2.linear.weight                                                 ├─131,072\n",
            "│    │    │    └─mlp.ln2.bn.weight                                                     ├─256\n",
            "│    │    │    └─mlp.ln2.bn.bias                                                       └─256\n",
            "│    │    └─Sequential: 3-12                                 [32, 49, 256]             1,238,130\n",
            "│    │    │    └─0.attn.attention_biases                                               ├─294\n",
            "│    │    │    └─0.attn.qkv.linear.weight                                              ├─98,304\n",
            "│    │    │    └─0.attn.qkv.bn.weight                                                  ├─384\n",
            "│    │    │    └─0.attn.qkv.bn.bias                                                    ├─384\n",
            "│    │    │    └─0.attn.proj.ln.linear.weight                                          ├─49,152\n",
            "│    │    │    └─0.attn.proj.ln.bn.weight                                              ├─256\n",
            "│    │    │    └─0.attn.proj.ln.bn.bias                                                ├─256\n",
            "│    │    │    └─0.mlp.ln1.linear.weight                                               ├─131,072\n",
            "│    │    │    └─0.mlp.ln1.bn.weight                                                   ├─512\n",
            "│    │    │    └─0.mlp.ln1.bn.bias                                                     ├─512\n",
            "│    │    │    └─0.mlp.ln2.linear.weight                                               ├─131,072\n",
            "│    │    │    └─0.mlp.ln2.bn.weight                                                   ├─256\n",
            "│    │    │    └─0.mlp.ln2.bn.bias                                                     ├─256\n",
            "│    │    │    └─1.attn.attention_biases                                               ├─294\n",
            "│    │    │    └─1.attn.qkv.linear.weight                                              ├─98,304\n",
            "│    │    │    └─1.attn.qkv.bn.weight                                                  ├─384\n",
            "│    │    │    └─1.attn.qkv.bn.bias                                                    ├─384\n",
            "│    │    │    └─1.attn.proj.ln.linear.weight                                          ├─49,152\n",
            "│    │    │    └─1.attn.proj.ln.bn.weight                                              ├─256\n",
            "│    │    │    └─1.attn.proj.ln.bn.bias                                                ├─256\n",
            "│    │    │    └─1.mlp.ln1.linear.weight                                               ├─131,072\n",
            "│    │    │    └─1.mlp.ln1.bn.weight                                                   ├─512\n",
            "│    │    │    └─1.mlp.ln1.bn.bias                                                     ├─512\n",
            "│    │    │    └─1.mlp.ln2.linear.weight                                               ├─131,072\n",
            "│    │    │    └─1.mlp.ln2.bn.weight                                                   ├─256\n",
            "│    │    │    └─1.mlp.ln2.bn.bias                                                     ├─256\n",
            "│    │    │    └─2.attn.attention_biases                                               ├─294\n",
            "│    │    │    └─2.attn.qkv.linear.weight                                              ├─98,304\n",
            "│    │    │    └─2.attn.qkv.bn.weight                                                  ├─384\n",
            "│    │    │    └─2.attn.qkv.bn.bias                                                    ├─384\n",
            "│    │    │    └─2.attn.proj.ln.linear.weight                                          ├─49,152\n",
            "│    │    │    └─2.attn.proj.ln.bn.weight                                              ├─256\n",
            "│    │    │    └─2.attn.proj.ln.bn.bias                                                ├─256\n",
            "│    │    │    └─2.mlp.ln1.linear.weight                                               ├─131,072\n",
            "│    │    │    └─2.mlp.ln1.bn.weight                                                   ├─512\n",
            "│    │    │    └─2.mlp.ln1.bn.bias                                                     ├─512\n",
            "│    │    │    └─2.mlp.ln2.linear.weight                                               ├─131,072\n",
            "│    │    │    └─2.mlp.ln2.bn.weight                                                   ├─256\n",
            "│    │    │    └─2.mlp.ln2.bn.bias                                                     └─256\n",
            "│    └─LevitStage: 2-10                                      [32, 16, 384]             --\n",
            "│    │    └─downsample.attn_downsample.attention_biases                                ├─784\n",
            "│    │    └─downsample.attn_downsample.kv.linear.weight                                ├─327,680\n",
            "│    │    └─downsample.attn_downsample.kv.bn.weight                                    ├─1,280\n",
            "│    │    └─downsample.attn_downsample.kv.bn.bias                                      ├─1,280\n",
            "│    │    └─downsample.attn_downsample.q.ln.linear.weight                              ├─65,536\n",
            "│    │    └─downsample.attn_downsample.q.ln.bn.weight                                  ├─256\n",
            "│    │    └─downsample.attn_downsample.q.ln.bn.bias                                    ├─256\n",
            "│    │    └─downsample.attn_downsample.proj.ln.linear.weight                           ├─393,216\n",
            "│    │    └─downsample.attn_downsample.proj.ln.bn.weight                               ├─384\n",
            "│    │    └─downsample.attn_downsample.proj.ln.bn.bias                                 ├─384\n",
            "│    │    └─downsample.mlp.ln1.linear.weight                                           ├─294,912\n",
            "│    │    └─downsample.mlp.ln1.bn.weight                                               ├─768\n",
            "│    │    └─downsample.mlp.ln1.bn.bias                                                 ├─768\n",
            "│    │    └─downsample.mlp.ln2.linear.weight                                           ├─294,912\n",
            "│    │    └─downsample.mlp.ln2.bn.weight                                               ├─384\n",
            "│    │    └─downsample.mlp.ln2.bn.bias                                                 ├─384\n",
            "│    │    └─blocks.0.attn.attention_biases                                             ├─128\n",
            "│    │    └─blocks.0.attn.qkv.linear.weight                                            ├─196,608\n",
            "│    │    └─blocks.0.attn.qkv.bn.weight                                                ├─512\n",
            "│    │    └─blocks.0.attn.qkv.bn.bias                                                  ├─512\n",
            "│    │    └─blocks.0.attn.proj.ln.linear.weight                                        ├─98,304\n",
            "│    │    └─blocks.0.attn.proj.ln.bn.weight                                            ├─384\n",
            "│    │    └─blocks.0.attn.proj.ln.bn.bias                                              ├─384\n",
            "│    │    └─blocks.0.mlp.ln1.linear.weight                                             ├─294,912\n",
            "│    │    └─blocks.0.mlp.ln1.bn.weight                                                 ├─768\n",
            "│    │    └─blocks.0.mlp.ln1.bn.bias                                                   ├─768\n",
            "│    │    └─blocks.0.mlp.ln2.linear.weight                                             ├─294,912\n",
            "│    │    └─blocks.0.mlp.ln2.bn.weight                                                 ├─384\n",
            "│    │    └─blocks.0.mlp.ln2.bn.bias                                                   ├─384\n",
            "│    │    └─blocks.1.attn.attention_biases                                             ├─128\n",
            "│    │    └─blocks.1.attn.qkv.linear.weight                                            ├─196,608\n",
            "│    │    └─blocks.1.attn.qkv.bn.weight                                                ├─512\n",
            "│    │    └─blocks.1.attn.qkv.bn.bias                                                  ├─512\n",
            "│    │    └─blocks.1.attn.proj.ln.linear.weight                                        ├─98,304\n",
            "│    │    └─blocks.1.attn.proj.ln.bn.weight                                            ├─384\n",
            "│    │    └─blocks.1.attn.proj.ln.bn.bias                                              ├─384\n",
            "│    │    └─blocks.1.mlp.ln1.linear.weight                                             ├─294,912\n",
            "│    │    └─blocks.1.mlp.ln1.bn.weight                                                 ├─768\n",
            "│    │    └─blocks.1.mlp.ln1.bn.bias                                                   ├─768\n",
            "│    │    └─blocks.1.mlp.ln2.linear.weight                                             ├─294,912\n",
            "│    │    └─blocks.1.mlp.ln2.bn.weight                                                 ├─384\n",
            "│    │    └─blocks.1.mlp.ln2.bn.bias                                                   ├─384\n",
            "│    │    └─blocks.2.attn.attention_biases                                             ├─128\n",
            "│    │    └─blocks.2.attn.qkv.linear.weight                                            ├─196,608\n",
            "│    │    └─blocks.2.attn.qkv.bn.weight                                                ├─512\n",
            "│    │    └─blocks.2.attn.qkv.bn.bias                                                  ├─512\n",
            "│    │    └─blocks.2.attn.proj.ln.linear.weight                                        ├─98,304\n",
            "│    │    └─blocks.2.attn.proj.ln.bn.weight                                            ├─384\n",
            "│    │    └─blocks.2.attn.proj.ln.bn.bias                                              ├─384\n",
            "│    │    └─blocks.2.mlp.ln1.linear.weight                                             ├─294,912\n",
            "│    │    └─blocks.2.mlp.ln1.bn.weight                                                 ├─768\n",
            "│    │    └─blocks.2.mlp.ln1.bn.bias                                                   ├─768\n",
            "│    │    └─blocks.2.mlp.ln2.linear.weight                                             ├─294,912\n",
            "│    │    └─blocks.2.mlp.ln2.bn.weight                                                 ├─384\n",
            "│    │    └─blocks.2.mlp.ln2.bn.bias                                                   ├─384\n",
            "│    │    └─blocks.3.attn.attention_biases                                             ├─128\n",
            "│    │    └─blocks.3.attn.qkv.linear.weight                                            ├─196,608\n",
            "│    │    └─blocks.3.attn.qkv.bn.weight                                                ├─512\n",
            "│    │    └─blocks.3.attn.qkv.bn.bias                                                  ├─512\n",
            "│    │    └─blocks.3.attn.proj.ln.linear.weight                                        ├─98,304\n",
            "│    │    └─blocks.3.attn.proj.ln.bn.weight                                            ├─384\n",
            "│    │    └─blocks.3.attn.proj.ln.bn.bias                                              ├─384\n",
            "│    │    └─blocks.3.mlp.ln1.linear.weight                                             ├─294,912\n",
            "│    │    └─blocks.3.mlp.ln1.bn.weight                                                 ├─768\n",
            "│    │    └─blocks.3.mlp.ln1.bn.bias                                                   ├─768\n",
            "│    │    └─blocks.3.mlp.ln2.linear.weight                                             ├─294,912\n",
            "│    │    └─blocks.3.mlp.ln2.bn.weight                                                 ├─384\n",
            "│    │    └─blocks.3.mlp.ln2.bn.bias                                                   └─384\n",
            "│    │    └─LevitDownsample: 3-13                            [32, 16, 384]             1,383,184\n",
            "│    │    │    └─attn_downsample.attention_biases                                      ├─784\n",
            "│    │    │    └─attn_downsample.kv.linear.weight                                      ├─327,680\n",
            "│    │    │    └─attn_downsample.kv.bn.weight                                          ├─1,280\n",
            "│    │    │    └─attn_downsample.kv.bn.bias                                            ├─1,280\n",
            "│    │    │    └─attn_downsample.q.ln.linear.weight                                    ├─65,536\n",
            "│    │    │    └─attn_downsample.q.ln.bn.weight                                        ├─256\n",
            "│    │    │    └─attn_downsample.q.ln.bn.bias                                          ├─256\n",
            "│    │    │    └─attn_downsample.proj.ln.linear.weight                                 ├─393,216\n",
            "│    │    │    └─attn_downsample.proj.ln.bn.weight                                     ├─384\n",
            "│    │    │    └─attn_downsample.proj.ln.bn.bias                                       ├─384\n",
            "│    │    │    └─mlp.ln1.linear.weight                                                 ├─294,912\n",
            "│    │    │    └─mlp.ln1.bn.weight                                                     ├─768\n",
            "│    │    │    └─mlp.ln1.bn.bias                                                       ├─768\n",
            "│    │    │    └─mlp.ln2.linear.weight                                                 ├─294,912\n",
            "│    │    │    └─mlp.ln2.bn.weight                                                     ├─384\n",
            "│    │    │    └─mlp.ln2.bn.bias                                                       └─384\n",
            "│    │    └─Sequential: 3-14                                 [32, 16, 384]             3,555,840\n",
            "│    │    │    └─0.attn.attention_biases                                               ├─128\n",
            "│    │    │    └─0.attn.qkv.linear.weight                                              ├─196,608\n",
            "│    │    │    └─0.attn.qkv.bn.weight                                                  ├─512\n",
            "│    │    │    └─0.attn.qkv.bn.bias                                                    ├─512\n",
            "│    │    │    └─0.attn.proj.ln.linear.weight                                          ├─98,304\n",
            "│    │    │    └─0.attn.proj.ln.bn.weight                                              ├─384\n",
            "│    │    │    └─0.attn.proj.ln.bn.bias                                                ├─384\n",
            "│    │    │    └─0.mlp.ln1.linear.weight                                               ├─294,912\n",
            "│    │    │    └─0.mlp.ln1.bn.weight                                                   ├─768\n",
            "│    │    │    └─0.mlp.ln1.bn.bias                                                     ├─768\n",
            "│    │    │    └─0.mlp.ln2.linear.weight                                               ├─294,912\n",
            "│    │    │    └─0.mlp.ln2.bn.weight                                                   ├─384\n",
            "│    │    │    └─0.mlp.ln2.bn.bias                                                     ├─384\n",
            "│    │    │    └─1.attn.attention_biases                                               ├─128\n",
            "│    │    │    └─1.attn.qkv.linear.weight                                              ├─196,608\n",
            "│    │    │    └─1.attn.qkv.bn.weight                                                  ├─512\n",
            "│    │    │    └─1.attn.qkv.bn.bias                                                    ├─512\n",
            "│    │    │    └─1.attn.proj.ln.linear.weight                                          ├─98,304\n",
            "│    │    │    └─1.attn.proj.ln.bn.weight                                              ├─384\n",
            "│    │    │    └─1.attn.proj.ln.bn.bias                                                ├─384\n",
            "│    │    │    └─1.mlp.ln1.linear.weight                                               ├─294,912\n",
            "│    │    │    └─1.mlp.ln1.bn.weight                                                   ├─768\n",
            "│    │    │    └─1.mlp.ln1.bn.bias                                                     ├─768\n",
            "│    │    │    └─1.mlp.ln2.linear.weight                                               ├─294,912\n",
            "│    │    │    └─1.mlp.ln2.bn.weight                                                   ├─384\n",
            "│    │    │    └─1.mlp.ln2.bn.bias                                                     ├─384\n",
            "│    │    │    └─2.attn.attention_biases                                               ├─128\n",
            "│    │    │    └─2.attn.qkv.linear.weight                                              ├─196,608\n",
            "│    │    │    └─2.attn.qkv.bn.weight                                                  ├─512\n",
            "│    │    │    └─2.attn.qkv.bn.bias                                                    ├─512\n",
            "│    │    │    └─2.attn.proj.ln.linear.weight                                          ├─98,304\n",
            "│    │    │    └─2.attn.proj.ln.bn.weight                                              ├─384\n",
            "│    │    │    └─2.attn.proj.ln.bn.bias                                                ├─384\n",
            "│    │    │    └─2.mlp.ln1.linear.weight                                               ├─294,912\n",
            "│    │    │    └─2.mlp.ln1.bn.weight                                                   ├─768\n",
            "│    │    │    └─2.mlp.ln1.bn.bias                                                     ├─768\n",
            "│    │    │    └─2.mlp.ln2.linear.weight                                               ├─294,912\n",
            "│    │    │    └─2.mlp.ln2.bn.weight                                                   ├─384\n",
            "│    │    │    └─2.mlp.ln2.bn.bias                                                     ├─384\n",
            "│    │    │    └─3.attn.attention_biases                                               ├─128\n",
            "│    │    │    └─3.attn.qkv.linear.weight                                              ├─196,608\n",
            "│    │    │    └─3.attn.qkv.bn.weight                                                  ├─512\n",
            "│    │    │    └─3.attn.qkv.bn.bias                                                    ├─512\n",
            "│    │    │    └─3.attn.proj.ln.linear.weight                                          ├─98,304\n",
            "│    │    │    └─3.attn.proj.ln.bn.weight                                              ├─384\n",
            "│    │    │    └─3.attn.proj.ln.bn.bias                                                ├─384\n",
            "│    │    │    └─3.mlp.ln1.linear.weight                                               ├─294,912\n",
            "│    │    │    └─3.mlp.ln1.bn.weight                                                   ├─768\n",
            "│    │    │    └─3.mlp.ln1.bn.bias                                                     ├─768\n",
            "│    │    │    └─3.mlp.ln2.linear.weight                                               ├─294,912\n",
            "│    │    │    └─3.mlp.ln2.bn.weight                                                   ├─384\n",
            "│    │    │    └─3.mlp.ln2.bn.bias                                                     └─384\n",
            "├─NormLinear: 1-3                                            [32, 9]                   --\n",
            "│    └─bn.weight                                                                       ├─384\n",
            "│    └─bn.bias                                                                         ├─384\n",
            "│    └─linear.weight                                                                   ├─3,456\n",
            "│    └─linear.bias                                                                     └─9\n",
            "│    └─BatchNorm1d: 2-11                                     [32, 384]                 768\n",
            "│    │    └─weight                                                                     ├─384\n",
            "│    │    └─bias                                                                       └─384\n",
            "│    └─Dropout: 2-12                                         [32, 384]                 --\n",
            "│    └─Linear: 2-13                                          [32, 9]                   3,465\n",
            "│    │    └─weight                                                                     ├─3,456\n",
            "│    │    └─bias                                                                       └─9\n",
            "├─NormLinear: 1-4                                            [32, 9]                   --\n",
            "│    └─bn.weight                                                                       ├─384\n",
            "│    └─bn.bias                                                                         ├─384\n",
            "│    └─linear.weight                                                                   ├─3,456\n",
            "│    └─linear.bias                                                                     └─9\n",
            "│    └─BatchNorm1d: 2-14                                     [32, 384]                 768\n",
            "│    │    └─weight                                                                     ├─384\n",
            "│    │    └─bias                                                                       └─384\n",
            "│    └─Dropout: 2-15                                         [32, 384]                 --\n",
            "│    └─Linear: 2-16                                          [32, 9]                   3,465\n",
            "│    │    └─weight                                                                     ├─3,456\n",
            "│    │    └─bias                                                                       └─9\n",
            "==============================================================================================================\n",
            "Total params: 7,013,988\n",
            "Trainable params: 7,013,988\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (Units.GIGABYTES): 1.84\n",
            "==============================================================================================================\n",
            "Input size (MB): 19.27\n",
            "Forward/backward pass size (MB): 660.02\n",
            "Params size (MB): 28.03\n",
            "Estimated Total Size (MB): 707.32\n",
            "==============================================================================================================\n"
          ]
        }
      ],
      "source": [
        "print(summary(model, input_size=(32, 3, 224, 224), verbose=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 319,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2XJWJO0JTIMT",
        "outputId": "249b89b7-8583-4d33-9046-521087af0832"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LevitDistilled(\n",
              "  (stem): Stem16(\n",
              "    (conv1): ConvNorm(\n",
              "      (linear): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (act1): Hardswish()\n",
              "    (conv2): ConvNorm(\n",
              "      (linear): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (act2): Hardswish()\n",
              "    (conv3): ConvNorm(\n",
              "      (linear): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (act3): Hardswish()\n",
              "    (conv4): ConvNorm(\n",
              "      (linear): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (stages): Sequential(\n",
              "    (0): LevitStage(\n",
              "      (downsample): Identity()\n",
              "      (blocks): Sequential(\n",
              "        (0): LevitBlock(\n",
              "          (attn): Attention(\n",
              "            (qkv): LinearNorm(\n",
              "              (linear): Linear(in_features=128, out_features=256, bias=False)\n",
              "              (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "            (proj): Sequential(\n",
              "              (act): Hardswish()\n",
              "              (ln): LinearNorm(\n",
              "                (linear): Linear(in_features=128, out_features=128, bias=False)\n",
              "                (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (drop_path1): Identity()\n",
              "          (mlp): LevitMlp(\n",
              "            (ln1): LinearNorm(\n",
              "              (linear): Linear(in_features=128, out_features=256, bias=False)\n",
              "              (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "            (act): Hardswish()\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "            (ln2): LinearNorm(\n",
              "              (linear): Linear(in_features=256, out_features=128, bias=False)\n",
              "              (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "        (1): LevitBlock(\n",
              "          (attn): Attention(\n",
              "            (qkv): LinearNorm(\n",
              "              (linear): Linear(in_features=128, out_features=256, bias=False)\n",
              "              (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "            (proj): Sequential(\n",
              "              (act): Hardswish()\n",
              "              (ln): LinearNorm(\n",
              "                (linear): Linear(in_features=128, out_features=128, bias=False)\n",
              "                (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (drop_path1): Identity()\n",
              "          (mlp): LevitMlp(\n",
              "            (ln1): LinearNorm(\n",
              "              (linear): Linear(in_features=128, out_features=256, bias=False)\n",
              "              (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "            (act): Hardswish()\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "            (ln2): LinearNorm(\n",
              "              (linear): Linear(in_features=256, out_features=128, bias=False)\n",
              "              (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (1): LevitStage(\n",
              "      (downsample): LevitDownsample(\n",
              "        (attn_downsample): AttentionDownsample(\n",
              "          (kv): LinearNorm(\n",
              "            (linear): Linear(in_features=128, out_features=640, bias=False)\n",
              "            (bn): BatchNorm1d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "          (q): Sequential(\n",
              "            (down): Downsample()\n",
              "            (ln): LinearNorm(\n",
              "              (linear): Linear(in_features=128, out_features=128, bias=False)\n",
              "              (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (proj): Sequential(\n",
              "            (act): Hardswish()\n",
              "            (ln): LinearNorm(\n",
              "              (linear): Linear(in_features=512, out_features=256, bias=False)\n",
              "              (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (mlp): LevitMlp(\n",
              "          (ln1): LinearNorm(\n",
              "            (linear): Linear(in_features=256, out_features=512, bias=False)\n",
              "            (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "          (act): Hardswish()\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "          (ln2): LinearNorm(\n",
              "            (linear): Linear(in_features=512, out_features=256, bias=False)\n",
              "            (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "      )\n",
              "      (blocks): Sequential(\n",
              "        (0): LevitBlock(\n",
              "          (attn): Attention(\n",
              "            (qkv): LinearNorm(\n",
              "              (linear): Linear(in_features=256, out_features=384, bias=False)\n",
              "              (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "            (proj): Sequential(\n",
              "              (act): Hardswish()\n",
              "              (ln): LinearNorm(\n",
              "                (linear): Linear(in_features=192, out_features=256, bias=False)\n",
              "                (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (drop_path1): Identity()\n",
              "          (mlp): LevitMlp(\n",
              "            (ln1): LinearNorm(\n",
              "              (linear): Linear(in_features=256, out_features=512, bias=False)\n",
              "              (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "            (act): Hardswish()\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "            (ln2): LinearNorm(\n",
              "              (linear): Linear(in_features=512, out_features=256, bias=False)\n",
              "              (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "        (1): LevitBlock(\n",
              "          (attn): Attention(\n",
              "            (qkv): LinearNorm(\n",
              "              (linear): Linear(in_features=256, out_features=384, bias=False)\n",
              "              (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "            (proj): Sequential(\n",
              "              (act): Hardswish()\n",
              "              (ln): LinearNorm(\n",
              "                (linear): Linear(in_features=192, out_features=256, bias=False)\n",
              "                (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (drop_path1): Identity()\n",
              "          (mlp): LevitMlp(\n",
              "            (ln1): LinearNorm(\n",
              "              (linear): Linear(in_features=256, out_features=512, bias=False)\n",
              "              (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "            (act): Hardswish()\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "            (ln2): LinearNorm(\n",
              "              (linear): Linear(in_features=512, out_features=256, bias=False)\n",
              "              (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "        (2): LevitBlock(\n",
              "          (attn): Attention(\n",
              "            (qkv): LinearNorm(\n",
              "              (linear): Linear(in_features=256, out_features=384, bias=False)\n",
              "              (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "            (proj): Sequential(\n",
              "              (act): Hardswish()\n",
              "              (ln): LinearNorm(\n",
              "                (linear): Linear(in_features=192, out_features=256, bias=False)\n",
              "                (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (drop_path1): Identity()\n",
              "          (mlp): LevitMlp(\n",
              "            (ln1): LinearNorm(\n",
              "              (linear): Linear(in_features=256, out_features=512, bias=False)\n",
              "              (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "            (act): Hardswish()\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "            (ln2): LinearNorm(\n",
              "              (linear): Linear(in_features=512, out_features=256, bias=False)\n",
              "              (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (2): LevitStage(\n",
              "      (downsample): LevitDownsample(\n",
              "        (attn_downsample): AttentionDownsample(\n",
              "          (kv): LinearNorm(\n",
              "            (linear): Linear(in_features=256, out_features=1280, bias=False)\n",
              "            (bn): BatchNorm1d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "          (q): Sequential(\n",
              "            (down): Downsample()\n",
              "            (ln): LinearNorm(\n",
              "              (linear): Linear(in_features=256, out_features=256, bias=False)\n",
              "              (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (proj): Sequential(\n",
              "            (act): Hardswish()\n",
              "            (ln): LinearNorm(\n",
              "              (linear): Linear(in_features=1024, out_features=384, bias=False)\n",
              "              (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (mlp): LevitMlp(\n",
              "          (ln1): LinearNorm(\n",
              "            (linear): Linear(in_features=384, out_features=768, bias=False)\n",
              "            (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "          (act): Hardswish()\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "          (ln2): LinearNorm(\n",
              "            (linear): Linear(in_features=768, out_features=384, bias=False)\n",
              "            (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "      )\n",
              "      (blocks): Sequential(\n",
              "        (0): LevitBlock(\n",
              "          (attn): Attention(\n",
              "            (qkv): LinearNorm(\n",
              "              (linear): Linear(in_features=384, out_features=512, bias=False)\n",
              "              (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "            (proj): Sequential(\n",
              "              (act): Hardswish()\n",
              "              (ln): LinearNorm(\n",
              "                (linear): Linear(in_features=256, out_features=384, bias=False)\n",
              "                (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (drop_path1): Identity()\n",
              "          (mlp): LevitMlp(\n",
              "            (ln1): LinearNorm(\n",
              "              (linear): Linear(in_features=384, out_features=768, bias=False)\n",
              "              (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "            (act): Hardswish()\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "            (ln2): LinearNorm(\n",
              "              (linear): Linear(in_features=768, out_features=384, bias=False)\n",
              "              (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "        (1): LevitBlock(\n",
              "          (attn): Attention(\n",
              "            (qkv): LinearNorm(\n",
              "              (linear): Linear(in_features=384, out_features=512, bias=False)\n",
              "              (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "            (proj): Sequential(\n",
              "              (act): Hardswish()\n",
              "              (ln): LinearNorm(\n",
              "                (linear): Linear(in_features=256, out_features=384, bias=False)\n",
              "                (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (drop_path1): Identity()\n",
              "          (mlp): LevitMlp(\n",
              "            (ln1): LinearNorm(\n",
              "              (linear): Linear(in_features=384, out_features=768, bias=False)\n",
              "              (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "            (act): Hardswish()\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "            (ln2): LinearNorm(\n",
              "              (linear): Linear(in_features=768, out_features=384, bias=False)\n",
              "              (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "        (2): LevitBlock(\n",
              "          (attn): Attention(\n",
              "            (qkv): LinearNorm(\n",
              "              (linear): Linear(in_features=384, out_features=512, bias=False)\n",
              "              (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "            (proj): Sequential(\n",
              "              (act): Hardswish()\n",
              "              (ln): LinearNorm(\n",
              "                (linear): Linear(in_features=256, out_features=384, bias=False)\n",
              "                (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (drop_path1): Identity()\n",
              "          (mlp): LevitMlp(\n",
              "            (ln1): LinearNorm(\n",
              "              (linear): Linear(in_features=384, out_features=768, bias=False)\n",
              "              (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "            (act): Hardswish()\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "            (ln2): LinearNorm(\n",
              "              (linear): Linear(in_features=768, out_features=384, bias=False)\n",
              "              (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "        (3): LevitBlock(\n",
              "          (attn): Attention(\n",
              "            (qkv): LinearNorm(\n",
              "              (linear): Linear(in_features=384, out_features=512, bias=False)\n",
              "              (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "            (proj): Sequential(\n",
              "              (act): Hardswish()\n",
              "              (ln): LinearNorm(\n",
              "                (linear): Linear(in_features=256, out_features=384, bias=False)\n",
              "                (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (drop_path1): Identity()\n",
              "          (mlp): LevitMlp(\n",
              "            (ln1): LinearNorm(\n",
              "              (linear): Linear(in_features=384, out_features=768, bias=False)\n",
              "              (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "            (act): Hardswish()\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "            (ln2): LinearNorm(\n",
              "              (linear): Linear(in_features=768, out_features=384, bias=False)\n",
              "              (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (head): NormLinear(\n",
              "    (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (drop): Dropout(p=0.0, inplace=False)\n",
              "    (linear): Linear(in_features=384, out_features=9, bias=True)\n",
              "  )\n",
              "  (head_dist): NormLinear(\n",
              "    (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (drop): Dropout(p=0.0, inplace=False)\n",
              "    (linear): Linear(in_features=384, out_features=9, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 319
        }
      ],
      "source": [
        "import torch.nn.init as init\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)  # CUDA 연산 시 동일한 결과 보장\n",
        "    torch.cuda.manual_seed_all(seed)  # 멀티-GPU 환경에서 동일한 결과 보장\n",
        "    torch.backends.cudnn.deterministic = True  # CuDNN 연산을 deterministic하게 설정\n",
        "    torch.backends.cudnn.benchmark = False  # 연산 속도를 희생하고 일관된 연산을 수행\n",
        "\n",
        "def initialize_weights(m):\n",
        "    if isinstance(m, nn.Conv2d):  # Conv 레이어 초기화\n",
        "        init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "        if m.bias is not None:\n",
        "            init.constant_(m.bias, 0)\n",
        "    elif isinstance(m, nn.Linear):  # Linear 레이어 초기화\n",
        "        init.xavier_uniform_(m.weight)\n",
        "        if m.bias is not None:\n",
        "            init.constant_(m.bias, 0)\n",
        "    elif isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):  # BatchNorm 초기화\n",
        "        init.constant_(m.weight, 1)\n",
        "        init.constant_(m.bias, 0)\n",
        "\n",
        "set_seed(42)  # 랜덤 시드 고정\n",
        "model.apply(initialize_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 320,
      "metadata": {
        "id": "fnlxaQSs7NsL"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dir = 'val/CRC-VAL-HE-7K/'\n",
        "dataset = datasets.ImageFolder(root=test_dir, transform=transform)\n",
        "test_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "Qe6c884rPvE7"
      },
      "execution_count": 321,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 322,
      "metadata": {
        "id": "4TSObUGwcYxd"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(lr=learning_rate, params=model.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 323,
      "metadata": {
        "id": "1No-7jzGgzrV"
      },
      "outputs": [],
      "source": [
        "train_losses = []\n",
        "train_accuracies = []\n",
        "val_losses = []\n",
        "val_accuracies = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 324,
      "metadata": {
        "id": "Yf0ZPGLtz_gB"
      },
      "outputs": [],
      "source": [
        "def train(model, train_loader, criterion, optimizer, device, epoch):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for i, (inputs, labels) in enumerate(tqdm(train_loader, desc=\"Training\")):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Train Loss: {epoch_loss:.4f}, Train Accuracy: {accuracy:.2f}%\")\n",
        "    train_losses.append(epoch_loss)\n",
        "    train_accuracies.append(accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 325,
      "metadata": {
        "id": "rbdjXJ520Btp"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, data_loader, criterion, device, epoch, phase=\"Validation\"):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, labels) in enumerate(tqdm(data_loader, desc=f\"{phase}\")):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            # Save all labels and predictions for balanced accuracy\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "    epoch_loss = running_loss / len(data_loader)\n",
        "    accuracy = 100 * correct / total\n",
        "    balanced_acc = balanced_accuracy_score(all_labels, all_predictions)\n",
        "\n",
        "    print(f\"{phase} Loss: {epoch_loss:.4f}, {phase} Accuracy: {accuracy:.2f}%\")\n",
        "    print(f\"Balanced Accuracy: {balanced_acc:.4f}\")\n",
        "\n",
        "    val_losses.append(epoch_loss)\n",
        "    val_accuracies.append(accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 326,
      "metadata": {
        "id": "GfBjePI-0DkY"
      },
      "outputs": [],
      "source": [
        "def measure_inference_time(model, data_loader, device):\n",
        "    model.eval()\n",
        "    times = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, _ in data_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            start_time = torch.cuda.Event(enable_timing=True)\n",
        "            end_time = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "            start_time.record()\n",
        "            _ = model(inputs)  # inference 수행\n",
        "            end_time.record()\n",
        "\n",
        "            # 시간 측정\n",
        "            torch.cuda.synchronize()  # CUDA에서 모든 커널이 완료될 때까지 대기\n",
        "            elapsed_time = start_time.elapsed_time(end_time)  # 밀리초 단위로 반환\n",
        "            times.append(elapsed_time)\n",
        "\n",
        "    # 통계량 계산\n",
        "    times_np = np.array(times)\n",
        "    total_inferences = len(times_np)\n",
        "    avg_time = np.mean(times_np)\n",
        "    std_dev = np.std(times_np)\n",
        "    max_time = np.max(times_np)\n",
        "    min_time = np.min(times_np)\n",
        "\n",
        "    # 결과 출력\n",
        "    print(f\"Inference Time Measurement Results:\")\n",
        "    print(f\"Total Inferences: {total_inferences}\")\n",
        "    print(f\"Average Time: {avg_time:.2f} ms\")\n",
        "    print(f\"Standard Deviation: {std_dev:.2f} ms\")\n",
        "    print(f\"Maximum Time: {max_time:.2f} ms\")\n",
        "    print(f\"Minimum Time: {min_time:.2f} ms\")\n",
        "\n",
        "    return times"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "times = measure_inference_time(model, test_loader, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6sipSSrgUbj",
        "outputId": "3c63272c-3624-4ac3-c2c8-2965c6a82b59"
      },
      "execution_count": 327,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference Time Measurement Results:\n",
            "Total Inferences: 225\n",
            "Average Time: 16.12 ms\n",
            "Standard Deviation: 0.93 ms\n",
            "Maximum Time: 25.00 ms\n",
            "Minimum Time: 15.32 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 328,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c77rmtOz0XuA",
        "outputId": "d2b3b54f-fba2-481c-ba7b-db339b065b71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                           aten::matmul         4.35%     972.699us        34.16%       7.643ms     112.402us       0.000us         0.00%       2.786ms      40.966us            68  \n",
            "                                           aten::linear         1.19%     266.657us        22.74%       5.087ms     105.985us       0.000us         0.00%       1.839ms      38.306us            48  \n",
            "                                               aten::mm         8.70%       1.947ms        12.70%       2.842ms      61.773us       1.744ms        38.02%       1.744ms      37.914us            46  \n",
            "                                              aten::bmm         3.66%     819.503us         4.69%       1.048ms      47.655us     755.098us        16.46%     755.098us      34.323us            22  \n",
            "                                       aten::batch_norm         1.59%     356.712us        30.63%       6.854ms     131.803us       0.000us         0.00%     753.815us      14.496us            52  \n",
            "                           aten::_batch_norm_impl_index         2.01%     448.788us        29.04%       6.497ms     124.943us       0.000us         0.00%     753.815us      14.496us            52  \n",
            "                                aten::native_batch_norm         6.22%       1.391ms        22.74%       5.089ms     106.013us     523.162us        11.41%     653.400us      13.613us            48  \n",
            "                        ampere_sgemm_64x32_sliced1x4_tn         0.00%       0.000us         0.00%       0.000us       0.000us     561.243us        12.24%     561.243us      31.180us            18  \n",
            "                                ampere_sgemm_128x128_nn         0.00%       0.000us         0.00%       0.000us       0.000us     533.469us        11.63%     533.469us     106.694us             5  \n",
            "                                            aten::copy_         5.22%       1.168ms        12.90%       2.885ms      29.146us     498.074us        10.86%     498.074us       5.031us            99  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 22.375ms\n",
            "Self CUDA time total: 4.587ms\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from torch import profiler\n",
        "\n",
        "dummy_input = torch.randn(32, 3, 224, 224).cuda()\n",
        "\n",
        "# Profiling inference\n",
        "with profiler.profile(\n",
        "    activities=[\n",
        "       profiler.ProfilerActivity.CPU,\n",
        "        profiler.ProfilerActivity.CUDA,  # Include if using GPU\n",
        "    ],\n",
        "    on_trace_ready=profiler.tensorboard_trace_handler(\"./logs\"),  # Optional logging\n",
        "    record_shapes=True,\n",
        "    with_stack=True\n",
        ") as prof:\n",
        "    with torch.no_grad():\n",
        "        model(dummy_input)\n",
        "\n",
        "\n",
        "# Print results\n",
        "print(prof.key_averages().table(sort_by=\"cuda_time_total\" if torch.cuda.is_available() else \"cpu_time_total\", row_limit=10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 329,
      "metadata": {
        "id": "jXvKsupvZYjZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score, recall_score, precision_score, roc_curve, auc\n",
        "\n",
        "def score_evaluate(model, data_loader, criterion, device, phase=\"Validation\"):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(data_loader, desc=f\"{phase}\"):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "\n",
        "    overall_f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
        "    overall_recall = recall_score(all_labels, all_preds, average=\"macro\")\n",
        "    overall_precision = precision_score(all_labels, all_preds, average=\"macro\")\n",
        "\n",
        "    f1_per_class = f1_score(all_labels, all_preds, average=None)\n",
        "    recall_per_class = recall_score(all_labels, all_preds, average=None)\n",
        "    precision_per_class = precision_score(all_labels, all_preds, average=None)\n",
        "    class_labels = sorted(set(all_labels))\n",
        "\n",
        "    epoch_loss = running_loss / len(data_loader)\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"{phase} Loss: {epoch_loss:.4f}, {phase} Accuracy: {accuracy:.2f}%\")\n",
        "    val_losses.append(epoch_loss)\n",
        "    val_accuracies.append(accuracy)\n",
        "\n",
        "    print(f\"Overall - F1: {overall_f1:.4f}, Recall: {overall_recall:.4f}, Precision: {overall_precision:.4f}\")\n",
        "    print(\"Per-Class Metrics:\")\n",
        "    for i, label in enumerate(class_labels):\n",
        "        print(f\"Class {label} - F1: {f1_per_class[i]:.4f}, Recall: {recall_per_class[i]:.4f}, Precision: {precision_per_class[i]:.4f}\")\n",
        "\n",
        "    return overall_f1, overall_recall, overall_precision, f1_per_class, recall_per_class, precision_per_class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 330,
      "metadata": {
        "id": "oe9DO2tS-BpI"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "def roc_auc(model, data_loader, device, num_classes):\n",
        "\n",
        "    y = [\"F1\", \"Precision\", \"Recall\"]\n",
        "\n",
        "    model.eval()\n",
        "    all_labels = []\n",
        "    all_scores = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(data_loader, desc=\"ROC AUC\"):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_scores.extend(outputs.cpu().numpy())\n",
        "\n",
        "    all_labels = label_binarize(all_labels, classes=list(range(num_classes)))\n",
        "    print(f\"Binarized all_labels shape: {all_labels.shape}\")\n",
        "    print(f\"All_scores shape: {np.array(all_scores).shape}\")\n",
        "\n",
        "    fpr, tpr, _ = roc_curve(all_labels.ravel(), np.array(all_scores).ravel())\n",
        "    roc_auc_value = auc(fpr, tpr)\n",
        "\n",
        "    plt.figure(figsize=(9, 7))\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc_value:0.4f})')\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"Overall ROC AUC: {roc_auc_value:.4f}\")\n",
        "\n",
        "    return fpr, tpr, roc_auc_value"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load('LeViT_128S_NCT-CRC-HE-100K.pth'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYa8t9xMgmeT",
        "outputId": "a9dbe291-2403-449d-88d5-d7c4eec1119d"
      },
      "execution_count": 331,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-331-66941f807298>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('LeViT_128S_NCT-CRC-HE-100K.pth'))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 331
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "overall_f1, overall_recall, overall_precision, f1_per_class, recall_per_class, precision_per_class = score_evaluate(model, test_loader, criterion, device, phase=\"Test\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2y3cywDpjUQ",
        "outputId": "638ef829-d725-4d6c-d8ad-1b3e050c9891"
      },
      "execution_count": 332,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Test: 100%|██████████| 225/225 [00:12<00:00, 17.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 1.3281, Test Accuracy: 83.77%\n",
            "Overall - F1: 0.8200, Recall: 0.8398, Precision: 0.8525\n",
            "Per-Class Metrics:\n",
            "Class 0 - F1: 0.7279, Recall: 0.5747, Precision: 0.9923\n",
            "Class 1 - F1: 0.9517, Recall: 1.0000, Precision: 0.9078\n",
            "Class 2 - F1: 0.7931, Recall: 0.9558, Precision: 0.6778\n",
            "Class 3 - F1: 0.9674, Recall: 0.9369, Precision: 1.0000\n",
            "Class 4 - F1: 0.9225, Recall: 0.8966, Precision: 0.9498\n",
            "Class 5 - F1: 0.5844, Recall: 0.8564, Precision: 0.4436\n",
            "Class 6 - F1: 0.8770, Recall: 0.9433, Precision: 0.8195\n",
            "Class 7 - F1: 0.6156, Recall: 0.4584, Precision: 0.9369\n",
            "Class 8 - F1: 0.9405, Recall: 0.9359, Precision: 0.9451\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = [\"ADI\", \"BACK\", \"DEB\", \"LYM\", \"MUC\", \"MUS\", \"NORM\", 'STR', \"TUM\"]\n",
        "y = [\"F1\", \"Precision\", \"Recall\"]\n",
        "\n",
        "f1_score_list = []\n",
        "precision_score_list = []\n",
        "recall_score_list = []\n",
        "\n",
        "for i, class_name in enumerate(x):\n",
        "    f1_score_list.append(f1_per_class[i])\n",
        "    precision_score_list.append(precision_per_class[i])\n",
        "    recall_score_list.append(recall_per_class[i])\n",
        "\n",
        "x.append(\"Overall\")\n",
        "f1_score_list.append(overall_f1)\n",
        "precision_score_list.append(overall_precision)\n",
        "recall_score_list.append(overall_recall)\n",
        "\n",
        "score_list = [f1_score_list, precision_score_list, recall_score_list]\n",
        "score_list = np.array(score_list)\n",
        "plt.figure(figsize=(20, 12))\n",
        "score_cm = plt.matshow(score_list, cmap=plt.cm.Blues, alpha=0.3)\n",
        "plt.xticks(range(len(x)), x)\n",
        "plt.yticks(range(len(y)), y)\n",
        "for i in range(len(y)):\n",
        "    for j in range(len(x)):\n",
        "        # 행렬의 각각의 수치를 각 칸의 중앙에 넣어준다\n",
        "        plt.text(x=j, y=i,\n",
        "                     s=\"{:.2f}\".format(score_list[i, j]),\n",
        "                     va='center',\n",
        "                     ha='center',\n",
        "                     )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "p4pYjMPUpy2m",
        "outputId": "d9242a2f-16ae-4328-a720-ac829ef31121"
      },
      "execution_count": 333,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2000x1200 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1600x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z_vaWpvS2WkS"
      },
      "execution_count": 296,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}