{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!wget -O CRC-VAL-HE-7K.zip https://zenodo.org/records/1214456/files/CRC-VAL-HE-7K.zip\n",
        "!unzip -qq CRC-VAL-HE-7K.zip -d val"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QrjpYwInzmwA",
        "outputId": "2d8be20f-c921-4fbb-e79b-7a8f9f189396"
      },
      "execution_count": 261,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-27 14:31:58--  https://zenodo.org/records/1214456/files/CRC-VAL-HE-7K.zip\n",
            "Resolving zenodo.org (zenodo.org)... 188.185.43.25, 188.185.48.194, 188.185.45.92, ...\n",
            "Connecting to zenodo.org (zenodo.org)|188.185.43.25|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 800276929 (763M) [application/octet-stream]\n",
            "Saving to: ‘CRC-VAL-HE-7K.zip’\n",
            "\n",
            "CRC-VAL-HE-7K.zip   100%[===================>] 763.20M   562KB/s    in 24m 5s  \n",
            "\n",
            "2025-02-27 14:56:03 (541 KB/s) - ‘CRC-VAL-HE-7K.zip’ saved [800276929/800276929]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 262,
      "metadata": {
        "id": "Wa8mdzSwvzMe"
      },
      "outputs": [],
      "source": [
        "import timm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, ConcatDataset\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "import itertools\n",
        "from torchinfo import summary\n",
        "from sklearn.metrics import accuracy_score, balanced_accuracy_score, classification_report, confusion_matrix\n",
        "from PIL import Image\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import random\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Subset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 298,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "lbCq25bWzJSQ",
        "outputId": "2376c729-d205-4e02-fe14-997c2a524806"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VisionTransformer(\n",
            "  (patch_embed): PatchEmbed(\n",
            "    (proj): Conv2d(3, 192, kernel_size=(16, 16), stride=(16, 16))\n",
            "    (norm): Identity()\n",
            "  )\n",
            "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
            "  (patch_drop): Identity()\n",
            "  (norm_pre): Identity()\n",
            "  (blocks): Sequential(\n",
            "    (0): Block(\n",
            "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
            "        (q_norm): Identity()\n",
            "        (k_norm): Identity()\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls1): Identity()\n",
            "      (drop_path1): Identity()\n",
            "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
            "        (act): GELU(approximate='none')\n",
            "        (drop1): Dropout(p=0.0, inplace=False)\n",
            "        (norm): Identity()\n",
            "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
            "        (drop2): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls2): Identity()\n",
            "      (drop_path2): Identity()\n",
            "    )\n",
            "    (1): Block(\n",
            "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
            "        (q_norm): Identity()\n",
            "        (k_norm): Identity()\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls1): Identity()\n",
            "      (drop_path1): Identity()\n",
            "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
            "        (act): GELU(approximate='none')\n",
            "        (drop1): Dropout(p=0.0, inplace=False)\n",
            "        (norm): Identity()\n",
            "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
            "        (drop2): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls2): Identity()\n",
            "      (drop_path2): Identity()\n",
            "    )\n",
            "    (2): Block(\n",
            "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
            "        (q_norm): Identity()\n",
            "        (k_norm): Identity()\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls1): Identity()\n",
            "      (drop_path1): Identity()\n",
            "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
            "        (act): GELU(approximate='none')\n",
            "        (drop1): Dropout(p=0.0, inplace=False)\n",
            "        (norm): Identity()\n",
            "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
            "        (drop2): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls2): Identity()\n",
            "      (drop_path2): Identity()\n",
            "    )\n",
            "    (3): Block(\n",
            "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
            "        (q_norm): Identity()\n",
            "        (k_norm): Identity()\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls1): Identity()\n",
            "      (drop_path1): Identity()\n",
            "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
            "        (act): GELU(approximate='none')\n",
            "        (drop1): Dropout(p=0.0, inplace=False)\n",
            "        (norm): Identity()\n",
            "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
            "        (drop2): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls2): Identity()\n",
            "      (drop_path2): Identity()\n",
            "    )\n",
            "    (4): Block(\n",
            "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
            "        (q_norm): Identity()\n",
            "        (k_norm): Identity()\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls1): Identity()\n",
            "      (drop_path1): Identity()\n",
            "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
            "        (act): GELU(approximate='none')\n",
            "        (drop1): Dropout(p=0.0, inplace=False)\n",
            "        (norm): Identity()\n",
            "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
            "        (drop2): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls2): Identity()\n",
            "      (drop_path2): Identity()\n",
            "    )\n",
            "    (5): Block(\n",
            "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
            "        (q_norm): Identity()\n",
            "        (k_norm): Identity()\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls1): Identity()\n",
            "      (drop_path1): Identity()\n",
            "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
            "        (act): GELU(approximate='none')\n",
            "        (drop1): Dropout(p=0.0, inplace=False)\n",
            "        (norm): Identity()\n",
            "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
            "        (drop2): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls2): Identity()\n",
            "      (drop_path2): Identity()\n",
            "    )\n",
            "    (6): Block(\n",
            "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
            "        (q_norm): Identity()\n",
            "        (k_norm): Identity()\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls1): Identity()\n",
            "      (drop_path1): Identity()\n",
            "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
            "        (act): GELU(approximate='none')\n",
            "        (drop1): Dropout(p=0.0, inplace=False)\n",
            "        (norm): Identity()\n",
            "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
            "        (drop2): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls2): Identity()\n",
            "      (drop_path2): Identity()\n",
            "    )\n",
            "    (7): Block(\n",
            "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
            "        (q_norm): Identity()\n",
            "        (k_norm): Identity()\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls1): Identity()\n",
            "      (drop_path1): Identity()\n",
            "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
            "        (act): GELU(approximate='none')\n",
            "        (drop1): Dropout(p=0.0, inplace=False)\n",
            "        (norm): Identity()\n",
            "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
            "        (drop2): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls2): Identity()\n",
            "      (drop_path2): Identity()\n",
            "    )\n",
            "    (8): Block(\n",
            "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
            "        (q_norm): Identity()\n",
            "        (k_norm): Identity()\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls1): Identity()\n",
            "      (drop_path1): Identity()\n",
            "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
            "        (act): GELU(approximate='none')\n",
            "        (drop1): Dropout(p=0.0, inplace=False)\n",
            "        (norm): Identity()\n",
            "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
            "        (drop2): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls2): Identity()\n",
            "      (drop_path2): Identity()\n",
            "    )\n",
            "    (9): Block(\n",
            "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
            "        (q_norm): Identity()\n",
            "        (k_norm): Identity()\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls1): Identity()\n",
            "      (drop_path1): Identity()\n",
            "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
            "        (act): GELU(approximate='none')\n",
            "        (drop1): Dropout(p=0.0, inplace=False)\n",
            "        (norm): Identity()\n",
            "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
            "        (drop2): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls2): Identity()\n",
            "      (drop_path2): Identity()\n",
            "    )\n",
            "    (10): Block(\n",
            "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
            "        (q_norm): Identity()\n",
            "        (k_norm): Identity()\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls1): Identity()\n",
            "      (drop_path1): Identity()\n",
            "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
            "        (act): GELU(approximate='none')\n",
            "        (drop1): Dropout(p=0.0, inplace=False)\n",
            "        (norm): Identity()\n",
            "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
            "        (drop2): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls2): Identity()\n",
            "      (drop_path2): Identity()\n",
            "    )\n",
            "    (11): Block(\n",
            "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
            "        (q_norm): Identity()\n",
            "        (k_norm): Identity()\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls1): Identity()\n",
            "      (drop_path1): Identity()\n",
            "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
            "        (act): GELU(approximate='none')\n",
            "        (drop1): Dropout(p=0.0, inplace=False)\n",
            "        (norm): Identity()\n",
            "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
            "        (drop2): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls2): Identity()\n",
            "      (drop_path2): Identity()\n",
            "    )\n",
            "  )\n",
            "  (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "  (fc_norm): Identity()\n",
            "  (head_drop): Dropout(p=0.0, inplace=False)\n",
            "  (head): Linear(in_features=192, out_features=9, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model = timm.create_model('vit_tiny_patch16_224', pretrained=False, num_classes=9)\n",
        "print(model)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "batch_size = 32\n",
        "learning_rate = 5e-4\n",
        "num_epochs = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 299,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNpMsNYAzLDF",
        "outputId": "9125b9c5-055c-4055-d7cd-176bb65f76a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==========================================================================================\n",
            "Layer (type:depth-idx)                   Output Shape              Param #\n",
            "==========================================================================================\n",
            "VisionTransformer                        [32, 9]                   38,016\n",
            "├─PatchEmbed: 1-1                        [32, 196, 192]            --\n",
            "│    └─Conv2d: 2-1                       [32, 192, 14, 14]         147,648\n",
            "│    └─Identity: 2-2                     [32, 196, 192]            --\n",
            "├─Dropout: 1-2                           [32, 197, 192]            --\n",
            "├─Identity: 1-3                          [32, 197, 192]            --\n",
            "├─Identity: 1-4                          [32, 197, 192]            --\n",
            "├─Sequential: 1-5                        [32, 197, 192]            --\n",
            "│    └─Block: 2-3                        [32, 197, 192]            --\n",
            "│    │    └─LayerNorm: 3-1               [32, 197, 192]            384\n",
            "│    │    └─Attention: 3-2               [32, 197, 192]            148,224\n",
            "│    │    └─Identity: 3-3                [32, 197, 192]            --\n",
            "│    │    └─Identity: 3-4                [32, 197, 192]            --\n",
            "│    │    └─LayerNorm: 3-5               [32, 197, 192]            384\n",
            "│    │    └─Mlp: 3-6                     [32, 197, 192]            295,872\n",
            "│    │    └─Identity: 3-7                [32, 197, 192]            --\n",
            "│    │    └─Identity: 3-8                [32, 197, 192]            --\n",
            "│    └─Block: 2-4                        [32, 197, 192]            --\n",
            "│    │    └─LayerNorm: 3-9               [32, 197, 192]            384\n",
            "│    │    └─Attention: 3-10              [32, 197, 192]            148,224\n",
            "│    │    └─Identity: 3-11               [32, 197, 192]            --\n",
            "│    │    └─Identity: 3-12               [32, 197, 192]            --\n",
            "│    │    └─LayerNorm: 3-13              [32, 197, 192]            384\n",
            "│    │    └─Mlp: 3-14                    [32, 197, 192]            295,872\n",
            "│    │    └─Identity: 3-15               [32, 197, 192]            --\n",
            "│    │    └─Identity: 3-16               [32, 197, 192]            --\n",
            "│    └─Block: 2-5                        [32, 197, 192]            --\n",
            "│    │    └─LayerNorm: 3-17              [32, 197, 192]            384\n",
            "│    │    └─Attention: 3-18              [32, 197, 192]            148,224\n",
            "│    │    └─Identity: 3-19               [32, 197, 192]            --\n",
            "│    │    └─Identity: 3-20               [32, 197, 192]            --\n",
            "│    │    └─LayerNorm: 3-21              [32, 197, 192]            384\n",
            "│    │    └─Mlp: 3-22                    [32, 197, 192]            295,872\n",
            "│    │    └─Identity: 3-23               [32, 197, 192]            --\n",
            "│    │    └─Identity: 3-24               [32, 197, 192]            --\n",
            "│    └─Block: 2-6                        [32, 197, 192]            --\n",
            "│    │    └─LayerNorm: 3-25              [32, 197, 192]            384\n",
            "│    │    └─Attention: 3-26              [32, 197, 192]            148,224\n",
            "│    │    └─Identity: 3-27               [32, 197, 192]            --\n",
            "│    │    └─Identity: 3-28               [32, 197, 192]            --\n",
            "│    │    └─LayerNorm: 3-29              [32, 197, 192]            384\n",
            "│    │    └─Mlp: 3-30                    [32, 197, 192]            295,872\n",
            "│    │    └─Identity: 3-31               [32, 197, 192]            --\n",
            "│    │    └─Identity: 3-32               [32, 197, 192]            --\n",
            "│    └─Block: 2-7                        [32, 197, 192]            --\n",
            "│    │    └─LayerNorm: 3-33              [32, 197, 192]            384\n",
            "│    │    └─Attention: 3-34              [32, 197, 192]            148,224\n",
            "│    │    └─Identity: 3-35               [32, 197, 192]            --\n",
            "│    │    └─Identity: 3-36               [32, 197, 192]            --\n",
            "│    │    └─LayerNorm: 3-37              [32, 197, 192]            384\n",
            "│    │    └─Mlp: 3-38                    [32, 197, 192]            295,872\n",
            "│    │    └─Identity: 3-39               [32, 197, 192]            --\n",
            "│    │    └─Identity: 3-40               [32, 197, 192]            --\n",
            "│    └─Block: 2-8                        [32, 197, 192]            --\n",
            "│    │    └─LayerNorm: 3-41              [32, 197, 192]            384\n",
            "│    │    └─Attention: 3-42              [32, 197, 192]            148,224\n",
            "│    │    └─Identity: 3-43               [32, 197, 192]            --\n",
            "│    │    └─Identity: 3-44               [32, 197, 192]            --\n",
            "│    │    └─LayerNorm: 3-45              [32, 197, 192]            384\n",
            "│    │    └─Mlp: 3-46                    [32, 197, 192]            295,872\n",
            "│    │    └─Identity: 3-47               [32, 197, 192]            --\n",
            "│    │    └─Identity: 3-48               [32, 197, 192]            --\n",
            "│    └─Block: 2-9                        [32, 197, 192]            --\n",
            "│    │    └─LayerNorm: 3-49              [32, 197, 192]            384\n",
            "│    │    └─Attention: 3-50              [32, 197, 192]            148,224\n",
            "│    │    └─Identity: 3-51               [32, 197, 192]            --\n",
            "│    │    └─Identity: 3-52               [32, 197, 192]            --\n",
            "│    │    └─LayerNorm: 3-53              [32, 197, 192]            384\n",
            "│    │    └─Mlp: 3-54                    [32, 197, 192]            295,872\n",
            "│    │    └─Identity: 3-55               [32, 197, 192]            --\n",
            "│    │    └─Identity: 3-56               [32, 197, 192]            --\n",
            "│    └─Block: 2-10                       [32, 197, 192]            --\n",
            "│    │    └─LayerNorm: 3-57              [32, 197, 192]            384\n",
            "│    │    └─Attention: 3-58              [32, 197, 192]            148,224\n",
            "│    │    └─Identity: 3-59               [32, 197, 192]            --\n",
            "│    │    └─Identity: 3-60               [32, 197, 192]            --\n",
            "│    │    └─LayerNorm: 3-61              [32, 197, 192]            384\n",
            "│    │    └─Mlp: 3-62                    [32, 197, 192]            295,872\n",
            "│    │    └─Identity: 3-63               [32, 197, 192]            --\n",
            "│    │    └─Identity: 3-64               [32, 197, 192]            --\n",
            "│    └─Block: 2-11                       [32, 197, 192]            --\n",
            "│    │    └─LayerNorm: 3-65              [32, 197, 192]            384\n",
            "│    │    └─Attention: 3-66              [32, 197, 192]            148,224\n",
            "│    │    └─Identity: 3-67               [32, 197, 192]            --\n",
            "│    │    └─Identity: 3-68               [32, 197, 192]            --\n",
            "│    │    └─LayerNorm: 3-69              [32, 197, 192]            384\n",
            "│    │    └─Mlp: 3-70                    [32, 197, 192]            295,872\n",
            "│    │    └─Identity: 3-71               [32, 197, 192]            --\n",
            "│    │    └─Identity: 3-72               [32, 197, 192]            --\n",
            "│    └─Block: 2-12                       [32, 197, 192]            --\n",
            "│    │    └─LayerNorm: 3-73              [32, 197, 192]            384\n",
            "│    │    └─Attention: 3-74              [32, 197, 192]            148,224\n",
            "│    │    └─Identity: 3-75               [32, 197, 192]            --\n",
            "│    │    └─Identity: 3-76               [32, 197, 192]            --\n",
            "│    │    └─LayerNorm: 3-77              [32, 197, 192]            384\n",
            "│    │    └─Mlp: 3-78                    [32, 197, 192]            295,872\n",
            "│    │    └─Identity: 3-79               [32, 197, 192]            --\n",
            "│    │    └─Identity: 3-80               [32, 197, 192]            --\n",
            "│    └─Block: 2-13                       [32, 197, 192]            --\n",
            "│    │    └─LayerNorm: 3-81              [32, 197, 192]            384\n",
            "│    │    └─Attention: 3-82              [32, 197, 192]            148,224\n",
            "│    │    └─Identity: 3-83               [32, 197, 192]            --\n",
            "│    │    └─Identity: 3-84               [32, 197, 192]            --\n",
            "│    │    └─LayerNorm: 3-85              [32, 197, 192]            384\n",
            "│    │    └─Mlp: 3-86                    [32, 197, 192]            295,872\n",
            "│    │    └─Identity: 3-87               [32, 197, 192]            --\n",
            "│    │    └─Identity: 3-88               [32, 197, 192]            --\n",
            "│    └─Block: 2-14                       [32, 197, 192]            --\n",
            "│    │    └─LayerNorm: 3-89              [32, 197, 192]            384\n",
            "│    │    └─Attention: 3-90              [32, 197, 192]            148,224\n",
            "│    │    └─Identity: 3-91               [32, 197, 192]            --\n",
            "│    │    └─Identity: 3-92               [32, 197, 192]            --\n",
            "│    │    └─LayerNorm: 3-93              [32, 197, 192]            384\n",
            "│    │    └─Mlp: 3-94                    [32, 197, 192]            295,872\n",
            "│    │    └─Identity: 3-95               [32, 197, 192]            --\n",
            "│    │    └─Identity: 3-96               [32, 197, 192]            --\n",
            "├─LayerNorm: 1-6                         [32, 197, 192]            384\n",
            "├─Identity: 1-7                          [32, 192]                 --\n",
            "├─Dropout: 1-8                           [32, 192]                 --\n",
            "├─Linear: 1-9                            [32, 9]                   1,737\n",
            "==========================================================================================\n",
            "Total params: 5,526,153\n",
            "Trainable params: 5,526,153\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (Units.GIGABYTES): 1.10\n",
            "==========================================================================================\n",
            "Input size (MB): 19.27\n",
            "Forward/backward pass size (MB): 1297.47\n",
            "Params size (MB): 21.95\n",
            "Estimated Total Size (MB): 1338.69\n",
            "==========================================================================================\n"
          ]
        }
      ],
      "source": [
        "print(summary(model, input_size=(32, 3, 224, 224)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 300,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "qDELjYy2zP90",
        "outputId": "d8fd2faf-c5ed-48fb-f2c0-ae039d3677cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==========================================================================================\n",
            "Layer (type:depth-idx)                   Output Shape              Param #\n",
            "==========================================================================================\n",
            "VisionTransformer                        [32, 9]                   38,016\n",
            "├─cls_token                                                        ├─192\n",
            "├─pos_embed                                                        └─37,824\n",
            "├─PatchEmbed: 1-1                        [32, 196, 192]            --\n",
            "│    └─proj.weight                                                 ├─147,456\n",
            "│    └─proj.bias                                                   └─192\n",
            "│    └─Conv2d: 2-1                       [32, 192, 14, 14]         147,648\n",
            "│    │    └─weight                                                 ├─147,456\n",
            "│    │    └─bias                                                   └─192\n",
            "│    └─Identity: 2-2                     [32, 196, 192]            --\n",
            "├─Dropout: 1-2                           [32, 197, 192]            --\n",
            "├─Identity: 1-3                          [32, 197, 192]            --\n",
            "├─Identity: 1-4                          [32, 197, 192]            --\n",
            "├─Sequential: 1-5                        [32, 197, 192]            --\n",
            "│    └─0.norm1.weight                                              ├─192\n",
            "│    └─0.norm1.bias                                                ├─192\n",
            "│    └─0.attn.qkv.weight                                           ├─110,592\n",
            "│    └─0.attn.qkv.bias                                             ├─576\n",
            "│    └─0.attn.proj.weight                                          ├─36,864\n",
            "│    └─0.attn.proj.bias                                            ├─192\n",
            "│    └─0.norm2.weight                                              ├─192\n",
            "│    └─0.norm2.bias                                                ├─192\n",
            "│    └─0.mlp.fc1.weight                                            ├─147,456\n",
            "│    └─0.mlp.fc1.bias                                              ├─768\n",
            "│    └─0.mlp.fc2.weight                                            ├─147,456\n",
            "│    └─0.mlp.fc2.bias                                              ├─192\n",
            "│    └─1.norm1.weight                                              ├─192\n",
            "│    └─1.norm1.bias                                                ├─192\n",
            "│    └─1.attn.qkv.weight                                           ├─110,592\n",
            "│    └─1.attn.qkv.bias                                             ├─576\n",
            "│    └─1.attn.proj.weight                                          ├─36,864\n",
            "│    └─1.attn.proj.bias                                            ├─192\n",
            "│    └─1.norm2.weight                                              ├─192\n",
            "│    └─1.norm2.bias                                                ├─192\n",
            "│    └─1.mlp.fc1.weight                                            ├─147,456\n",
            "│    └─1.mlp.fc1.bias                                              ├─768\n",
            "│    └─1.mlp.fc2.weight                                            ├─147,456\n",
            "│    └─1.mlp.fc2.bias                                              ├─192\n",
            "│    └─2.norm1.weight                                              ├─192\n",
            "│    └─2.norm1.bias                                                ├─192\n",
            "│    └─2.attn.qkv.weight                                           ├─110,592\n",
            "│    └─2.attn.qkv.bias                                             ├─576\n",
            "│    └─2.attn.proj.weight                                          ├─36,864\n",
            "│    └─2.attn.proj.bias                                            ├─192\n",
            "│    └─2.norm2.weight                                              ├─192\n",
            "│    └─2.norm2.bias                                                ├─192\n",
            "│    └─2.mlp.fc1.weight                                            ├─147,456\n",
            "│    └─2.mlp.fc1.bias                                              ├─768\n",
            "│    └─2.mlp.fc2.weight                                            ├─147,456\n",
            "│    └─2.mlp.fc2.bias                                              ├─192\n",
            "│    └─3.norm1.weight                                              ├─192\n",
            "│    └─3.norm1.bias                                                ├─192\n",
            "│    └─3.attn.qkv.weight                                           ├─110,592\n",
            "│    └─3.attn.qkv.bias                                             ├─576\n",
            "│    └─3.attn.proj.weight                                          ├─36,864\n",
            "│    └─3.attn.proj.bias                                            ├─192\n",
            "│    └─3.norm2.weight                                              ├─192\n",
            "│    └─3.norm2.bias                                                ├─192\n",
            "│    └─3.mlp.fc1.weight                                            ├─147,456\n",
            "│    └─3.mlp.fc1.bias                                              ├─768\n",
            "│    └─3.mlp.fc2.weight                                            ├─147,456\n",
            "│    └─3.mlp.fc2.bias                                              ├─192\n",
            "│    └─4.norm1.weight                                              ├─192\n",
            "│    └─4.norm1.bias                                                ├─192\n",
            "│    └─4.attn.qkv.weight                                           ├─110,592\n",
            "│    └─4.attn.qkv.bias                                             ├─576\n",
            "│    └─4.attn.proj.weight                                          ├─36,864\n",
            "│    └─4.attn.proj.bias                                            ├─192\n",
            "│    └─4.norm2.weight                                              ├─192\n",
            "│    └─4.norm2.bias                                                ├─192\n",
            "│    └─4.mlp.fc1.weight                                            ├─147,456\n",
            "│    └─4.mlp.fc1.bias                                              ├─768\n",
            "│    └─4.mlp.fc2.weight                                            ├─147,456\n",
            "│    └─4.mlp.fc2.bias                                              ├─192\n",
            "│    └─5.norm1.weight                                              ├─192\n",
            "│    └─5.norm1.bias                                                ├─192\n",
            "│    └─5.attn.qkv.weight                                           ├─110,592\n",
            "│    └─5.attn.qkv.bias                                             ├─576\n",
            "│    └─5.attn.proj.weight                                          ├─36,864\n",
            "│    └─5.attn.proj.bias                                            ├─192\n",
            "│    └─5.norm2.weight                                              ├─192\n",
            "│    └─5.norm2.bias                                                ├─192\n",
            "│    └─5.mlp.fc1.weight                                            ├─147,456\n",
            "│    └─5.mlp.fc1.bias                                              ├─768\n",
            "│    └─5.mlp.fc2.weight                                            ├─147,456\n",
            "│    └─5.mlp.fc2.bias                                              ├─192\n",
            "│    └─6.norm1.weight                                              ├─192\n",
            "│    └─6.norm1.bias                                                ├─192\n",
            "│    └─6.attn.qkv.weight                                           ├─110,592\n",
            "│    └─6.attn.qkv.bias                                             ├─576\n",
            "│    └─6.attn.proj.weight                                          ├─36,864\n",
            "│    └─6.attn.proj.bias                                            ├─192\n",
            "│    └─6.norm2.weight                                              ├─192\n",
            "│    └─6.norm2.bias                                                ├─192\n",
            "│    └─6.mlp.fc1.weight                                            ├─147,456\n",
            "│    └─6.mlp.fc1.bias                                              ├─768\n",
            "│    └─6.mlp.fc2.weight                                            ├─147,456\n",
            "│    └─6.mlp.fc2.bias                                              ├─192\n",
            "│    └─7.norm1.weight                                              ├─192\n",
            "│    └─7.norm1.bias                                                ├─192\n",
            "│    └─7.attn.qkv.weight                                           ├─110,592\n",
            "│    └─7.attn.qkv.bias                                             ├─576\n",
            "│    └─7.attn.proj.weight                                          ├─36,864\n",
            "│    └─7.attn.proj.bias                                            ├─192\n",
            "│    └─7.norm2.weight                                              ├─192\n",
            "│    └─7.norm2.bias                                                ├─192\n",
            "│    └─7.mlp.fc1.weight                                            ├─147,456\n",
            "│    └─7.mlp.fc1.bias                                              ├─768\n",
            "│    └─7.mlp.fc2.weight                                            ├─147,456\n",
            "│    └─7.mlp.fc2.bias                                              ├─192\n",
            "│    └─8.norm1.weight                                              ├─192\n",
            "│    └─8.norm1.bias                                                ├─192\n",
            "│    └─8.attn.qkv.weight                                           ├─110,592\n",
            "│    └─8.attn.qkv.bias                                             ├─576\n",
            "│    └─8.attn.proj.weight                                          ├─36,864\n",
            "│    └─8.attn.proj.bias                                            ├─192\n",
            "│    └─8.norm2.weight                                              ├─192\n",
            "│    └─8.norm2.bias                                                ├─192\n",
            "│    └─8.mlp.fc1.weight                                            ├─147,456\n",
            "│    └─8.mlp.fc1.bias                                              ├─768\n",
            "│    └─8.mlp.fc2.weight                                            ├─147,456\n",
            "│    └─8.mlp.fc2.bias                                              ├─192\n",
            "│    └─9.norm1.weight                                              ├─192\n",
            "│    └─9.norm1.bias                                                ├─192\n",
            "│    └─9.attn.qkv.weight                                           ├─110,592\n",
            "│    └─9.attn.qkv.bias                                             ├─576\n",
            "│    └─9.attn.proj.weight                                          ├─36,864\n",
            "│    └─9.attn.proj.bias                                            ├─192\n",
            "│    └─9.norm2.weight                                              ├─192\n",
            "│    └─9.norm2.bias                                                ├─192\n",
            "│    └─9.mlp.fc1.weight                                            ├─147,456\n",
            "│    └─9.mlp.fc1.bias                                              ├─768\n",
            "│    └─9.mlp.fc2.weight                                            ├─147,456\n",
            "│    └─9.mlp.fc2.bias                                              ├─192\n",
            "│    └─10.norm1.weight                                             ├─192\n",
            "│    └─10.norm1.bias                                               ├─192\n",
            "│    └─10.attn.qkv.weight                                          ├─110,592\n",
            "│    └─10.attn.qkv.bias                                            ├─576\n",
            "│    └─10.attn.proj.weight                                         ├─36,864\n",
            "│    └─10.attn.proj.bias                                           ├─192\n",
            "│    └─10.norm2.weight                                             ├─192\n",
            "│    └─10.norm2.bias                                               ├─192\n",
            "│    └─10.mlp.fc1.weight                                           ├─147,456\n",
            "│    └─10.mlp.fc1.bias                                             ├─768\n",
            "│    └─10.mlp.fc2.weight                                           ├─147,456\n",
            "│    └─10.mlp.fc2.bias                                             ├─192\n",
            "│    └─11.norm1.weight                                             ├─192\n",
            "│    └─11.norm1.bias                                               ├─192\n",
            "│    └─11.attn.qkv.weight                                          ├─110,592\n",
            "│    └─11.attn.qkv.bias                                            ├─576\n",
            "│    └─11.attn.proj.weight                                         ├─36,864\n",
            "│    └─11.attn.proj.bias                                           ├─192\n",
            "│    └─11.norm2.weight                                             ├─192\n",
            "│    └─11.norm2.bias                                               ├─192\n",
            "│    └─11.mlp.fc1.weight                                           ├─147,456\n",
            "│    └─11.mlp.fc1.bias                                             ├─768\n",
            "│    └─11.mlp.fc2.weight                                           ├─147,456\n",
            "│    └─11.mlp.fc2.bias                                             └─192\n",
            "│    └─Block: 2-3                        [32, 197, 192]            --\n",
            "│    │    └─norm1.weight                                           ├─192\n",
            "│    │    └─norm1.bias                                             ├─192\n",
            "│    │    └─attn.qkv.weight                                        ├─110,592\n",
            "│    │    └─attn.qkv.bias                                          ├─576\n",
            "│    │    └─attn.proj.weight                                       ├─36,864\n",
            "│    │    └─attn.proj.bias                                         ├─192\n",
            "│    │    └─norm2.weight                                           ├─192\n",
            "│    │    └─norm2.bias                                             ├─192\n",
            "│    │    └─mlp.fc1.weight                                         ├─147,456\n",
            "│    │    └─mlp.fc1.bias                                           ├─768\n",
            "│    │    └─mlp.fc2.weight                                         ├─147,456\n",
            "│    │    └─mlp.fc2.bias                                           └─192\n",
            "│    │    └─LayerNorm: 3-1               [32, 197, 192]            384\n",
            "│    │    │    └─weight                                            ├─192\n",
            "│    │    │    └─bias                                              └─192\n",
            "│    │    └─Attention: 3-2               [32, 197, 192]            148,224\n",
            "│    │    │    └─qkv.weight                                        ├─110,592\n",
            "│    │    │    └─qkv.bias                                          ├─576\n",
            "│    │    │    └─proj.weight                                       ├─36,864\n",
            "│    │    │    └─proj.bias                                         └─192\n",
            "│    │    └─Identity: 3-3                [32, 197, 192]            --\n",
            "│    │    └─Identity: 3-4                [32, 197, 192]            --\n",
            "│    │    └─LayerNorm: 3-5               [32, 197, 192]            384\n",
            "│    │    │    └─weight                                            ├─192\n",
            "│    │    │    └─bias                                              └─192\n",
            "│    │    └─Mlp: 3-6                     [32, 197, 192]            295,872\n",
            "│    │    │    └─fc1.weight                                        ├─147,456\n",
            "│    │    │    └─fc1.bias                                          ├─768\n",
            "│    │    │    └─fc2.weight                                        ├─147,456\n",
            "│    │    │    └─fc2.bias                                          └─192\n",
            "│    │    └─Identity: 3-7                [32, 197, 192]            --\n",
            "│    │    └─Identity: 3-8                [32, 197, 192]            --\n",
            "│    └─Block: 2-4                        [32, 197, 192]            --\n",
            "│    │    └─norm1.weight                                           ├─192\n",
            "│    │    └─norm1.bias                                             ├─192\n",
            "│    │    └─attn.qkv.weight                                        ├─110,592\n",
            "│    │    └─attn.qkv.bias                                          ├─576\n",
            "│    │    └─attn.proj.weight                                       ├─36,864\n",
            "│    │    └─attn.proj.bias                                         ├─192\n",
            "│    │    └─norm2.weight                                           ├─192\n",
            "│    │    └─norm2.bias                                             ├─192\n",
            "│    │    └─mlp.fc1.weight                                         ├─147,456\n",
            "│    │    └─mlp.fc1.bias                                           ├─768\n",
            "│    │    └─mlp.fc2.weight                                         ├─147,456\n",
            "│    │    └─mlp.fc2.bias                                           └─192\n",
            "│    │    └─LayerNorm: 3-9               [32, 197, 192]            384\n",
            "│    │    │    └─weight                                            ├─192\n",
            "│    │    │    └─bias                                              └─192\n",
            "│    │    └─Attention: 3-10              [32, 197, 192]            148,224\n",
            "│    │    │    └─qkv.weight                                        ├─110,592\n",
            "│    │    │    └─qkv.bias                                          ├─576\n",
            "│    │    │    └─proj.weight                                       ├─36,864\n",
            "│    │    │    └─proj.bias                                         └─192\n",
            "│    │    └─Identity: 3-11               [32, 197, 192]            --\n",
            "│    │    └─Identity: 3-12               [32, 197, 192]            --\n",
            "│    │    └─LayerNorm: 3-13              [32, 197, 192]            384\n",
            "│    │    │    └─weight                                            ├─192\n",
            "│    │    │    └─bias                                              └─192\n",
            "│    │    └─Mlp: 3-14                    [32, 197, 192]            295,872\n",
            "│    │    │    └─fc1.weight                                        ├─147,456\n",
            "│    │    │    └─fc1.bias                                          ├─768\n",
            "│    │    │    └─fc2.weight                                        ├─147,456\n",
            "│    │    │    └─fc2.bias                                          └─192\n",
            "│    │    └─Identity: 3-15               [32, 197, 192]            --\n",
            "│    │    └─Identity: 3-16               [32, 197, 192]            --\n",
            "│    └─Block: 2-5                        [32, 197, 192]            --\n",
            "│    │    └─norm1.weight                                           ├─192\n",
            "│    │    └─norm1.bias                                             ├─192\n",
            "│    │    └─attn.qkv.weight                                        ├─110,592\n",
            "│    │    └─attn.qkv.bias                                          ├─576\n",
            "│    │    └─attn.proj.weight                                       ├─36,864\n",
            "│    │    └─attn.proj.bias                                         ├─192\n",
            "│    │    └─norm2.weight                                           ├─192\n",
            "│    │    └─norm2.bias                                             ├─192\n",
            "│    │    └─mlp.fc1.weight                                         ├─147,456\n",
            "│    │    └─mlp.fc1.bias                                           ├─768\n",
            "│    │    └─mlp.fc2.weight                                         ├─147,456\n",
            "│    │    └─mlp.fc2.bias                                           └─192\n",
            "│    │    └─LayerNorm: 3-17              [32, 197, 192]            384\n",
            "│    │    │    └─weight                                            ├─192\n",
            "│    │    │    └─bias                                              └─192\n",
            "│    │    └─Attention: 3-18              [32, 197, 192]            148,224\n",
            "│    │    │    └─qkv.weight                                        ├─110,592\n",
            "│    │    │    └─qkv.bias                                          ├─576\n",
            "│    │    │    └─proj.weight                                       ├─36,864\n",
            "│    │    │    └─proj.bias                                         └─192\n",
            "│    │    └─Identity: 3-19               [32, 197, 192]            --\n",
            "│    │    └─Identity: 3-20               [32, 197, 192]            --\n",
            "│    │    └─LayerNorm: 3-21              [32, 197, 192]            384\n",
            "│    │    │    └─weight                                            ├─192\n",
            "│    │    │    └─bias                                              └─192\n",
            "│    │    └─Mlp: 3-22                    [32, 197, 192]            295,872\n",
            "│    │    │    └─fc1.weight                                        ├─147,456\n",
            "│    │    │    └─fc1.bias                                          ├─768\n",
            "│    │    │    └─fc2.weight                                        ├─147,456\n",
            "│    │    │    └─fc2.bias                                          └─192\n",
            "│    │    └─Identity: 3-23               [32, 197, 192]            --\n",
            "│    │    └─Identity: 3-24               [32, 197, 192]            --\n",
            "│    └─Block: 2-6                        [32, 197, 192]            --\n",
            "│    │    └─norm1.weight                                           ├─192\n",
            "│    │    └─norm1.bias                                             ├─192\n",
            "│    │    └─attn.qkv.weight                                        ├─110,592\n",
            "│    │    └─attn.qkv.bias                                          ├─576\n",
            "│    │    └─attn.proj.weight                                       ├─36,864\n",
            "│    │    └─attn.proj.bias                                         ├─192\n",
            "│    │    └─norm2.weight                                           ├─192\n",
            "│    │    └─norm2.bias                                             ├─192\n",
            "│    │    └─mlp.fc1.weight                                         ├─147,456\n",
            "│    │    └─mlp.fc1.bias                                           ├─768\n",
            "│    │    └─mlp.fc2.weight                                         ├─147,456\n",
            "│    │    └─mlp.fc2.bias                                           └─192\n",
            "│    │    └─LayerNorm: 3-25              [32, 197, 192]            384\n",
            "│    │    │    └─weight                                            ├─192\n",
            "│    │    │    └─bias                                              └─192\n",
            "│    │    └─Attention: 3-26              [32, 197, 192]            148,224\n",
            "│    │    │    └─qkv.weight                                        ├─110,592\n",
            "│    │    │    └─qkv.bias                                          ├─576\n",
            "│    │    │    └─proj.weight                                       ├─36,864\n",
            "│    │    │    └─proj.bias                                         └─192\n",
            "│    │    └─Identity: 3-27               [32, 197, 192]            --\n",
            "│    │    └─Identity: 3-28               [32, 197, 192]            --\n",
            "│    │    └─LayerNorm: 3-29              [32, 197, 192]            384\n",
            "│    │    │    └─weight                                            ├─192\n",
            "│    │    │    └─bias                                              └─192\n",
            "│    │    └─Mlp: 3-30                    [32, 197, 192]            295,872\n",
            "│    │    │    └─fc1.weight                                        ├─147,456\n",
            "│    │    │    └─fc1.bias                                          ├─768\n",
            "│    │    │    └─fc2.weight                                        ├─147,456\n",
            "│    │    │    └─fc2.bias                                          └─192\n",
            "│    │    └─Identity: 3-31               [32, 197, 192]            --\n",
            "│    │    └─Identity: 3-32               [32, 197, 192]            --\n",
            "│    └─Block: 2-7                        [32, 197, 192]            --\n",
            "│    │    └─norm1.weight                                           ├─192\n",
            "│    │    └─norm1.bias                                             ├─192\n",
            "│    │    └─attn.qkv.weight                                        ├─110,592\n",
            "│    │    └─attn.qkv.bias                                          ├─576\n",
            "│    │    └─attn.proj.weight                                       ├─36,864\n",
            "│    │    └─attn.proj.bias                                         ├─192\n",
            "│    │    └─norm2.weight                                           ├─192\n",
            "│    │    └─norm2.bias                                             ├─192\n",
            "│    │    └─mlp.fc1.weight                                         ├─147,456\n",
            "│    │    └─mlp.fc1.bias                                           ├─768\n",
            "│    │    └─mlp.fc2.weight                                         ├─147,456\n",
            "│    │    └─mlp.fc2.bias                                           └─192\n",
            "│    │    └─LayerNorm: 3-33              [32, 197, 192]            384\n",
            "│    │    │    └─weight                                            ├─192\n",
            "│    │    │    └─bias                                              └─192\n",
            "│    │    └─Attention: 3-34              [32, 197, 192]            148,224\n",
            "│    │    │    └─qkv.weight                                        ├─110,592\n",
            "│    │    │    └─qkv.bias                                          ├─576\n",
            "│    │    │    └─proj.weight                                       ├─36,864\n",
            "│    │    │    └─proj.bias                                         └─192\n",
            "│    │    └─Identity: 3-35               [32, 197, 192]            --\n",
            "│    │    └─Identity: 3-36               [32, 197, 192]            --\n",
            "│    │    └─LayerNorm: 3-37              [32, 197, 192]            384\n",
            "│    │    │    └─weight                                            ├─192\n",
            "│    │    │    └─bias                                              └─192\n",
            "│    │    └─Mlp: 3-38                    [32, 197, 192]            295,872\n",
            "│    │    │    └─fc1.weight                                        ├─147,456\n",
            "│    │    │    └─fc1.bias                                          ├─768\n",
            "│    │    │    └─fc2.weight                                        ├─147,456\n",
            "│    │    │    └─fc2.bias                                          └─192\n",
            "│    │    └─Identity: 3-39               [32, 197, 192]            --\n",
            "│    │    └─Identity: 3-40               [32, 197, 192]            --\n",
            "│    └─Block: 2-8                        [32, 197, 192]            --\n",
            "│    │    └─norm1.weight                                           ├─192\n",
            "│    │    └─norm1.bias                                             ├─192\n",
            "│    │    └─attn.qkv.weight                                        ├─110,592\n",
            "│    │    └─attn.qkv.bias                                          ├─576\n",
            "│    │    └─attn.proj.weight                                       ├─36,864\n",
            "│    │    └─attn.proj.bias                                         ├─192\n",
            "│    │    └─norm2.weight                                           ├─192\n",
            "│    │    └─norm2.bias                                             ├─192\n",
            "│    │    └─mlp.fc1.weight                                         ├─147,456\n",
            "│    │    └─mlp.fc1.bias                                           ├─768\n",
            "│    │    └─mlp.fc2.weight                                         ├─147,456\n",
            "│    │    └─mlp.fc2.bias                                           └─192\n",
            "│    │    └─LayerNorm: 3-41              [32, 197, 192]            384\n",
            "│    │    │    └─weight                                            ├─192\n",
            "│    │    │    └─bias                                              └─192\n",
            "│    │    └─Attention: 3-42              [32, 197, 192]            148,224\n",
            "│    │    │    └─qkv.weight                                        ├─110,592\n",
            "│    │    │    └─qkv.bias                                          ├─576\n",
            "│    │    │    └─proj.weight                                       ├─36,864\n",
            "│    │    │    └─proj.bias                                         └─192\n",
            "│    │    └─Identity: 3-43               [32, 197, 192]            --\n",
            "│    │    └─Identity: 3-44               [32, 197, 192]            --\n",
            "│    │    └─LayerNorm: 3-45              [32, 197, 192]            384\n",
            "│    │    │    └─weight                                            ├─192\n",
            "│    │    │    └─bias                                              └─192\n",
            "│    │    └─Mlp: 3-46                    [32, 197, 192]            295,872\n",
            "│    │    │    └─fc1.weight                                        ├─147,456\n",
            "│    │    │    └─fc1.bias                                          ├─768\n",
            "│    │    │    └─fc2.weight                                        ├─147,456\n",
            "│    │    │    └─fc2.bias                                          └─192\n",
            "│    │    └─Identity: 3-47               [32, 197, 192]            --\n",
            "│    │    └─Identity: 3-48               [32, 197, 192]            --\n",
            "│    └─Block: 2-9                        [32, 197, 192]            --\n",
            "│    │    └─norm1.weight                                           ├─192\n",
            "│    │    └─norm1.bias                                             ├─192\n",
            "│    │    └─attn.qkv.weight                                        ├─110,592\n",
            "│    │    └─attn.qkv.bias                                          ├─576\n",
            "│    │    └─attn.proj.weight                                       ├─36,864\n",
            "│    │    └─attn.proj.bias                                         ├─192\n",
            "│    │    └─norm2.weight                                           ├─192\n",
            "│    │    └─norm2.bias                                             ├─192\n",
            "│    │    └─mlp.fc1.weight                                         ├─147,456\n",
            "│    │    └─mlp.fc1.bias                                           ├─768\n",
            "│    │    └─mlp.fc2.weight                                         ├─147,456\n",
            "│    │    └─mlp.fc2.bias                                           └─192\n",
            "│    │    └─LayerNorm: 3-49              [32, 197, 192]            384\n",
            "│    │    │    └─weight                                            ├─192\n",
            "│    │    │    └─bias                                              └─192\n",
            "│    │    └─Attention: 3-50              [32, 197, 192]            148,224\n",
            "│    │    │    └─qkv.weight                                        ├─110,592\n",
            "│    │    │    └─qkv.bias                                          ├─576\n",
            "│    │    │    └─proj.weight                                       ├─36,864\n",
            "│    │    │    └─proj.bias                                         └─192\n",
            "│    │    └─Identity: 3-51               [32, 197, 192]            --\n",
            "│    │    └─Identity: 3-52               [32, 197, 192]            --\n",
            "│    │    └─LayerNorm: 3-53              [32, 197, 192]            384\n",
            "│    │    │    └─weight                                            ├─192\n",
            "│    │    │    └─bias                                              └─192\n",
            "│    │    └─Mlp: 3-54                    [32, 197, 192]            295,872\n",
            "│    │    │    └─fc1.weight                                        ├─147,456\n",
            "│    │    │    └─fc1.bias                                          ├─768\n",
            "│    │    │    └─fc2.weight                                        ├─147,456\n",
            "│    │    │    └─fc2.bias                                          └─192\n",
            "│    │    └─Identity: 3-55               [32, 197, 192]            --\n",
            "│    │    └─Identity: 3-56               [32, 197, 192]            --\n",
            "│    └─Block: 2-10                       [32, 197, 192]            --\n",
            "│    │    └─norm1.weight                                           ├─192\n",
            "│    │    └─norm1.bias                                             ├─192\n",
            "│    │    └─attn.qkv.weight                                        ├─110,592\n",
            "│    │    └─attn.qkv.bias                                          ├─576\n",
            "│    │    └─attn.proj.weight                                       ├─36,864\n",
            "│    │    └─attn.proj.bias                                         ├─192\n",
            "│    │    └─norm2.weight                                           ├─192\n",
            "│    │    └─norm2.bias                                             ├─192\n",
            "│    │    └─mlp.fc1.weight                                         ├─147,456\n",
            "│    │    └─mlp.fc1.bias                                           ├─768\n",
            "│    │    └─mlp.fc2.weight                                         ├─147,456\n",
            "│    │    └─mlp.fc2.bias                                           └─192\n",
            "│    │    └─LayerNorm: 3-57              [32, 197, 192]            384\n",
            "│    │    │    └─weight                                            ├─192\n",
            "│    │    │    └─bias                                              └─192\n",
            "│    │    └─Attention: 3-58              [32, 197, 192]            148,224\n",
            "│    │    │    └─qkv.weight                                        ├─110,592\n",
            "│    │    │    └─qkv.bias                                          ├─576\n",
            "│    │    │    └─proj.weight                                       ├─36,864\n",
            "│    │    │    └─proj.bias                                         └─192\n",
            "│    │    └─Identity: 3-59               [32, 197, 192]            --\n",
            "│    │    └─Identity: 3-60               [32, 197, 192]            --\n",
            "│    │    └─LayerNorm: 3-61              [32, 197, 192]            384\n",
            "│    │    │    └─weight                                            ├─192\n",
            "│    │    │    └─bias                                              └─192\n",
            "│    │    └─Mlp: 3-62                    [32, 197, 192]            295,872\n",
            "│    │    │    └─fc1.weight                                        ├─147,456\n",
            "│    │    │    └─fc1.bias                                          ├─768\n",
            "│    │    │    └─fc2.weight                                        ├─147,456\n",
            "│    │    │    └─fc2.bias                                          └─192\n",
            "│    │    └─Identity: 3-63               [32, 197, 192]            --\n",
            "│    │    └─Identity: 3-64               [32, 197, 192]            --\n",
            "│    └─Block: 2-11                       [32, 197, 192]            --\n",
            "│    │    └─norm1.weight                                           ├─192\n",
            "│    │    └─norm1.bias                                             ├─192\n",
            "│    │    └─attn.qkv.weight                                        ├─110,592\n",
            "│    │    └─attn.qkv.bias                                          ├─576\n",
            "│    │    └─attn.proj.weight                                       ├─36,864\n",
            "│    │    └─attn.proj.bias                                         ├─192\n",
            "│    │    └─norm2.weight                                           ├─192\n",
            "│    │    └─norm2.bias                                             ├─192\n",
            "│    │    └─mlp.fc1.weight                                         ├─147,456\n",
            "│    │    └─mlp.fc1.bias                                           ├─768\n",
            "│    │    └─mlp.fc2.weight                                         ├─147,456\n",
            "│    │    └─mlp.fc2.bias                                           └─192\n",
            "│    │    └─LayerNorm: 3-65              [32, 197, 192]            384\n",
            "│    │    │    └─weight                                            ├─192\n",
            "│    │    │    └─bias                                              └─192\n",
            "│    │    └─Attention: 3-66              [32, 197, 192]            148,224\n",
            "│    │    │    └─qkv.weight                                        ├─110,592\n",
            "│    │    │    └─qkv.bias                                          ├─576\n",
            "│    │    │    └─proj.weight                                       ├─36,864\n",
            "│    │    │    └─proj.bias                                         └─192\n",
            "│    │    └─Identity: 3-67               [32, 197, 192]            --\n",
            "│    │    └─Identity: 3-68               [32, 197, 192]            --\n",
            "│    │    └─LayerNorm: 3-69              [32, 197, 192]            384\n",
            "│    │    │    └─weight                                            ├─192\n",
            "│    │    │    └─bias                                              └─192\n",
            "│    │    └─Mlp: 3-70                    [32, 197, 192]            295,872\n",
            "│    │    │    └─fc1.weight                                        ├─147,456\n",
            "│    │    │    └─fc1.bias                                          ├─768\n",
            "│    │    │    └─fc2.weight                                        ├─147,456\n",
            "│    │    │    └─fc2.bias                                          └─192\n",
            "│    │    └─Identity: 3-71               [32, 197, 192]            --\n",
            "│    │    └─Identity: 3-72               [32, 197, 192]            --\n",
            "│    └─Block: 2-12                       [32, 197, 192]            --\n",
            "│    │    └─norm1.weight                                           ├─192\n",
            "│    │    └─norm1.bias                                             ├─192\n",
            "│    │    └─attn.qkv.weight                                        ├─110,592\n",
            "│    │    └─attn.qkv.bias                                          ├─576\n",
            "│    │    └─attn.proj.weight                                       ├─36,864\n",
            "│    │    └─attn.proj.bias                                         ├─192\n",
            "│    │    └─norm2.weight                                           ├─192\n",
            "│    │    └─norm2.bias                                             ├─192\n",
            "│    │    └─mlp.fc1.weight                                         ├─147,456\n",
            "│    │    └─mlp.fc1.bias                                           ├─768\n",
            "│    │    └─mlp.fc2.weight                                         ├─147,456\n",
            "│    │    └─mlp.fc2.bias                                           └─192\n",
            "│    │    └─LayerNorm: 3-73              [32, 197, 192]            384\n",
            "│    │    │    └─weight                                            ├─192\n",
            "│    │    │    └─bias                                              └─192\n",
            "│    │    └─Attention: 3-74              [32, 197, 192]            148,224\n",
            "│    │    │    └─qkv.weight                                        ├─110,592\n",
            "│    │    │    └─qkv.bias                                          ├─576\n",
            "│    │    │    └─proj.weight                                       ├─36,864\n",
            "│    │    │    └─proj.bias                                         └─192\n",
            "│    │    └─Identity: 3-75               [32, 197, 192]            --\n",
            "│    │    └─Identity: 3-76               [32, 197, 192]            --\n",
            "│    │    └─LayerNorm: 3-77              [32, 197, 192]            384\n",
            "│    │    │    └─weight                                            ├─192\n",
            "│    │    │    └─bias                                              └─192\n",
            "│    │    └─Mlp: 3-78                    [32, 197, 192]            295,872\n",
            "│    │    │    └─fc1.weight                                        ├─147,456\n",
            "│    │    │    └─fc1.bias                                          ├─768\n",
            "│    │    │    └─fc2.weight                                        ├─147,456\n",
            "│    │    │    └─fc2.bias                                          └─192\n",
            "│    │    └─Identity: 3-79               [32, 197, 192]            --\n",
            "│    │    └─Identity: 3-80               [32, 197, 192]            --\n",
            "│    └─Block: 2-13                       [32, 197, 192]            --\n",
            "│    │    └─norm1.weight                                           ├─192\n",
            "│    │    └─norm1.bias                                             ├─192\n",
            "│    │    └─attn.qkv.weight                                        ├─110,592\n",
            "│    │    └─attn.qkv.bias                                          ├─576\n",
            "│    │    └─attn.proj.weight                                       ├─36,864\n",
            "│    │    └─attn.proj.bias                                         ├─192\n",
            "│    │    └─norm2.weight                                           ├─192\n",
            "│    │    └─norm2.bias                                             ├─192\n",
            "│    │    └─mlp.fc1.weight                                         ├─147,456\n",
            "│    │    └─mlp.fc1.bias                                           ├─768\n",
            "│    │    └─mlp.fc2.weight                                         ├─147,456\n",
            "│    │    └─mlp.fc2.bias                                           └─192\n",
            "│    │    └─LayerNorm: 3-81              [32, 197, 192]            384\n",
            "│    │    │    └─weight                                            ├─192\n",
            "│    │    │    └─bias                                              └─192\n",
            "│    │    └─Attention: 3-82              [32, 197, 192]            148,224\n",
            "│    │    │    └─qkv.weight                                        ├─110,592\n",
            "│    │    │    └─qkv.bias                                          ├─576\n",
            "│    │    │    └─proj.weight                                       ├─36,864\n",
            "│    │    │    └─proj.bias                                         └─192\n",
            "│    │    └─Identity: 3-83               [32, 197, 192]            --\n",
            "│    │    └─Identity: 3-84               [32, 197, 192]            --\n",
            "│    │    └─LayerNorm: 3-85              [32, 197, 192]            384\n",
            "│    │    │    └─weight                                            ├─192\n",
            "│    │    │    └─bias                                              └─192\n",
            "│    │    └─Mlp: 3-86                    [32, 197, 192]            295,872\n",
            "│    │    │    └─fc1.weight                                        ├─147,456\n",
            "│    │    │    └─fc1.bias                                          ├─768\n",
            "│    │    │    └─fc2.weight                                        ├─147,456\n",
            "│    │    │    └─fc2.bias                                          └─192\n",
            "│    │    └─Identity: 3-87               [32, 197, 192]            --\n",
            "│    │    └─Identity: 3-88               [32, 197, 192]            --\n",
            "│    └─Block: 2-14                       [32, 197, 192]            --\n",
            "│    │    └─norm1.weight                                           ├─192\n",
            "│    │    └─norm1.bias                                             ├─192\n",
            "│    │    └─attn.qkv.weight                                        ├─110,592\n",
            "│    │    └─attn.qkv.bias                                          ├─576\n",
            "│    │    └─attn.proj.weight                                       ├─36,864\n",
            "│    │    └─attn.proj.bias                                         ├─192\n",
            "│    │    └─norm2.weight                                           ├─192\n",
            "│    │    └─norm2.bias                                             ├─192\n",
            "│    │    └─mlp.fc1.weight                                         ├─147,456\n",
            "│    │    └─mlp.fc1.bias                                           ├─768\n",
            "│    │    └─mlp.fc2.weight                                         ├─147,456\n",
            "│    │    └─mlp.fc2.bias                                           └─192\n",
            "│    │    └─LayerNorm: 3-89              [32, 197, 192]            384\n",
            "│    │    │    └─weight                                            ├─192\n",
            "│    │    │    └─bias                                              └─192\n",
            "│    │    └─Attention: 3-90              [32, 197, 192]            148,224\n",
            "│    │    │    └─qkv.weight                                        ├─110,592\n",
            "│    │    │    └─qkv.bias                                          ├─576\n",
            "│    │    │    └─proj.weight                                       ├─36,864\n",
            "│    │    │    └─proj.bias                                         └─192\n",
            "│    │    └─Identity: 3-91               [32, 197, 192]            --\n",
            "│    │    └─Identity: 3-92               [32, 197, 192]            --\n",
            "│    │    └─LayerNorm: 3-93              [32, 197, 192]            384\n",
            "│    │    │    └─weight                                            ├─192\n",
            "│    │    │    └─bias                                              └─192\n",
            "│    │    └─Mlp: 3-94                    [32, 197, 192]            295,872\n",
            "│    │    │    └─fc1.weight                                        ├─147,456\n",
            "│    │    │    └─fc1.bias                                          ├─768\n",
            "│    │    │    └─fc2.weight                                        ├─147,456\n",
            "│    │    │    └─fc2.bias                                          └─192\n",
            "│    │    └─Identity: 3-95               [32, 197, 192]            --\n",
            "│    │    └─Identity: 3-96               [32, 197, 192]            --\n",
            "├─LayerNorm: 1-6                         [32, 197, 192]            384\n",
            "│    └─weight                                                      ├─192\n",
            "│    └─bias                                                        └─192\n",
            "├─Identity: 1-7                          [32, 192]                 --\n",
            "├─Dropout: 1-8                           [32, 192]                 --\n",
            "├─Linear: 1-9                            [32, 9]                   1,737\n",
            "│    └─weight                                                      ├─1,728\n",
            "│    └─bias                                                        └─9\n",
            "==========================================================================================\n",
            "Total params: 5,526,153\n",
            "Trainable params: 5,526,153\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (Units.GIGABYTES): 1.10\n",
            "==========================================================================================\n",
            "Input size (MB): 19.27\n",
            "Forward/backward pass size (MB): 1297.47\n",
            "Params size (MB): 21.95\n",
            "Estimated Total Size (MB): 1338.69\n",
            "==========================================================================================\n",
            "==========================================================================================\n",
            "Layer (type:depth-idx)                   Output Shape              Param #\n",
            "==========================================================================================\n",
            "VisionTransformer                        [32, 9]                   38,016\n",
            "├─cls_token                                                        ├─192\n",
            "├─pos_embed                                                        └─37,824\n",
            "├─PatchEmbed: 1-1                        [32, 196, 192]            --\n",
            "│    └─proj.weight                                                 ├─147,456\n",
            "│    └─proj.bias                                                   └─192\n",
            "│    └─Conv2d: 2-1                       [32, 192, 14, 14]         147,648\n",
            "│    │    └─weight                                                 ├─147,456\n",
            "│    │    └─bias                                                   └─192\n",
            "│    └─Identity: 2-2                     [32, 196, 192]            --\n",
            "├─Dropout: 1-2                           [32, 197, 192]            --\n",
            "├─Identity: 1-3                          [32, 197, 192]            --\n",
            "├─Identity: 1-4                          [32, 197, 192]            --\n",
            "├─Sequential: 1-5                        [32, 197, 192]            --\n",
            "│    └─0.norm1.weight                                              ├─192\n",
            "│    └─0.norm1.bias                                                ├─192\n",
            "│    └─0.attn.qkv.weight                                           ├─110,592\n",
            "│    └─0.attn.qkv.bias                                             ├─576\n",
            "│    └─0.attn.proj.weight                                          ├─36,864\n",
            "│    └─0.attn.proj.bias                                            ├─192\n",
            "│    └─0.norm2.weight                                              ├─192\n",
            "│    └─0.norm2.bias                                                ├─192\n",
            "│    └─0.mlp.fc1.weight                                            ├─147,456\n",
            "│    └─0.mlp.fc1.bias                                              ├─768\n",
            "│    └─0.mlp.fc2.weight                                            ├─147,456\n",
            "│    └─0.mlp.fc2.bias                                              ├─192\n",
            "│    └─1.norm1.weight                                              ├─192\n",
            "│    └─1.norm1.bias                                                ├─192\n",
            "│    └─1.attn.qkv.weight                                           ├─110,592\n",
            "│    └─1.attn.qkv.bias                                             ├─576\n",
            "│    └─1.attn.proj.weight                                          ├─36,864\n",
            "│    └─1.attn.proj.bias                                            ├─192\n",
            "│    └─1.norm2.weight                                              ├─192\n",
            "│    └─1.norm2.bias                                                ├─192\n",
            "│    └─1.mlp.fc1.weight                                            ├─147,456\n",
            "│    └─1.mlp.fc1.bias                                              ├─768\n",
            "│    └─1.mlp.fc2.weight                                            ├─147,456\n",
            "│    └─1.mlp.fc2.bias                                              ├─192\n",
            "│    └─2.norm1.weight                                              ├─192\n",
            "│    └─2.norm1.bias                                                ├─192\n",
            "│    └─2.attn.qkv.weight                                           ├─110,592\n",
            "│    └─2.attn.qkv.bias                                             ├─576\n",
            "│    └─2.attn.proj.weight                                          ├─36,864\n",
            "│    └─2.attn.proj.bias                                            ├─192\n",
            "│    └─2.norm2.weight                                              ├─192\n",
            "│    └─2.norm2.bias                                                ├─192\n",
            "│    └─2.mlp.fc1.weight                                            ├─147,456\n",
            "│    └─2.mlp.fc1.bias                                              ├─768\n",
            "│    └─2.mlp.fc2.weight                                            ├─147,456\n",
            "│    └─2.mlp.fc2.bias                                              ├─192\n",
            "│    └─3.norm1.weight                                              ├─192\n",
            "│    └─3.norm1.bias                                                ├─192\n",
            "│    └─3.attn.qkv.weight                                           ├─110,592\n",
            "│    └─3.attn.qkv.bias                                             ├─576\n",
            "│    └─3.attn.proj.weight                                          ├─36,864\n",
            "│    └─3.attn.proj.bias                                            ├─192\n",
            "│    └─3.norm2.weight                                              ├─192\n",
            "│    └─3.norm2.bias                                                ├─192\n",
            "│    └─3.mlp.fc1.weight                                            ├─147,456\n",
            "│    └─3.mlp.fc1.bias                                              ├─768\n",
            "│    └─3.mlp.fc2.weight                                            ├─147,456\n",
            "│    └─3.mlp.fc2.bias                                              ├─192\n",
            "│    └─4.norm1.weight                                              ├─192\n",
            "│    └─4.norm1.bias                                                ├─192\n",
            "│    └─4.attn.qkv.weight                                           ├─110,592\n",
            "│    └─4.attn.qkv.bias                                             ├─576\n",
            "│    └─4.attn.proj.weight                                          ├─36,864\n",
            "│    └─4.attn.proj.bias                                            ├─192\n",
            "│    └─4.norm2.weight                                              ├─192\n",
            "│    └─4.norm2.bias                                                ├─192\n",
            "│    └─4.mlp.fc1.weight                                            ├─147,456\n",
            "│    └─4.mlp.fc1.bias                                              ├─768\n",
            "│    └─4.mlp.fc2.weight                                            ├─147,456\n",
            "│    └─4.mlp.fc2.bias                                              ├─192\n",
            "│    └─5.norm1.weight                                              ├─192\n",
            "│    └─5.norm1.bias                                                ├─192\n",
            "│    └─5.attn.qkv.weight                                           ├─110,592\n",
            "│    └─5.attn.qkv.bias                                             ├─576\n",
            "│    └─5.attn.proj.weight                                          ├─36,864\n",
            "│    └─5.attn.proj.bias                                            ├─192\n",
            "│    └─5.norm2.weight                                              ├─192\n",
            "│    └─5.norm2.bias                                                ├─192\n",
            "│    └─5.mlp.fc1.weight                                            ├─147,456\n",
            "│    └─5.mlp.fc1.bias                                              ├─768\n",
            "│    └─5.mlp.fc2.weight                                            ├─147,456\n",
            "│    └─5.mlp.fc2.bias                                              ├─192\n",
            "│    └─6.norm1.weight                                              ├─192\n",
            "│    └─6.norm1.bias                                                ├─192\n",
            "│    └─6.attn.qkv.weight                                           ├─110,592\n",
            "│    └─6.attn.qkv.bias                                             ├─576\n",
            "│    └─6.attn.proj.weight                                          ├─36,864\n",
            "│    └─6.attn.proj.bias                                            ├─192\n",
            "│    └─6.norm2.weight                                              ├─192\n",
            "│    └─6.norm2.bias                                                ├─192\n",
            "│    └─6.mlp.fc1.weight                                            ├─147,456\n",
            "│    └─6.mlp.fc1.bias                                              ├─768\n",
            "│    └─6.mlp.fc2.weight                                            ├─147,456\n",
            "│    └─6.mlp.fc2.bias                                              ├─192\n",
            "│    └─7.norm1.weight                                              ├─192\n",
            "│    └─7.norm1.bias                                                ├─192\n",
            "│    └─7.attn.qkv.weight                                           ├─110,592\n",
            "│    └─7.attn.qkv.bias                                             ├─576\n",
            "│    └─7.attn.proj.weight                                          ├─36,864\n",
            "│    └─7.attn.proj.bias                                            ├─192\n",
            "│    └─7.norm2.weight                                              ├─192\n",
            "│    └─7.norm2.bias                                                ├─192\n",
            "│    └─7.mlp.fc1.weight                                            ├─147,456\n",
            "│    └─7.mlp.fc1.bias                                              ├─768\n",
            "│    └─7.mlp.fc2.weight                                            ├─147,456\n",
            "│    └─7.mlp.fc2.bias                                              ├─192\n",
            "│    └─8.norm1.weight                                              ├─192\n",
            "│    └─8.norm1.bias                                                ├─192\n",
            "│    └─8.attn.qkv.weight                                           ├─110,592\n",
            "│    └─8.attn.qkv.bias                                             ├─576\n",
            "│    └─8.attn.proj.weight                                          ├─36,864\n",
            "│    └─8.attn.proj.bias                                            ├─192\n",
            "│    └─8.norm2.weight                                              ├─192\n",
            "│    └─8.norm2.bias                                                ├─192\n",
            "│    └─8.mlp.fc1.weight                                            ├─147,456\n",
            "│    └─8.mlp.fc1.bias                                              ├─768\n",
            "│    └─8.mlp.fc2.weight                                            ├─147,456\n",
            "│    └─8.mlp.fc2.bias                                              ├─192\n",
            "│    └─9.norm1.weight                                              ├─192\n",
            "│    └─9.norm1.bias                                                ├─192\n",
            "│    └─9.attn.qkv.weight                                           ├─110,592\n",
            "│    └─9.attn.qkv.bias                                             ├─576\n",
            "│    └─9.attn.proj.weight                                          ├─36,864\n",
            "│    └─9.attn.proj.bias                                            ├─192\n",
            "│    └─9.norm2.weight                                              ├─192\n",
            "│    └─9.norm2.bias                                                ├─192\n",
            "│    └─9.mlp.fc1.weight                                            ├─147,456\n",
            "│    └─9.mlp.fc1.bias                                              ├─768\n",
            "│    └─9.mlp.fc2.weight                                            ├─147,456\n",
            "│    └─9.mlp.fc2.bias                                              ├─192\n",
            "│    └─10.norm1.weight                                             ├─192\n",
            "│    └─10.norm1.bias                                               ├─192\n",
            "│    └─10.attn.qkv.weight                                          ├─110,592\n",
            "│    └─10.attn.qkv.bias                                            ├─576\n",
            "│    └─10.attn.proj.weight                                         ├─36,864\n",
            "│    └─10.attn.proj.bias                                           ├─192\n",
            "│    └─10.norm2.weight                                             ├─192\n",
            "│    └─10.norm2.bias                                               ├─192\n",
            "│    └─10.mlp.fc1.weight                                           ├─147,456\n",
            "│    └─10.mlp.fc1.bias                                             ├─768\n",
            "│    └─10.mlp.fc2.weight                                           ├─147,456\n",
            "│    └─10.mlp.fc2.bias                                             ├─192\n",
            "│    └─11.norm1.weight                                             ├─192\n",
            "│    └─11.norm1.bias                                               ├─192\n",
            "│    └─11.attn.qkv.weight                                          ├─110,592\n",
            "│    └─11.attn.qkv.bias                                            ├─576\n",
            "│    └─11.attn.proj.weight                                         ├─36,864\n",
            "│    └─11.attn.proj.bias                                           ├─192\n",
            "│    └─11.norm2.weight                                             ├─192\n",
            "│    └─11.norm2.bias                                               ├─192\n",
            "│    └─11.mlp.fc1.weight                                           ├─147,456\n",
            "│    └─11.mlp.fc1.bias                                             ├─768\n",
            "│    └─11.mlp.fc2.weight                                           ├─147,456\n",
            "│    └─11.mlp.fc2.bias                                             └─192\n",
            "│    └─Block: 2-3                        [32, 197, 192]            --\n",
            "│    │    └─norm1.weight                                           ├─192\n",
            "│    │    └─norm1.bias                                             ├─192\n",
            "│    │    └─attn.qkv.weight                                        ├─110,592\n",
            "│    │    └─attn.qkv.bias                                          ├─576\n",
            "│    │    └─attn.proj.weight                                       ├─36,864\n",
            "│    │    └─attn.proj.bias                                         ├─192\n",
            "│    │    └─norm2.weight                                           ├─192\n",
            "│    │    └─norm2.bias                                             ├─192\n",
            "│    │    └─mlp.fc1.weight                                         ├─147,456\n",
            "│    │    └─mlp.fc1.bias                                           ├─768\n",
            "│    │    └─mlp.fc2.weight                                         ├─147,456\n",
            "│    │    └─mlp.fc2.bias                                           └─192\n",
            "│    │    └─LayerNorm: 3-1               [32, 197, 192]            384\n",
            "│    │    │    └─weight                                            ├─192\n",
            "│    │    │    └─bias                                              └─192\n",
            "│    │    └─Attention: 3-2               [32, 197, 192]            148,224\n",
            "│    │    │    └─qkv.weight                                        ├─110,592\n",
            "│    │    │    └─qkv.bias                                          ├─576\n",
            "│    │    │    └─proj.weight                                       ├─36,864\n",
            "│    │    │    └─proj.bias                                         └─192\n",
            "│    │    └─Identity: 3-3                [32, 197, 192]            --\n",
            "│    │    └─Identity: 3-4                [32, 197, 192]            --\n",
            "│    │    └─LayerNorm: 3-5               [32, 197, 192]            384\n",
            "│    │    │    └─weight                                            ├─192\n",
            "│    │    │    └─bias                                              └─192\n",
            "│    │    └─Mlp: 3-6                     [32, 197, 192]            295,872\n",
            "│    │    │    └─fc1.weight                                        ├─147,456\n",
            "│    │    │    └─fc1.bias                                          ├─768\n",
            "│    │    │    └─fc2.weight                                        ├─147,456\n",
            "│    │    │    └─fc2.bias                                          └─192\n",
            "│    │    └─Identity: 3-7                [32, 197, 192]            --\n",
            "│    │    └─Identity: 3-8                [32, 197, 192]            --\n",
            "│    └─Block: 2-4                        [32, 197, 192]            --\n",
            "│    │    └─norm1.weight                                           ├─192\n",
            "│    │    └─norm1.bias                                             ├─192\n",
            "│    │    └─attn.qkv.weight                                        ├─110,592\n",
            "│    │    └─attn.qkv.bias                                          ├─576\n",
            "│    │    └─attn.proj.weight                                       ├─36,864\n",
            "│    │    └─attn.proj.bias                                         ├─192\n",
            "│    │    └─norm2.weight                                           ├─192\n",
            "│    │    └─norm2.bias                                             ├─192\n",
            "│    │    └─mlp.fc1.weight                                         ├─147,456\n",
            "│    │    └─mlp.fc1.bias                                           ├─768\n",
            "│    │    └─mlp.fc2.weight                                         ├─147,456\n",
            "│    │    └─mlp.fc2.bias                                           └─192\n",
            "│    │    └─LayerNorm: 3-9               [32, 197, 192]            384\n",
            "│    │    │    └─weight                                            ├─192\n",
            "│    │    │    └─bias                                              └─192\n",
            "│    │    └─Attention: 3-10              [32, 197, 192]            148,224\n",
            "│    │    │    └─qkv.weight                                        ├─110,592\n",
            "│    │    │    └─qkv.bias                                          ├─576\n",
            "│    │    │    └─proj.weight                                       ├─36,864\n",
            "│    │    │    └─proj.bias                                         └─192\n",
            "│    │    └─Identity: 3-11               [32, 197, 192]            --\n",
            "│    │    └─Identity: 3-12               [32, 197, 192]            --\n",
            "│    │    └─LayerNorm: 3-13              [32, 197, 192]            384\n",
            "│    │    │    └─weight                                            ├─192\n",
            "│    │    │    └─bias                                              └─192\n",
            "│    │    └─Mlp: 3-14                    [32, 197, 192]            295,872\n",
            "│    │    │    └─fc1.weight                                        ├─147,456\n",
            "│    │    │    └─fc1.bias                                          ├─768\n",
            "│    │    │    └─fc2.weight                                        ├─147,456\n",
            "│    │    │    └─fc2.bias                                          └─192\n",
            "│    │    └─Identity: 3-15               [32, 197, 192]            --\n",
            "│    │    └─Identity: 3-16               [32, 197, 192]            --\n",
            "│    └─Block: 2-5                        [32, 197, 192]            --\n",
            "│    │    └─norm1.weight                                           ├─192\n",
            "│    │    └─norm1.bias                                             ├─192\n",
            "│    │    └─attn.qkv.weight                                        ├─110,592\n",
            "│    │    └─attn.qkv.bias                                          ├─576\n",
            "│    │    └─attn.proj.weight                                       ├─36,864\n",
            "│    │    └─attn.proj.bias                                         ├─192\n",
            "│    │    └─norm2.weight                                           ├─192\n",
            "│    │    └─norm2.bias                                             ├─192\n",
            "│    │    └─mlp.fc1.weight                                         ├─147,456\n",
            "│    │    └─mlp.fc1.bias                                           ├─768\n",
            "│    │    └─mlp.fc2.weight                                         ├─147,456\n",
            "│    │    └─mlp.fc2.bias                                           └─192\n",
            "│    │    └─LayerNorm: 3-17              [32, 197, 192]            384\n",
            "│    │    │    └─weight                                            ├─192\n",
            "│    │    │    └─bias                                              └─192\n",
            "│    │    └─Attention: 3-18              [32, 197, 192]            148,224\n",
            "│    │    │    └─qkv.weight                                        ├─110,592\n",
            "│    │    │    └─qkv.bias                                          ├─576\n",
            "│    │    │    └─proj.weight                                       ├─36,864\n",
            "│    │    │    └─proj.bias                                         └─192\n",
            "│    │    └─Identity: 3-19               [32, 197, 192]            --\n",
            "│    │    └─Identity: 3-20               [32, 197, 192]            --\n",
            "│    │    └─LayerNorm: 3-21              [32, 197, 192]            384\n",
            "│    │    │    └─weight                                            ├─192\n",
            "│    │    │    └─bias                                              └─192\n",
            "│    │    └─Mlp: 3-22                    [32, 197, 192]            295,872\n",
            "│    │    │    └─fc1.weight                                        ├─147,456\n",
            "│    │    │    └─fc1.bias                                          ├─768\n",
            "│    │    │    └─fc2.weight                                        ├─147,456\n",
            "│    │    │    └─fc2.bias                                          └─192\n",
            "│    │    └─Identity: 3-23               [32, 197, 192]            --\n",
            "│    │    └─Identity: 3-24               [32, 197, 192]            --\n",
            "│    └─Block: 2-6                        [32, 197, 192]            --\n",
            "│    │    └─norm1.weight                                           ├─192\n",
            "│    │    └─norm1.bias                                             ├─192\n",
            "│    │    └─attn.qkv.weight                                        ├─110,592\n",
            "│    │    └─attn.qkv.bias                                          ├─576\n",
            "│    │    └─attn.proj.weight                                       ├─36,864\n",
            "│    │    └─attn.proj.bias                                         ├─192\n",
            "│    │    └─norm2.weight                                           ├─192\n",
            "│    │    └─norm2.bias                                             ├─192\n",
            "│    │    └─mlp.fc1.weight                                         ├─147,456\n",
            "│    │    └─mlp.fc1.bias                                           ├─768\n",
            "│    │    └─mlp.fc2.weight                                         ├─147,456\n",
            "│    │    └─mlp.fc2.bias                                           └─192\n",
            "│    │    └─LayerNorm: 3-25              [32, 197, 192]            384\n",
            "│    │    │    └─weight                                            ├─192\n",
            "│    │    │    └─bias                                              └─192\n",
            "│    │    └─Attention: 3-26              [32, 197, 192]            148,224\n",
            "│    │    │    └─qkv.weight                                        ├─110,592\n",
            "│    │    │    └─qkv.bias                                          ├─576\n",
            "│    │    │    └─proj.weight                                       ├─36,864\n",
            "│    │    │    └─proj.bias                                         └─192\n",
            "│    │    └─Identity: 3-27               [32, 197, 192]            --\n",
            "│    │    └─Identity: 3-28               [32, 197, 192]            --\n",
            "│    │    └─LayerNorm: 3-29              [32, 197, 192]            384\n",
            "│    │    │    └─weight                                            ├─192\n",
            "│    │    │    └─bias                                              └─192\n",
            "│    │    └─Mlp: 3-30                    [32, 197, 192]            295,872\n",
            "│    │    │    └─fc1.weight                                        ├─147,456\n",
            "│    │    │    └─fc1.bias                                          ├─768\n",
            "│    │    │    └─fc2.weight                                        ├─147,456\n",
            "│    │    │    └─fc2.bias                                          └─192\n",
            "│    │    └─Identity: 3-31               [32, 197, 192]            --\n",
            "│    │    └─Identity: 3-32               [32, 197, 192]            --\n",
            "│    └─Block: 2-7                        [32, 197, 192]            --\n",
            "│    │    └─norm1.weight                                           ├─192\n",
            "│    │    └─norm1.bias                                             ├─192\n",
            "│    │    └─attn.qkv.weight                                        ├─110,592\n",
            "│    │    └─attn.qkv.bias                                          ├─576\n",
            "│    │    └─attn.proj.weight                                       ├─36,864\n",
            "│    │    └─attn.proj.bias                                         ├─192\n",
            "│    │    └─norm2.weight                                           ├─192\n",
            "│    │    └─norm2.bias                                             ├─192\n",
            "│    │    └─mlp.fc1.weight                                         ├─147,456\n",
            "│    │    └─mlp.fc1.bias                                           ├─768\n",
            "│    │    └─mlp.fc2.weight                                         ├─147,456\n",
            "│    │    └─mlp.fc2.bias                                           └─192\n",
            "│    │    └─LayerNorm: 3-33              [32, 197, 192]            384\n",
            "│    │    │    └─weight                                            ├─192\n",
            "│    │    │    └─bias                                              └─192\n",
            "│    │    └─Attention: 3-34              [32, 197, 192]            148,224\n",
            "│    │    │    └─qkv.weight                                        ├─110,592\n",
            "│    │    │    └─qkv.bias                                          ├─576\n",
            "│    │    │    └─proj.weight                                       ├─36,864\n",
            "│    │    │    └─proj.bias                                         └─192\n",
            "│    │    └─Identity: 3-35               [32, 197, 192]            --\n",
            "│    │    └─Identity: 3-36               [32, 197, 192]            --\n",
            "│    │    └─LayerNorm: 3-37              [32, 197, 192]            384\n",
            "│    │    │    └─weight                                            ├─192\n",
            "│    │    │    └─bias                                              └─192\n",
            "│    │    └─Mlp: 3-38                    [32, 197, 192]            295,872\n",
            "│    │    │    └─fc1.weight                                        ├─147,456\n",
            "│    │    │    └─fc1.bias                                          ├─768\n",
            "│    │    │    └─fc2.weight                                        ├─147,456\n",
            "│    │    │    └─fc2.bias                                          └─192\n",
            "│    │    └─Identity: 3-39               [32, 197, 192]            --\n",
            "│    │    └─Identity: 3-40               [32, 197, 192]            --\n",
            "│    └─Block: 2-8                        [32, 197, 192]            --\n",
            "│    │    └─norm1.weight                                           ├─192\n",
            "│    │    └─norm1.bias                                             ├─192\n",
            "│    │    └─attn.qkv.weight                                        ├─110,592\n",
            "│    │    └─attn.qkv.bias                                          ├─576\n",
            "│    │    └─attn.proj.weight                                       ├─36,864\n",
            "│    │    └─attn.proj.bias                                         ├─192\n",
            "│    │    └─norm2.weight                                           ├─192\n",
            "│    │    └─norm2.bias                                             ├─192\n",
            "│    │    └─mlp.fc1.weight                                         ├─147,456\n",
            "│    │    └─mlp.fc1.bias                                           ├─768\n",
            "│    │    └─mlp.fc2.weight                                         ├─147,456\n",
            "│    │    └─mlp.fc2.bias                                           └─192\n",
            "│    │    └─LayerNorm: 3-41              [32, 197, 192]            384\n",
            "│    │    │    └─weight                                            ├─192\n",
            "│    │    │    └─bias                                              └─192\n",
            "│    │    └─Attention: 3-42              [32, 197, 192]            148,224\n",
            "│    │    │    └─qkv.weight                                        ├─110,592\n",
            "│    │    │    └─qkv.bias                                          ├─576\n",
            "│    │    │    └─proj.weight                                       ├─36,864\n",
            "│    │    │    └─proj.bias                                         └─192\n",
            "│    │    └─Identity: 3-43               [32, 197, 192]            --\n",
            "│    │    └─Identity: 3-44               [32, 197, 192]            --\n",
            "│    │    └─LayerNorm: 3-45              [32, 197, 192]            384\n",
            "│    │    │    └─weight                                            ├─192\n",
            "│    │    │    └─bias                                              └─192\n",
            "│    │    └─Mlp: 3-46                    [32, 197, 192]            295,872\n",
            "│    │    │    └─fc1.weight                                        ├─147,456\n",
            "│    │    │    └─fc1.bias                                          ├─768\n",
            "│    │    │    └─fc2.weight                                        ├─147,456\n",
            "│    │    │    └─fc2.bias                                          └─192\n",
            "│    │    └─Identity: 3-47               [32, 197, 192]            --\n",
            "│    │    └─Identity: 3-48               [32, 197, 192]            --\n",
            "│    └─Block: 2-9                        [32, 197, 192]            --\n",
            "│    │    └─norm1.weight                                           ├─192\n",
            "│    │    └─norm1.bias                                             ├─192\n",
            "│    │    └─attn.qkv.weight                                        ├─110,592\n",
            "│    │    └─attn.qkv.bias                                          ├─576\n",
            "│    │    └─attn.proj.weight                                       ├─36,864\n",
            "│    │    └─attn.proj.bias                                         ├─192\n",
            "│    │    └─norm2.weight                                           ├─192\n",
            "│    │    └─norm2.bias                                             ├─192\n",
            "│    │    └─mlp.fc1.weight                                         ├─147,456\n",
            "│    │    └─mlp.fc1.bias                                           ├─768\n",
            "│    │    └─mlp.fc2.weight                                         ├─147,456\n",
            "│    │    └─mlp.fc2.bias                                           └─192\n",
            "│    │    └─LayerNorm: 3-49              [32, 197, 192]            384\n",
            "│    │    │    └─weight                                            ├─192\n",
            "│    │    │    └─bias                                              └─192\n",
            "│    │    └─Attention: 3-50              [32, 197, 192]            148,224\n",
            "│    │    │    └─qkv.weight                                        ├─110,592\n",
            "│    │    │    └─qkv.bias                                          ├─576\n",
            "│    │    │    └─proj.weight                                       ├─36,864\n",
            "│    │    │    └─proj.bias                                         └─192\n",
            "│    │    └─Identity: 3-51               [32, 197, 192]            --\n",
            "│    │    └─Identity: 3-52               [32, 197, 192]            --\n",
            "│    │    └─LayerNorm: 3-53              [32, 197, 192]            384\n",
            "│    │    │    └─weight                                            ├─192\n",
            "│    │    │    └─bias                                              └─192\n",
            "│    │    └─Mlp: 3-54                    [32, 197, 192]            295,872\n",
            "│    │    │    └─fc1.weight                                        ├─147,456\n",
            "│    │    │    └─fc1.bias                                          ├─768\n",
            "│    │    │    └─fc2.weight                                        ├─147,456\n",
            "│    │    │    └─fc2.bias                                          └─192\n",
            "│    │    └─Identity: 3-55               [32, 197, 192]            --\n",
            "│    │    └─Identity: 3-56               [32, 197, 192]            --\n",
            "│    └─Block: 2-10                       [32, 197, 192]            --\n",
            "│    │    └─norm1.weight                                           ├─192\n",
            "│    │    └─norm1.bias                                             ├─192\n",
            "│    │    └─attn.qkv.weight                                        ├─110,592\n",
            "│    │    └─attn.qkv.bias                                          ├─576\n",
            "│    │    └─attn.proj.weight                                       ├─36,864\n",
            "│    │    └─attn.proj.bias                                         ├─192\n",
            "│    │    └─norm2.weight                                           ├─192\n",
            "│    │    └─norm2.bias                                             ├─192\n",
            "│    │    └─mlp.fc1.weight                                         ├─147,456\n",
            "│    │    └─mlp.fc1.bias                                           ├─768\n",
            "│    │    └─mlp.fc2.weight                                         ├─147,456\n",
            "│    │    └─mlp.fc2.bias                                           └─192\n",
            "│    │    └─LayerNorm: 3-57              [32, 197, 192]            384\n",
            "│    │    │    └─weight                                            ├─192\n",
            "│    │    │    └─bias                                              └─192\n",
            "│    │    └─Attention: 3-58              [32, 197, 192]            148,224\n",
            "│    │    │    └─qkv.weight                                        ├─110,592\n",
            "│    │    │    └─qkv.bias                                          ├─576\n",
            "│    │    │    └─proj.weight                                       ├─36,864\n",
            "│    │    │    └─proj.bias                                         └─192\n",
            "│    │    └─Identity: 3-59               [32, 197, 192]            --\n",
            "│    │    └─Identity: 3-60               [32, 197, 192]            --\n",
            "│    │    └─LayerNorm: 3-61              [32, 197, 192]            384\n",
            "│    │    │    └─weight                                            ├─192\n",
            "│    │    │    └─bias                                              └─192\n",
            "│    │    └─Mlp: 3-62                    [32, 197, 192]            295,872\n",
            "│    │    │    └─fc1.weight                                        ├─147,456\n",
            "│    │    │    └─fc1.bias                                          ├─768\n",
            "│    │    │    └─fc2.weight                                        ├─147,456\n",
            "│    │    │    └─fc2.bias                                          └─192\n",
            "│    │    └─Identity: 3-63               [32, 197, 192]            --\n",
            "│    │    └─Identity: 3-64               [32, 197, 192]            --\n",
            "│    └─Block: 2-11                       [32, 197, 192]            --\n",
            "│    │    └─norm1.weight                                           ├─192\n",
            "│    │    └─norm1.bias                                             ├─192\n",
            "│    │    └─attn.qkv.weight                                        ├─110,592\n",
            "│    │    └─attn.qkv.bias                                          ├─576\n",
            "│    │    └─attn.proj.weight                                       ├─36,864\n",
            "│    │    └─attn.proj.bias                                         ├─192\n",
            "│    │    └─norm2.weight                                           ├─192\n",
            "│    │    └─norm2.bias                                             ├─192\n",
            "│    │    └─mlp.fc1.weight                                         ├─147,456\n",
            "│    │    └─mlp.fc1.bias                                           ├─768\n",
            "│    │    └─mlp.fc2.weight                                         ├─147,456\n",
            "│    │    └─mlp.fc2.bias                                           └─192\n",
            "│    │    └─LayerNorm: 3-65              [32, 197, 192]            384\n",
            "│    │    │    └─weight                                            ├─192\n",
            "│    │    │    └─bias                                              └─192\n",
            "│    │    └─Attention: 3-66              [32, 197, 192]            148,224\n",
            "│    │    │    └─qkv.weight                                        ├─110,592\n",
            "│    │    │    └─qkv.bias                                          ├─576\n",
            "│    │    │    └─proj.weight                                       ├─36,864\n",
            "│    │    │    └─proj.bias                                         └─192\n",
            "│    │    └─Identity: 3-67               [32, 197, 192]            --\n",
            "│    │    └─Identity: 3-68               [32, 197, 192]            --\n",
            "│    │    └─LayerNorm: 3-69              [32, 197, 192]            384\n",
            "│    │    │    └─weight                                            ├─192\n",
            "│    │    │    └─bias                                              └─192\n",
            "│    │    └─Mlp: 3-70                    [32, 197, 192]            295,872\n",
            "│    │    │    └─fc1.weight                                        ├─147,456\n",
            "│    │    │    └─fc1.bias                                          ├─768\n",
            "│    │    │    └─fc2.weight                                        ├─147,456\n",
            "│    │    │    └─fc2.bias                                          └─192\n",
            "│    │    └─Identity: 3-71               [32, 197, 192]            --\n",
            "│    │    └─Identity: 3-72               [32, 197, 192]            --\n",
            "│    └─Block: 2-12                       [32, 197, 192]            --\n",
            "│    │    └─norm1.weight                                           ├─192\n",
            "│    │    └─norm1.bias                                             ├─192\n",
            "│    │    └─attn.qkv.weight                                        ├─110,592\n",
            "│    │    └─attn.qkv.bias                                          ├─576\n",
            "│    │    └─attn.proj.weight                                       ├─36,864\n",
            "│    │    └─attn.proj.bias                                         ├─192\n",
            "│    │    └─norm2.weight                                           ├─192\n",
            "│    │    └─norm2.bias                                             ├─192\n",
            "│    │    └─mlp.fc1.weight                                         ├─147,456\n",
            "│    │    └─mlp.fc1.bias                                           ├─768\n",
            "│    │    └─mlp.fc2.weight                                         ├─147,456\n",
            "│    │    └─mlp.fc2.bias                                           └─192\n",
            "│    │    └─LayerNorm: 3-73              [32, 197, 192]            384\n",
            "│    │    │    └─weight                                            ├─192\n",
            "│    │    │    └─bias                                              └─192\n",
            "│    │    └─Attention: 3-74              [32, 197, 192]            148,224\n",
            "│    │    │    └─qkv.weight                                        ├─110,592\n",
            "│    │    │    └─qkv.bias                                          ├─576\n",
            "│    │    │    └─proj.weight                                       ├─36,864\n",
            "│    │    │    └─proj.bias                                         └─192\n",
            "│    │    └─Identity: 3-75               [32, 197, 192]            --\n",
            "│    │    └─Identity: 3-76               [32, 197, 192]            --\n",
            "│    │    └─LayerNorm: 3-77              [32, 197, 192]            384\n",
            "│    │    │    └─weight                                            ├─192\n",
            "│    │    │    └─bias                                              └─192\n",
            "│    │    └─Mlp: 3-78                    [32, 197, 192]            295,872\n",
            "│    │    │    └─fc1.weight                                        ├─147,456\n",
            "│    │    │    └─fc1.bias                                          ├─768\n",
            "│    │    │    └─fc2.weight                                        ├─147,456\n",
            "│    │    │    └─fc2.bias                                          └─192\n",
            "│    │    └─Identity: 3-79               [32, 197, 192]            --\n",
            "│    │    └─Identity: 3-80               [32, 197, 192]            --\n",
            "│    └─Block: 2-13                       [32, 197, 192]            --\n",
            "│    │    └─norm1.weight                                           ├─192\n",
            "│    │    └─norm1.bias                                             ├─192\n",
            "│    │    └─attn.qkv.weight                                        ├─110,592\n",
            "│    │    └─attn.qkv.bias                                          ├─576\n",
            "│    │    └─attn.proj.weight                                       ├─36,864\n",
            "│    │    └─attn.proj.bias                                         ├─192\n",
            "│    │    └─norm2.weight                                           ├─192\n",
            "│    │    └─norm2.bias                                             ├─192\n",
            "│    │    └─mlp.fc1.weight                                         ├─147,456\n",
            "│    │    └─mlp.fc1.bias                                           ├─768\n",
            "│    │    └─mlp.fc2.weight                                         ├─147,456\n",
            "│    │    └─mlp.fc2.bias                                           └─192\n",
            "│    │    └─LayerNorm: 3-81              [32, 197, 192]            384\n",
            "│    │    │    └─weight                                            ├─192\n",
            "│    │    │    └─bias                                              └─192\n",
            "│    │    └─Attention: 3-82              [32, 197, 192]            148,224\n",
            "│    │    │    └─qkv.weight                                        ├─110,592\n",
            "│    │    │    └─qkv.bias                                          ├─576\n",
            "│    │    │    └─proj.weight                                       ├─36,864\n",
            "│    │    │    └─proj.bias                                         └─192\n",
            "│    │    └─Identity: 3-83               [32, 197, 192]            --\n",
            "│    │    └─Identity: 3-84               [32, 197, 192]            --\n",
            "│    │    └─LayerNorm: 3-85              [32, 197, 192]            384\n",
            "│    │    │    └─weight                                            ├─192\n",
            "│    │    │    └─bias                                              └─192\n",
            "│    │    └─Mlp: 3-86                    [32, 197, 192]            295,872\n",
            "│    │    │    └─fc1.weight                                        ├─147,456\n",
            "│    │    │    └─fc1.bias                                          ├─768\n",
            "│    │    │    └─fc2.weight                                        ├─147,456\n",
            "│    │    │    └─fc2.bias                                          └─192\n",
            "│    │    └─Identity: 3-87               [32, 197, 192]            --\n",
            "│    │    └─Identity: 3-88               [32, 197, 192]            --\n",
            "│    └─Block: 2-14                       [32, 197, 192]            --\n",
            "│    │    └─norm1.weight                                           ├─192\n",
            "│    │    └─norm1.bias                                             ├─192\n",
            "│    │    └─attn.qkv.weight                                        ├─110,592\n",
            "│    │    └─attn.qkv.bias                                          ├─576\n",
            "│    │    └─attn.proj.weight                                       ├─36,864\n",
            "│    │    └─attn.proj.bias                                         ├─192\n",
            "│    │    └─norm2.weight                                           ├─192\n",
            "│    │    └─norm2.bias                                             ├─192\n",
            "│    │    └─mlp.fc1.weight                                         ├─147,456\n",
            "│    │    └─mlp.fc1.bias                                           ├─768\n",
            "│    │    └─mlp.fc2.weight                                         ├─147,456\n",
            "│    │    └─mlp.fc2.bias                                           └─192\n",
            "│    │    └─LayerNorm: 3-89              [32, 197, 192]            384\n",
            "│    │    │    └─weight                                            ├─192\n",
            "│    │    │    └─bias                                              └─192\n",
            "│    │    └─Attention: 3-90              [32, 197, 192]            148,224\n",
            "│    │    │    └─qkv.weight                                        ├─110,592\n",
            "│    │    │    └─qkv.bias                                          ├─576\n",
            "│    │    │    └─proj.weight                                       ├─36,864\n",
            "│    │    │    └─proj.bias                                         └─192\n",
            "│    │    └─Identity: 3-91               [32, 197, 192]            --\n",
            "│    │    └─Identity: 3-92               [32, 197, 192]            --\n",
            "│    │    └─LayerNorm: 3-93              [32, 197, 192]            384\n",
            "│    │    │    └─weight                                            ├─192\n",
            "│    │    │    └─bias                                              └─192\n",
            "│    │    └─Mlp: 3-94                    [32, 197, 192]            295,872\n",
            "│    │    │    └─fc1.weight                                        ├─147,456\n",
            "│    │    │    └─fc1.bias                                          ├─768\n",
            "│    │    │    └─fc2.weight                                        ├─147,456\n",
            "│    │    │    └─fc2.bias                                          └─192\n",
            "│    │    └─Identity: 3-95               [32, 197, 192]            --\n",
            "│    │    └─Identity: 3-96               [32, 197, 192]            --\n",
            "├─LayerNorm: 1-6                         [32, 197, 192]            384\n",
            "│    └─weight                                                      ├─192\n",
            "│    └─bias                                                        └─192\n",
            "├─Identity: 1-7                          [32, 192]                 --\n",
            "├─Dropout: 1-8                           [32, 192]                 --\n",
            "├─Linear: 1-9                            [32, 9]                   1,737\n",
            "│    └─weight                                                      ├─1,728\n",
            "│    └─bias                                                        └─9\n",
            "==========================================================================================\n",
            "Total params: 5,526,153\n",
            "Trainable params: 5,526,153\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (Units.GIGABYTES): 1.10\n",
            "==========================================================================================\n",
            "Input size (MB): 19.27\n",
            "Forward/backward pass size (MB): 1297.47\n",
            "Params size (MB): 21.95\n",
            "Estimated Total Size (MB): 1338.69\n",
            "==========================================================================================\n"
          ]
        }
      ],
      "source": [
        "print(summary(model, input_size=(32, 3, 224, 224), verbose=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 301,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2XJWJO0JTIMT",
        "outputId": "245d678a-1a47-407c-c7a0-51a582b9262c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VisionTransformer(\n",
              "  (patch_embed): PatchEmbed(\n",
              "    (proj): Conv2d(3, 192, kernel_size=(16, 16), stride=(16, 16))\n",
              "    (norm): Identity()\n",
              "  )\n",
              "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
              "  (patch_drop): Identity()\n",
              "  (norm_pre): Identity()\n",
              "  (blocks): Sequential(\n",
              "    (0): Block(\n",
              "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
              "        (q_norm): Identity()\n",
              "        (k_norm): Identity()\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (1): Block(\n",
              "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
              "        (q_norm): Identity()\n",
              "        (k_norm): Identity()\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (2): Block(\n",
              "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
              "        (q_norm): Identity()\n",
              "        (k_norm): Identity()\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (3): Block(\n",
              "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
              "        (q_norm): Identity()\n",
              "        (k_norm): Identity()\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (4): Block(\n",
              "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
              "        (q_norm): Identity()\n",
              "        (k_norm): Identity()\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (5): Block(\n",
              "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
              "        (q_norm): Identity()\n",
              "        (k_norm): Identity()\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (6): Block(\n",
              "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
              "        (q_norm): Identity()\n",
              "        (k_norm): Identity()\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (7): Block(\n",
              "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
              "        (q_norm): Identity()\n",
              "        (k_norm): Identity()\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (8): Block(\n",
              "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
              "        (q_norm): Identity()\n",
              "        (k_norm): Identity()\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (9): Block(\n",
              "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
              "        (q_norm): Identity()\n",
              "        (k_norm): Identity()\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (10): Block(\n",
              "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
              "        (q_norm): Identity()\n",
              "        (k_norm): Identity()\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (11): Block(\n",
              "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
              "        (q_norm): Identity()\n",
              "        (k_norm): Identity()\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "  )\n",
              "  (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
              "  (fc_norm): Identity()\n",
              "  (head_drop): Dropout(p=0.0, inplace=False)\n",
              "  (head): Linear(in_features=192, out_features=9, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 301
        }
      ],
      "source": [
        "import torch.nn.init as init\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)  # CUDA 연산 시 동일한 결과 보장\n",
        "    torch.cuda.manual_seed_all(seed)  # 멀티-GPU 환경에서 동일한 결과 보장\n",
        "    torch.backends.cudnn.deterministic = True  # CuDNN 연산을 deterministic하게 설정\n",
        "    torch.backends.cudnn.benchmark = False  # 연산 속도를 희생하고 일관된 연산을 수행\n",
        "\n",
        "def initialize_weights(m):\n",
        "    if isinstance(m, nn.Conv2d):  # Conv 레이어 초기화\n",
        "        init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "        if m.bias is not None:\n",
        "            init.constant_(m.bias, 0)\n",
        "    elif isinstance(m, nn.Linear):  # Linear 레이어 초기화\n",
        "        init.xavier_uniform_(m.weight)\n",
        "        if m.bias is not None:\n",
        "            init.constant_(m.bias, 0)\n",
        "    elif isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):  # BatchNorm 초기화\n",
        "        init.constant_(m.weight, 1)\n",
        "        init.constant_(m.bias, 0)\n",
        "\n",
        "set_seed(42)  # 랜덤 시드 고정\n",
        "model.apply(initialize_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 302,
      "metadata": {
        "id": "fnlxaQSs7NsL"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dir = 'val/CRC-VAL-HE-7K/'\n",
        "dataset = datasets.ImageFolder(root=test_dir, transform=transform)\n",
        "test_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "Qe6c884rPvE7"
      },
      "execution_count": 303,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 304,
      "metadata": {
        "id": "4TSObUGwcYxd"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(lr=learning_rate, params=model.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 305,
      "metadata": {
        "id": "1No-7jzGgzrV"
      },
      "outputs": [],
      "source": [
        "train_losses = []\n",
        "train_accuracies = []\n",
        "val_losses = []\n",
        "val_accuracies = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 306,
      "metadata": {
        "id": "Yf0ZPGLtz_gB"
      },
      "outputs": [],
      "source": [
        "def train(model, train_loader, criterion, optimizer, device, epoch):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for i, (inputs, labels) in enumerate(tqdm(train_loader, desc=\"Training\")):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Train Loss: {epoch_loss:.4f}, Train Accuracy: {accuracy:.2f}%\")\n",
        "    train_losses.append(epoch_loss)\n",
        "    train_accuracies.append(accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 307,
      "metadata": {
        "id": "rbdjXJ520Btp"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, data_loader, criterion, device, epoch, phase=\"Validation\"):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, labels) in enumerate(tqdm(data_loader, desc=f\"{phase}\")):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            # Save all labels and predictions for balanced accuracy\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "    epoch_loss = running_loss / len(data_loader)\n",
        "    accuracy = 100 * correct / total\n",
        "    balanced_acc = balanced_accuracy_score(all_labels, all_predictions)\n",
        "\n",
        "    print(f\"{phase} Loss: {epoch_loss:.4f}, {phase} Accuracy: {accuracy:.2f}%\")\n",
        "    print(f\"Balanced Accuracy: {balanced_acc:.4f}\")\n",
        "\n",
        "    val_losses.append(epoch_loss)\n",
        "    val_accuracies.append(accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 308,
      "metadata": {
        "id": "GfBjePI-0DkY"
      },
      "outputs": [],
      "source": [
        "def measure_inference_time(model, data_loader, device):\n",
        "    model.eval()\n",
        "    times = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, _ in data_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            start_time = torch.cuda.Event(enable_timing=True)\n",
        "            end_time = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "            start_time.record()\n",
        "            _ = model(inputs)  # inference 수행\n",
        "            end_time.record()\n",
        "\n",
        "            # 시간 측정\n",
        "            torch.cuda.synchronize()  # CUDA에서 모든 커널이 완료될 때까지 대기\n",
        "            elapsed_time = start_time.elapsed_time(end_time)  # 밀리초 단위로 반환\n",
        "            times.append(elapsed_time)\n",
        "\n",
        "    # 통계량 계산\n",
        "    times_np = np.array(times)\n",
        "    total_inferences = len(times_np)\n",
        "    avg_time = np.mean(times_np)\n",
        "    std_dev = np.std(times_np)\n",
        "    max_time = np.max(times_np)\n",
        "    min_time = np.min(times_np)\n",
        "\n",
        "    # 결과 출력\n",
        "    print(f\"Inference Time Measurement Results:\")\n",
        "    print(f\"Total Inferences: {total_inferences}\")\n",
        "    print(f\"Average Time: {avg_time:.2f} ms\")\n",
        "    print(f\"Standard Deviation: {std_dev:.2f} ms\")\n",
        "    print(f\"Maximum Time: {max_time:.2f} ms\")\n",
        "    print(f\"Minimum Time: {min_time:.2f} ms\")\n",
        "\n",
        "    return times"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "times = measure_inference_time(model, test_loader, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6sipSSrgUbj",
        "outputId": "40d37a80-e089-4c23-b66e-d86a505aa5eb"
      },
      "execution_count": 309,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference Time Measurement Results:\n",
            "Total Inferences: 225\n",
            "Average Time: 10.07 ms\n",
            "Standard Deviation: 0.33 ms\n",
            "Maximum Time: 14.05 ms\n",
            "Minimum Time: 9.87 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 310,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c77rmtOz0XuA",
        "outputId": "dc8b4bb7-6032-4117-8b61-899851720e8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                           aten::linear         5.56%     570.120us        39.16%       4.016ms      81.969us       0.000us         0.00%       6.428ms     131.178us            49  \n",
            "                                            aten::addmm        18.96%       1.945ms        25.72%       2.638ms      53.833us       6.428ms        67.29%       6.428ms     131.178us            49  \n",
            "                                 ampere_sgemm_32x128_tn         0.00%       0.000us         0.00%       0.000us       0.000us       4.548ms        47.61%       4.548ms     126.332us            36  \n",
            "                                 ampere_sgemm_128x64_tn         0.00%       0.000us         0.00%       0.000us       0.000us       1.866ms        19.54%       1.866ms     155.527us            12  \n",
            "                     aten::scaled_dot_product_attention         3.26%     334.346us        14.45%       1.482ms     123.529us       0.000us         0.00%       1.556ms     129.695us            12  \n",
            "          aten::_scaled_dot_product_efficient_attention         2.93%     300.536us        11.19%       1.148ms      95.667us       0.000us         0.00%       1.556ms     129.695us            12  \n",
            "                     aten::_efficient_attention_forward         2.52%     258.665us         6.56%     672.678us      56.057us       1.556ms        16.29%       1.556ms     129.695us            12  \n",
            "fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEf...         0.00%       0.000us         0.00%       0.000us       0.000us       1.556ms        16.29%       1.556ms     129.695us            12  \n",
            "                                       aten::layer_norm         1.47%     151.187us        16.60%       1.703ms      68.107us       0.000us         0.00%     649.821us      25.993us            25  \n",
            "                                aten::native_layer_norm         6.84%     701.113us        15.13%       1.551ms      62.060us     649.821us         6.80%     649.821us      25.993us            25  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 10.257ms\n",
            "Self CUDA time total: 9.552ms\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from torch import profiler\n",
        "\n",
        "dummy_input = torch.randn(32, 3, 224, 224).cuda()\n",
        "\n",
        "# Profiling inference\n",
        "with profiler.profile(\n",
        "    activities=[\n",
        "       profiler.ProfilerActivity.CPU,\n",
        "        profiler.ProfilerActivity.CUDA,  # Include if using GPU\n",
        "    ],\n",
        "    on_trace_ready=profiler.tensorboard_trace_handler(\"./logs\"),  # Optional logging\n",
        "    record_shapes=True,\n",
        "    with_stack=True\n",
        ") as prof:\n",
        "    with torch.no_grad():\n",
        "        model(dummy_input)\n",
        "\n",
        "\n",
        "# Print results\n",
        "print(prof.key_averages().table(sort_by=\"cuda_time_total\" if torch.cuda.is_available() else \"cpu_time_total\", row_limit=10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 311,
      "metadata": {
        "id": "jXvKsupvZYjZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score, recall_score, precision_score, roc_curve, auc\n",
        "\n",
        "def score_evaluate(model, data_loader, criterion, device, phase=\"Validation\"):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(data_loader, desc=f\"{phase}\"):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "\n",
        "    overall_f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
        "    overall_recall = recall_score(all_labels, all_preds, average=\"macro\")\n",
        "    overall_precision = precision_score(all_labels, all_preds, average=\"macro\")\n",
        "\n",
        "    f1_per_class = f1_score(all_labels, all_preds, average=None)\n",
        "    recall_per_class = recall_score(all_labels, all_preds, average=None)\n",
        "    precision_per_class = precision_score(all_labels, all_preds, average=None)\n",
        "    class_labels = sorted(set(all_labels))\n",
        "\n",
        "    epoch_loss = running_loss / len(data_loader)\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"{phase} Loss: {epoch_loss:.4f}, {phase} Accuracy: {accuracy:.2f}%\")\n",
        "    val_losses.append(epoch_loss)\n",
        "    val_accuracies.append(accuracy)\n",
        "\n",
        "    print(f\"Overall - F1: {overall_f1:.4f}, Recall: {overall_recall:.4f}, Precision: {overall_precision:.4f}\")\n",
        "    print(\"Per-Class Metrics:\")\n",
        "    for i, label in enumerate(class_labels):\n",
        "        print(f\"Class {label} - F1: {f1_per_class[i]:.4f}, Recall: {recall_per_class[i]:.4f}, Precision: {precision_per_class[i]:.4f}\")\n",
        "\n",
        "    return overall_f1, overall_recall, overall_precision, f1_per_class, recall_per_class, precision_per_class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 312,
      "metadata": {
        "id": "oe9DO2tS-BpI"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "def roc_auc(model, data_loader, device, num_classes):\n",
        "\n",
        "    y = [\"F1\", \"Precision\", \"Recall\"]\n",
        "\n",
        "    model.eval()\n",
        "    all_labels = []\n",
        "    all_scores = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(data_loader, desc=\"ROC AUC\"):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_scores.extend(outputs.cpu().numpy())\n",
        "\n",
        "    all_labels = label_binarize(all_labels, classes=list(range(num_classes)))\n",
        "    print(f\"Binarized all_labels shape: {all_labels.shape}\")\n",
        "    print(f\"All_scores shape: {np.array(all_scores).shape}\")\n",
        "\n",
        "    fpr, tpr, _ = roc_curve(all_labels.ravel(), np.array(all_scores).ravel())\n",
        "    roc_auc_value = auc(fpr, tpr)\n",
        "\n",
        "    plt.figure(figsize=(9, 7))\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc_value:0.4f})')\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"Overall ROC AUC: {roc_auc_value:.4f}\")\n",
        "\n",
        "    return fpr, tpr, roc_auc_value"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load('ViT_tiny_NCT-CRC-HE-100K.pth'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYa8t9xMgmeT",
        "outputId": "04e80182-1dd9-41f7-e1e5-01f05b7fcffb"
      },
      "execution_count": 313,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-313-faa0d92db662>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('ViT_tiny_NCT-CRC-HE-100K.pth'))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 313
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "overall_f1, overall_recall, overall_precision, f1_per_class, recall_per_class, precision_per_class = score_evaluate(model, test_loader, criterion, device, phase=\"Test\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2y3cywDpjUQ",
        "outputId": "0e26e881-2805-467b-cf5f-5b7aedcf020d"
      },
      "execution_count": 314,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Test: 100%|██████████| 225/225 [00:11<00:00, 20.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.7262, Test Accuracy: 84.78%\n",
            "Overall - F1: 0.8058, Recall: 0.8147, Precision: 0.8236\n",
            "Per-Class Metrics:\n",
            "Class 0 - F1: 0.9679, Recall: 0.9791, Precision: 0.9569\n",
            "Class 1 - F1: 0.9680, Recall: 1.0000, Precision: 0.9380\n",
            "Class 2 - F1: 0.7064, Recall: 0.9086, Precision: 0.5779\n",
            "Class 3 - F1: 0.9324, Recall: 0.9905, Precision: 0.8808\n",
            "Class 4 - F1: 0.8447, Recall: 0.7411, Precision: 0.9821\n",
            "Class 5 - F1: 0.7896, Recall: 0.7449, Precision: 0.8400\n",
            "Class 6 - F1: 0.6545, Recall: 0.6289, Precision: 0.6823\n",
            "Class 7 - F1: 0.5317, Recall: 0.4086, Precision: 0.7611\n",
            "Class 8 - F1: 0.8567, Recall: 0.9311, Precision: 0.7934\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = [\"ADI\", \"BACK\", \"DEB\", \"LYM\", \"MUC\", \"MUS\", \"NORM\", 'STR', \"TUM\"]\n",
        "y = [\"F1\", \"Precision\", \"Recall\"]\n",
        "\n",
        "f1_score_list = []\n",
        "precision_score_list = []\n",
        "recall_score_list = []\n",
        "\n",
        "for i, class_name in enumerate(x):\n",
        "    f1_score_list.append(f1_per_class[i])\n",
        "    precision_score_list.append(precision_per_class[i])\n",
        "    recall_score_list.append(recall_per_class[i])\n",
        "\n",
        "x.append(\"Overall\")\n",
        "f1_score_list.append(overall_f1)\n",
        "precision_score_list.append(overall_precision)\n",
        "recall_score_list.append(overall_recall)\n",
        "\n",
        "score_list = [f1_score_list, precision_score_list, recall_score_list]\n",
        "score_list = np.array(score_list)\n",
        "plt.figure(figsize=(20, 12))\n",
        "score_cm = plt.matshow(score_list, cmap=plt.cm.Blues, alpha=0.3)\n",
        "plt.xticks(range(len(x)), x)\n",
        "plt.yticks(range(len(y)), y)\n",
        "for i in range(len(y)):\n",
        "    for j in range(len(x)):\n",
        "        # 행렬의 각각의 수치를 각 칸의 중앙에 넣어준다\n",
        "        plt.text(x=j, y=i,\n",
        "                     s=\"{:.2f}\".format(score_list[i, j]),\n",
        "                     va='center',\n",
        "                     ha='center',\n",
        "                     )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "p4pYjMPUpy2m",
        "outputId": "5c78057b-2ccb-4f11-ffb0-65541669d400"
      },
      "execution_count": 315,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2000x1200 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1600x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z_vaWpvS2WkS"
      },
      "execution_count": 296,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}