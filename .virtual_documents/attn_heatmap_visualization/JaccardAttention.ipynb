import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader, random_split
from tqdm import tqdm


class LinearNorm(nn.Module):
    """
    A helper module for linear layers with normalization.
    """
    def __init__(self, in_features, out_features):
        super(LinearNorm, self).__init__()
        self.linear = nn.Linear(in_features, out_features)
        self.norm = nn.LayerNorm(out_features)

    def forward(self, x):
        return self.norm(self.linear(x))


class JaccardAttention(nn.Module):
    def __init__(self, dim, num_heads):
        super(JaccardAttention, self).__init__()
        self.num_heads = num_heads
        head_dim = dim // num_heads
        self.scale = head_dim ** -0.5
        inner_dim = head_dim * num_heads * 3  # For Q, K, V
        self.qkv = LinearNorm(dim, inner_dim)

        # Optional attention bias for positional bias
        self.attention_bias = None

        self.proj = nn.Sequential(
            nn.Hardswish(),  # Non-linear activation for projection
            LinearNorm(dim, dim)  # Project back to input dimension
        )

    def forward(self, x):
        B, N, C = x.shape  # B: batch size, N: sequence length, C: embedding dimension
        head_dim = C // self.num_heads

        # Initialize or adjust attention bias
        if self.attention_bias is None or self.attention_bias.size(1) != N:
            self.attention_bias = nn.Parameter(torch.zeros(self.num_heads, N, N))

        # Compute Q, K, V
        qkv = self.qkv(x)  # (B, N, 3 * dim)
        qkv = qkv.view(B, N, 3, self.num_heads, head_dim)  # Split into heads
        q, k, v = qkv.permute(2, 0, 3, 1, 4)  # (3, B, num_heads, N, head_dim)

        # Compute Jaccard Similarity-based Attention
        q_int = q.to(torch.int32)  # Convert to integer for bit-wise operations
        k_int = k.to(torch.int32)

        # Calculate intersection and union
        intersection = torch.sum((q_int.unsqueeze(3) & k_int.unsqueeze(2)), dim=-1)  # (B, num_heads, N, N)
        union = torch.sum((q_int.unsqueeze(3) | k_int.unsqueeze(2)), dim=-1)  # (B, num_heads, N, N)

        attn = intersection.float() / (union.float() + 1e-10)  # Avoid division by zero
        attn = attn * self.scale  # Apply scaling
        attn += self.attention_bias.to(x.device)  # Add attention bias

        attn = attn.softmax(dim=-1)  # Normalize attention scores
        x = (attn @ v).transpose(1, 2).reshape(B, N, C)  # Compute attention output
        return self.proj(x)  # Project back to input space


class JaccardViT(nn.Module):
    def __init__(self, img_size=32, patch_size=4, num_classes=10, dim=64, depth=6, heads=8, mlp_dim=128):
        super(JaccardViT, self).__init__()
        assert img_size % patch_size == 0, "Image size must be divisible by patch size."
        num_patches = (img_size // patch_size) ** 2
        patch_dim = 3 * patch_size ** 2  # For CIFAR-10, channels = 3

        self.patch_embedding = nn.Linear(patch_dim, dim)
        self.position_embedding = nn.Parameter(torch.randn(1, num_patches, dim))

        # Replace TransformerEncoder with JaccardAttention
        self.layers = nn.ModuleList([
            JaccardAttention(dim=dim, num_heads=heads) for _ in range(depth)
        ])

        self.mlp_head = nn.Sequential(
            nn.LayerNorm(dim),
            nn.Linear(dim, num_classes)
        )

    def forward(self, x):
        B, C, H, W = x.shape
        x = x.unfold(2, 4, 4).unfold(3, 4, 4)  # Patchifying
        x = x.permute(0, 2, 3, 1, 4, 5).reshape(B, -1, 3 * 4 * 4)  # (B, Patches, Patch Dim)
        x = self.patch_embedding(x)
        x += self.position_embedding

        for layer in self.layers:
            x = layer(x) + x  # Residual connection

        x = x.mean(dim=1)  # Global Average Pooling
        return self.mlp_head(x)


def load_data(batch_size=64):
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
    ])

    train_data = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
    test_data = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)

    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)
    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)

    return train_loader, test_loader


def train_and_test(model, train_loader, test_loader, device, num_epochs=10, lr=1e-3):
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)

    for epoch in range(num_epochs):
        # Training phase
        model.train()
        train_loss, train_correct, train_total = 0, 0, 0
        for inputs, labels in tqdm(train_loader, desc=f"Epoch {epoch + 1}/{num_epochs} [Training]"):
            inputs, labels = inputs.to(device), labels.to(device)
            optimizer.zero_grad()

            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            _, preds = torch.max(outputs, 1)
            train_correct += (preds == labels).sum().item()
            train_total += labels.size(0)

        train_acc = train_correct / train_total
        print(f"Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}")

    # test after train
    model.eval()
    test_correct, test_total = 0, 0
    with torch.no_grad():
      for inputs, labels in tqdm(test_loader, desc=f"Epoch {epoch + 1}/{num_epochs} [Testing]"):
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)
            test_correct += (preds == labels).sum().item()
            test_total += labels.size(0)

    test_acc = test_correct / test_total
    print(f"Epoch {epoch + 1}/{num_epochs}, Test Acc: {test_acc:.4f}")


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


train_loader, test_loader = load_data(batch_size=64)


model = JaccardViT(img_size=32, patch_size=4, num_classes=10, dim=64, depth=6, heads=8, mlp_dim=128)
model.to(device)


train_and_test(model, train_loader, test_loader, device, num_epochs=10, lr=1e-3)


class SimpleViT(nn.Module):
    def __init__(self, img_size=32, patch_size=4, num_classes=10, dim=64, depth=6, heads=8, mlp_dim=128):
        super(SimpleViT, self).__init__()
        assert img_size % patch_size == 0, "Image size must be divisible by patch size."
        num_patches = (img_size // patch_size) ** 2
        patch_dim = 3 * patch_size ** 2  # For CIFAR-10, channels = 3

        self.patch_embedding = nn.Linear(patch_dim, dim)
        self.position_embedding = nn.Parameter(torch.randn(1, num_patches, dim))

        self.transformer = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(d_model=dim, nhead=heads, dim_feedforward=mlp_dim),
            num_layers=depth
        )

        self.mlp_head = nn.Sequential(
            nn.LayerNorm(dim),
            nn.Linear(dim, num_classes)
        )

    def forward(self, x):
        B, C, H, W = x.shape
        x = x.unfold(2, 4, 4).unfold(3, 4, 4)  # Patchifying
        x = x.permute(0, 2, 3, 1, 4, 5).reshape(B, -1, 3 * 4 * 4)  # (B, Patches, Patch Dim)
        x = self.patch_embedding(x)
        x += self.position_embedding

        x = self.transformer(x)
        x = x.mean(dim=1)  # Global Average Pooling
        return self.mlp_head(x)


vitmodel = SimpleViT(img_size=32, patch_size=4, num_classes=10, dim=64, depth=6, heads=8, mlp_dim=128)
vitmodel.to(device)
train_and_test(vitmodel, train_loader, test_loader, device, num_epochs=10, lr=1e-3)


import numpy as np
def measure_inference_time(model, data_loader, device):
    model.eval()
    times = []

    with torch.no_grad():
        for inputs, _ in data_loader:
            inputs = inputs.to(device)
            start_time = torch.cuda.Event(enable_timing=True)
            end_time = torch.cuda.Event(enable_timing=True)

            start_time.record()
            _ = model(inputs)  # inference 수행
            end_time.record()

            # 시간 측정
            torch.cuda.synchronize()  # CUDA에서 모든 커널이 완료될 때까지 대기
            elapsed_time = start_time.elapsed_time(end_time)  # 밀리초 단위로 반환
            times.append(elapsed_time)

    # 통계량 계산
    times_np = np.array(times)
    total_inferences = len(times_np)
    avg_time = np.mean(times_np)
    std_dev = np.std(times_np)
    max_time = np.max(times_np)
    min_time = np.min(times_np)

    # 결과 출력
    print(f"Inference Time Measurement Results:")
    print(f"Total Inferences: {total_inferences}")
    print(f"Average Time: {avg_time:.2f} ms")
    print(f"Standard Deviation: {std_dev:.2f} ms")
    print(f"Maximum Time: {max_time:.2f} ms")
    print(f"Minimum Time: {min_time:.2f} ms")

    return times


times = measure_inference_time(model, test_loader, device)


times = measure_inference_time(vitmodel, test_loader, device)



