{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5343a4f2-80d3-4705-9d37-3e1fc95141df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement fla (from versions: none)\n",
      "ERROR: No matching distribution found for fla\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fla'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m \n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfla\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgla\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fused_chunk_gla, chunk_gla, fused_recurrent_gla\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'fla'"
     ]
    }
   ],
   "source": [
    "#!pip3 install triton\n",
    "!pip3 install fla\n",
    "import torch\n",
    "from einops import rearrange\n",
    "#import triton \n",
    "#import triton.language as tl\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn \n",
    "\n",
    "from fla.ops.gla import fused_chunk_gla, chunk_gla, fused_recurrent_gla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3faab7-481a-4120-a98a-5326482c102e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatedLinearAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embed_dim = config.d_model\n",
    "        self.num_heads = config.n_head\n",
    "        \n",
    "        self.gate_fn = nn.functional.silu\n",
    "        assert config.use_gk and not config.use_gv, \"Only use_gk is supported for simplicity.\"\n",
    "\n",
    "        self.q_proj = nn.Linear(self.embed_dim, self.embed_dim//2, bias=False)\n",
    "        self.k_proj = nn.Linear(self.embed_dim, self.embed_dim//2, bias=False)\n",
    "        self.k_gate =  nn.Sequential(nn.Linear(self.embed_dim, 16, bias=False), nn.Linear(16, self.embed_dim // 2))\n",
    "\n",
    "        self.v_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n",
    "        self.g_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=True)\n",
    "        self.out_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n",
    "\n",
    "        self.head_dim = self.embed_dim // self.num_heads\n",
    "        self.key_dim = self.embed_dim // self.num_heads\n",
    "        self.scaling = self.key_dim ** -0.5\n",
    "        self.group_norm = nn.LayerNorm(self.head_dim, eps=1e-5, elementwise_affine=False)\n",
    "\n",
    "        self.post_init()\n",
    "\n",
    "\n",
    "\n",
    "    def post_init(self):\n",
    "        nn.init.xavier_uniform_(self.q_proj.weight, gain=2 ** -2.5)\n",
    "        nn.init.xavier_uniform_(self.k_proj.weight, gain=2 ** -2.5)\n",
    "        if isinstance(self.k_gate, nn.Sequential):\n",
    "            nn.init.xavier_uniform_(self.k_gate[0].weight, gain=2 ** -2.5)\n",
    "            nn.init.xavier_uniform_(self.k_gate[1].weight, gain=2 ** -2.5)\n",
    "        else:\n",
    "            nn.init.xavier_uniform_(self.k_gate.weight, gain=2 ** -2.5)\n",
    "\n",
    "    def forward(self, x, hidden_states=None):\n",
    "        q = self.q_proj(x)\n",
    "        k = self.k_proj(x) * self.scaling\n",
    "        k_gate = self.k_gate(x)\n",
    "        v = self.v_proj(x)\n",
    "        g = self.g_proj(x)\n",
    "\n",
    "        output, new_hidden_states = self.gated_linear_attention(q, k, v, k_gate, hidden_states=hidden_states)\n",
    "        output = self.gate_fn(g) * output\n",
    "        output = self.out_proj(output)\n",
    "        return output, new_hidden_states\n",
    "\n",
    "\n",
    "    def gated_linear_attention(self, q, k, v, gk, normalizer=16, hidden_states=None):\n",
    "        q = rearrange(q, 'b l (h d) -> b h l d', h = self.num_heads).contiguous()\n",
    "        k = rearrange(k, 'b l (h d) -> b h l d', h = self.num_heads).contiguous()\n",
    "        v = rearrange(v, 'b l (h d) -> b h l d', h = self.num_heads).contiguous()\n",
    "        gk = rearrange(gk, 'b l (h d) -> b h l d', h = self.num_heads).contiguous()\n",
    "        gk = F.logsigmoid(gk) / normalizer\n",
    "\n",
    "        if self.training:\n",
    "            o, new_hidden_states = fused_chunk_gla(q, k, v, gk, initial_state=hidden_states, output_final_state=True)\n",
    "        else:\n",
    "            o = fused_recurrent_gla(q, k, v, gk)\n",
    "            new_hidden_states = None\n",
    "        o = self.group_norm(o)\n",
    "        o = rearrange(o, 'b h l d -> b l (h d)')\n",
    "        return o, new_hidden_states\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    BATCH, H, N_CTX, D_MODEL = 32, 4, 2048, 1024\n",
    "\n",
    "    GLA = GatedLinearAttention(D_MODEL, H, use_gk=True, use_gv=False).cuda().to(torch.bfloat16)\n",
    "\n",
    "    x = torch.randn((BATCH, N_CTX, D_MODEL), dtype=torch.bfloat16,\n",
    "     device=\"cuda\", requires_grad=True)\n",
    "    \n",
    "    y, _ = GLA(x)\n",
    "    print(y.shape)\n",
    "    y.sum().backward()\n",
    "    print(x.grad.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
