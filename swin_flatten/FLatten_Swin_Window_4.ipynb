{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bEMpr27c0RCi",
        "outputId": "d86b304b-23be-4240-f2b5-1c77b82e8d9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (1.0.9)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from timm) (2.4.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.19.1+cu121)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.24.7)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (24.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->timm) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (10.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->timm) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->timm) (1.3.0)\n",
            "Requirement already satisfied: pytorch-ignite in /usr/local/lib/python3.10/dist-packages (0.5.1)\n",
            "Requirement already satisfied: torch<3,>=1.3 in /usr/local/lib/python3.10/dist-packages (from pytorch-ignite) (2.4.1+cu121)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pytorch-ignite) (24.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.3->pytorch-ignite) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.3->pytorch-ignite) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.3->pytorch-ignite) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.3->pytorch-ignite) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.3->pytorch-ignite) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.3->pytorch-ignite) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3,>=1.3->pytorch-ignite) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<3,>=1.3->pytorch-ignite) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "# --------------------------------------------------------\n",
        "# Swin Transformer\n",
        "# Copyright (c) 2021 Microsoft\n",
        "# Licensed under The MIT License [see LICENSE for details]\n",
        "# Written by Ze Liu\n",
        "# --------------------------------------------------------\n",
        "\n",
        "!pip3 install timm\n",
        "!pip3 install pytorch-ignite\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn as nn\n",
        "import torch.utils.checkpoint as checkpoint\n",
        "\n",
        "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
        "from einops import rearrange\n",
        "\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
        "import ignite.metrics\n",
        "import ignite.contrib.handlers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "cxGT3QPy0q0H"
      },
      "outputs": [],
      "source": [
        "class Mlp(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = act_layer()\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "wncXpbHk09rc"
      },
      "outputs": [],
      "source": [
        "def window_partition(x, window_size):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        x: (B, H, W, C)\n",
        "        window_size (int): window size\n",
        "\n",
        "    Returns:\n",
        "        windows: (num_windows*B, window_size, window_size, C)\n",
        "    \"\"\"\n",
        "    B, H, W, C = x.shape\n",
        "    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n",
        "    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n",
        "    return windows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "uvsOSiH-1DdL"
      },
      "outputs": [],
      "source": [
        "def window_reverse(windows, window_size, H, W):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        windows: (num_windows*B, window_size, window_size, C)\n",
        "        window_size (int): Window size\n",
        "        H (int): Height of image\n",
        "        W (int): Width of image\n",
        "\n",
        "    Returns:\n",
        "        x: (B, H, W, C)\n",
        "    \"\"\"\n",
        "    B = int(windows.shape[0] / (H * W / window_size / window_size))\n",
        "    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n",
        "    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "Fp57DIJ41EBj"
      },
      "outputs": [],
      "source": [
        "class WindowAttention(nn.Module):\n",
        "    r\"\"\" Window based multi-head self attention (W-MSA) module with relative position bias.\n",
        "    It supports both of shifted and non-shifted window.\n",
        "\n",
        "    Args:\n",
        "        dim (int): Number of input channels.\n",
        "        window_size (tuple[int]): The height and width of the window.\n",
        "        num_heads (int): Number of attention heads.\n",
        "        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n",
        "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n",
        "        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n",
        "        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
        "\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.window_size = window_size  # Wh, Ww\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "\n",
        "        # define a parameter table of relative position bias\n",
        "        self.relative_position_bias_table = nn.Parameter(\n",
        "            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n",
        "\n",
        "        # get pair-wise relative position index for each token inside the window\n",
        "        coords_h = torch.arange(self.window_size[0])\n",
        "        coords_w = torch.arange(self.window_size[1])\n",
        "        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n",
        "        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n",
        "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n",
        "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
        "        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n",
        "        relative_coords[:, :, 1] += self.window_size[1] - 1\n",
        "        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n",
        "        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n",
        "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "        trunc_normal_(self.relative_position_bias_table, std=.02)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: input features with shape of (num_windows*B, N, C)\n",
        "            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None\n",
        "        \"\"\"\n",
        "        B_, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n",
        "\n",
        "        q = q * self.scale\n",
        "        attn = (q @ k.transpose(-2, -1))\n",
        "\n",
        "        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n",
        "            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)  # Wh*Ww,Wh*Ww,nH\n",
        "        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n",
        "        attn = attn + relative_position_bias.unsqueeze(0)\n",
        "\n",
        "        if mask is not None:\n",
        "            nW = mask.shape[0]\n",
        "            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n",
        "            attn = attn.view(-1, self.num_heads, N, N)\n",
        "            attn = self.softmax(attn)\n",
        "        else:\n",
        "            attn = self.softmax(attn)\n",
        "\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x\n",
        "\n",
        "    def extra_repr(self) -> str:\n",
        "        return f'dim={self.dim}, window_size={self.window_size}, num_heads={self.num_heads}'\n",
        "\n",
        "    def flops(self, N):\n",
        "        # calculate flops for 1 window with token length of N\n",
        "        flops = 0\n",
        "        # qkv = self.qkv(x)\n",
        "        flops += N * self.dim * 3 * self.dim\n",
        "        # attn = (q @ k.transpose(-2, -1))\n",
        "        flops += self.num_heads * N * (self.dim // self.num_heads) * N\n",
        "        #  x = (attn @ v)\n",
        "        flops += self.num_heads * N * N * (self.dim // self.num_heads)\n",
        "        # x = self.proj(x)\n",
        "        flops += N * self.dim * self.dim\n",
        "        return flops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "e_UlHriB1I6l"
      },
      "outputs": [],
      "source": [
        "class FocusedLinearAttention(nn.Module):\n",
        "    r\"\"\" Window based multi-head self attention (W-MSA) module with relative position bias.\n",
        "    It supports both of shifted and non-shifted window.\n",
        "\n",
        "    Args:\n",
        "        dim (int): Number of input channels.\n",
        "        window_size (tuple[int]): The height and width of the window.\n",
        "        num_heads (int): Number of attention heads.\n",
        "        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n",
        "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n",
        "        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n",
        "        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.,\n",
        "                 focusing_factor=3, kernel_size=5):\n",
        "\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.window_size = window_size  # Wh, Ww\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "\n",
        "        self.focusing_factor = focusing_factor  # Used to sharpen attention distribution\n",
        "\n",
        "    # Linear layer to project input to query, key, and value\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)  # Output projection\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "        # Depth-wise convolution for capturing local spatial information\n",
        "        self.dwc = nn.Conv2d(in_channels=head_dim, out_channels=head_dim, kernel_size=kernel_size,\n",
        "                            groups=head_dim, padding=kernel_size // 2)\n",
        "\n",
        "        # Learnable scale parameter\n",
        "        self.scale = nn.Parameter(torch.zeros(size=(1, 1, dim)))\n",
        "\n",
        "        # Learnable positional encoding\n",
        "        self.positional_encoding = nn.Parameter(torch.zeros(size=(1, window_size[0] * window_size[1], dim)))\n",
        "\n",
        "        print('Linear Attention window{} f{} kernel{}'.\n",
        "              format(window_size, focusing_factor, kernel_size))\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        B, N, C = x.shape\n",
        "\n",
        "        # Project input to query, key, and value\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, C).permute(2, 0, 1, 3)\n",
        "        q, k, v = qkv.unbind(0)\n",
        "\n",
        "        # Add positional encoding to keys\n",
        "        k = k + self.positional_encoding\n",
        "\n",
        "        focusing_factor = self.focusing_factor\n",
        "        kernel_function = nn.ReLU()\n",
        "\n",
        "        # Apply ReLU and add small epsilon to avoid zero values\n",
        "        q = kernel_function(q) + 1e-6\n",
        "        k = kernel_function(k) + 1e-6\n",
        "\n",
        "        # Compute scale using Softplus for stability\n",
        "        scale = nn.Softplus()(self.scale)\n",
        "        q = q / scale\n",
        "        k = k / scale\n",
        "\n",
        "        # Store original norms\n",
        "        q_norm = q.norm(dim=-1, keepdim=True)\n",
        "        k_norm = k.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        # Apply focusing factor\n",
        "        q = q ** focusing_factor\n",
        "        k = k ** focusing_factor\n",
        "\n",
        "        # Renormalize to original norms\n",
        "        q = (q / q.norm(dim=-1, keepdim=True)) * q_norm\n",
        "        k = (k / k.norm(dim=-1, keepdim=True)) * k_norm\n",
        "\n",
        "        # Reshape for multi-head attention\n",
        "        q = q.reshape(B, N, self.num_heads, -1).permute(0, 2, 1, 3)\n",
        "        k = k.reshape(B, N, self.num_heads, -1).permute(0, 2, 1, 3)\n",
        "        v = v.reshape(B, N, self.num_heads, -1).permute(0, 2, 1, 3)\n",
        "\n",
        "        # Compute linear attention\n",
        "        z = 1 / (q @ k.mean(dim=-2, keepdim=True).transpose(-2, -1) + 1e-6)\n",
        "        kv = (k.transpose(-2, -1) * (N ** -0.5)) @ (v * (N ** -0.5))\n",
        "        x = q @ kv * z\n",
        "\n",
        "        # Reshape output\n",
        "        H = W = int(N ** 0.5)\n",
        "        x = x.transpose(1, 2).reshape(B, N, C)\n",
        "\n",
        "        # Apply depth-wise convolution to capture local spatial information\n",
        "        v = v.reshape(B * self.num_heads, H, W, -1).permute(0, 3, 1, 2)\n",
        "        x = x + self.dwc(v).reshape(B, C, N).permute(0, 2, 1)\n",
        "\n",
        "        # Final projection and dropout\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x\n",
        "\n",
        "    def eval(self):\n",
        "        super(FocusedLinearAttention, self).eval()\n",
        "        print('eval')\n",
        "\n",
        "    def extra_repr(self) -> str:\n",
        "        return f'dim={self.dim}, window_size={self.window_size}, num_heads={self.num_heads}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "DIR8e1tu1O5O"
      },
      "outputs": [],
      "source": [
        "class SwinTransformerBlock(nn.Module):\n",
        "    r\"\"\" Swin Transformer Block.\n",
        "\n",
        "    Args:\n",
        "        dim (int): Number of input channels.\n",
        "        input_resolution (tuple[int]): Input resulotion.\n",
        "        num_heads (int): Number of attention heads.\n",
        "        window_size (int): Window size.\n",
        "        shift_size (int): Shift size for SW-MSA.\n",
        "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
        "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
        "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
        "        drop (float, optional): Dropout rate. Default: 0.0\n",
        "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
        "        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n",
        "        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU\n",
        "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dim, input_resolution, num_heads, window_size=7, shift_size=0,\n",
        "                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.,\n",
        "                 act_layer=nn.GELU, norm_layer=nn.LayerNorm,\n",
        "                 focusing_factor=3, kernel_size=5, attn_type='L'):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.input_resolution = input_resolution\n",
        "        self.num_heads = num_heads\n",
        "        self.window_size = window_size\n",
        "        self.shift_size = shift_size\n",
        "        self.mlp_ratio = mlp_ratio\n",
        "        if min(self.input_resolution) <= self.window_size:\n",
        "            # if window size is larger than input resolution, we don't partition windows\n",
        "            self.shift_size = 0\n",
        "            self.window_size = min(self.input_resolution)\n",
        "        assert 0 <= self.shift_size < self.window_size, \"shift_size must in 0-window_size\"\n",
        "\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        assert attn_type in ['L', 'S']\n",
        "        if attn_type == 'L':\n",
        "            self.attn = FocusedLinearAttention(\n",
        "                dim, window_size=to_2tuple(self.window_size), num_heads=num_heads,\n",
        "                qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop,\n",
        "                focusing_factor=focusing_factor, kernel_size=kernel_size)\n",
        "        else:\n",
        "            self.attn = WindowAttention(\n",
        "                dim, window_size=to_2tuple(self.window_size), num_heads=num_heads,\n",
        "                qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
        "\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm2 = norm_layer(dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
        "\n",
        "        if self.shift_size > 0:\n",
        "            # calculate attention mask for SW-MSA\n",
        "            H, W = self.input_resolution\n",
        "            img_mask = torch.zeros((1, H, W, 1))  # 1 H W 1\n",
        "            h_slices = (slice(0, -self.window_size),\n",
        "                        slice(-self.window_size, -self.shift_size),\n",
        "                        slice(-self.shift_size, None))\n",
        "            w_slices = (slice(0, -self.window_size),\n",
        "                        slice(-self.window_size, -self.shift_size),\n",
        "                        slice(-self.shift_size, None))\n",
        "            cnt = 0\n",
        "            for h in h_slices:\n",
        "                for w in w_slices:\n",
        "                    img_mask[:, h, w, :] = cnt\n",
        "                    cnt += 1\n",
        "\n",
        "            mask_windows = window_partition(img_mask, self.window_size)  # nW, window_size, window_size, 1\n",
        "            mask_windows = mask_windows.view(-1, self.window_size * self.window_size)\n",
        "            attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n",
        "            attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n",
        "        else:\n",
        "            attn_mask = None\n",
        "\n",
        "        self.register_buffer(\"attn_mask\", attn_mask)\n",
        "\n",
        "    def forward(self, x):\n",
        "        H, W = self.input_resolution\n",
        "        B, L, C = x.shape\n",
        "        assert L == H * W, \"input feature has wrong size\"\n",
        "\n",
        "        shortcut = x\n",
        "        x = self.norm1(x)\n",
        "        x = x.view(B, H, W, C)\n",
        "\n",
        "        # cyclic shift\n",
        "        if self.shift_size > 0:\n",
        "            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n",
        "        else:\n",
        "            shifted_x = x\n",
        "\n",
        "        # partition windows\n",
        "        x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, window_size, C\n",
        "        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)  # nW*B, window_size*window_size, C\n",
        "\n",
        "        # W-MSA/SW-MSA\n",
        "        attn_windows = self.attn(x_windows, mask=self.attn_mask)  # nW*B, window_size*window_size, C\n",
        "\n",
        "        # merge windows\n",
        "        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)\n",
        "        shifted_x = window_reverse(attn_windows, self.window_size, H, W)  # B H' W' C\n",
        "\n",
        "        # reverse cyclic shift\n",
        "        if self.shift_size > 0:\n",
        "            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n",
        "        else:\n",
        "            x = shifted_x\n",
        "        x = x.view(B, H * W, C)\n",
        "\n",
        "        # FFN\n",
        "        x = shortcut + self.drop_path(x)\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "\n",
        "        return x\n",
        "\n",
        "    def extra_repr(self) -> str:\n",
        "        return f\"dim={self.dim}, input_resolution={self.input_resolution}, num_heads={self.num_heads}, \" \\\n",
        "               f\"window_size={self.window_size}, shift_size={self.shift_size}, mlp_ratio={self.mlp_ratio}\"\n",
        "\n",
        "    def flops(self):\n",
        "        flops = 0\n",
        "        H, W = self.input_resolution\n",
        "        # norm1\n",
        "        flops += self.dim * H * W\n",
        "        # W-MSA/SW-MSA\n",
        "        nW = H * W / self.window_size / self.window_size\n",
        "        flops += nW * self.attn.flops(self.window_size * self.window_size)\n",
        "        # mlp\n",
        "        flops += 2 * H * W * self.dim * self.dim * self.mlp_ratio\n",
        "        # norm2\n",
        "        flops += self.dim * H * W\n",
        "        return flops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "blyYs6kZ1TeT"
      },
      "outputs": [],
      "source": [
        "class PatchMerging(nn.Module):\n",
        "    r\"\"\" Patch Merging Layer.\n",
        "\n",
        "    Args:\n",
        "        input_resolution (tuple[int]): Resolution of input feature.\n",
        "        dim (int): Number of input channels.\n",
        "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_resolution, dim, norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.input_resolution = input_resolution\n",
        "        self.dim = dim\n",
        "        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n",
        "        self.norm = norm_layer(4 * dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: B, H*W, C\n",
        "        \"\"\"\n",
        "        H, W = self.input_resolution\n",
        "        B, L, C = x.shape\n",
        "        assert L == H * W, \"input feature has wrong size\"\n",
        "        assert H % 2 == 0 and W % 2 == 0, f\"x size ({H}*{W}) are not even.\"\n",
        "\n",
        "        x = x.view(B, H, W, C)\n",
        "\n",
        "        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C\n",
        "        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C\n",
        "        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C\n",
        "        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C\n",
        "        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C\n",
        "        x = x.view(B, -1, 4 * C)  # B H/2*W/2 4*C\n",
        "\n",
        "        x = self.norm(x)\n",
        "        x = self.reduction(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def extra_repr(self) -> str:\n",
        "        return f\"input_resolution={self.input_resolution}, dim={self.dim}\"\n",
        "\n",
        "    def flops(self):\n",
        "        H, W = self.input_resolution\n",
        "        flops = H * W * self.dim\n",
        "        flops += (H // 2) * (W // 2) * 4 * self.dim * 2 * self.dim\n",
        "        return flops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "XNJN4Z_01WBE"
      },
      "outputs": [],
      "source": [
        "class BasicLayer(nn.Module):\n",
        "    \"\"\" A basic Swin Transformer layer for one stage.\n",
        "\n",
        "    Args:\n",
        "        dim (int): Number of input channels.\n",
        "        input_resolution (tuple[int]): Input resolution.\n",
        "        depth (int): Number of blocks.\n",
        "        num_heads (int): Number of attention heads.\n",
        "        window_size (int): Local window size.\n",
        "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
        "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
        "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
        "        drop (float, optional): Dropout rate. Default: 0.0\n",
        "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
        "        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n",
        "        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n",
        "        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n",
        "        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dim, input_resolution, depth, num_heads, window_size,\n",
        "                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0.,\n",
        "                 drop_path=0., norm_layer=nn.LayerNorm, downsample=None, use_checkpoint=False,\n",
        "                 focusing_factor=3, kernel_size=5, attn_type='L'):\n",
        "\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.input_resolution = input_resolution\n",
        "        self.depth = depth\n",
        "        self.use_checkpoint = use_checkpoint\n",
        "\n",
        "        # build blocks\n",
        "        attn_types = [(attn_type if attn_type[0] != 'M' else ('L' if i < int(attn_type[1:]) else 'S')) for i in range(depth)]\n",
        "        window_sizes = [(window_size if attn_types[i] == 'L' else (7 if window_size <= 56 else 12)) for i in range(depth)]\n",
        "        self.blocks = nn.ModuleList([\n",
        "            SwinTransformerBlock(dim=dim, input_resolution=input_resolution,\n",
        "                                 num_heads=num_heads, window_size=window_sizes[i],\n",
        "                                 shift_size=0 if (i % 2 == 0) else window_sizes[i] // 2,\n",
        "                                 mlp_ratio=mlp_ratio,\n",
        "                                 qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                                 drop=drop, attn_drop=attn_drop,\n",
        "                                 drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n",
        "                                 norm_layer=norm_layer,\n",
        "                                 focusing_factor=focusing_factor,\n",
        "                                 kernel_size=kernel_size,\n",
        "                                 attn_type=attn_types[i])\n",
        "            for i in range(depth)])\n",
        "\n",
        "        # patch merging layer\n",
        "        if downsample is not None:\n",
        "            self.downsample = downsample(input_resolution, dim=dim, norm_layer=norm_layer)\n",
        "        else:\n",
        "            self.downsample = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        for blk in self.blocks:\n",
        "            if self.use_checkpoint:\n",
        "                x = checkpoint.checkpoint(blk, x)\n",
        "            else:\n",
        "                x = blk(x)\n",
        "        if self.downsample is not None:\n",
        "            x = self.downsample(x)\n",
        "        return x\n",
        "\n",
        "    def extra_repr(self) -> str:\n",
        "        return f\"dim={self.dim}, input_resolution={self.input_resolution}, depth={self.depth}\"\n",
        "\n",
        "    def flops(self):\n",
        "        flops = 0\n",
        "        for blk in self.blocks:\n",
        "            flops += blk.flops()\n",
        "        if self.downsample is not None:\n",
        "            flops += self.downsample.flops()\n",
        "        return flops\n",
        "\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "    r\"\"\" Image to Patch Embedding\n",
        "\n",
        "    Args:\n",
        "        img_size (int): Image size.  Default: 224.\n",
        "        patch_size (int): Patch token size. Default: 4.\n",
        "        in_chans (int): Number of input image channels. Default: 3.\n",
        "        embed_dim (int): Number of linear projection output channels. Default: 96.\n",
        "        norm_layer (nn.Module, optional): Normalization layer. Default: None\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):\n",
        "        super().__init__()\n",
        "        img_size = to_2tuple(img_size)\n",
        "        patch_size = to_2tuple(patch_size)\n",
        "        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.patches_resolution = patches_resolution\n",
        "        self.num_patches = patches_resolution[0] * patches_resolution[1]\n",
        "\n",
        "        self.in_chans = in_chans\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "        if norm_layer is not None:\n",
        "            self.norm = norm_layer(embed_dim)\n",
        "        else:\n",
        "            self.norm = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        # FIXME look at relaxing size constraints\n",
        "        assert H == self.img_size[0] and W == self.img_size[1], \\\n",
        "            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
        "        x = self.proj(x).flatten(2).transpose(1, 2)  # B Ph*Pw C\n",
        "        if self.norm is not None:\n",
        "            x = self.norm(x)\n",
        "        return x\n",
        "\n",
        "    def flops(self):\n",
        "        Ho, Wo = self.patches_resolution\n",
        "        flops = Ho * Wo * self.embed_dim * self.in_chans * (self.patch_size[0] * self.patch_size[1])\n",
        "        if self.norm is not None:\n",
        "            flops += Ho * Wo * self.embed_dim\n",
        "        return flops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "LuascylH1a-6"
      },
      "outputs": [],
      "source": [
        "class FLattenSwinTransformer(nn.Module):\n",
        "    r\"\"\" Swin Transformer\n",
        "        A PyTorch impl of : `Swin Transformer: Hierarchical Vision Transformer using Shifted Windows`  -\n",
        "          https://arxiv.org/pdf/2103.14030\n",
        "\n",
        "    Args:\n",
        "        img_size (int | tuple(int)): Input image size. Default 224\n",
        "        patch_size (int | tuple(int)): Patch size. Default: 4\n",
        "        in_chans (int): Number of input image channels. Default: 3\n",
        "        num_classes (int): Number of classes for classification head. Default: 1000\n",
        "        embed_dim (int): Patch embedding dimension. Default: 96\n",
        "        depths (tuple(int)): Depth of each Swin Transformer layer.\n",
        "        num_heads (tuple(int)): Number of attention heads in different layers.\n",
        "        window_size (int): Window size. Default: 7\n",
        "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4\n",
        "        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True\n",
        "        qk_scale (float): Override default qk scale of head_dim ** -0.5 if set. Default: None\n",
        "        drop_rate (float): Dropout rate. Default: 0\n",
        "        attn_drop_rate (float): Attention dropout rate. Default: 0\n",
        "        drop_path_rate (float): Stochastic depth rate. Default: 0.1\n",
        "        norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.\n",
        "        ape (bool): If True, add absolute position embedding to the patch embedding. Default: False\n",
        "        patch_norm (bool): If True, add normalization after patch embedding. Default: True\n",
        "        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, img_size=32, patch_size=4, in_chans=3, num_classes=1000,\n",
        "                 embed_dim=96, depths=[2, 2, 6, 2], num_heads=[3, 6, 12, 24],\n",
        "                 window_size=7, mlp_ratio=4., qkv_bias=True, qk_scale=None,\n",
        "                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,\n",
        "                 norm_layer=nn.LayerNorm, ape=False, patch_norm=True,\n",
        "                 use_checkpoint=False,\n",
        "                 focusing_factor=3, kernel_size=5, attn_type='LLLL', **kwargs):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_classes = num_classes\n",
        "        self.num_layers = len(depths)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.ape = ape\n",
        "        self.patch_norm = patch_norm\n",
        "        self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))\n",
        "        self.mlp_ratio = mlp_ratio\n",
        "\n",
        "        # split image into non-overlapping patches\n",
        "        self.patch_embed = PatchEmbed(\n",
        "            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim,\n",
        "            norm_layer=norm_layer if self.patch_norm else None)\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "        patches_resolution = self.patch_embed.patches_resolution\n",
        "        self.patches_resolution = patches_resolution\n",
        "\n",
        "        # absolute position embedding\n",
        "        if self.ape:\n",
        "            self.absolute_pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n",
        "            trunc_normal_(self.absolute_pos_embed, std=.02)\n",
        "\n",
        "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
        "\n",
        "        # stochastic depth\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n",
        "\n",
        "        # build layers\n",
        "        self.layers = nn.ModuleList()\n",
        "        for i_layer in range(self.num_layers):\n",
        "            layer = BasicLayer(dim=int(embed_dim * 2 ** i_layer),\n",
        "                               input_resolution=(patches_resolution[0] // (2 ** i_layer),\n",
        "                                                 patches_resolution[1] // (2 ** i_layer)),\n",
        "                               depth=depths[i_layer],\n",
        "                               num_heads=num_heads[i_layer],\n",
        "                               window_size=window_size,\n",
        "                               mlp_ratio=self.mlp_ratio,\n",
        "                               qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                               drop=drop_rate, attn_drop=attn_drop_rate,\n",
        "                               drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],\n",
        "                               norm_layer=norm_layer,\n",
        "                               downsample=PatchMerging if (i_layer < self.num_layers - 1) else None,\n",
        "                               use_checkpoint=use_checkpoint,\n",
        "                               focusing_factor=focusing_factor,\n",
        "                               kernel_size=kernel_size,\n",
        "                               attn_type=attn_type[i_layer] + (attn_type[self.num_layers:] if attn_type[i_layer] == 'M' else ''))\n",
        "            self.layers.append(layer)\n",
        "\n",
        "        self.norm = norm_layer(self.num_features)\n",
        "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "    @torch.jit.ignore\n",
        "    def no_weight_decay(self):\n",
        "        return {'absolute_pos_embed'}\n",
        "\n",
        "    @torch.jit.ignore\n",
        "    def no_weight_decay_keywords(self):\n",
        "        return {'relative_position_bias_table'}\n",
        "\n",
        "    def forward_features(self, x):\n",
        "        x = self.patch_embed(x)\n",
        "        if self.ape:\n",
        "            x = x + self.absolute_pos_embed\n",
        "        x = self.pos_drop(x)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "\n",
        "        x = self.norm(x)  # B L C\n",
        "        x = self.avgpool(x.transpose(1, 2))  # B C 1\n",
        "        x = torch.flatten(x, 1)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.forward_features(x)\n",
        "        x = self.head(x)\n",
        "        return x\n",
        "\n",
        "    def flops(self):\n",
        "        flops = 0\n",
        "        flops += self.patch_embed.flops()\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            flops += layer.flops()\n",
        "        flops += self.num_features * self.patches_resolution[0] * self.patches_resolution[1] // (2 ** self.num_layers)\n",
        "        flops += self.num_features * self.num_classes\n",
        "        return flops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "CH3oe5YA1en5"
      },
      "outputs": [],
      "source": [
        "DATA_DIR='./data'\n",
        "\n",
        "IMAGE_SIZE = 32\n",
        "\n",
        "NUM_CLASSES = 10\n",
        "NUM_WORKERS = 8\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 100\n",
        "\n",
        "LEARNING_RATE = 1e-3\n",
        "WEIGHT_DECAY = 1e-1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CIG2Ai-N1rHK",
        "outputId": "4d641b02-cebf-4dd2-da65-46ad48144db5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "device: cuda\n"
          ]
        }
      ],
      "source": [
        "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(\"device:\", DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c73u1cDN2wC4",
        "outputId": "ef5015da-5064-4dd4-d2fa-79d4631d2d7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.PILToTensor(),\n",
        "    transforms.ConvertImageDtype(torch.float)\n",
        "])\n",
        "\n",
        "train_dset = datasets.CIFAR10(root=DATA_DIR, train=True, download=True, transform=train_transform)\n",
        "test_dset = datasets.CIFAR10(root=DATA_DIR, train=False, download=True, transform=transforms.ToTensor())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "KKpPWqWt2xoz"
      },
      "outputs": [],
      "source": [
        "def dataset_show_image(dset, idx):\n",
        "    X, Y = dset[idx]\n",
        "    title = \"Ground truth: {}\".format(dset.classes[Y])\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111)\n",
        "    ax.set_axis_off()\n",
        "    ax.imshow(np.moveaxis(X.numpy(), 0, -1))\n",
        "    ax.set_title(title)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "nL_w38GL3LeA",
        "outputId": "d173c5f6-0fe9-42f0-bd1f-98849ac90963"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGbCAYAAAAr/4yjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAhM0lEQVR4nO3da5CcdbXv8fX0/TKXnmtISDIhFwIBCYiQc1RyAZGbpHCDUaBOkWCK4iaCiFrwgoTCAhSEQkRESiih8IWWJXUoPMJmS1GessrbDh5gB5kwCQkJSWaSuWS6e/r2Py/YWTJMMGvFDJD4/VTxYiZr1jzdT/f8upN5fkQhhCAAAIhI7MM+AADARwehAABQhAIAQBEKAABFKAAAFKEAAFCEAgBAEQoAAEUoAAAUoYBDXhRFsmbNmg/7MN7XrFmz5HOf+9wH/n1XrlwpTU1NptmP+n2IDw6h8C+ir69Prr32Wjn66KMll8tJLpeTBQsWyDXXXCN//etfP+zDm3Rbt26VNWvWyLp16yZl/6uvvipr1qyRjRs3Tsp+4IOS+LAPAJPv6aefli9+8YuSSCTk0ksvlYULF0osFpP169fLL3/5S/nhD38ofX190tPT82Ef6qTZunWrrF27VmbNmiUnnnjiQd//6quvytq1a2Xp0qUya9asg75/spVKJUkk+HEAQuGwt2HDBvnSl74kPT098vzzz8vUqVPH/fldd90lDz74oMRi//hN4+joqOTz+ck81I+UYrEouVzuwz6MD0wmk/mwDwEfEfz10WHuO9/5joyOjsqjjz46IRBERBKJhFx33XUyY8YM/dzev4vesGGDnHvuudLc3CyXXnqpiLwTDjfeeKPMmDFD0um0zJ8/X+6++255d9nuxo0bJYoieeyxxyZ8v/f+3fWaNWskiiLp7e2VlStXSqFQkNbWVlm1apUUi8VxXzs2NiY33HCDdHV1SXNzsyxfvly2bNmy3/vghRdekFNOOUVERFatWiVRFI07vqVLl8rxxx8vf/7zn2Xx4sWSy+Xk5ptv3ufx7jVr1ixZuXKliIg89thj8oUvfEFERJYtW6b7X3jhhXFf87vf/U5OPfVUyWQyMnv2bPnpT386Ye+GDRtkw4YN+71N1WpV1q5dK/PmzZNMJiMdHR3y6U9/Wp577rkJs2+99ZZccMEF0tTUJF1dXfL1r39d6vX6uJn3Oy/r16+XFStWSEtLi3R0dMhXv/pVKZfL+z0+HLoIhcPc008/LXPnzpVFixa5vq5Wq8lZZ50l3d3dcvfdd8uFF14oIQRZvny53HvvvXL22WfL9773PZk/f77cdNNN8rWvfe2fOs4VK1bIyMiI3HHHHbJixQp57LHHZO3ateNmVq9eLffdd5989rOflTvvvFOSyaScd955+9197LHHym233SYiIldccYU8/vjj8vjjj8vixYt1ZmBgQM455xw58cQT5b777pNly5aZj33x4sVy3XXXiYjIzTffrPuPPfZYnent7ZWLLrpIzjzzTLnnnnukra1NVq5cKa+88sq4XWeccYacccYZ+/2ea9askbVr18qyZcvkgQcekFtuuUVmzpwpf/nLX8bN1et1Oeuss6Sjo0PuvvtuWbJkidxzzz3y8MMPm27bihUrpFwuyx133CHnnnuu3H///XLFFVeYvhaHqIDD1tDQUBCRcMEFF0z4s927d4edO3fqf8ViUf/ssssuCyISvvWtb437ml/96ldBRMLtt98+7vMXXXRRiKIo9Pb2hhBC6OvrCyISHn300QnfV0TCrbfeqh/feuutQUTC5ZdfPm7u85//fOjo6NCP161bF0QkXH311ePmLrnkkgk79+WPf/zj+x7TkiVLgoiEhx56aL/Hu1dPT0+47LLL9OOf//znQUTCb3/7233Oikh48cUX9XM7duwI6XQ63HjjjRNme3p6/uFtCSGEhQsXhvPOO+8fzuw9j7fddtu4z5900knh5JNPHve59zsvy5cvHzd39dVXBxEJL7300n6PEYcm3ikcxoaHh0VE9vlriUuXLpWuri797wc/+MGEmauuumrcx88884zE43F9VbzXjTfeKCEE+fWvf33Ax3rllVeO+/i0006TgYEBvQ3PPPOMiMiE73399dcf8Pd8t3Q6LatWrToou/ZlwYIFctppp+nHXV1dMn/+fHnjjTfGzW3cuNH0G0yFQkFeeeUVef311/c7u6/79r3f9/1cc8014z7+yle+IiJ/Px84/BAKh7Hm5mYREdmzZ8+EP/vRj34kzz33nDzxxBP7/NpEIiHTp08f97lNmzbJtGnTdO9ee/+aZNOmTQd8rDNnzhz3cVtbm4iI7N69W3fHYjGZM2fOuLn58+cf8Pd8tyOPPFJSqdRB2bUv7719Iu/cxr23z+u2226TwcFBOfroo+VjH/uY3HTTTfv81eJMJiNdXV0H/H3nzZs37uM5c+ZILBbjV28PY4TCYay1tVWmTp0qL7/88oQ/W7RokXzmM5+RT33qU/v82nQ6vd/fSHo/URTt8/Pv/cfNd4vH4/v8fPiA/m+x2WzWNf+Pbsu+HOzbt3jxYtmwYYP85Cc/keOPP14eeeQR+fjHPy6PPPKI6fseqPc7tzh8EAqHufPOO096e3vlD3/4wz+9q6enR7Zu3SojIyPjPr9+/Xr9c5G/v8ofHBwcN/fPvJPo6emRRqMx4TdzXnvtNdPXH+gPs7a2tgm3o1KpyLZt2w7K/n9Ge3u7rFq1Sn72s5/J5s2b5YQTTjjoVyW/96+nent7pdFoHJLXYsCGUDjMfeMb35BcLieXX365bN++fcKfe16pnnvuuVKv1+WBBx4Y9/l7771XoiiSc845R0REWlpapLOzU1588cVxcw8++OAB3IJ37N19//33j/v8fffdZ/r6vddYvPcH/P7MmTNnwu14+OGHJ7xTOND972X9ldSBgYFxHzc1NcncuXNlbGzsn/r+7/Xef2v6/ve/LyJ/Px84/HDx2mFu3rx58uSTT8rFF18s8+fP1yuaQwjS19cnTz75pMRisQn/frAv559/vixbtkxuueUW2bhxoyxcuFCeffZZeeqpp+T6668f9/f9q1evljvvvFNWr14tn/jEJ+TFF1+Uv/3tbwd8O0488US5+OKL5cEHH5ShoSH55Cc/Kc8//7z09vaavn7OnDlSKBTkoYcekubmZsnn87Jo0SI56qij/uHXrV69Wq688kq58MIL5cwzz5SXXnpJfvOb30hnZ+eE44vH43LXXXfJ0NCQpNNpOf3006W7u9t1O/f+Our+/s5+wYIFsnTpUjn55JOlvb1d/vSnP8kvfvELufbaa13fb3/6+vpk+fLlcvbZZ8vvf/97eeKJJ+SSSy6RhQsXHtTvg4+QD/V3n/CB6e3tDVdddVWYO3duyGQyIZvNhmOOOSZceeWVYd26deNmL7vsspDP5/e5Z2RkJNxwww1h2rRpIZlMhnnz5oXvfve7odFojJsrFovhy1/+cmhtbQ3Nzc1hxYoVYceOHe/7q487d+4c9/WPPvpoEJHQ19ennyuVSuG6664LHR0dIZ/Ph/PPPz9s3rzZ9CupIYTw1FNPhQULFoREIjHu11OXLFkSjjvuuH1+Tb1eD9/85jdDZ2dnyOVy4ayzzgq9vb0TfiU1hBB+/OMfh9mzZ4d4PD7u11N7enr2+eujS5YsCUuWLBn3OeuvpN5+++3h1FNPDYVCQc/lt7/97VCpVHTm/c7j3vv83d7vvLz66qvhoosuCs3NzaGtrS1ce+21oVQq7ff4cOiKQviA/iUPwCFj78VxO3funPCuCIc3/k0BAKAIBQCAIhQAAIp/UwAAKN4pAAAUoQAAUOaL1/r7+12La7WaeZY+lQ/ev8R97v2LUee8Z9zbRxwc22P+5XZRw7U6cswH8T0GI+dr2I/K34xP5nPNexunTJmy3xneKQAAFKEAAFCEAgBAEQoAAEUoAAAUoQAAUIQCAEARCgAARSgAABShAABQhAIAQJm7j+Lx+GQeBz5g/xLdR05Ro+6ad7XOxHz3d8PTCxScz81g3x3FfN06kXi6krzdRHQfvddk3EbeKQAAFKEAAFCEAgBAEQoAAEUoAAAUoQAAUIQCAEARCgAARSgAABShAABQ5poL7+XUH5VLzLFvh+r5cVUGeG9j8FQ0iLiaKLxVFI7Xa2PVmmtzIpm0D9d990k8mszHlfP8/Aug5gIAMKkIBQCAIhQAAIpQAAAoQgEAoAgFAIAiFAAAilAAAChCAQCgCAUAgCIUAADK3H3k6pw5gHmMd6h2E32kOB+CdW+/V8P+DWoNX29PtVY3z77+xhuu3VOO6DbPNioV1+6u9jbzbCbt6GASkQbPiQkm4+cs7xQAAIpQAAAoQgEAoAgFAIAiFAAAilAAAChCAQCgCAUAgCIUAACKUAAAKHPNhbd2wTNPJcYHbzLv849ORYfvNsaTKdd8Pdj3l/aMuXYPDo2aZ7f373LtzjbnzbMdzc2u3bHI/jozcr4mjSJfVcikcjx/DrWfbrxTAAAoQgEAoAgFAIAiFAAAilAAAChCAQCgCAUAgCIUAACKUAAAKEIBAKAIBQCAMncfxWK+Bo/QONQaP/wc1Tf//QWTchgi4u8yik1i91Hd0fbSaPj6bOJx++uYSqXq2r1zYNg1PzxaNs+Wxuqu3aNFe1dSLJ3z7S5VzLNNOd+DtuYY9zVNueqGPlIOtW433ikAABShAABQhAIAQBEKAABFKAAAFKEAAFCEAgBAEQoAAEUoAAAUoQAAUOaai9Fiybe5Yb/ePRGPu1YHx+54wrfbMx9FvgoATy1GrDG5eR1zVFF4+wX2jNnrH0Lw3YfZhPkhK+VqzbV7m7PmYsdu+3zDc3+LSNXRF1Ec2ePavaN/l3l2y1vbXLsXzJttnp0za7prdzz4qkJcj63gfL55Tqez5cLzY8X1PDbvBADgvxEKAABFKAAAFKEAAFCEAgBAEQoAAEUoAAAUoQAAUIQCAEARCgAARSgAAJS5SGawNOZa3JTLm2djiaRrd71h77RxVwg5qkTiztqRmKP8KIpNcl47emEiZ/fR29veMs+2t7e7dmczKfPsWLno2p1L23eLiBzR1WmeDc6OmtGivT8qn/Idd6Vs7zGLxxqu3XvG7D8nas7HVRTZe69EvL1a3mOZrM2+L3BWh5nwTgEAoAgFAIAiFAAAilAAAChCAQCgCAUAgCIUAACKUAAAKEIBAKAIBQCAMl83nmjpcC2uO2oaqrG4a7dE9cmZFZF6wz4fc15jHjnmg0zC9evv3u+4lD7mvE6/VrFXHUTBd37EUXFSaLZXrYiIVKvO+zxur2fJNTW7VntqLqJ42rU7cvSzpLO+CprI8WCpRb7XpMHXuOGqi/A+xsXx/PTdg85ajEnoueCdAgBAEQoAAEUoAAAUoQAAUIQCAEARCgAARSgAABShAABQhAIAQBEKAABFKAAAlLn76Cc/fcK1OGo4ukESvnaQpuaMeXbuUTNdu085YYF5NuGM1OC4T4Kz0yR4y1siR0eNo29IRKStvd08m0rbz6WISHA0w6RSvk6gjjZfB1cQ+3wilXLtTiXMT02RpO8+LNfs53NweLdr9+DQkHl2ZGjQtbtaLLnmJbI/hzo6Cq7V8+bONs8mU45zKb46I0/XlBXvFAAAilAAAChCAQCgCAUAgCIUAACKUAAAKEIBAKAIBQCAIhQAAIpQAAAoQgEAoMylHKVi2bW4UrLPJz09LyIyYq9XkZxzd/3YY8yz5VBx7Y45uo/Sqaxrt7MqSeqOLwiOniQRkdb2LvNszLlbYvbXMZVGw7U67uwnksh+LL4jEWmI/fxs3PSGa/dbO3aYZ3cNDLh2l0r2fqL6mK9Tq1LyPd/Gxorm2ekzprh2z5wx3Tybd3YfiePce7rArHinAABQhAIAQBEKAABFKAAAFKEAAFCEAgBAEQoAAEUoAAAUoQAAUIQCAECZr79e8W8XuhaPFe2Xu+ezvkqHyHEZeNZ5iXnk6CMYHh527W7UqubZZCLj2p3I+uZDIm6eLVV99QKhYb/PY47aChGRZCJpnk04bqOISDLpqwyIYpNXFVJ11JCUG/bHlYhIvqXJPNtWKLh21yv2Y8nEfc/7wQFHv42IbHlro3l27lFzXbvjMftj3FMpIyISdzxWvPU2FrxTAAAoQgEAoAgFAIAiFAAAilAAAChCAQCgCAUAgCIUAACKUAAAKEIBAKAIBQCAMhd4NKqOUiARiTvyxtdQI9KUyptns5m0a3epbO8zKlbrrt0b39honk2lfL0wM4/qcc33bd5qnn36/zzv2l2N2fuJMumUa3fOcT7zzj6o1pYW13yhtdk8e9JJJ7h2d3W2mWfnTD/StTsW2Z9x8cj3urFSHjPPJhz9QSIipe521/y0qQX77JFTXbvrdftzv1h0dlM5uuCcp8eEdwoAAEUoAAAUoQAAUIQCAEARCgAARSgAABShAABQhAIAQBEKAABFKAAAlPk681/972ddixtV+6XdMam4djelcubZZmd1wax5082zXR1Nrt0dU2eaZ9s7u127M3lfpcPgf20yz778X5tdu0shmGcTzo6ThNh3Nzvvk7kzfVUh//PUj5tnO/L2SgwRkXzcXgERItdqqVRq5tla3V5bISJSHBo0z1brvvqHbM53PgsFex3O9re3u3b39+8yz2bzvsqaKUfYn/u5nK/Gp7Nl/49D3ikAABShAABQhAIAQBEKAABFKAAAFKEAAFCEAgBAEQoAAEUoAAAUoQAAUIQCAECZC1b+9J8vuxZnkinzbGVs2LU7mbJn2aL/cYpr96a37D0/A9tcq+X4444zz6ayvp6X4pivPyqZsXemnPTxE1y7yyV7X04qae/4ERGZN/so8+xxx8537Z7WWXDNt+TsnTaNsu/8bH57p3l2x+7drt3b+u27R/eMunYPDg6aZytVX69SMuV7rKTS9udQvWbv1BIRqVbt/VG5gq/36nix/5xobfXtnn1E135neKcAAFCEAgBAEQoAAEUoAAAUoQAAUIQCAEARCgAARSgAABShAABQhAIAQJmvG9+5ZZNrcXtbm3n2yOndrt0LTphnnk2mI9fuV9b9wTw7JeOromiK6ubZHf2+Do18S6trvqPFfuzLz17s2h2L7K81Wlt9x93Z0WGe3bVrwLW7b9PrrvmhQXs9y/DQiGv3yHDRPDs46qui2DU8ZJ6tVauu3clk0jybSttnRURicd9r2NYW+3O/UCi4drd12+sl0rmca3cqa5/fUyq7dlvwTgEAoAgFAIAiFAAAilAAAChCAQCgCAUAgCIUAACKUAAAKEIBAKAIBQCAIhQAAMrcffTW3151LR5uaTLPfu6zV7p2n332GebZf/+PZ127uwv2TpPuXN61O5uwd7FkooZr95TWFtd8s2M+k/N1PNUkmGdTaefuuv1+efu1t1y739yx3TVfqdpvZyLje6w0N7ebZ7szvm6dasXXZ+SRTNn7jOLOLiPvfHOz/bnc0mKffedY7M/lPaP2HisRke3b+82z5bJvt3xi4X5HeKcAAFCEAgBAEQoAAEUoAAAUoQAAUIQCAEARCgAARSgAABShAABQhAIAQBEKAABl7j4qF0ddiz+28Hjz7OlnnO7a3VHoMM9+atFi1+5YzN5n05xMu3a3NNn7b+IpXydQIpV1zQfH7WxIxbV7aPeAebYl4bsPGxI3z86eb38Mioh0Tz/aNb9r97B5trlQcO2u1u3nJwq+13bJmP0+bDR8HVzlctk8u2d0j2t3aNRd83uK9v2bt21z7S6X7J1D1aL9PhERqdfttzOX9z1/LHinAABQhAIAQBEKAABFKAAAFKEAAFCEAgBAEQoAAEUoAAAUoQAAUIQCAECZay5mH7PQtfiL/2u1ebZYT7p2v9a73TzbiHy7My1N5tlqiFy7dw06LtNv2C+jFxGp10uu+ch85kUaMubaPTI8Yp6Nb6+6dm/dscM8Ozbm290o11zz+Zy9tuSN17e4dve9+aZ5Nkr4HuPtnfaamMqY79wPDQ2ZZwf6+127g6P+QUQkFrNXdESOWRGRfNZeK1PI2B8nIiKZjL26orTH97y34J0CAEARCgAARSgAABShAABQhAIAQBEKAABFKAAAFKEAAFCEAgBAEQoAAEUoAACUuQHnwksucS1uO2K6efall329MJWKvdOm0vB1mtQlbp4NDV+mxsXelRRJcO2u1323Mzj2x9wvHey7qzXfcfcP2HuvajVfL4yz/kYKLQXzbKXi6xDaNTBqH47bH7MiIv39ZfPsWNV3H9ZK9t31SsW1O55yFHaJSC6TMs+m487ncs1+n1fKvg4uEXvHUzafce7eP94pAAAUoQAAUIQCAEARCgAARSgAABShAABQhAIAQBEKAABFKAAAFKEAAFDm68b/c92fXIv/+v/WmWcjybp2x+NJ82wimfbtTnguG7cfh4hI3FFHkEj58jqT8V3unkzajz2V9t2HsZT9fMaD7z5sSbXZjyPd5NpdjdvrBUREyvWaebbmay2RVC5nnq0WfRUaxdFh82yl5tsdVR2VDs7+lErdWf0yWjTPjo74bmfOUbnR1ep7HCZy9udyyvf0MeGdAgBAEQoAAEUoAAAUoQAAUIQCAEARCgAARSgAABShAABQhAIAQBEKAABFKAAAlLnA43cv/rtrcXF40DybStp7XkREsrlmx7S9o0REJB7s88GZqbGkp/socu3OpH3dR5mMvc8olfGdn0Suw34cqVbX7lTM0XvlfMkTZXz3eRTZu3iqYxXX7rFS2b676tvdiBr2YcdtFBFJiGM+Zn8+iIhI2lf005q3z7fmfT8nmrIp82w66bi/RSQZ2fujorqvs8mCdwoAAEUoAAAUoQAAUIQCAEARCgAARSgAABShAABQhAIAQBEKAABFKAAAlPna7ildLa7F20o7zbP1+qBrd0t7u3k2EfkujR/u322eHRkede2u1u11BI2a7/L10PBdSu/iqJYQEUllu82zIel7XNUiex1BzNlzkUtlXfP5rL3+o16tuXZLw1EXkfbdzshRoZJJ+eofso76lPamvGv39CZPvY3I9Kmd5tmcryVGxsoj5tlYsFeWiIgk4vbzU2jxPWYteKcAAFCEAgBAEQoAAEUoAAAUoQAAUIQCAEARCgAARSgAABShAABQhAIAQBEKAABlLjYJ1aJrcWs+ZZ4dKfu6Qar1PebZ+ccc59odptp7lXb2D7h27xjoN8/uGay7dheLvvNTr9u7eBo13/nJJ1rNs8ecMMe1e+uwvXNm5/Cga3ep4uuyKpVL5tm42PtsRETSSfvzJ5/0dVMV8va+nK5CwbX7iGlHmGfnHjnFtbs7HXfN7xkdNs/u2mXvahMRiafsr6dz+TbX7qZm+/np6PDttuCdAgBAEQoAAEUoAAAUoQAAUIQCAEARCgAARSgAABShAABQhAIAQBEKAABlrrkY2LrFtbhetVcjlCS4dhc3v2mebY/7KgA6M3nzbHLMVy2RjTXMs6W47z4JwV5b8Q5HjUbkPD8le53Haaf4akiOO/Zj5tk339zk2j0wuNs1PzZWsQ83fPdhImavdMjGfLs7M2nzbCFvfz6IiNQdj6u3++3PYxGR1/q3ueajjL0qpKW7w7U729Jsns01++7D9k77sTS12itlrHinAABQhAIAQBEKAABFKAAAFKEAAFCEAgBAEQoAAEUoAAAUoQAAUIQCAEARCgAAZe4+OmJqu2vxljftXUm1MWdvT2Sf7/vba67VQ6mcedabqKONqn22Zp8VEWnUvd1H9r6ceBS5No+VR8yzf/m/z7p2L803mWePj/nOUKnV3mcjItKo2Xt+oprv/JQr9u6wofqYa/eOAXs31ab12127+0vD5tly0ve4ynb7fga1HVEwz6Zb7M97EZF41t6rlGttce1O5+xdSVHc/CPcjHcKAABFKAAAFKEAAFCEAgBAEQoAAEUoAAAUoQAAUIQCAEARCgAARSgAABShAABQ5uKMGfNmuBYPj9o7UEa32LtY3mHvTCk7O4F21Rrm2VTk6x2pBPux1IO9V0dERIL9uL2i4Ouo8VQl9f71j67dm0fsnVBdsaxrdwj2PigRkbqjW2lPzHd+3g727qPesaJr95aavSupmPM9xptnTDXPTjmqx7U7U/B1CEnMcexx3+vjpiZ7B1euxdepFUumzbMhOviv63mnAABQhAIAQBEKAABFKAAAFKEAAFCEAgBAEQoAAEUoAAAUoQAAUIQCAECZrwNvaWt3Le6a0m2e3easufCULjR8zQUyJvZ6iapzt6e6oi6TV1vhFcR5Qx0nqFoquVaP9u80z8bSBdfu+Ji9WkJEZKvjsbJO7NUSIiK9Cfv5H21Kunbnp7eZZ7umTXPt7uiaYp5N53Ou3RXn4zA4ql/Sibhrd9wxH497d9vrOWLO3aadB30jAOCQRSgAABShAABQhAIAQBEKAABFKAAAFKEAAFCEAgBAEQoAAEUoAAAUoQAAUOaSjWwm71qczqTNs8mUL5vqVXunSfAUJYlILfL0qzj7iTyrvQcenP1EDo3IdyzBMb+n4bsP11eK5tnWVNa3u7zdNf9KbdQ8u6vF1/PTPuMo8+zUWb5+osJUe49ZOt/k2h1r2M991dFNJCIST6R880n7z6BEyrc7itlvZ71u78gSEYkcz59YdPBf1/NOAQCgCAUAgCIUAACKUAAAKEIBAKAIBQCAIhQAAIpQAAAoQgEAoAgFAIAy11xU6zXX4tHSiHm2uZBx7S6Pjpln684ahbrjsvG6t1nC8QWR78p4EXHWYjgEZ+VGiJsfVjIa8z2uflcZMs9uKvp278r5XiMlpswwzx5xZJdr91FdnebZjtYO1+6Yo7pi1NXNIlJ21MQkEnHX7oyjOkdEJJOzV/MkUr6fQZmsvbYknfHtTiaTrvmDjXcKAABFKAAAFKEAAFCEAgBAEQoAAEUoAAAUoQAAUIQCAEARCgAARSgAABShAABQju4je9+QiEg8Ze9Aaeuyd5SIiFSbUubZWtXXfeQZrzp7lYKj+yjmWy2Rs/soiuzzwTErIiIJe3dLIuHbXc3az/1Ya7tr9+zWbtd8W3uLebapxd4HJSLSlLP3AqUzvt3lmr1YqyK+Eq7g6O2JJ33HLd7HoWM+mbI/rkRE4o7epqTzdsbj9t3B2U1lwTsFAIAiFAAAilAAAChCAQCgCAUAgCIUAACKUAAAKEIBAKAIBQCAIhQAAMp8/XU86bvEvNDeZJ5tyvmyqV6xX9rtrbmo1e3zwVktEYvZL3ePnHkdc1YAxGL2S+ljCd+xJJL285N11AWIiDQ32ytRpjS1unY3pbOu+XzKPp9K2+sfREQqjvE9Kd/5KdVr5tl65NudcVScpOK++gdvFUXMURcRxXy3MwT7Y7xSqbp2p1L2+VTS9/yx4J0CAEARCgAARSgAABShAABQhAIAQBEKAABFKAAAFKEAAFCEAgBAEQoAAEUoAABUFDwlHgCAwxrvFAAAilAAAChCAQCgCAUAgCIUAACKUAAAKEIBAKAIBQCAIhQAAOr/A4t35BY2aLkMAAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "dataset_show_image(test_dset, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "IGs1A1fy3O3h"
      },
      "outputs": [],
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_dset, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                                           num_workers=NUM_WORKERS, pin_memory=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dset, batch_size=BATCH_SIZE, shuffle=False,\n",
        "                                          num_workers=NUM_WORKERS, pin_memory=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWEj9CuI3Rjj",
        "outputId": "34daaffb-66fe-47a1-af42-101d1b24671e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Linear Attention window(4, 4) f3 kernel5\n",
            "Linear Attention window(4, 4) f3 kernel5\n",
            "Linear Attention window(4, 4) f3 kernel5\n",
            "Linear Attention window(4, 4) f3 kernel5\n",
            "Linear Attention window(2, 2) f3 kernel5\n",
            "Linear Attention window(2, 2) f3 kernel5\n",
            "Linear Attention window(2, 2) f3 kernel5\n",
            "Linear Attention window(2, 2) f3 kernel5\n",
            "Linear Attention window(2, 2) f3 kernel5\n",
            "Linear Attention window(2, 2) f3 kernel5\n",
            "Linear Attention window(1, 1) f3 kernel5\n",
            "Linear Attention window(1, 1) f3 kernel5\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "FLattenSwinTransformer(\n",
              "  (patch_embed): PatchEmbed(\n",
              "    (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
              "    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (pos_drop): Dropout(p=0.1, inplace=False)\n",
              "  (layers): ModuleList(\n",
              "    (0): BasicLayer(\n",
              "      dim=96, input_resolution=(8, 8), depth=2\n",
              "      (blocks): ModuleList(\n",
              "        (0): SwinTransformerBlock(\n",
              "          dim=96, input_resolution=(8, 8), num_heads=3, window_size=4, shift_size=0, mlp_ratio=4.0\n",
              "          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
              "          (attn): FocusedLinearAttention(\n",
              "            dim=96, window_size=(4, 4), num_heads=3\n",
              "            (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
              "            (attn_drop): Dropout(p=0.1, inplace=False)\n",
              "            (proj): Linear(in_features=96, out_features=96, bias=True)\n",
              "            (proj_drop): Dropout(p=0.1, inplace=False)\n",
              "            (softmax): Softmax(dim=-1)\n",
              "            (dwc): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=32)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
              "            (drop): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): SwinTransformerBlock(\n",
              "          dim=96, input_resolution=(8, 8), num_heads=3, window_size=4, shift_size=2, mlp_ratio=4.0\n",
              "          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
              "          (attn): FocusedLinearAttention(\n",
              "            dim=96, window_size=(4, 4), num_heads=3\n",
              "            (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
              "            (attn_drop): Dropout(p=0.1, inplace=False)\n",
              "            (proj): Linear(in_features=96, out_features=96, bias=True)\n",
              "            (proj_drop): Dropout(p=0.1, inplace=False)\n",
              "            (softmax): Softmax(dim=-1)\n",
              "            (dwc): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=32)\n",
              "          )\n",
              "          (drop_path): DropPath(drop_prob=0.009)\n",
              "          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
              "            (drop): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (downsample): PatchMerging(\n",
              "        input_resolution=(8, 8), dim=96\n",
              "        (reduction): Linear(in_features=384, out_features=192, bias=False)\n",
              "        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicLayer(\n",
              "      dim=192, input_resolution=(4, 4), depth=2\n",
              "      (blocks): ModuleList(\n",
              "        (0): SwinTransformerBlock(\n",
              "          dim=192, input_resolution=(4, 4), num_heads=6, window_size=4, shift_size=0, mlp_ratio=4.0\n",
              "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
              "          (attn): FocusedLinearAttention(\n",
              "            dim=192, window_size=(4, 4), num_heads=6\n",
              "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
              "            (attn_drop): Dropout(p=0.1, inplace=False)\n",
              "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
              "            (proj_drop): Dropout(p=0.1, inplace=False)\n",
              "            (softmax): Softmax(dim=-1)\n",
              "            (dwc): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=32)\n",
              "          )\n",
              "          (drop_path): DropPath(drop_prob=0.018)\n",
              "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
              "            (drop): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): SwinTransformerBlock(\n",
              "          dim=192, input_resolution=(4, 4), num_heads=6, window_size=4, shift_size=0, mlp_ratio=4.0\n",
              "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
              "          (attn): FocusedLinearAttention(\n",
              "            dim=192, window_size=(4, 4), num_heads=6\n",
              "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
              "            (attn_drop): Dropout(p=0.1, inplace=False)\n",
              "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
              "            (proj_drop): Dropout(p=0.1, inplace=False)\n",
              "            (softmax): Softmax(dim=-1)\n",
              "            (dwc): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=32)\n",
              "          )\n",
              "          (drop_path): DropPath(drop_prob=0.027)\n",
              "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
              "            (drop): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (downsample): PatchMerging(\n",
              "        input_resolution=(4, 4), dim=192\n",
              "        (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
              "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "    (2): BasicLayer(\n",
              "      dim=384, input_resolution=(2, 2), depth=6\n",
              "      (blocks): ModuleList(\n",
              "        (0): SwinTransformerBlock(\n",
              "          dim=384, input_resolution=(2, 2), num_heads=12, window_size=2, shift_size=0, mlp_ratio=4.0\n",
              "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "          (attn): FocusedLinearAttention(\n",
              "            dim=384, window_size=(2, 2), num_heads=12\n",
              "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
              "            (attn_drop): Dropout(p=0.1, inplace=False)\n",
              "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "            (proj_drop): Dropout(p=0.1, inplace=False)\n",
              "            (softmax): Softmax(dim=-1)\n",
              "            (dwc): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=32)\n",
              "          )\n",
              "          (drop_path): DropPath(drop_prob=0.036)\n",
              "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "            (drop): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): SwinTransformerBlock(\n",
              "          dim=384, input_resolution=(2, 2), num_heads=12, window_size=2, shift_size=0, mlp_ratio=4.0\n",
              "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "          (attn): FocusedLinearAttention(\n",
              "            dim=384, window_size=(2, 2), num_heads=12\n",
              "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
              "            (attn_drop): Dropout(p=0.1, inplace=False)\n",
              "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "            (proj_drop): Dropout(p=0.1, inplace=False)\n",
              "            (softmax): Softmax(dim=-1)\n",
              "            (dwc): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=32)\n",
              "          )\n",
              "          (drop_path): DropPath(drop_prob=0.045)\n",
              "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "            (drop): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): SwinTransformerBlock(\n",
              "          dim=384, input_resolution=(2, 2), num_heads=12, window_size=2, shift_size=0, mlp_ratio=4.0\n",
              "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "          (attn): FocusedLinearAttention(\n",
              "            dim=384, window_size=(2, 2), num_heads=12\n",
              "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
              "            (attn_drop): Dropout(p=0.1, inplace=False)\n",
              "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "            (proj_drop): Dropout(p=0.1, inplace=False)\n",
              "            (softmax): Softmax(dim=-1)\n",
              "            (dwc): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=32)\n",
              "          )\n",
              "          (drop_path): DropPath(drop_prob=0.055)\n",
              "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "            (drop): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): SwinTransformerBlock(\n",
              "          dim=384, input_resolution=(2, 2), num_heads=12, window_size=2, shift_size=0, mlp_ratio=4.0\n",
              "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "          (attn): FocusedLinearAttention(\n",
              "            dim=384, window_size=(2, 2), num_heads=12\n",
              "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
              "            (attn_drop): Dropout(p=0.1, inplace=False)\n",
              "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "            (proj_drop): Dropout(p=0.1, inplace=False)\n",
              "            (softmax): Softmax(dim=-1)\n",
              "            (dwc): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=32)\n",
              "          )\n",
              "          (drop_path): DropPath(drop_prob=0.064)\n",
              "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "            (drop): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): SwinTransformerBlock(\n",
              "          dim=384, input_resolution=(2, 2), num_heads=12, window_size=2, shift_size=0, mlp_ratio=4.0\n",
              "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "          (attn): FocusedLinearAttention(\n",
              "            dim=384, window_size=(2, 2), num_heads=12\n",
              "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
              "            (attn_drop): Dropout(p=0.1, inplace=False)\n",
              "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "            (proj_drop): Dropout(p=0.1, inplace=False)\n",
              "            (softmax): Softmax(dim=-1)\n",
              "            (dwc): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=32)\n",
              "          )\n",
              "          (drop_path): DropPath(drop_prob=0.073)\n",
              "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "            (drop): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): SwinTransformerBlock(\n",
              "          dim=384, input_resolution=(2, 2), num_heads=12, window_size=2, shift_size=0, mlp_ratio=4.0\n",
              "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "          (attn): FocusedLinearAttention(\n",
              "            dim=384, window_size=(2, 2), num_heads=12\n",
              "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
              "            (attn_drop): Dropout(p=0.1, inplace=False)\n",
              "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "            (proj_drop): Dropout(p=0.1, inplace=False)\n",
              "            (softmax): Softmax(dim=-1)\n",
              "            (dwc): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=32)\n",
              "          )\n",
              "          (drop_path): DropPath(drop_prob=0.082)\n",
              "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "            (drop): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (downsample): PatchMerging(\n",
              "        input_resolution=(2, 2), dim=384\n",
              "        (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
              "        (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "    (3): BasicLayer(\n",
              "      dim=768, input_resolution=(1, 1), depth=2\n",
              "      (blocks): ModuleList(\n",
              "        (0): SwinTransformerBlock(\n",
              "          dim=768, input_resolution=(1, 1), num_heads=24, window_size=1, shift_size=0, mlp_ratio=4.0\n",
              "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (attn): FocusedLinearAttention(\n",
              "            dim=768, window_size=(1, 1), num_heads=24\n",
              "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "            (attn_drop): Dropout(p=0.1, inplace=False)\n",
              "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (proj_drop): Dropout(p=0.1, inplace=False)\n",
              "            (softmax): Softmax(dim=-1)\n",
              "            (dwc): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=32)\n",
              "          )\n",
              "          (drop_path): DropPath(drop_prob=0.091)\n",
              "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (drop): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): SwinTransformerBlock(\n",
              "          dim=768, input_resolution=(1, 1), num_heads=24, window_size=1, shift_size=0, mlp_ratio=4.0\n",
              "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (attn): FocusedLinearAttention(\n",
              "            dim=768, window_size=(1, 1), num_heads=24\n",
              "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "            (attn_drop): Dropout(p=0.1, inplace=False)\n",
              "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (proj_drop): Dropout(p=0.1, inplace=False)\n",
              "            (softmax): Softmax(dim=-1)\n",
              "            (dwc): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=32)\n",
              "          )\n",
              "          (drop_path): DropPath(drop_prob=0.100)\n",
              "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (drop): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  (avgpool): AdaptiveAvgPool1d(output_size=1)\n",
              "  (head): Linear(in_features=768, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Data loading\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(32),  # Swin Transformer expects 224x224 input\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "# Model instantiation\n",
        "model = FLattenSwinTransformer(\n",
        "    img_size=32,\n",
        "    patch_size=4,\n",
        "    in_chans=3,\n",
        "    num_classes=10,  # CIFAR-10 has 10 classes\n",
        "    embed_dim=96,\n",
        "    depths=[2, 2, 6, 2],\n",
        "    num_heads=[3, 6, 12, 24],\n",
        "    window_size=4,\n",
        "    mlp_ratio=4.,\n",
        "    qkv_bias=True,\n",
        "    drop_rate=0.1,\n",
        "    attn_drop_rate=0.1,\n",
        "    drop_path_rate=0.1,\n",
        "    ape=False,\n",
        "    patch_norm=True,\n",
        "    use_checkpoint=False,\n",
        "    focusing_factor=3,\n",
        "    kernel_size=5,\n",
        "    attn_type='LLLL'\n",
        ")\n",
        "\n",
        "model.to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqpdKvMZ6RWm",
        "outputId": "21ff8c31-b326-4f89-f11a-e2633d8435fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of parameters: 27,538,090\n",
            "Epoch [1/100] Batch [0/1563] Loss: 2.5473 Acc: 3.12%\n",
            "Epoch [1/100] Batch [100/1563] Loss: 2.3404 Acc: 9.34%\n",
            "Epoch [1/100] Batch [200/1563] Loss: 2.2921 Acc: 9.27%\n",
            "Epoch [1/100] Batch [300/1563] Loss: 2.3384 Acc: 9.94%\n",
            "Epoch [1/100] Batch [400/1563] Loss: 2.4226 Acc: 10.27%\n",
            "Epoch [1/100] Batch [500/1563] Loss: 2.3307 Acc: 10.32%\n",
            "Epoch [1/100] Batch [600/1563] Loss: 2.2687 Acc: 10.34%\n",
            "Epoch [1/100] Batch [700/1563] Loss: 2.2369 Acc: 10.32%\n",
            "Epoch [1/100] Batch [800/1563] Loss: 2.3148 Acc: 10.24%\n",
            "Epoch [1/100] Batch [900/1563] Loss: 2.3438 Acc: 10.24%\n",
            "Epoch [1/100] Batch [1000/1563] Loss: 2.2814 Acc: 10.24%\n",
            "Epoch [1/100] Batch [1100/1563] Loss: 2.3201 Acc: 10.31%\n",
            "Epoch [1/100] Batch [1200/1563] Loss: 2.2960 Acc: 10.22%\n",
            "Epoch [1/100] Batch [1300/1563] Loss: 2.3491 Acc: 10.19%\n",
            "Epoch [1/100] Batch [1400/1563] Loss: 2.3458 Acc: 10.23%\n",
            "Epoch [1/100] Batch [1500/1563] Loss: 2.2691 Acc: 10.22%\n",
            "Epoch [1/100], Loss: 2.3477, Accuracy: 10.23%\n",
            "Test Loss: 2.3104, Test Accuracy: 10.00%\n",
            "Epoch [2/100] Batch [0/1563] Loss: 2.3400 Acc: 0.00%\n",
            "Epoch [2/100] Batch [100/1563] Loss: 2.2907 Acc: 9.62%\n",
            "Epoch [2/100] Batch [200/1563] Loss: 2.2713 Acc: 9.87%\n",
            "Epoch [2/100] Batch [300/1563] Loss: 2.3278 Acc: 9.80%\n",
            "Epoch [2/100] Batch [400/1563] Loss: 2.3136 Acc: 9.69%\n",
            "Epoch [2/100] Batch [500/1563] Loss: 2.2972 Acc: 9.49%\n",
            "Epoch [2/100] Batch [600/1563] Loss: 2.3165 Acc: 9.48%\n",
            "Epoch [2/100] Batch [700/1563] Loss: 2.3193 Acc: 9.37%\n",
            "Epoch [2/100] Batch [800/1563] Loss: 2.3196 Acc: 9.43%\n",
            "Epoch [2/100] Batch [900/1563] Loss: 2.3252 Acc: 9.47%\n",
            "Epoch [2/100] Batch [1000/1563] Loss: 2.2935 Acc: 9.50%\n",
            "Epoch [2/100] Batch [1100/1563] Loss: 2.3099 Acc: 9.51%\n",
            "Epoch [2/100] Batch [1200/1563] Loss: 2.3048 Acc: 9.60%\n",
            "Epoch [2/100] Batch [1300/1563] Loss: 2.2979 Acc: 9.64%\n",
            "Epoch [2/100] Batch [1400/1563] Loss: 2.2966 Acc: 9.67%\n",
            "Epoch [2/100] Batch [1500/1563] Loss: 2.3148 Acc: 9.73%\n",
            "Epoch [2/100], Loss: 2.3078, Accuracy: 9.77%\n",
            "Test Loss: 2.3042, Test Accuracy: 10.00%\n",
            "Epoch [3/100] Batch [0/1563] Loss: 2.3085 Acc: 9.38%\n",
            "Epoch [3/100] Batch [100/1563] Loss: 2.3218 Acc: 10.83%\n",
            "Epoch [3/100] Batch [200/1563] Loss: 2.3069 Acc: 10.45%\n",
            "Epoch [3/100] Batch [300/1563] Loss: 2.2936 Acc: 10.45%\n",
            "Epoch [3/100] Batch [400/1563] Loss: 2.2979 Acc: 10.24%\n",
            "Epoch [3/100] Batch [500/1563] Loss: 2.2950 Acc: 10.12%\n",
            "Epoch [3/100] Batch [600/1563] Loss: 2.3145 Acc: 10.06%\n",
            "Epoch [3/100] Batch [700/1563] Loss: 2.2976 Acc: 10.03%\n",
            "Epoch [3/100] Batch [800/1563] Loss: 2.3003 Acc: 10.07%\n",
            "Epoch [3/100] Batch [900/1563] Loss: 2.3113 Acc: 9.99%\n",
            "Epoch [3/100] Batch [1000/1563] Loss: 2.3069 Acc: 10.02%\n",
            "Epoch [3/100] Batch [1100/1563] Loss: 2.3088 Acc: 10.07%\n",
            "Epoch [3/100] Batch [1200/1563] Loss: 2.3006 Acc: 10.04%\n",
            "Epoch [3/100] Batch [1300/1563] Loss: 2.2853 Acc: 10.06%\n",
            "Epoch [3/100] Batch [1400/1563] Loss: 2.3100 Acc: 10.10%\n",
            "Epoch [3/100] Batch [1500/1563] Loss: 2.3040 Acc: 10.11%\n",
            "Epoch [3/100], Loss: 2.3053, Accuracy: 10.11%\n",
            "Test Loss: 2.3115, Test Accuracy: 10.00%\n",
            "Epoch [4/100] Batch [0/1563] Loss: 2.3379 Acc: 9.38%\n",
            "Epoch [4/100] Batch [100/1563] Loss: 2.3061 Acc: 9.13%\n",
            "Epoch [4/100] Batch [200/1563] Loss: 2.2824 Acc: 9.87%\n",
            "Epoch [4/100] Batch [300/1563] Loss: 2.2879 Acc: 9.90%\n",
            "Epoch [4/100] Batch [400/1563] Loss: 2.3204 Acc: 9.80%\n",
            "Epoch [4/100] Batch [500/1563] Loss: 2.2848 Acc: 9.97%\n",
            "Epoch [4/100] Batch [600/1563] Loss: 2.3122 Acc: 9.80%\n",
            "Epoch [4/100] Batch [700/1563] Loss: 2.2838 Acc: 9.86%\n",
            "Epoch [4/100] Batch [800/1563] Loss: 2.3232 Acc: 9.90%\n",
            "Epoch [4/100] Batch [900/1563] Loss: 2.3116 Acc: 9.95%\n",
            "Epoch [4/100] Batch [1000/1563] Loss: 2.3006 Acc: 9.92%\n",
            "Epoch [4/100] Batch [1100/1563] Loss: 2.2969 Acc: 9.89%\n",
            "Epoch [4/100] Batch [1200/1563] Loss: 2.3320 Acc: 10.03%\n",
            "Epoch [4/100] Batch [1300/1563] Loss: 2.3250 Acc: 9.96%\n",
            "Epoch [4/100] Batch [1400/1563] Loss: 2.3082 Acc: 9.98%\n",
            "Epoch [4/100] Batch [1500/1563] Loss: 2.2934 Acc: 9.94%\n",
            "Epoch [4/100], Loss: 2.3049, Accuracy: 9.96%\n",
            "Test Loss: 2.3040, Test Accuracy: 10.00%\n",
            "Epoch [5/100] Batch [0/1563] Loss: 2.3098 Acc: 3.12%\n",
            "Epoch [5/100] Batch [100/1563] Loss: 2.3189 Acc: 9.25%\n",
            "Epoch [5/100] Batch [200/1563] Loss: 2.3045 Acc: 9.72%\n",
            "Epoch [5/100] Batch [300/1563] Loss: 2.3014 Acc: 9.82%\n",
            "Epoch [5/100] Batch [400/1563] Loss: 2.3031 Acc: 9.77%\n",
            "Epoch [5/100] Batch [500/1563] Loss: 2.3203 Acc: 9.85%\n",
            "Epoch [5/100] Batch [600/1563] Loss: 2.3050 Acc: 9.92%\n",
            "Epoch [5/100] Batch [700/1563] Loss: 2.2896 Acc: 10.03%\n",
            "Epoch [5/100] Batch [800/1563] Loss: 2.3106 Acc: 9.81%\n",
            "Epoch [5/100] Batch [900/1563] Loss: 2.3113 Acc: 9.94%\n",
            "Epoch [5/100] Batch [1000/1563] Loss: 2.2890 Acc: 9.89%\n",
            "Epoch [5/100] Batch [1100/1563] Loss: 2.3034 Acc: 9.99%\n",
            "Epoch [5/100] Batch [1200/1563] Loss: 2.3109 Acc: 9.97%\n",
            "Epoch [5/100] Batch [1300/1563] Loss: 2.3017 Acc: 9.93%\n",
            "Epoch [5/100] Batch [1400/1563] Loss: 2.3018 Acc: 9.89%\n",
            "Epoch [5/100] Batch [1500/1563] Loss: 2.3106 Acc: 9.86%\n",
            "Epoch [5/100], Loss: 2.3046, Accuracy: 9.88%\n",
            "Test Loss: 2.3026, Test Accuracy: 10.00%\n",
            "Epoch [6/100] Batch [0/1563] Loss: 2.3047 Acc: 6.25%\n",
            "Epoch [6/100] Batch [100/1563] Loss: 2.2949 Acc: 9.75%\n",
            "Epoch [6/100] Batch [200/1563] Loss: 2.2829 Acc: 9.98%\n",
            "Epoch [6/100] Batch [300/1563] Loss: 2.3197 Acc: 11.29%\n",
            "Epoch [6/100] Batch [400/1563] Loss: 2.2253 Acc: 11.95%\n",
            "Epoch [6/100] Batch [500/1563] Loss: 2.1886 Acc: 12.77%\n",
            "Epoch [6/100] Batch [600/1563] Loss: 2.2492 Acc: 13.00%\n",
            "Epoch [6/100] Batch [700/1563] Loss: 2.2867 Acc: 13.24%\n",
            "Epoch [6/100] Batch [800/1563] Loss: 2.2444 Acc: 13.71%\n",
            "Epoch [6/100] Batch [900/1563] Loss: 2.3018 Acc: 14.22%\n",
            "Epoch [6/100] Batch [1000/1563] Loss: 2.0941 Acc: 14.61%\n",
            "Epoch [6/100] Batch [1100/1563] Loss: 2.2659 Acc: 14.30%\n",
            "Epoch [6/100] Batch [1200/1563] Loss: 2.1724 Acc: 14.34%\n",
            "Epoch [6/100] Batch [1300/1563] Loss: 2.3373 Acc: 14.56%\n",
            "Epoch [6/100] Batch [1400/1563] Loss: 2.1669 Acc: 14.87%\n",
            "Epoch [6/100] Batch [1500/1563] Loss: 2.2496 Acc: 15.12%\n",
            "Epoch [6/100], Loss: 2.2403, Accuracy: 15.23%\n",
            "Test Loss: 2.1861, Test Accuracy: 18.11%\n",
            "Epoch [7/100] Batch [0/1563] Loss: 2.3423 Acc: 3.12%\n",
            "Epoch [7/100] Batch [100/1563] Loss: 2.1995 Acc: 18.47%\n",
            "Epoch [7/100] Batch [200/1563] Loss: 2.2838 Acc: 16.98%\n",
            "Epoch [7/100] Batch [300/1563] Loss: 2.3257 Acc: 16.43%\n",
            "Epoch [7/100] Batch [400/1563] Loss: 2.1775 Acc: 15.72%\n",
            "Epoch [7/100] Batch [500/1563] Loss: 2.3103 Acc: 15.72%\n",
            "Epoch [7/100] Batch [600/1563] Loss: 2.3296 Acc: 14.85%\n",
            "Epoch [7/100] Batch [700/1563] Loss: 2.3407 Acc: 14.22%\n",
            "Epoch [7/100] Batch [800/1563] Loss: 2.2975 Acc: 13.76%\n",
            "Epoch [7/100] Batch [900/1563] Loss: 2.3212 Acc: 13.31%\n",
            "Epoch [7/100] Batch [1000/1563] Loss: 2.2832 Acc: 13.07%\n",
            "Epoch [7/100] Batch [1100/1563] Loss: 2.3066 Acc: 12.83%\n",
            "Epoch [7/100] Batch [1200/1563] Loss: 2.2902 Acc: 12.65%\n",
            "Epoch [7/100] Batch [1300/1563] Loss: 2.3044 Acc: 12.46%\n",
            "Epoch [7/100] Batch [1400/1563] Loss: 2.3039 Acc: 12.27%\n",
            "Epoch [7/100] Batch [1500/1563] Loss: 2.2921 Acc: 12.18%\n",
            "Epoch [7/100], Loss: 2.2820, Accuracy: 12.14%\n",
            "Test Loss: 2.3097, Test Accuracy: 9.95%\n",
            "Epoch [8/100] Batch [0/1563] Loss: 2.3062 Acc: 15.62%\n",
            "Epoch [8/100] Batch [100/1563] Loss: 2.2904 Acc: 8.97%\n",
            "Epoch [8/100] Batch [200/1563] Loss: 2.2979 Acc: 9.30%\n",
            "Epoch [8/100] Batch [300/1563] Loss: 2.3057 Acc: 9.31%\n",
            "Epoch [8/100] Batch [400/1563] Loss: 2.2975 Acc: 9.51%\n",
            "Epoch [8/100] Batch [500/1563] Loss: 2.3001 Acc: 9.45%\n",
            "Epoch [8/100] Batch [600/1563] Loss: 2.3062 Acc: 9.49%\n",
            "Epoch [8/100] Batch [700/1563] Loss: 2.2243 Acc: 9.59%\n",
            "Epoch [8/100] Batch [800/1563] Loss: 2.0729 Acc: 10.37%\n",
            "Epoch [8/100] Batch [900/1563] Loss: 2.3088 Acc: 10.39%\n",
            "Epoch [8/100] Batch [1000/1563] Loss: 2.3081 Acc: 10.33%\n",
            "Epoch [8/100] Batch [1100/1563] Loss: 2.2880 Acc: 10.34%\n",
            "Epoch [8/100] Batch [1200/1563] Loss: 2.3045 Acc: 10.37%\n",
            "Epoch [8/100] Batch [1300/1563] Loss: 2.2986 Acc: 10.37%\n",
            "Epoch [8/100] Batch [1400/1563] Loss: 2.2881 Acc: 10.34%\n",
            "Epoch [8/100] Batch [1500/1563] Loss: 2.3067 Acc: 10.31%\n",
            "Epoch [8/100], Loss: 2.2978, Accuracy: 10.28%\n",
            "Test Loss: 2.3028, Test Accuracy: 10.00%\n",
            "Epoch [9/100] Batch [0/1563] Loss: 2.3039 Acc: 15.62%\n",
            "Epoch [9/100] Batch [100/1563] Loss: 2.3042 Acc: 10.12%\n",
            "Epoch [9/100] Batch [200/1563] Loss: 2.3002 Acc: 9.83%\n",
            "Epoch [9/100] Batch [300/1563] Loss: 2.3052 Acc: 9.83%\n",
            "Epoch [9/100] Batch [400/1563] Loss: 2.3022 Acc: 9.91%\n",
            "Epoch [9/100] Batch [500/1563] Loss: 2.3000 Acc: 9.94%\n",
            "Epoch [9/100] Batch [600/1563] Loss: 2.2936 Acc: 9.89%\n",
            "Epoch [9/100] Batch [700/1563] Loss: 2.2920 Acc: 9.99%\n",
            "Epoch [9/100] Batch [800/1563] Loss: 2.3005 Acc: 10.03%\n",
            "Epoch [9/100] Batch [900/1563] Loss: 2.3037 Acc: 9.95%\n",
            "Epoch [9/100] Batch [1000/1563] Loss: 2.2958 Acc: 9.88%\n",
            "Epoch [9/100] Batch [1100/1563] Loss: 2.2871 Acc: 9.90%\n",
            "Epoch [9/100] Batch [1200/1563] Loss: 2.3006 Acc: 9.85%\n",
            "Epoch [9/100] Batch [1300/1563] Loss: 2.3069 Acc: 9.85%\n",
            "Epoch [9/100] Batch [1400/1563] Loss: 2.2970 Acc: 9.92%\n",
            "Epoch [9/100] Batch [1500/1563] Loss: 2.3051 Acc: 9.97%\n",
            "Epoch [9/100], Loss: 2.3034, Accuracy: 10.01%\n",
            "Test Loss: 2.3030, Test Accuracy: 10.00%\n",
            "Epoch [10/100] Batch [0/1563] Loss: 2.2959 Acc: 18.75%\n",
            "Epoch [10/100] Batch [100/1563] Loss: 2.2971 Acc: 9.75%\n",
            "Epoch [10/100] Batch [200/1563] Loss: 2.2949 Acc: 10.17%\n",
            "Epoch [10/100] Batch [300/1563] Loss: 2.3136 Acc: 9.88%\n",
            "Epoch [10/100] Batch [400/1563] Loss: 2.3015 Acc: 9.91%\n",
            "Epoch [10/100] Batch [500/1563] Loss: 2.3057 Acc: 9.89%\n",
            "Epoch [10/100] Batch [600/1563] Loss: 2.2982 Acc: 9.81%\n",
            "Epoch [10/100] Batch [700/1563] Loss: 2.3018 Acc: 9.90%\n",
            "Epoch [10/100] Batch [800/1563] Loss: 2.2979 Acc: 9.88%\n",
            "Epoch [10/100] Batch [900/1563] Loss: 2.2899 Acc: 9.97%\n",
            "Epoch [10/100] Batch [1000/1563] Loss: 2.2975 Acc: 9.96%\n",
            "Epoch [10/100] Batch [1100/1563] Loss: 2.2943 Acc: 9.98%\n",
            "Epoch [10/100] Batch [1200/1563] Loss: 2.3043 Acc: 10.02%\n",
            "Epoch [10/100] Batch [1300/1563] Loss: 2.2975 Acc: 10.01%\n",
            "Epoch [10/100] Batch [1400/1563] Loss: 2.3069 Acc: 10.01%\n",
            "Epoch [10/100] Batch [1500/1563] Loss: 2.3041 Acc: 10.01%\n",
            "Epoch [10/100], Loss: 2.3033, Accuracy: 10.01%\n",
            "Test Loss: 2.3034, Test Accuracy: 10.00%\n",
            "Epoch [11/100] Batch [0/1563] Loss: 2.3107 Acc: 9.38%\n",
            "Epoch [11/100] Batch [100/1563] Loss: 2.3076 Acc: 9.03%\n",
            "Epoch [11/100] Batch [200/1563] Loss: 2.3012 Acc: 9.33%\n",
            "Epoch [11/100] Batch [300/1563] Loss: 2.3068 Acc: 9.46%\n",
            "Epoch [11/100] Batch [400/1563] Loss: 2.3152 Acc: 9.66%\n",
            "Epoch [11/100] Batch [500/1563] Loss: 2.3111 Acc: 9.76%\n",
            "Epoch [11/100] Batch [600/1563] Loss: 2.2993 Acc: 9.71%\n",
            "Epoch [11/100] Batch [700/1563] Loss: 2.3017 Acc: 9.83%\n",
            "Epoch [11/100] Batch [800/1563] Loss: 2.3027 Acc: 9.82%\n",
            "Epoch [11/100] Batch [900/1563] Loss: 2.3012 Acc: 9.81%\n",
            "Epoch [11/100] Batch [1000/1563] Loss: 2.3108 Acc: 9.77%\n",
            "Epoch [11/100] Batch [1100/1563] Loss: 2.3095 Acc: 9.71%\n",
            "Epoch [11/100] Batch [1200/1563] Loss: 2.3003 Acc: 9.72%\n",
            "Epoch [11/100] Batch [1300/1563] Loss: 2.3043 Acc: 9.78%\n",
            "Epoch [11/100] Batch [1400/1563] Loss: 2.3011 Acc: 9.84%\n",
            "Epoch [11/100] Batch [1500/1563] Loss: 2.3128 Acc: 9.87%\n",
            "Epoch [11/100], Loss: 2.3034, Accuracy: 9.91%\n",
            "Test Loss: 2.3032, Test Accuracy: 10.00%\n",
            "Epoch [12/100] Batch [0/1563] Loss: 2.2976 Acc: 3.12%\n",
            "Epoch [12/100] Batch [100/1563] Loss: 2.3045 Acc: 9.07%\n",
            "Epoch [12/100] Batch [200/1563] Loss: 2.3122 Acc: 9.45%\n",
            "Epoch [12/100] Batch [300/1563] Loss: 2.3090 Acc: 9.78%\n",
            "Epoch [12/100] Batch [400/1563] Loss: 2.3033 Acc: 9.84%\n",
            "Epoch [12/100] Batch [500/1563] Loss: 2.3078 Acc: 9.90%\n",
            "Epoch [12/100] Batch [600/1563] Loss: 2.2994 Acc: 9.99%\n",
            "Epoch [12/100] Batch [700/1563] Loss: 2.2987 Acc: 9.96%\n",
            "Epoch [12/100] Batch [800/1563] Loss: 2.3080 Acc: 9.93%\n",
            "Epoch [12/100] Batch [900/1563] Loss: 2.3016 Acc: 9.98%\n",
            "Epoch [12/100] Batch [1000/1563] Loss: 2.3060 Acc: 10.05%\n",
            "Epoch [12/100] Batch [1100/1563] Loss: 2.3065 Acc: 9.95%\n",
            "Epoch [12/100] Batch [1200/1563] Loss: 2.3036 Acc: 9.97%\n",
            "Epoch [12/100] Batch [1300/1563] Loss: 2.3084 Acc: 9.94%\n",
            "Epoch [12/100] Batch [1400/1563] Loss: 2.3050 Acc: 9.92%\n",
            "Epoch [12/100] Batch [1500/1563] Loss: 2.3150 Acc: 9.94%\n",
            "Epoch [12/100], Loss: 2.3035, Accuracy: 9.92%\n",
            "Test Loss: 2.3030, Test Accuracy: 10.00%\n",
            "Epoch [13/100] Batch [0/1563] Loss: 2.3021 Acc: 6.25%\n",
            "Epoch [13/100] Batch [100/1563] Loss: 2.2966 Acc: 9.03%\n",
            "Epoch [13/100] Batch [200/1563] Loss: 2.3023 Acc: 9.36%\n",
            "Epoch [13/100] Batch [300/1563] Loss: 2.3054 Acc: 9.80%\n",
            "Epoch [13/100] Batch [400/1563] Loss: 2.3005 Acc: 9.97%\n",
            "Epoch [13/100] Batch [500/1563] Loss: 2.3000 Acc: 10.25%\n",
            "Epoch [13/100] Batch [600/1563] Loss: 2.3032 Acc: 10.18%\n",
            "Epoch [13/100] Batch [700/1563] Loss: 2.3027 Acc: 10.04%\n",
            "Epoch [13/100] Batch [800/1563] Loss: 2.3098 Acc: 10.07%\n",
            "Epoch [13/100] Batch [900/1563] Loss: 2.3025 Acc: 10.02%\n",
            "Epoch [13/100] Batch [1000/1563] Loss: 2.3009 Acc: 10.07%\n",
            "Epoch [13/100] Batch [1100/1563] Loss: 2.3150 Acc: 10.10%\n",
            "Epoch [13/100] Batch [1200/1563] Loss: 2.3082 Acc: 10.10%\n",
            "Epoch [13/100] Batch [1300/1563] Loss: 2.3010 Acc: 10.08%\n",
            "Epoch [13/100] Batch [1400/1563] Loss: 2.2987 Acc: 10.04%\n",
            "Epoch [13/100] Batch [1500/1563] Loss: 2.3091 Acc: 10.07%\n",
            "Epoch [13/100], Loss: 2.3034, Accuracy: 10.06%\n",
            "Test Loss: 2.3027, Test Accuracy: 10.00%\n",
            "Epoch [14/100] Batch [0/1563] Loss: 2.3008 Acc: 15.62%\n",
            "Epoch [14/100] Batch [100/1563] Loss: 2.3054 Acc: 9.25%\n",
            "Epoch [14/100] Batch [200/1563] Loss: 2.3042 Acc: 9.25%\n",
            "Epoch [14/100] Batch [300/1563] Loss: 2.3100 Acc: 9.44%\n",
            "Epoch [14/100] Batch [400/1563] Loss: 2.2939 Acc: 9.69%\n",
            "Epoch [14/100] Batch [500/1563] Loss: 2.2968 Acc: 9.78%\n",
            "Epoch [14/100] Batch [600/1563] Loss: 2.3033 Acc: 9.96%\n",
            "Epoch [14/100] Batch [700/1563] Loss: 2.3009 Acc: 9.93%\n",
            "Epoch [14/100] Batch [800/1563] Loss: 2.3027 Acc: 9.91%\n",
            "Epoch [14/100] Batch [900/1563] Loss: 2.3028 Acc: 9.95%\n",
            "Epoch [14/100] Batch [1000/1563] Loss: 2.3010 Acc: 9.87%\n",
            "Epoch [14/100] Batch [1100/1563] Loss: 2.3030 Acc: 9.89%\n",
            "Epoch [14/100] Batch [1200/1563] Loss: 2.3086 Acc: 9.80%\n",
            "Epoch [14/100] Batch [1300/1563] Loss: 2.2926 Acc: 9.90%\n",
            "Epoch [14/100] Batch [1400/1563] Loss: 2.3094 Acc: 9.93%\n",
            "Epoch [14/100] Batch [1500/1563] Loss: 2.3065 Acc: 9.90%\n",
            "Epoch [14/100], Loss: 2.3034, Accuracy: 9.91%\n",
            "Test Loss: 2.3029, Test Accuracy: 10.00%\n",
            "Epoch [15/100] Batch [0/1563] Loss: 2.3019 Acc: 3.12%\n",
            "Epoch [15/100] Batch [100/1563] Loss: 2.3085 Acc: 9.72%\n",
            "Epoch [15/100] Batch [200/1563] Loss: 2.3040 Acc: 9.69%\n",
            "Epoch [15/100] Batch [300/1563] Loss: 2.2972 Acc: 9.91%\n",
            "Epoch [15/100] Batch [400/1563] Loss: 2.3095 Acc: 9.97%\n",
            "Epoch [15/100] Batch [500/1563] Loss: 2.3029 Acc: 10.02%\n",
            "Epoch [15/100] Batch [600/1563] Loss: 2.2962 Acc: 9.92%\n",
            "Epoch [15/100] Batch [700/1563] Loss: 2.3055 Acc: 9.93%\n",
            "Epoch [15/100] Batch [800/1563] Loss: 2.3041 Acc: 9.88%\n",
            "Epoch [15/100] Batch [900/1563] Loss: 2.3071 Acc: 9.90%\n",
            "Epoch [15/100] Batch [1000/1563] Loss: 2.2983 Acc: 9.97%\n",
            "Epoch [15/100] Batch [1100/1563] Loss: 2.2843 Acc: 9.99%\n",
            "Epoch [15/100] Batch [1200/1563] Loss: 2.3119 Acc: 9.98%\n",
            "Epoch [15/100] Batch [1300/1563] Loss: 2.3053 Acc: 9.95%\n",
            "Epoch [15/100] Batch [1400/1563] Loss: 2.2965 Acc: 9.87%\n",
            "Epoch [15/100] Batch [1500/1563] Loss: 2.2990 Acc: 9.84%\n",
            "Epoch [15/100], Loss: 2.3034, Accuracy: 9.84%\n",
            "Test Loss: 2.3029, Test Accuracy: 10.00%\n",
            "Epoch [16/100] Batch [0/1563] Loss: 2.3048 Acc: 9.38%\n",
            "Epoch [16/100] Batch [100/1563] Loss: 2.3009 Acc: 10.24%\n",
            "Epoch [16/100] Batch [200/1563] Loss: 2.3011 Acc: 9.83%\n",
            "Epoch [16/100] Batch [300/1563] Loss: 2.3009 Acc: 9.68%\n",
            "Epoch [16/100] Batch [400/1563] Loss: 2.3131 Acc: 9.65%\n",
            "Epoch [16/100] Batch [500/1563] Loss: 2.2936 Acc: 9.71%\n",
            "Epoch [16/100] Batch [600/1563] Loss: 2.2932 Acc: 9.60%\n",
            "Epoch [16/100] Batch [700/1563] Loss: 2.3112 Acc: 9.73%\n",
            "Epoch [16/100] Batch [800/1563] Loss: 2.3124 Acc: 9.66%\n",
            "Epoch [16/100] Batch [900/1563] Loss: 2.3015 Acc: 9.66%\n",
            "Epoch [16/100] Batch [1000/1563] Loss: 2.2982 Acc: 9.72%\n",
            "Epoch [16/100] Batch [1100/1563] Loss: 2.3027 Acc: 9.89%\n",
            "Epoch [16/100] Batch [1200/1563] Loss: 2.3021 Acc: 9.90%\n",
            "Epoch [16/100] Batch [1300/1563] Loss: 2.2961 Acc: 9.89%\n",
            "Epoch [16/100] Batch [1400/1563] Loss: 2.2954 Acc: 9.90%\n",
            "Epoch [16/100] Batch [1500/1563] Loss: 2.3040 Acc: 9.97%\n",
            "Epoch [16/100], Loss: 2.3032, Accuracy: 9.99%\n",
            "Test Loss: 2.3050, Test Accuracy: 10.00%\n",
            "Epoch [17/100] Batch [0/1563] Loss: 2.2993 Acc: 15.62%\n",
            "Epoch [17/100] Batch [100/1563] Loss: 2.3018 Acc: 10.02%\n",
            "Epoch [17/100] Batch [200/1563] Loss: 2.3099 Acc: 9.90%\n",
            "Epoch [17/100] Batch [300/1563] Loss: 2.3047 Acc: 9.96%\n",
            "Epoch [17/100] Batch [400/1563] Loss: 2.3073 Acc: 9.80%\n",
            "Epoch [17/100] Batch [500/1563] Loss: 2.3060 Acc: 9.79%\n",
            "Epoch [17/100] Batch [600/1563] Loss: 2.3003 Acc: 9.94%\n",
            "Epoch [17/100] Batch [700/1563] Loss: 2.3038 Acc: 9.93%\n",
            "Epoch [17/100] Batch [800/1563] Loss: 2.2958 Acc: 9.93%\n",
            "Epoch [17/100] Batch [900/1563] Loss: 2.2926 Acc: 9.94%\n",
            "Epoch [17/100] Batch [1000/1563] Loss: 2.2996 Acc: 10.05%\n",
            "Epoch [17/100] Batch [1100/1563] Loss: 2.3023 Acc: 10.01%\n",
            "Epoch [17/100] Batch [1200/1563] Loss: 2.3003 Acc: 10.02%\n",
            "Epoch [17/100] Batch [1300/1563] Loss: 2.3046 Acc: 10.00%\n",
            "Epoch [17/100] Batch [1400/1563] Loss: 2.3112 Acc: 9.95%\n",
            "Epoch [17/100] Batch [1500/1563] Loss: 2.3075 Acc: 9.97%\n",
            "Epoch [17/100], Loss: 2.3034, Accuracy: 9.95%\n",
            "Test Loss: 2.3034, Test Accuracy: 10.00%\n",
            "Epoch [18/100] Batch [0/1563] Loss: 2.3096 Acc: 9.38%\n",
            "Epoch [18/100] Batch [100/1563] Loss: 2.2974 Acc: 9.93%\n",
            "Epoch [18/100] Batch [200/1563] Loss: 2.3014 Acc: 9.65%\n",
            "Epoch [18/100] Batch [300/1563] Loss: 2.3083 Acc: 9.98%\n",
            "Epoch [18/100] Batch [400/1563] Loss: 2.3041 Acc: 9.66%\n",
            "Epoch [18/100] Batch [500/1563] Loss: 2.2995 Acc: 9.95%\n",
            "Epoch [18/100] Batch [600/1563] Loss: 2.2963 Acc: 10.07%\n",
            "Epoch [18/100] Batch [700/1563] Loss: 2.3167 Acc: 10.03%\n",
            "Epoch [18/100] Batch [800/1563] Loss: 2.3027 Acc: 10.00%\n",
            "Epoch [18/100] Batch [900/1563] Loss: 2.3015 Acc: 9.93%\n",
            "Epoch [18/100] Batch [1000/1563] Loss: 2.3084 Acc: 9.91%\n",
            "Epoch [18/100] Batch [1100/1563] Loss: 2.2980 Acc: 9.79%\n",
            "Epoch [18/100] Batch [1200/1563] Loss: 2.3051 Acc: 9.79%\n",
            "Epoch [18/100] Batch [1300/1563] Loss: 2.3178 Acc: 9.78%\n",
            "Epoch [18/100] Batch [1400/1563] Loss: 2.3029 Acc: 9.81%\n",
            "Epoch [18/100] Batch [1500/1563] Loss: 2.3089 Acc: 9.77%\n",
            "Epoch [18/100], Loss: 2.3034, Accuracy: 9.77%\n",
            "Test Loss: 2.3032, Test Accuracy: 10.00%\n",
            "Epoch [19/100] Batch [0/1563] Loss: 2.3088 Acc: 3.12%\n",
            "Epoch [19/100] Batch [100/1563] Loss: 2.3111 Acc: 10.67%\n",
            "Epoch [19/100] Batch [200/1563] Loss: 2.2980 Acc: 10.68%\n",
            "Epoch [19/100] Batch [300/1563] Loss: 2.3027 Acc: 10.12%\n",
            "Epoch [19/100] Batch [400/1563] Loss: 2.2958 Acc: 10.07%\n",
            "Epoch [19/100] Batch [500/1563] Loss: 2.2978 Acc: 9.99%\n",
            "Epoch [19/100] Batch [600/1563] Loss: 2.2988 Acc: 9.97%\n",
            "Epoch [19/100] Batch [700/1563] Loss: 2.3151 Acc: 9.87%\n",
            "Epoch [19/100] Batch [800/1563] Loss: 2.3121 Acc: 9.94%\n",
            "Epoch [19/100] Batch [900/1563] Loss: 2.2973 Acc: 10.03%\n",
            "Epoch [19/100] Batch [1000/1563] Loss: 2.2968 Acc: 9.87%\n",
            "Epoch [19/100] Batch [1100/1563] Loss: 2.3117 Acc: 9.83%\n",
            "Epoch [19/100] Batch [1200/1563] Loss: 2.3039 Acc: 9.79%\n",
            "Epoch [19/100] Batch [1300/1563] Loss: 2.3176 Acc: 9.84%\n",
            "Epoch [19/100] Batch [1400/1563] Loss: 2.3045 Acc: 9.91%\n",
            "Epoch [19/100] Batch [1500/1563] Loss: 2.3063 Acc: 9.86%\n",
            "Epoch [19/100], Loss: 2.3034, Accuracy: 9.89%\n",
            "Test Loss: 2.3032, Test Accuracy: 10.00%\n",
            "Epoch [20/100] Batch [0/1563] Loss: 2.3003 Acc: 9.38%\n",
            "Epoch [20/100] Batch [100/1563] Loss: 2.3000 Acc: 9.25%\n",
            "Epoch [20/100] Batch [200/1563] Loss: 2.2963 Acc: 9.44%\n",
            "Epoch [20/100] Batch [300/1563] Loss: 2.3118 Acc: 9.71%\n",
            "Epoch [20/100] Batch [400/1563] Loss: 2.3070 Acc: 9.88%\n",
            "Epoch [20/100] Batch [500/1563] Loss: 2.3048 Acc: 9.84%\n",
            "Epoch [20/100] Batch [600/1563] Loss: 2.3002 Acc: 9.73%\n",
            "Epoch [20/100] Batch [700/1563] Loss: 2.3067 Acc: 9.79%\n",
            "Epoch [20/100] Batch [800/1563] Loss: 2.2984 Acc: 9.89%\n",
            "Epoch [20/100] Batch [900/1563] Loss: 2.2913 Acc: 9.88%\n",
            "Epoch [20/100] Batch [1000/1563] Loss: 2.3063 Acc: 9.98%\n",
            "Epoch [20/100] Batch [1100/1563] Loss: 2.3032 Acc: 9.93%\n",
            "Epoch [20/100] Batch [1200/1563] Loss: 2.3030 Acc: 9.91%\n",
            "Epoch [20/100] Batch [1300/1563] Loss: 2.3047 Acc: 9.93%\n",
            "Epoch [20/100] Batch [1400/1563] Loss: 2.2997 Acc: 10.01%\n",
            "Epoch [20/100] Batch [1500/1563] Loss: 2.3010 Acc: 10.01%\n",
            "Epoch [20/100], Loss: 2.3033, Accuracy: 9.96%\n",
            "Test Loss: 2.3033, Test Accuracy: 10.00%\n",
            "Epoch [21/100] Batch [0/1563] Loss: 2.3008 Acc: 15.62%\n",
            "Epoch [21/100] Batch [100/1563] Loss: 2.3035 Acc: 9.41%\n",
            "Epoch [21/100] Batch [200/1563] Loss: 2.3048 Acc: 9.75%\n",
            "Epoch [21/100] Batch [300/1563] Loss: 2.3177 Acc: 9.87%\n",
            "Epoch [21/100] Batch [400/1563] Loss: 2.3031 Acc: 9.62%\n",
            "Epoch [21/100] Batch [500/1563] Loss: 2.2959 Acc: 9.84%\n",
            "Epoch [21/100] Batch [600/1563] Loss: 2.3062 Acc: 9.64%\n",
            "Epoch [21/100] Batch [700/1563] Loss: 2.2934 Acc: 9.63%\n",
            "Epoch [21/100] Batch [800/1563] Loss: 2.2983 Acc: 9.71%\n",
            "Epoch [21/100] Batch [900/1563] Loss: 2.3010 Acc: 9.75%\n",
            "Epoch [21/100] Batch [1000/1563] Loss: 2.3166 Acc: 9.87%\n",
            "Epoch [21/100] Batch [1100/1563] Loss: 2.3021 Acc: 9.88%\n",
            "Epoch [21/100] Batch [1200/1563] Loss: 2.3035 Acc: 9.84%\n",
            "Epoch [21/100] Batch [1300/1563] Loss: 2.3019 Acc: 9.82%\n",
            "Epoch [21/100] Batch [1400/1563] Loss: 2.2996 Acc: 9.79%\n",
            "Epoch [21/100] Batch [1500/1563] Loss: 2.3013 Acc: 9.80%\n",
            "Epoch [21/100], Loss: 2.3034, Accuracy: 9.82%\n",
            "Test Loss: 2.3039, Test Accuracy: 10.00%\n",
            "Epoch [22/100] Batch [0/1563] Loss: 2.3150 Acc: 6.25%\n",
            "Epoch [22/100] Batch [100/1563] Loss: 2.3050 Acc: 9.93%\n",
            "Epoch [22/100] Batch [200/1563] Loss: 2.3040 Acc: 10.28%\n",
            "Epoch [22/100] Batch [300/1563] Loss: 2.3030 Acc: 10.02%\n",
            "Epoch [22/100] Batch [400/1563] Loss: 2.3054 Acc: 10.08%\n",
            "Epoch [22/100] Batch [500/1563] Loss: 2.3199 Acc: 9.98%\n",
            "Epoch [22/100] Batch [600/1563] Loss: 2.2999 Acc: 10.22%\n",
            "Epoch [22/100] Batch [700/1563] Loss: 2.3013 Acc: 10.18%\n",
            "Epoch [22/100] Batch [800/1563] Loss: 2.2940 Acc: 10.22%\n",
            "Epoch [22/100] Batch [900/1563] Loss: 2.3099 Acc: 10.11%\n",
            "Epoch [22/100] Batch [1000/1563] Loss: 2.3018 Acc: 10.08%\n",
            "Epoch [22/100] Batch [1100/1563] Loss: 2.3034 Acc: 10.07%\n",
            "Epoch [22/100] Batch [1200/1563] Loss: 2.3118 Acc: 10.06%\n",
            "Epoch [22/100] Batch [1300/1563] Loss: 2.3006 Acc: 10.04%\n",
            "Epoch [22/100] Batch [1400/1563] Loss: 2.3032 Acc: 10.05%\n",
            "Epoch [22/100] Batch [1500/1563] Loss: 2.3077 Acc: 10.05%\n",
            "Epoch [22/100], Loss: 2.3033, Accuracy: 10.07%\n",
            "Test Loss: 2.3030, Test Accuracy: 10.00%\n",
            "Epoch [23/100] Batch [0/1563] Loss: 2.2938 Acc: 15.62%\n",
            "Epoch [23/100] Batch [100/1563] Loss: 2.3016 Acc: 9.44%\n",
            "Epoch [23/100] Batch [200/1563] Loss: 2.2956 Acc: 10.04%\n",
            "Epoch [23/100] Batch [300/1563] Loss: 2.3005 Acc: 9.87%\n",
            "Epoch [23/100] Batch [400/1563] Loss: 2.3084 Acc: 10.09%\n",
            "Epoch [23/100] Batch [500/1563] Loss: 2.3002 Acc: 10.09%\n",
            "Epoch [23/100] Batch [600/1563] Loss: 2.3063 Acc: 9.99%\n",
            "Epoch [23/100] Batch [700/1563] Loss: 2.2976 Acc: 9.95%\n",
            "Epoch [23/100] Batch [800/1563] Loss: 2.3075 Acc: 9.99%\n",
            "Epoch [23/100] Batch [900/1563] Loss: 2.2975 Acc: 9.90%\n",
            "Epoch [23/100] Batch [1000/1563] Loss: 2.3017 Acc: 9.85%\n",
            "Epoch [23/100] Batch [1100/1563] Loss: 2.3111 Acc: 9.82%\n",
            "Epoch [23/100] Batch [1200/1563] Loss: 2.3106 Acc: 9.83%\n",
            "Epoch [23/100] Batch [1300/1563] Loss: 2.3043 Acc: 9.83%\n",
            "Epoch [23/100] Batch [1400/1563] Loss: 2.3003 Acc: 9.83%\n",
            "Epoch [23/100] Batch [1500/1563] Loss: 2.3032 Acc: 9.75%\n",
            "Epoch [23/100], Loss: 2.3033, Accuracy: 9.79%\n",
            "Test Loss: 2.3028, Test Accuracy: 10.00%\n",
            "Epoch [24/100] Batch [0/1563] Loss: 2.2938 Acc: 18.75%\n",
            "Epoch [24/100] Batch [100/1563] Loss: 2.3034 Acc: 10.77%\n",
            "Epoch [24/100] Batch [200/1563] Loss: 2.3184 Acc: 10.70%\n",
            "Epoch [24/100] Batch [300/1563] Loss: 2.2994 Acc: 10.69%\n",
            "Epoch [24/100] Batch [400/1563] Loss: 2.2985 Acc: 10.75%\n",
            "Epoch [24/100] Batch [500/1563] Loss: 2.3012 Acc: 10.45%\n",
            "Epoch [24/100] Batch [600/1563] Loss: 2.2949 Acc: 10.51%\n",
            "Epoch [24/100] Batch [700/1563] Loss: 2.3063 Acc: 10.56%\n",
            "Epoch [24/100] Batch [800/1563] Loss: 2.3019 Acc: 10.58%\n",
            "Epoch [24/100] Batch [900/1563] Loss: 2.3054 Acc: 10.52%\n",
            "Epoch [24/100] Batch [1000/1563] Loss: 2.2956 Acc: 10.44%\n",
            "Epoch [24/100] Batch [1100/1563] Loss: 2.3045 Acc: 10.37%\n",
            "Epoch [24/100] Batch [1200/1563] Loss: 2.2960 Acc: 10.33%\n",
            "Epoch [24/100] Batch [1300/1563] Loss: 2.2932 Acc: 10.30%\n",
            "Epoch [24/100] Batch [1400/1563] Loss: 2.2943 Acc: 10.25%\n",
            "Epoch [24/100] Batch [1500/1563] Loss: 2.3029 Acc: 10.29%\n",
            "Epoch [24/100], Loss: 2.3032, Accuracy: 10.26%\n",
            "Test Loss: 2.3030, Test Accuracy: 10.00%\n",
            "Epoch [25/100] Batch [0/1563] Loss: 2.3080 Acc: 3.12%\n",
            "Epoch [25/100] Batch [100/1563] Loss: 2.3050 Acc: 10.06%\n",
            "Epoch [25/100] Batch [200/1563] Loss: 2.3182 Acc: 9.86%\n",
            "Epoch [25/100] Batch [300/1563] Loss: 2.3001 Acc: 9.74%\n",
            "Epoch [25/100] Batch [400/1563] Loss: 2.3022 Acc: 9.55%\n",
            "Epoch [25/100] Batch [500/1563] Loss: 2.3095 Acc: 9.59%\n",
            "Epoch [25/100] Batch [600/1563] Loss: 2.3077 Acc: 9.75%\n",
            "Epoch [25/100] Batch [700/1563] Loss: 2.3049 Acc: 9.76%\n",
            "Epoch [25/100] Batch [800/1563] Loss: 2.3006 Acc: 9.86%\n",
            "Epoch [25/100] Batch [900/1563] Loss: 2.3003 Acc: 9.87%\n",
            "Epoch [25/100] Batch [1000/1563] Loss: 2.2960 Acc: 9.91%\n",
            "Epoch [25/100] Batch [1100/1563] Loss: 2.3079 Acc: 9.89%\n",
            "Epoch [25/100] Batch [1200/1563] Loss: 2.3017 Acc: 9.86%\n",
            "Epoch [25/100] Batch [1300/1563] Loss: 2.2976 Acc: 9.85%\n",
            "Epoch [25/100] Batch [1400/1563] Loss: 2.3114 Acc: 9.83%\n",
            "Epoch [25/100] Batch [1500/1563] Loss: 2.3034 Acc: 9.81%\n",
            "Epoch [25/100], Loss: 2.3032, Accuracy: 9.84%\n",
            "Test Loss: 2.3032, Test Accuracy: 10.00%\n",
            "Epoch [26/100] Batch [0/1563] Loss: 2.3058 Acc: 12.50%\n",
            "Epoch [26/100] Batch [100/1563] Loss: 2.3043 Acc: 10.40%\n",
            "Epoch [26/100] Batch [200/1563] Loss: 2.2982 Acc: 10.21%\n",
            "Epoch [26/100] Batch [300/1563] Loss: 2.3061 Acc: 10.35%\n",
            "Epoch [26/100] Batch [400/1563] Loss: 2.3070 Acc: 10.29%\n",
            "Epoch [26/100] Batch [500/1563] Loss: 2.3092 Acc: 10.18%\n",
            "Epoch [26/100] Batch [600/1563] Loss: 2.3065 Acc: 10.23%\n",
            "Epoch [26/100] Batch [700/1563] Loss: 2.3007 Acc: 10.18%\n",
            "Epoch [26/100] Batch [800/1563] Loss: 2.2938 Acc: 10.18%\n",
            "Epoch [26/100] Batch [900/1563] Loss: 2.3078 Acc: 10.14%\n",
            "Epoch [26/100] Batch [1000/1563] Loss: 2.3038 Acc: 10.07%\n",
            "Epoch [26/100] Batch [1100/1563] Loss: 2.2923 Acc: 10.12%\n",
            "Epoch [26/100] Batch [1200/1563] Loss: 2.3098 Acc: 10.05%\n",
            "Epoch [26/100] Batch [1300/1563] Loss: 2.3032 Acc: 10.13%\n",
            "Epoch [26/100] Batch [1400/1563] Loss: 2.2986 Acc: 10.09%\n",
            "Epoch [26/100] Batch [1500/1563] Loss: 2.3062 Acc: 10.08%\n",
            "Epoch [26/100], Loss: 2.3032, Accuracy: 10.05%\n",
            "Test Loss: 2.3039, Test Accuracy: 10.00%\n",
            "Epoch [27/100] Batch [0/1563] Loss: 2.2926 Acc: 25.00%\n",
            "Epoch [27/100] Batch [100/1563] Loss: 2.3039 Acc: 9.72%\n",
            "Epoch [27/100] Batch [200/1563] Loss: 2.3005 Acc: 9.69%\n",
            "Epoch [27/100] Batch [300/1563] Loss: 2.3050 Acc: 9.96%\n",
            "Epoch [27/100] Batch [400/1563] Loss: 2.3114 Acc: 10.05%\n",
            "Epoch [27/100] Batch [500/1563] Loss: 2.3018 Acc: 10.09%\n",
            "Epoch [27/100] Batch [600/1563] Loss: 2.3051 Acc: 10.04%\n",
            "Epoch [27/100] Batch [700/1563] Loss: 2.3041 Acc: 9.98%\n",
            "Epoch [27/100] Batch [800/1563] Loss: 2.2964 Acc: 10.05%\n",
            "Epoch [27/100] Batch [900/1563] Loss: 2.3146 Acc: 10.10%\n",
            "Epoch [27/100] Batch [1000/1563] Loss: 2.2986 Acc: 10.12%\n",
            "Epoch [27/100] Batch [1100/1563] Loss: 2.2968 Acc: 10.08%\n",
            "Epoch [27/100] Batch [1200/1563] Loss: 2.3039 Acc: 10.06%\n",
            "Epoch [27/100] Batch [1300/1563] Loss: 2.2976 Acc: 9.99%\n",
            "Epoch [27/100] Batch [1400/1563] Loss: 2.2995 Acc: 9.97%\n",
            "Epoch [27/100] Batch [1500/1563] Loss: 2.3242 Acc: 10.02%\n",
            "Epoch [27/100], Loss: 2.3032, Accuracy: 9.96%\n",
            "Test Loss: 2.3029, Test Accuracy: 10.00%\n",
            "Epoch [28/100] Batch [0/1563] Loss: 2.3126 Acc: 12.50%\n",
            "Epoch [28/100] Batch [100/1563] Loss: 2.3023 Acc: 9.68%\n",
            "Epoch [28/100] Batch [200/1563] Loss: 2.3050 Acc: 10.31%\n",
            "Epoch [28/100] Batch [300/1563] Loss: 2.3021 Acc: 10.25%\n",
            "Epoch [28/100] Batch [400/1563] Loss: 2.3122 Acc: 10.27%\n",
            "Epoch [28/100] Batch [500/1563] Loss: 2.3097 Acc: 10.15%\n",
            "Epoch [28/100] Batch [600/1563] Loss: 2.3050 Acc: 10.27%\n",
            "Epoch [28/100] Batch [700/1563] Loss: 2.3086 Acc: 10.20%\n",
            "Epoch [28/100] Batch [800/1563] Loss: 2.3016 Acc: 10.23%\n",
            "Epoch [28/100] Batch [900/1563] Loss: 2.2994 Acc: 10.25%\n",
            "Epoch [28/100] Batch [1000/1563] Loss: 2.3064 Acc: 10.26%\n",
            "Epoch [28/100] Batch [1100/1563] Loss: 2.3070 Acc: 10.27%\n",
            "Epoch [28/100] Batch [1200/1563] Loss: 2.2972 Acc: 10.27%\n",
            "Epoch [28/100] Batch [1300/1563] Loss: 2.3022 Acc: 10.23%\n",
            "Epoch [28/100] Batch [1400/1563] Loss: 2.3042 Acc: 10.19%\n",
            "Epoch [28/100] Batch [1500/1563] Loss: 2.3038 Acc: 10.14%\n",
            "Epoch [28/100], Loss: 2.3033, Accuracy: 10.15%\n",
            "Test Loss: 2.3028, Test Accuracy: 10.00%\n",
            "Epoch [29/100] Batch [0/1563] Loss: 2.3002 Acc: 15.62%\n",
            "Epoch [29/100] Batch [100/1563] Loss: 2.2960 Acc: 9.22%\n",
            "Epoch [29/100] Batch [200/1563] Loss: 2.2956 Acc: 9.58%\n",
            "Epoch [29/100] Batch [300/1563] Loss: 2.3056 Acc: 9.96%\n",
            "Epoch [29/100] Batch [400/1563] Loss: 2.3067 Acc: 9.80%\n",
            "Epoch [29/100] Batch [500/1563] Loss: 2.2964 Acc: 9.82%\n",
            "Epoch [29/100] Batch [600/1563] Loss: 2.3038 Acc: 9.92%\n",
            "Epoch [29/100] Batch [700/1563] Loss: 2.3083 Acc: 9.72%\n",
            "Epoch [29/100] Batch [800/1563] Loss: 2.3046 Acc: 9.78%\n",
            "Epoch [29/100] Batch [900/1563] Loss: 2.2946 Acc: 9.79%\n",
            "Epoch [29/100] Batch [1000/1563] Loss: 2.3039 Acc: 9.82%\n",
            "Epoch [29/100] Batch [1100/1563] Loss: 2.3047 Acc: 9.86%\n",
            "Epoch [29/100] Batch [1200/1563] Loss: 2.3028 Acc: 9.92%\n",
            "Epoch [29/100] Batch [1300/1563] Loss: 2.3061 Acc: 9.95%\n",
            "Epoch [29/100] Batch [1400/1563] Loss: 2.3006 Acc: 9.91%\n",
            "Epoch [29/100] Batch [1500/1563] Loss: 2.3074 Acc: 9.86%\n",
            "Epoch [29/100], Loss: 2.3032, Accuracy: 9.85%\n",
            "Test Loss: 2.3028, Test Accuracy: 10.00%\n",
            "Epoch [30/100] Batch [0/1563] Loss: 2.2986 Acc: 3.12%\n",
            "Epoch [30/100] Batch [100/1563] Loss: 2.3066 Acc: 9.90%\n",
            "Epoch [30/100] Batch [200/1563] Loss: 2.3036 Acc: 10.07%\n",
            "Epoch [30/100] Batch [300/1563] Loss: 2.3018 Acc: 9.89%\n",
            "Epoch [30/100] Batch [400/1563] Loss: 2.3020 Acc: 10.12%\n",
            "Epoch [30/100] Batch [500/1563] Loss: 2.3063 Acc: 10.07%\n",
            "Epoch [30/100] Batch [600/1563] Loss: 2.3132 Acc: 10.04%\n",
            "Epoch [30/100] Batch [700/1563] Loss: 2.3116 Acc: 10.00%\n",
            "Epoch [30/100] Batch [800/1563] Loss: 2.3011 Acc: 9.94%\n",
            "Epoch [30/100] Batch [900/1563] Loss: 2.3002 Acc: 9.88%\n",
            "Epoch [30/100] Batch [1000/1563] Loss: 2.3083 Acc: 9.90%\n",
            "Epoch [30/100] Batch [1100/1563] Loss: 2.2934 Acc: 9.91%\n",
            "Epoch [30/100] Batch [1200/1563] Loss: 2.3049 Acc: 9.85%\n",
            "Epoch [30/100] Batch [1300/1563] Loss: 2.3070 Acc: 9.86%\n",
            "Epoch [30/100] Batch [1400/1563] Loss: 2.3055 Acc: 9.91%\n",
            "Epoch [30/100] Batch [1500/1563] Loss: 2.3036 Acc: 9.88%\n",
            "Epoch [30/100], Loss: 2.3032, Accuracy: 9.89%\n",
            "Test Loss: 2.3028, Test Accuracy: 10.00%\n",
            "Epoch [31/100] Batch [0/1563] Loss: 2.3047 Acc: 9.38%\n",
            "Epoch [31/100] Batch [100/1563] Loss: 2.3006 Acc: 10.12%\n",
            "Epoch [31/100] Batch [200/1563] Loss: 2.2942 Acc: 10.21%\n",
            "Epoch [31/100] Batch [300/1563] Loss: 2.3025 Acc: 10.01%\n",
            "Epoch [31/100] Batch [400/1563] Loss: 2.3137 Acc: 9.95%\n",
            "Epoch [31/100] Batch [500/1563] Loss: 2.2791 Acc: 10.24%\n",
            "Epoch [31/100] Batch [600/1563] Loss: 2.3059 Acc: 10.22%\n",
            "Epoch [31/100] Batch [700/1563] Loss: 2.3045 Acc: 10.20%\n",
            "Epoch [31/100] Batch [800/1563] Loss: 2.3071 Acc: 10.14%\n",
            "Epoch [31/100] Batch [900/1563] Loss: 2.3021 Acc: 10.11%\n",
            "Epoch [31/100] Batch [1000/1563] Loss: 2.2958 Acc: 10.13%\n",
            "Epoch [31/100] Batch [1100/1563] Loss: 2.2991 Acc: 10.06%\n",
            "Epoch [31/100] Batch [1200/1563] Loss: 2.3087 Acc: 9.99%\n",
            "Epoch [31/100] Batch [1300/1563] Loss: 2.3033 Acc: 9.98%\n",
            "Epoch [31/100] Batch [1400/1563] Loss: 2.3107 Acc: 10.01%\n",
            "Epoch [31/100] Batch [1500/1563] Loss: 2.3088 Acc: 10.05%\n",
            "Epoch [31/100], Loss: 2.3032, Accuracy: 10.04%\n",
            "Test Loss: 2.3045, Test Accuracy: 10.00%\n",
            "Epoch [32/100] Batch [0/1563] Loss: 2.3027 Acc: 12.50%\n",
            "Epoch [32/100] Batch [100/1563] Loss: 2.3214 Acc: 10.27%\n",
            "Epoch [32/100] Batch [200/1563] Loss: 2.2855 Acc: 10.17%\n",
            "Epoch [32/100] Batch [300/1563] Loss: 2.2961 Acc: 10.15%\n",
            "Epoch [32/100] Batch [400/1563] Loss: 2.2976 Acc: 10.06%\n",
            "Epoch [32/100] Batch [500/1563] Loss: 2.2939 Acc: 10.05%\n",
            "Epoch [32/100] Batch [600/1563] Loss: 2.3040 Acc: 10.02%\n",
            "Epoch [32/100] Batch [700/1563] Loss: 2.3125 Acc: 10.02%\n",
            "Epoch [32/100] Batch [800/1563] Loss: 2.3156 Acc: 10.00%\n",
            "Epoch [32/100] Batch [900/1563] Loss: 2.3074 Acc: 10.01%\n",
            "Epoch [32/100] Batch [1000/1563] Loss: 2.3002 Acc: 10.01%\n",
            "Epoch [32/100] Batch [1100/1563] Loss: 2.3066 Acc: 10.06%\n",
            "Epoch [32/100] Batch [1200/1563] Loss: 2.3018 Acc: 10.09%\n",
            "Epoch [32/100] Batch [1300/1563] Loss: 2.3033 Acc: 10.02%\n",
            "Epoch [32/100] Batch [1400/1563] Loss: 2.3022 Acc: 10.03%\n",
            "Epoch [32/100] Batch [1500/1563] Loss: 2.2935 Acc: 10.01%\n",
            "Epoch [32/100], Loss: 2.3032, Accuracy: 10.01%\n",
            "Test Loss: 2.3033, Test Accuracy: 10.00%\n",
            "Epoch [33/100] Batch [0/1563] Loss: 2.3072 Acc: 9.38%\n",
            "Epoch [33/100] Batch [100/1563] Loss: 2.2962 Acc: 9.87%\n",
            "Epoch [33/100] Batch [200/1563] Loss: 2.2996 Acc: 10.37%\n",
            "Epoch [33/100] Batch [300/1563] Loss: 2.3029 Acc: 10.34%\n",
            "Epoch [33/100] Batch [400/1563] Loss: 2.3042 Acc: 10.22%\n",
            "Epoch [33/100] Batch [500/1563] Loss: 2.3190 Acc: 10.21%\n",
            "Epoch [33/100] Batch [600/1563] Loss: 2.3062 Acc: 10.17%\n",
            "Epoch [33/100] Batch [700/1563] Loss: 2.3041 Acc: 10.16%\n",
            "Epoch [33/100] Batch [800/1563] Loss: 2.2978 Acc: 10.17%\n",
            "Epoch [33/100] Batch [900/1563] Loss: 2.3040 Acc: 10.15%\n",
            "Epoch [33/100] Batch [1000/1563] Loss: 2.3013 Acc: 10.13%\n",
            "Epoch [33/100] Batch [1100/1563] Loss: 2.2947 Acc: 10.24%\n",
            "Epoch [33/100] Batch [1200/1563] Loss: 2.3013 Acc: 10.18%\n",
            "Epoch [33/100] Batch [1300/1563] Loss: 2.2989 Acc: 10.18%\n",
            "Epoch [33/100] Batch [1400/1563] Loss: 2.2996 Acc: 10.16%\n",
            "Epoch [33/100] Batch [1500/1563] Loss: 2.3104 Acc: 10.22%\n",
            "Epoch [33/100], Loss: 2.3030, Accuracy: 10.18%\n",
            "Test Loss: 2.3030, Test Accuracy: 10.00%\n",
            "Epoch [34/100] Batch [0/1563] Loss: 2.3028 Acc: 9.38%\n",
            "Epoch [34/100] Batch [100/1563] Loss: 2.3074 Acc: 9.90%\n",
            "Epoch [34/100] Batch [200/1563] Loss: 2.3052 Acc: 9.97%\n",
            "Epoch [34/100] Batch [300/1563] Loss: 2.3014 Acc: 9.76%\n",
            "Epoch [34/100] Batch [400/1563] Loss: 2.3142 Acc: 9.83%\n",
            "Epoch [34/100] Batch [500/1563] Loss: 2.3029 Acc: 9.81%\n",
            "Epoch [34/100] Batch [600/1563] Loss: 2.3014 Acc: 9.67%\n",
            "Epoch [34/100] Batch [700/1563] Loss: 2.3035 Acc: 9.67%\n",
            "Epoch [34/100] Batch [800/1563] Loss: 2.3070 Acc: 9.77%\n",
            "Epoch [34/100] Batch [900/1563] Loss: 2.3044 Acc: 9.86%\n",
            "Epoch [34/100] Batch [1000/1563] Loss: 2.2962 Acc: 9.82%\n",
            "Epoch [34/100] Batch [1100/1563] Loss: 2.3095 Acc: 9.87%\n",
            "Epoch [34/100] Batch [1200/1563] Loss: 2.3159 Acc: 9.84%\n",
            "Epoch [34/100] Batch [1300/1563] Loss: 2.3066 Acc: 9.87%\n",
            "Epoch [34/100] Batch [1400/1563] Loss: 2.3072 Acc: 9.88%\n",
            "Epoch [34/100] Batch [1500/1563] Loss: 2.3048 Acc: 9.91%\n",
            "Epoch [34/100], Loss: 2.3032, Accuracy: 9.89%\n",
            "Test Loss: 2.3028, Test Accuracy: 10.00%\n",
            "Epoch [35/100] Batch [0/1563] Loss: 2.3016 Acc: 12.50%\n",
            "Epoch [35/100] Batch [100/1563] Loss: 2.2969 Acc: 9.03%\n",
            "Epoch [35/100] Batch [200/1563] Loss: 2.3038 Acc: 9.65%\n",
            "Epoch [35/100] Batch [300/1563] Loss: 2.3076 Acc: 9.90%\n",
            "Epoch [35/100] Batch [400/1563] Loss: 2.3048 Acc: 9.95%\n",
            "Epoch [35/100] Batch [500/1563] Loss: 2.2979 Acc: 10.01%\n",
            "Epoch [35/100] Batch [600/1563] Loss: 2.3005 Acc: 10.16%\n",
            "Epoch [35/100] Batch [700/1563] Loss: 2.3048 Acc: 10.23%\n",
            "Epoch [35/100] Batch [800/1563] Loss: 2.2974 Acc: 10.15%\n",
            "Epoch [35/100] Batch [900/1563] Loss: 2.3066 Acc: 10.06%\n",
            "Epoch [35/100] Batch [1000/1563] Loss: 2.3009 Acc: 10.16%\n",
            "Epoch [35/100] Batch [1100/1563] Loss: 2.3143 Acc: 10.19%\n",
            "Epoch [35/100] Batch [1200/1563] Loss: 2.3042 Acc: 10.11%\n",
            "Epoch [35/100] Batch [1300/1563] Loss: 2.3076 Acc: 10.09%\n",
            "Epoch [35/100] Batch [1400/1563] Loss: 2.3008 Acc: 10.08%\n",
            "Epoch [35/100] Batch [1500/1563] Loss: 2.3063 Acc: 10.08%\n",
            "Epoch [35/100], Loss: 2.3031, Accuracy: 10.10%\n",
            "Test Loss: 2.3029, Test Accuracy: 10.00%\n",
            "Epoch [36/100] Batch [0/1563] Loss: 2.3061 Acc: 9.38%\n"
          ]
        }
      ],
      "source": [
        "# Optimizer and loss function\n",
        "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
        "\n",
        "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{EPOCHS}] Batch [{batch_idx}/{len(train_loader)}] \"\n",
        "                  f\"Loss: {loss.item():.4f} Acc: {100.*correct/total:.2f}%\")\n",
        "\n",
        "    train_accuracy = 100. * correct / total\n",
        "    # print(f\"Epoch [{epoch+1}/{EPOCHS}], Loss: {train_loss/len(train_loader):.4f}, \"\n",
        "          # f\"Accuracy: {train_accuracy:.2f}%\")\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    test_accuracy = 100. * correct / total\n",
        "    print(f\"Test Loss: {test_loss/len(test_loader):.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "    # Update learning rate\n",
        "    scheduler.step()\n",
        "\n",
        "print(\"Training finished!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4PmtkvvX7z53"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
