{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bEMpr27c0RCi",
    "outputId": "d86b304b-23be-4240-f2b5-1c77b82e8d9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (24.2)\n",
      "Collecting pip\n",
      "  Downloading pip-24.3.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "Downloading pip-24.3.1-py3-none-any.whl (1.8 MB)\n",
      "   ---------------------------------------- 0.0/1.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.8/1.8 MB 11.1 MB/s eta 0:00:00\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 24.2\n",
      "    Uninstalling pip-24.2:\n",
      "      Successfully uninstalled pip-24.2\n",
      "Successfully installed pip-24.3.1\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu112\n",
      "Requirement already satisfied: torch in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.4.1+cu118)\n",
      "Requirement already satisfied: torchvision in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.19.1+cu118)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.4.1+cu118)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: timm in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.0.9)\n",
      "Requirement already satisfied: pytorch-ignite in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.5.1)\n",
      "Requirement already satisfied: einops in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.8.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: torch in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from timm) (2.4.1+cu118)\n",
      "Requirement already satisfied: torchvision in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from timm) (0.19.1+cu118)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from timm) (6.0.2)\n",
      "Requirement already satisfied: huggingface_hub in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from timm) (0.25.0)\n",
      "Requirement already satisfied: safetensors in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from timm) (0.4.5)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pytorch-ignite) (24.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->timm) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->timm) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->timm) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->timm) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->timm) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->timm) (2024.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->timm) (75.1.0)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface_hub->timm) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface_hub->timm) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub->timm) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch->timm) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface_hub->timm) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface_hub->timm) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface_hub->timm) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface_hub->timm) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->torch->timm) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ignite\\handlers\\checkpoint.py:16: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.\n",
      "  from torch.distributed.optim import ZeroRedundancyOptimizer\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------\n",
    "# Swin Transformer\n",
    "# Copyright (c) 2021 Microsoft\n",
    "# Licensed under The MIT License [see LICENSE for details]\n",
    "# Written by Ze Liu\n",
    "# --------------------------------------------------------\n",
    "\n",
    "!python.exe -m pip install --upgrade pip\n",
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu112\n",
    "!pip3 install timm pytorch-ignite einops matplotlib\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn as nn\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "\n",
    "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
    "from einops import rearrange\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
    "import ignite.metrics\n",
    "import ignite.contrib.handlers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "cxGT3QPy0q0H"
   },
   "outputs": [],
   "source": [
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "wncXpbHk09rc"
   },
   "outputs": [],
   "source": [
    "def window_partition(x, window_size):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x: (B, H, W, C)\n",
    "        window_size (int): window size\n",
    "\n",
    "    Returns:\n",
    "        windows: (num_windows*B, window_size, window_size, C)\n",
    "    \"\"\"\n",
    "    B, H, W, C = x.shape\n",
    "    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n",
    "    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n",
    "    return windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "uvsOSiH-1DdL"
   },
   "outputs": [],
   "source": [
    "def window_reverse(windows, window_size, H, W):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        windows: (num_windows*B, window_size, window_size, C)\n",
    "        window_size (int): Window size\n",
    "        H (int): Height of image\n",
    "        W (int): Width of image\n",
    "\n",
    "    Returns:\n",
    "        x: (B, H, W, C)\n",
    "    \"\"\"\n",
    "    B = int(windows.shape[0] / (H * W / window_size / window_size))\n",
    "    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n",
    "    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Fp57DIJ41EBj"
   },
   "outputs": [],
   "source": [
    "class WindowAttention(nn.Module):\n",
    "    r\"\"\" Window based multi-head self attention (W-MSA) module with relative position bias.\n",
    "    It supports both of shifted and non-shifted window.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        window_size (tuple[int]): The height and width of the window.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n",
    "        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n",
    "        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.window_size = window_size  # Wh, Ww\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        # define a parameter table of relative position bias\n",
    "        self.relative_position_bias_table = nn.Parameter(\n",
    "            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n",
    "\n",
    "        # get pair-wise relative position index for each token inside the window\n",
    "        coords_h = torch.arange(self.window_size[0])\n",
    "        coords_w = torch.arange(self.window_size[1])\n",
    "        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n",
    "        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n",
    "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n",
    "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
    "        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n",
    "        relative_coords[:, :, 1] += self.window_size[1] - 1\n",
    "        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n",
    "        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n",
    "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "        trunc_normal_(self.relative_position_bias_table, std=.02)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input features with shape of (num_windows*B, N, C)\n",
    "            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None\n",
    "        \"\"\"\n",
    "        B_, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "        q = q * self.scale\n",
    "        attn = (q @ k.transpose(-2, -1))\n",
    "\n",
    "        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n",
    "            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)  # Wh*Ww,Wh*Ww,nH\n",
    "        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n",
    "        attn = attn + relative_position_bias.unsqueeze(0)\n",
    "\n",
    "        if mask is not None:\n",
    "            nW = mask.shape[0]\n",
    "            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n",
    "            attn = attn.view(-1, self.num_heads, N, N)\n",
    "            attn = self.softmax(attn)\n",
    "        else:\n",
    "            attn = self.softmax(attn)\n",
    "\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f'dim={self.dim}, window_size={self.window_size}, num_heads={self.num_heads}'\n",
    "\n",
    "    def flops(self, N):\n",
    "        # calculate flops for 1 window with token length of N\n",
    "        flops = 0\n",
    "        # qkv = self.qkv(x)\n",
    "        flops += N * self.dim * 3 * self.dim\n",
    "        # attn = (q @ k.transpose(-2, -1))\n",
    "        flops += self.num_heads * N * (self.dim // self.num_heads) * N\n",
    "        #  x = (attn @ v)\n",
    "        flops += self.num_heads * N * N * (self.dim // self.num_heads)\n",
    "        # x = self.proj(x)\n",
    "        flops += N * self.dim * self.dim\n",
    "        return flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "e_UlHriB1I6l"
   },
   "outputs": [],
   "source": [
    "class FocusedLinearAttention(nn.Module):\n",
    "    r\"\"\" Window based multi-head self attention (W-MSA) module with relative position bias.\n",
    "    It supports both of shifted and non-shifted window.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        window_size (tuple[int]): The height and width of the window.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n",
    "        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n",
    "        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.,\n",
    "                 focusing_factor=3, kernel_size=5):\n",
    "\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.window_size = window_size  # Wh, Ww\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "\n",
    "        self.focusing_factor = focusing_factor  # Used to sharpen attention distribution\n",
    "\n",
    "    # Linear layer to project input to query, key, and value\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)  # Output projection\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "        # Depth-wise convolution for capturing local spatial information\n",
    "        self.dwc = nn.Conv2d(in_channels=head_dim, out_channels=head_dim, kernel_size=kernel_size,\n",
    "                            groups=head_dim, padding=kernel_size // 2)\n",
    "\n",
    "        # Learnable scale parameter\n",
    "        self.scale = nn.Parameter(torch.zeros(size=(1, 1, dim)))\n",
    "\n",
    "        # Learnable positional encoding\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(size=(1, window_size[0] * window_size[1], dim)))\n",
    "\n",
    "        print('Linear Attention window{} f{} kernel{}'.\n",
    "              format(window_size, focusing_factor, kernel_size))\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        B, N, C = x.shape\n",
    "\n",
    "        # Project input to query, key, and value\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, C).permute(2, 0, 1, 3)\n",
    "        q, k, v = qkv.unbind(0)\n",
    "\n",
    "        # Add positional encoding to keys\n",
    "        k = k + self.positional_encoding\n",
    "\n",
    "        focusing_factor = self.focusing_factor\n",
    "        kernel_function = nn.ReLU()\n",
    "\n",
    "        # Apply ReLU and add small epsilon to avoid zero values\n",
    "        q = kernel_function(q) + 1e-6\n",
    "        k = kernel_function(k) + 1e-6\n",
    "\n",
    "        # Compute scale using Softplus for stability\n",
    "        scale = nn.Softplus()(self.scale)\n",
    "        q = q / scale\n",
    "        k = k / scale\n",
    "\n",
    "        # Store original norms\n",
    "        q_norm = q.norm(dim=-1, keepdim=True)\n",
    "        k_norm = k.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # Apply focusing factor\n",
    "        q = q ** focusing_factor\n",
    "        k = k ** focusing_factor\n",
    "\n",
    "        # Renormalize to original norms\n",
    "        q = (q / q.norm(dim=-1, keepdim=True)) * q_norm\n",
    "        k = (k / k.norm(dim=-1, keepdim=True)) * k_norm\n",
    "\n",
    "        # Reshape for multi-head attention\n",
    "        q = q.reshape(B, N, self.num_heads, -1).permute(0, 2, 1, 3)\n",
    "        k = k.reshape(B, N, self.num_heads, -1).permute(0, 2, 1, 3)\n",
    "        v = v.reshape(B, N, self.num_heads, -1).permute(0, 2, 1, 3)\n",
    "\n",
    "        # Compute linear attention\n",
    "        z = 1 / (q @ k.mean(dim=-2, keepdim=True).transpose(-2, -1) + 1e-6)\n",
    "        kv = (k.transpose(-2, -1) * (N ** -0.5)) @ (v * (N ** -0.5))\n",
    "        x = q @ kv * z\n",
    "\n",
    "        # Reshape output\n",
    "        H = W = int(N ** 0.5)\n",
    "        x = x.transpose(1, 2).reshape(B, N, C)\n",
    "\n",
    "        # Apply depth-wise convolution to capture local spatial information\n",
    "        v = v.reshape(B * self.num_heads, H, W, -1).permute(0, 3, 1, 2)\n",
    "        x = x + self.dwc(v).reshape(B, C, N).permute(0, 2, 1)\n",
    "\n",
    "        # Final projection and dropout\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "    def eval(self):\n",
    "        super(FocusedLinearAttention, self).eval()\n",
    "        print('eval')\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f'dim={self.dim}, window_size={self.window_size}, num_heads={self.num_heads}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "DIR8e1tu1O5O"
   },
   "outputs": [],
   "source": [
    "class SwinTransformerBlock(nn.Module):\n",
    "    r\"\"\" Swin Transformer Block.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        input_resolution (tuple[int]): Input resulotion.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        window_size (int): Window size.\n",
    "        shift_size (int): Shift size for SW-MSA.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
    "        drop (float, optional): Dropout rate. Default: 0.0\n",
    "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
    "        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n",
    "        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU\n",
    "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, input_resolution, num_heads, window_size=7, shift_size=0,\n",
    "                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.,\n",
    "                 act_layer=nn.GELU, norm_layer=nn.LayerNorm,\n",
    "                 focusing_factor=3, kernel_size=5, attn_type='L'):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.input_resolution = input_resolution\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        if min(self.input_resolution) <= self.window_size:\n",
    "            # if window size is larger than input resolution, we don't partition windows\n",
    "            self.shift_size = 0\n",
    "            self.window_size = min(self.input_resolution)\n",
    "        assert 0 <= self.shift_size < self.window_size, \"shift_size must in 0-window_size\"\n",
    "\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        assert attn_type in ['L', 'S']\n",
    "        if attn_type == 'L':\n",
    "            self.attn = FocusedLinearAttention(\n",
    "                dim, window_size=to_2tuple(self.window_size), num_heads=num_heads,\n",
    "                qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop,\n",
    "                focusing_factor=focusing_factor, kernel_size=kernel_size)\n",
    "        else:\n",
    "            self.attn = WindowAttention(\n",
    "                dim, window_size=to_2tuple(self.window_size), num_heads=num_heads,\n",
    "                qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "        if self.shift_size > 0:\n",
    "            # calculate attention mask for SW-MSA\n",
    "            H, W = self.input_resolution\n",
    "            img_mask = torch.zeros((1, H, W, 1))  # 1 H W 1\n",
    "            h_slices = (slice(0, -self.window_size),\n",
    "                        slice(-self.window_size, -self.shift_size),\n",
    "                        slice(-self.shift_size, None))\n",
    "            w_slices = (slice(0, -self.window_size),\n",
    "                        slice(-self.window_size, -self.shift_size),\n",
    "                        slice(-self.shift_size, None))\n",
    "            cnt = 0\n",
    "            for h in h_slices:\n",
    "                for w in w_slices:\n",
    "                    img_mask[:, h, w, :] = cnt\n",
    "                    cnt += 1\n",
    "\n",
    "            mask_windows = window_partition(img_mask, self.window_size)  # nW, window_size, window_size, 1\n",
    "            mask_windows = mask_windows.view(-1, self.window_size * self.window_size)\n",
    "            attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n",
    "            attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n",
    "        else:\n",
    "            attn_mask = None\n",
    "\n",
    "        self.register_buffer(\"attn_mask\", attn_mask)\n",
    "\n",
    "    def forward(self, x):\n",
    "        H, W = self.input_resolution\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = x.view(B, H, W, C)\n",
    "\n",
    "        # cyclic shift\n",
    "        if self.shift_size > 0:\n",
    "            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n",
    "        else:\n",
    "            shifted_x = x\n",
    "\n",
    "        # partition windows\n",
    "        x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, window_size, C\n",
    "        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)  # nW*B, window_size*window_size, C\n",
    "\n",
    "        # W-MSA/SW-MSA\n",
    "        attn_windows = self.attn(x_windows, mask=self.attn_mask)  # nW*B, window_size*window_size, C\n",
    "\n",
    "        # merge windows\n",
    "        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)\n",
    "        shifted_x = window_reverse(attn_windows, self.window_size, H, W)  # B H' W' C\n",
    "\n",
    "        # reverse cyclic shift\n",
    "        if self.shift_size > 0:\n",
    "            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n",
    "        else:\n",
    "            x = shifted_x\n",
    "        x = x.view(B, H * W, C)\n",
    "\n",
    "        # FFN\n",
    "        x = shortcut + self.drop_path(x)\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"dim={self.dim}, input_resolution={self.input_resolution}, num_heads={self.num_heads}, \" \\\n",
    "               f\"window_size={self.window_size}, shift_size={self.shift_size}, mlp_ratio={self.mlp_ratio}\"\n",
    "\n",
    "    def flops(self):\n",
    "        flops = 0\n",
    "        H, W = self.input_resolution\n",
    "        # norm1\n",
    "        flops += self.dim * H * W\n",
    "        # W-MSA/SW-MSA\n",
    "        nW = H * W / self.window_size / self.window_size\n",
    "        flops += nW * self.attn.flops(self.window_size * self.window_size)\n",
    "        # mlp\n",
    "        flops += 2 * H * W * self.dim * self.dim * self.mlp_ratio\n",
    "        # norm2\n",
    "        flops += self.dim * H * W\n",
    "        return flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "blyYs6kZ1TeT"
   },
   "outputs": [],
   "source": [
    "class PatchMerging(nn.Module):\n",
    "    r\"\"\" Patch Merging Layer.\n",
    "\n",
    "    Args:\n",
    "        input_resolution (tuple[int]): Resolution of input feature.\n",
    "        dim (int): Number of input channels.\n",
    "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_resolution, dim, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.input_resolution = input_resolution\n",
    "        self.dim = dim\n",
    "        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n",
    "        self.norm = norm_layer(4 * dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: B, H*W, C\n",
    "        \"\"\"\n",
    "        H, W = self.input_resolution\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "        assert H % 2 == 0 and W % 2 == 0, f\"x size ({H}*{W}) are not even.\"\n",
    "\n",
    "        x = x.view(B, H, W, C)\n",
    "\n",
    "        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C\n",
    "        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C\n",
    "        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C\n",
    "        x = x.view(B, -1, 4 * C)  # B H/2*W/2 4*C\n",
    "\n",
    "        x = self.norm(x)\n",
    "        x = self.reduction(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"input_resolution={self.input_resolution}, dim={self.dim}\"\n",
    "\n",
    "    def flops(self):\n",
    "        H, W = self.input_resolution\n",
    "        flops = H * W * self.dim\n",
    "        flops += (H // 2) * (W // 2) * 4 * self.dim * 2 * self.dim\n",
    "        return flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "XNJN4Z_01WBE"
   },
   "outputs": [],
   "source": [
    "class BasicLayer(nn.Module):\n",
    "    \"\"\" A basic Swin Transformer layer for one stage.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        input_resolution (tuple[int]): Input resolution.\n",
    "        depth (int): Number of blocks.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        window_size (int): Local window size.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
    "        drop (float, optional): Dropout rate. Default: 0.0\n",
    "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
    "        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n",
    "        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n",
    "        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n",
    "        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, input_resolution, depth, num_heads, window_size,\n",
    "                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., norm_layer=nn.LayerNorm, downsample=None, use_checkpoint=False,\n",
    "                 focusing_factor=3, kernel_size=5, attn_type='L'):\n",
    "\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.input_resolution = input_resolution\n",
    "        self.depth = depth\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "\n",
    "        # build blocks\n",
    "        attn_types = [(attn_type if attn_type[0] != 'M' else ('L' if i < int(attn_type[1:]) else 'S')) for i in range(depth)]\n",
    "        window_sizes = [(window_size if attn_types[i] == 'L' else (7 if window_size <= 56 else 12)) for i in range(depth)]\n",
    "        self.blocks = nn.ModuleList([\n",
    "            SwinTransformerBlock(dim=dim, input_resolution=input_resolution,\n",
    "                                 num_heads=num_heads, window_size=window_sizes[i],\n",
    "                                 shift_size=0 if (i % 2 == 0) else window_sizes[i] // 2,\n",
    "                                 mlp_ratio=mlp_ratio,\n",
    "                                 qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                                 drop=drop, attn_drop=attn_drop,\n",
    "                                 drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n",
    "                                 norm_layer=norm_layer,\n",
    "                                 focusing_factor=focusing_factor,\n",
    "                                 kernel_size=kernel_size,\n",
    "                                 attn_type=attn_types[i])\n",
    "            for i in range(depth)])\n",
    "\n",
    "        # patch merging layer\n",
    "        if downsample is not None:\n",
    "            self.downsample = downsample(input_resolution, dim=dim, norm_layer=norm_layer)\n",
    "        else:\n",
    "            self.downsample = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        for blk in self.blocks:\n",
    "            if self.use_checkpoint:\n",
    "                x = checkpoint.checkpoint(blk, x)\n",
    "            else:\n",
    "                x = blk(x)\n",
    "        if self.downsample is not None:\n",
    "            x = self.downsample(x)\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"dim={self.dim}, input_resolution={self.input_resolution}, depth={self.depth}\"\n",
    "\n",
    "    def flops(self):\n",
    "        flops = 0\n",
    "        for blk in self.blocks:\n",
    "            flops += blk.flops()\n",
    "        if self.downsample is not None:\n",
    "            flops += self.downsample.flops()\n",
    "        return flops\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    r\"\"\" Image to Patch Embedding\n",
    "\n",
    "    Args:\n",
    "        img_size (int): Image size.  Default: 224.\n",
    "        patch_size (int): Patch token size. Default: 4.\n",
    "        in_chans (int): Number of input image channels. Default: 3.\n",
    "        embed_dim (int): Number of linear projection output channels. Default: 96.\n",
    "        norm_layer (nn.Module, optional): Normalization layer. Default: None\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):\n",
    "        super().__init__()\n",
    "        img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.patches_resolution = patches_resolution\n",
    "        self.num_patches = patches_resolution[0] * patches_resolution[1]\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        if norm_layer is not None:\n",
    "            self.norm = norm_layer(embed_dim)\n",
    "        else:\n",
    "            self.norm = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        # FIXME look at relaxing size constraints\n",
    "        assert H == self.img_size[0] and W == self.img_size[1], \\\n",
    "            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)  # B Ph*Pw C\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "    def flops(self):\n",
    "        Ho, Wo = self.patches_resolution\n",
    "        flops = Ho * Wo * self.embed_dim * self.in_chans * (self.patch_size[0] * self.patch_size[1])\n",
    "        if self.norm is not None:\n",
    "            flops += Ho * Wo * self.embed_dim\n",
    "        return flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "LuascylH1a-6"
   },
   "outputs": [],
   "source": [
    "class FLattenSwinTransformer(nn.Module):\n",
    "    r\"\"\" Swin Transformer\n",
    "        A PyTorch impl of : `Swin Transformer: Hierarchical Vision Transformer using Shifted Windows`  -\n",
    "          https://arxiv.org/pdf/2103.14030\n",
    "\n",
    "    Args:\n",
    "        img_size (int | tuple(int)): Input image size. Default 224\n",
    "        patch_size (int | tuple(int)): Patch size. Default: 4\n",
    "        in_chans (int): Number of input image channels. Default: 3\n",
    "        num_classes (int): Number of classes for classification head. Default: 1000\n",
    "        embed_dim (int): Patch embedding dimension. Default: 96\n",
    "        depths (tuple(int)): Depth of each Swin Transformer layer.\n",
    "        num_heads (tuple(int)): Number of attention heads in different layers.\n",
    "        window_size (int): Window size. Default: 7\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4\n",
    "        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float): Override default qk scale of head_dim ** -0.5 if set. Default: None\n",
    "        drop_rate (float): Dropout rate. Default: 0\n",
    "        attn_drop_rate (float): Attention dropout rate. Default: 0\n",
    "        drop_path_rate (float): Stochastic depth rate. Default: 0.1\n",
    "        norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.\n",
    "        ape (bool): If True, add absolute position embedding to the patch embedding. Default: False\n",
    "        patch_norm (bool): If True, add normalization after patch embedding. Default: True\n",
    "        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size=32, patch_size=4, in_chans=3, num_classes=1000,\n",
    "                 embed_dim=96, depths=[2, 2, 6, 2], num_heads=[3, 6, 12, 24],\n",
    "                 window_size=7, mlp_ratio=4., qkv_bias=True, qk_scale=None,\n",
    "                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,\n",
    "                 norm_layer=nn.LayerNorm, ape=False, patch_norm=True,\n",
    "                 use_checkpoint=False,\n",
    "                 focusing_factor=3, kernel_size=5, attn_type='LLLL', **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.num_layers = len(depths)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.ape = ape\n",
    "        self.patch_norm = patch_norm\n",
    "        self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "\n",
    "        # split image into non-overlapping patches\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim,\n",
    "            norm_layer=norm_layer if self.patch_norm else None)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        patches_resolution = self.patch_embed.patches_resolution\n",
    "        self.patches_resolution = patches_resolution\n",
    "\n",
    "        # absolute position embedding\n",
    "        if self.ape:\n",
    "            self.absolute_pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n",
    "            trunc_normal_(self.absolute_pos_embed, std=.02)\n",
    "\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        # stochastic depth\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n",
    "\n",
    "        # build layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i_layer in range(self.num_layers):\n",
    "            layer = BasicLayer(dim=int(embed_dim * 2 ** i_layer),\n",
    "                               input_resolution=(patches_resolution[0] // (2 ** i_layer),\n",
    "                                                 patches_resolution[1] // (2 ** i_layer)),\n",
    "                               depth=depths[i_layer],\n",
    "                               num_heads=num_heads[i_layer],\n",
    "                               window_size=window_size,\n",
    "                               mlp_ratio=self.mlp_ratio,\n",
    "                               qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                               drop=drop_rate, attn_drop=attn_drop_rate,\n",
    "                               drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],\n",
    "                               norm_layer=norm_layer,\n",
    "                               downsample=PatchMerging if (i_layer < self.num_layers - 1) else None,\n",
    "                               use_checkpoint=use_checkpoint,\n",
    "                               focusing_factor=focusing_factor,\n",
    "                               kernel_size=kernel_size,\n",
    "                               attn_type=attn_type[i_layer] + (attn_type[self.num_layers:] if attn_type[i_layer] == 'M' else ''))\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        self.norm = norm_layer(self.num_features)\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'absolute_pos_embed'}\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay_keywords(self):\n",
    "        return {'relative_position_bias_table'}\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        if self.ape:\n",
    "            x = x + self.absolute_pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        x = self.norm(x)  # B L C\n",
    "        x = self.avgpool(x.transpose(1, 2))  # B C 1\n",
    "        x = torch.flatten(x, 1)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "    def flops(self):\n",
    "        flops = 0\n",
    "        flops += self.patch_embed.flops()\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            flops += layer.flops()\n",
    "        flops += self.num_features * self.patches_resolution[0] * self.patches_resolution[1] // (2 ** self.num_layers)\n",
    "        flops += self.num_features * self.num_classes\n",
    "        return flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "CH3oe5YA1en5"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Constants\n",
    "DATA_DIR = './data'\n",
    "IMAGE_SIZE = 224  # ImageNet images are typically resized to 224x224\n",
    "NUM_CLASSES = 1000  # ImageNet has 1000 classes\n",
    "NUM_WORKERS = 8\n",
    "BATCH_SIZE = 256  # Larger batch size for ImageNet\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CIG2Ai-N1rHK",
    "outputId": "4d641b02-cebf-4dd2-da65-46ad48144db5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 7222326297078981226\n",
      "xla_global_id: -1\n",
      "]\n",
      "2.4.1+cu118\n",
      "11.8\n",
      "True\n",
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)  # This will show the version of PyTorch\n",
    "print(torch.version.cuda)  # This will show the version of CUDA PyTorch is linked against\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "IGs1A1fy3O3h"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check device\n",
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zWEj9CuI3Rjj",
    "outputId": "34daaffb-66fe-47a1-af42-101d1b24671e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImageNet Loading Start...\n",
      "ImageNet Loading Ended...\n"
     ]
    }
   ],
   "source": [
    "print(\"ImageNet Loading Start...\")\n",
    "\n",
    "# Data loading\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),  # Resize to 256x256\n",
    "    transforms.CenterCrop(IMAGE_SIZE),  # Crop to 224x224\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet normalization\n",
    "])\n",
    "\n",
    "print(\"ImageNet Loading Ended...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to imagenet_train_data.pkl\n",
      "Dataset saved to imagenet_test_data.pkl\n"
     ]
    }
   ],
   "source": [
    "train_dataset = datasets.ImageNet(root=DATA_DIR, split='train', transform=transform)\n",
    "test_dataset = datasets.ImageNet(root=DATA_DIR, split='val', transform=transform)\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "#pickle output file path\n",
    "train_output = 'imagenet_train_data.pkl'\n",
    "test_output = 'imagenet_test_data.pkl' \n",
    "\n",
    "# Save the dataset with pickle\n",
    "with open(train_output, 'wb') as f:\n",
    "    pickle.dump(train_dataset, f)\n",
    "print(f\"Dataset saved to {train_output}\")\n",
    "\n",
    "with open(test_output, 'wb') as f:\n",
    "    pickle.dump(test_dataset, f)\n",
    "print(f\"Dataset saved to {test_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# import pickle\n",
    "\n",
    "train_input = 'imagenet_train_data.pkl'\n",
    "test_input = 'imagenet_test_data.pkl' \n",
    "\n",
    "# Load the dataset\n",
    "with open(train_input, 'rb') as f:\n",
    "    train_dataset = pickle.load(f)\n",
    "    \n",
    "with open(test_input, 'rb') as f:\n",
    "    test_dataset = pickle.load(f)\n",
    "print(\"Dataset loaded successfully\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Attention window(7, 7) f3 kernel5\n",
      "Linear Attention window(7, 7) f3 kernel5\n",
      "Linear Attention window(7, 7) f3 kernel5\n",
      "Linear Attention window(7, 7) f3 kernel5\n",
      "Linear Attention window(7, 7) f3 kernel5\n",
      "Linear Attention window(7, 7) f3 kernel5\n",
      "Linear Attention window(7, 7) f3 kernel5\n",
      "Linear Attention window(7, 7) f3 kernel5\n",
      "Linear Attention window(7, 7) f3 kernel5\n",
      "Linear Attention window(7, 7) f3 kernel5\n",
      "Linear Attention window(7, 7) f3 kernel5\n",
      "Linear Attention window(7, 7) f3 kernel5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FLattenSwinTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
       "    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.1, inplace=False)\n",
       "  (layers): ModuleList(\n",
       "    (0): BasicLayer(\n",
       "      dim=96, input_resolution=(56, 56), depth=2\n",
       "      (blocks): ModuleList(\n",
       "        (0): SwinTransformerBlock(\n",
       "          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): FocusedLinearAttention(\n",
       "            dim=96, window_size=(7, 7), num_heads=3\n",
       "            (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "            (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "            (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dwc): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=32)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): FocusedLinearAttention(\n",
       "            dim=96, window_size=(7, 7), num_heads=3\n",
       "            (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "            (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "            (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dwc): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=32)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.009)\n",
       "          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (downsample): PatchMerging(\n",
       "        input_resolution=(56, 56), dim=96\n",
       "        (reduction): Linear(in_features=384, out_features=192, bias=False)\n",
       "        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicLayer(\n",
       "      dim=192, input_resolution=(28, 28), depth=2\n",
       "      (blocks): ModuleList(\n",
       "        (0): SwinTransformerBlock(\n",
       "          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): FocusedLinearAttention(\n",
       "            dim=192, window_size=(7, 7), num_heads=6\n",
       "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "            (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dwc): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=32)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.018)\n",
       "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=3, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): FocusedLinearAttention(\n",
       "            dim=192, window_size=(7, 7), num_heads=6\n",
       "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "            (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dwc): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=32)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.027)\n",
       "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (downsample): PatchMerging(\n",
       "        input_resolution=(28, 28), dim=192\n",
       "        (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (2): BasicLayer(\n",
       "      dim=384, input_resolution=(14, 14), depth=6\n",
       "      (blocks): ModuleList(\n",
       "        (0): SwinTransformerBlock(\n",
       "          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): FocusedLinearAttention(\n",
       "            dim=384, window_size=(7, 7), num_heads=12\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dwc): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=32)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.036)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): FocusedLinearAttention(\n",
       "            dim=384, window_size=(7, 7), num_heads=12\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dwc): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=32)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.045)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): SwinTransformerBlock(\n",
       "          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): FocusedLinearAttention(\n",
       "            dim=384, window_size=(7, 7), num_heads=12\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dwc): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=32)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.055)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): SwinTransformerBlock(\n",
       "          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): FocusedLinearAttention(\n",
       "            dim=384, window_size=(7, 7), num_heads=12\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dwc): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=32)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.064)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): SwinTransformerBlock(\n",
       "          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): FocusedLinearAttention(\n",
       "            dim=384, window_size=(7, 7), num_heads=12\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dwc): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=32)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.073)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): SwinTransformerBlock(\n",
       "          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): FocusedLinearAttention(\n",
       "            dim=384, window_size=(7, 7), num_heads=12\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dwc): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=32)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.082)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (downsample): PatchMerging(\n",
       "        input_resolution=(14, 14), dim=384\n",
       "        (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
       "        (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (3): BasicLayer(\n",
       "      dim=768, input_resolution=(7, 7), depth=2\n",
       "      (blocks): ModuleList(\n",
       "        (0): SwinTransformerBlock(\n",
       "          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): FocusedLinearAttention(\n",
       "            dim=768, window_size=(7, 7), num_heads=24\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dwc): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=32)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.091)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): FocusedLinearAttention(\n",
       "            dim=768, window_size=(7, 7), num_heads=24\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dwc): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=32)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.100)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (avgpool): AdaptiveAvgPool1d(output_size=1)\n",
       "  (head): Linear(in_features=768, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model instantiation\n",
    "model = FLattenSwinTransformer(\n",
    "    img_size=IMAGE_SIZE,\n",
    "    patch_size=4,\n",
    "    in_chans=3,\n",
    "    num_classes=NUM_CLASSES,  # ImageNet has 1000 classes\n",
    "    embed_dim=96,\n",
    "    depths=[2, 2, 6, 2],\n",
    "    num_heads=[3, 6, 12, 24],\n",
    "    window_size=7,  # Typically larger window size for ImageNet\n",
    "    mlp_ratio=4.,\n",
    "    qkv_bias=True,\n",
    "    drop_rate=0.1,\n",
    "    attn_drop_rate=0.1,\n",
    "    drop_path_rate=0.1,\n",
    "    ape=False,\n",
    "    patch_norm=True,\n",
    "    use_checkpoint=False,\n",
    "    focusing_factor=3,\n",
    "    kernel_size=5,\n",
    "    attn_type='LLLL'\n",
    ")\n",
    "\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "KKpPWqWt2xoz"
   },
   "outputs": [],
   "source": [
    "def dataset_show_image(dset, idx):\n",
    "    X, Y = dset[idx]\n",
    "    title = \"Ground truth: {}\".format(dset.classes[Y])\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(np.moveaxis(X.numpy(), 0, -1))\n",
    "    ax.set_title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rqpdKvMZ6RWm",
    "outputId": "21ff8c31-b326-4f89-f11a-e2633d8435fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 28,495,816\n"
     ]
    }
   ],
   "source": [
    "# Optimizer and loss function\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Initialize lists to store losses\n",
    "train_losses = []\n",
    "val_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erase CUDA cache before training\n",
    "\n",
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "4PmtkvvX7z53"
   },
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 12.00 GiB of which 0 bytes is free. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m inputs, targets \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(DEVICE), targets\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m     11\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 12\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[0;32m     14\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[10], line 121\u001b[0m, in \u001b[0;36mFLattenSwinTransformer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m--> 121\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead(x)\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "Cell \u001b[1;32mIn[10], line 113\u001b[0m, in \u001b[0;36mFLattenSwinTransformer.forward_features\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    110\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_drop(x)\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m--> 113\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)  \u001b[38;5;66;03m# B L C\u001b[39;00m\n\u001b[0;32m    116\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavgpool(x\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m))  \u001b[38;5;66;03m# B C 1\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[9], line 60\u001b[0m, in \u001b[0;36mBasicLayer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     58\u001b[0m         x \u001b[38;5;241m=\u001b[39m checkpoint\u001b[38;5;241m.\u001b[39mcheckpoint(blk, x)\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 60\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample(x)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[7], line 99\u001b[0m, in \u001b[0;36mSwinTransformerBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     96\u001b[0m x_windows \u001b[38;5;241m=\u001b[39m x_windows\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size, C)  \u001b[38;5;66;03m# nW*B, window_size*window_size, C\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# W-MSA/SW-MSA\u001b[39;00m\n\u001b[1;32m---> 99\u001b[0m attn_windows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_windows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# nW*B, window_size*window_size, C\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m# merge windows\u001b[39;00m\n\u001b[0;32m    102\u001b[0m attn_windows \u001b[38;5;241m=\u001b[39m attn_windows\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size, C)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[6], line 88\u001b[0m, in \u001b[0;36mFocusedLinearAttention.forward\u001b[1;34m(self, x, mask)\u001b[0m\n\u001b[0;32m     85\u001b[0m v \u001b[38;5;241m=\u001b[39m v\u001b[38;5;241m.\u001b[39mreshape(B, N, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m# Compute linear attention\u001b[39;00m\n\u001b[1;32m---> 88\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m (\u001b[43mq\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-6\u001b[39;49m)\n\u001b[0;32m     89\u001b[0m kv \u001b[38;5;241m=\u001b[39m (k\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m (N \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m)) \u001b[38;5;241m@\u001b[39m (v \u001b[38;5;241m*\u001b[39m (N \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m))\n\u001b[0;32m     90\u001b[0m x \u001b[38;5;241m=\u001b[39m q \u001b[38;5;241m@\u001b[39m kv \u001b[38;5;241m*\u001b[39m z\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 12.00 GiB of which 0 bytes is free. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(50):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        #_, predicted = outputs.max(1)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    train_accuracy = 100. * correct / total\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)  # Store average training loss\n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}], Training Loss: {avg_train_loss:.4f}, \"\n",
    "          f\"Accuracy: {train_accuracy:.2f}%\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            #_, predicted = outputs.max(1)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    avg_val_loss = test_loss / len(test_loader)\n",
    "    val_losses.append(avg_val_loss)  # Store average validation loss\n",
    "    test_accuracy = 100. * correct / total\n",
    "    print(f\"Test Loss: {avg_val_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n",
    "\n",
    "    # Update learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "print(\"Training finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Training Loss', color='blue', marker='o')\n",
    "plt.plot(val_losses, label='Validation Loss', color='orange', marker='o')\n",
    "plt.title('Training and Validation Loss per Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
