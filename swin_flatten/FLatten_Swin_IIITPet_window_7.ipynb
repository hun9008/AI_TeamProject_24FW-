{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e27c9b0-1dba-4ed6-bd24-c0e5054a09fd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5e27c9b0-1dba-4ed6-bd24-c0e5054a09fd",
    "outputId": "681f9c2d-4c33-42d3-f4aa-debd5ee558a0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ignite\\handlers\\checkpoint.py:16: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.\n",
      "  from torch.distributed.optim import ZeroRedundancyOptimizer\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------\n",
    "# Swin Transformer\n",
    "# Copyright (c) 2021 Microsoft\n",
    "# Licensed under The MIT License [see LICENSE for details]\n",
    "# Written by Ze Liu\n",
    "# --------------------------------------------------------\n",
    "\n",
    "\n",
    "#!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu112\n",
    "#!pip3 install timm pytorch-ignite einops matplotlib\n",
    "#!pip install pytorch-ignite\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn as nn\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "\n",
    "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
    "from einops import rearrange\n",
    "\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
    "import ignite.metrics\n",
    "import ignite.contrib.handlers\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fcb0a4c-b75c-4896-b9a3-edd6f1ae9e2e",
   "metadata": {
    "id": "5fcb0a4c-b75c-4896-b9a3-edd6f1ae9e2e"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "IMAGE_SIZE = 224\n",
    "NUM_WORKERS = 8\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3c83c5a-d2d6-4b8c-a6c1-600d8110f0fb",
   "metadata": {
    "id": "d3c83c5a-d2d6-4b8c-a6c1-600d8110f0fb"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split, ConcatDataset\n",
    "\n",
    "trainval_data = datasets.OxfordIIITPet(root=\"data\", split=\"trainval\", target_types=\"category\", download=True, transform=transform)\n",
    "test_data = datasets.OxfordIIITPet(root=\"data\", split=\"test\", target_types=\"category\", download=True, transform=transform)\n",
    "combined_data = ConcatDataset([trainval_data, test_data])\n",
    "\n",
    "train_size = int(0.7 * len(combined_data))\n",
    "val_size = int(0.15 * len(combined_data))\n",
    "test_size = len(combined_data) - train_size - val_size\n",
    "train_data, val_data, test_data = random_split(combined_data, [train_size, val_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5875826-44f4-484d-9a52-bfa6b0262ae3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b5875826-44f4-484d-9a52-bfa6b0262ae3",
    "outputId": "3ee1c41c-eca2-4ebb-c639-4c298f3187f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 5144\n",
      "Validation set size: 1102\n",
      "Test set size: 1103\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"Train set size: {len(train_data)}\")\n",
    "print(f\"Validation set size: {len(val_data)}\")\n",
    "print(f\"Test set size: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a365973-7b57-4a1a-9cf1-adf693ec60fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------\n",
    "# Swin Transformer\n",
    "# Copyright (c) 2021 Microsoft\n",
    "# Licensed under The MIT License [see LICENSE for details]\n",
    "# Written by Ze Liu\n",
    "# --------------------------------------------------------\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
    "from einops import rearrange\n",
    "\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def window_partition(x, window_size):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x: (B, H, W, C)\n",
    "        window_size (int): window size\n",
    "\n",
    "    Returns:\n",
    "        windows: (num_windows*B, window_size, window_size, C)\n",
    "    \"\"\"\n",
    "    B, H, W, C = x.shape\n",
    "    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n",
    "    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n",
    "    return windows\n",
    "\n",
    "\n",
    "def window_reverse(windows, window_size, H, W):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        windows: (num_windows*B, window_size, window_size, C)\n",
    "        window_size (int): Window size\n",
    "        H (int): Height of image\n",
    "        W (int): Width of image\n",
    "\n",
    "    Returns:\n",
    "        x: (B, H, W, C)\n",
    "    \"\"\"\n",
    "    B = int(windows.shape[0] / (H * W / window_size / window_size))\n",
    "    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n",
    "    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n",
    "    return x\n",
    "\n",
    "\n",
    "class WindowAttention(nn.Module):\n",
    "    r\"\"\" Window based multi-head self attention (W-MSA) module with relative position bias.\n",
    "    It supports both of shifted and non-shifted window.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        window_size (tuple[int]): The height and width of the window.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n",
    "        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n",
    "        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.window_size = window_size  # Wh, Ww\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        # define a parameter table of relative position bias\n",
    "        self.relative_position_bias_table = nn.Parameter(\n",
    "            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n",
    "\n",
    "        # get pair-wise relative position index for each token inside the window\n",
    "        coords_h = torch.arange(self.window_size[0])\n",
    "        coords_w = torch.arange(self.window_size[1])\n",
    "        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n",
    "        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n",
    "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n",
    "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
    "        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n",
    "        relative_coords[:, :, 1] += self.window_size[1] - 1\n",
    "        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n",
    "        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n",
    "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "        trunc_normal_(self.relative_position_bias_table, std=.02)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input features with shape of (num_windows*B, N, C)\n",
    "            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None\n",
    "        \"\"\"\n",
    "        B_, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "        q = q * self.scale\n",
    "        attn = (q @ k.transpose(-2, -1))\n",
    "\n",
    "        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n",
    "            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)  # Wh*Ww,Wh*Ww,nH\n",
    "        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n",
    "        attn = attn + relative_position_bias.unsqueeze(0)\n",
    "\n",
    "        if mask is not None:\n",
    "            nW = mask.shape[0]\n",
    "            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n",
    "            attn = attn.view(-1, self.num_heads, N, N)\n",
    "            attn = self.softmax(attn)\n",
    "        else:\n",
    "            attn = self.softmax(attn)\n",
    "\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f'dim={self.dim}, window_size={self.window_size}, num_heads={self.num_heads}'\n",
    "\n",
    "    def flops(self, N):\n",
    "        # calculate flops for 1 window with token length of N\n",
    "        flops = 0\n",
    "        # qkv = self.qkv(x)\n",
    "        flops += N * self.dim * 3 * self.dim\n",
    "        # attn = (q @ k.transpose(-2, -1))\n",
    "        flops += self.num_heads * N * (self.dim // self.num_heads) * N\n",
    "        #  x = (attn @ v)\n",
    "        flops += self.num_heads * N * N * (self.dim // self.num_heads)\n",
    "        # x = self.proj(x)\n",
    "        flops += N * self.dim * self.dim\n",
    "        return flops\n",
    "\n",
    "\n",
    "class FocusedLinearAttention(nn.Module):\n",
    "    r\"\"\" Window based multi-head self attention (W-MSA) module with relative position bias.\n",
    "    It supports both of shifted and non-shifted window.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        window_size (tuple[int]): The height and width of the window.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n",
    "        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n",
    "        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.,\n",
    "                 focusing_factor=3, kernel_size=5):\n",
    "\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.window_size = window_size  # Wh, Ww\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "\n",
    "        self.focusing_factor = focusing_factor\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "        self.dwc = nn.Conv2d(in_channels=head_dim, out_channels=head_dim, kernel_size=kernel_size,\n",
    "                             groups=head_dim, padding=kernel_size // 2)\n",
    "        self.scale = nn.Parameter(torch.zeros(size=(1, 1, dim)))\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(size=(1, window_size[0] * window_size[1], dim)))\n",
    "        print('Linear Attention window{} f{} kernel{}'.\n",
    "              format(window_size, focusing_factor, kernel_size))\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input features with shape of (num_windows*B, N, C)\n",
    "            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None\n",
    "        \"\"\"\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, C).permute(2, 0, 1, 3)\n",
    "        q, k, v = qkv.unbind(0)\n",
    "        k = k + self.positional_encoding\n",
    "        focusing_factor = self.focusing_factor\n",
    "        kernel_function = nn.ReLU()\n",
    "        q = kernel_function(q) + 1e-6\n",
    "        k = kernel_function(k) + 1e-6\n",
    "        scale = nn.Softplus()(self.scale)\n",
    "        q = q / scale\n",
    "        k = k / scale\n",
    "        q_norm = q.norm(dim=-1, keepdim=True)\n",
    "        k_norm = k.norm(dim=-1, keepdim=True)\n",
    "        q = q ** focusing_factor\n",
    "        k = k ** focusing_factor\n",
    "        q = (q / q.norm(dim=-1, keepdim=True)) * q_norm\n",
    "        k = (k / k.norm(dim=-1, keepdim=True)) * k_norm\n",
    "\n",
    "        q = q.reshape(B, N, self.num_heads, -1).permute(0, 2, 1, 3)\n",
    "        k = k.reshape(B, N, self.num_heads, -1).permute(0, 2, 1, 3)\n",
    "        v = v.reshape(B, N, self.num_heads, -1).permute(0, 2, 1, 3)\n",
    "\n",
    "        z = 1 / (q @ k.mean(dim=-2, keepdim=True).transpose(-2, -1) + 1e-6)\n",
    "        kv = (k.transpose(-2, -1) * (N ** -0.5)) @ (v * (N ** -0.5))\n",
    "        x = q @ kv * z\n",
    "\n",
    "        H = W = int(N ** 0.5)\n",
    "        x = x.transpose(1, 2).reshape(B, N, C)\n",
    "        v = v.reshape(B * self.num_heads, H, W, -1).permute(0, 3, 1, 2)\n",
    "        x = x + self.dwc(v).reshape(B, C, N).permute(0, 2, 1)\n",
    "\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "    def eval(self):\n",
    "        super().eval()\n",
    "        print('eval')\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f'dim={self.dim}, window_size={self.window_size}, num_heads={self.num_heads}'\n",
    "\n",
    "\n",
    "class SwinTransformerBlock(nn.Module):\n",
    "    r\"\"\" Swin Transformer Block.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        input_resolution (tuple[int]): Input resulotion.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        window_size (int): Window size.\n",
    "        shift_size (int): Shift size for SW-MSA.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
    "        drop (float, optional): Dropout rate. Default: 0.0\n",
    "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
    "        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n",
    "        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU\n",
    "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, input_resolution, num_heads, window_size=7, shift_size=0,\n",
    "                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.,\n",
    "                 act_layer=nn.GELU, norm_layer=nn.LayerNorm,\n",
    "                 focusing_factor=3, kernel_size=5, attn_type='L'):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.input_resolution = input_resolution\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        if min(self.input_resolution) <= self.window_size:\n",
    "            # if window size is larger than input resolution, we don't partition windows\n",
    "            self.shift_size = 0\n",
    "            self.window_size = min(self.input_resolution)\n",
    "        assert 0 <= self.shift_size < self.window_size, \"shift_size must in 0-window_size\"\n",
    "\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        assert attn_type in ['L', 'S']\n",
    "        if attn_type == 'L':\n",
    "            self.attn = FocusedLinearAttention(\n",
    "                dim, window_size=to_2tuple(self.window_size), num_heads=num_heads,\n",
    "                qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop,\n",
    "                focusing_factor=focusing_factor, kernel_size=kernel_size)\n",
    "        else:\n",
    "            self.attn = WindowAttention(\n",
    "                dim, window_size=to_2tuple(self.window_size), num_heads=num_heads,\n",
    "                qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "        if self.shift_size > 0:\n",
    "            # calculate attention mask for SW-MSA\n",
    "            H, W = self.input_resolution\n",
    "            img_mask = torch.zeros((1, H, W, 1))  # 1 H W 1\n",
    "            h_slices = (slice(0, -self.window_size),\n",
    "                        slice(-self.window_size, -self.shift_size),\n",
    "                        slice(-self.shift_size, None))\n",
    "            w_slices = (slice(0, -self.window_size),\n",
    "                        slice(-self.window_size, -self.shift_size),\n",
    "                        slice(-self.shift_size, None))\n",
    "            cnt = 0\n",
    "            for h in h_slices:\n",
    "                for w in w_slices:\n",
    "                    img_mask[:, h, w, :] = cnt\n",
    "                    cnt += 1\n",
    "\n",
    "            mask_windows = window_partition(img_mask, self.window_size)  # nW, window_size, window_size, 1\n",
    "            mask_windows = mask_windows.view(-1, self.window_size * self.window_size)\n",
    "            attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n",
    "            attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n",
    "        else:\n",
    "            attn_mask = None\n",
    "\n",
    "        self.register_buffer(\"attn_mask\", attn_mask)\n",
    "\n",
    "    def forward(self, x):\n",
    "        H, W = self.input_resolution\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = x.view(B, H, W, C)\n",
    "\n",
    "        # cyclic shift\n",
    "        if self.shift_size > 0:\n",
    "            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n",
    "        else:\n",
    "            shifted_x = x\n",
    "\n",
    "        # partition windows\n",
    "        x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, window_size, C\n",
    "        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)  # nW*B, window_size*window_size, C\n",
    "\n",
    "        # W-MSA/SW-MSA\n",
    "        attn_windows = self.attn(x_windows, mask=self.attn_mask)  # nW*B, window_size*window_size, C\n",
    "\n",
    "        # merge windows\n",
    "        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)\n",
    "        shifted_x = window_reverse(attn_windows, self.window_size, H, W)  # B H' W' C\n",
    "\n",
    "        # reverse cyclic shift\n",
    "        if self.shift_size > 0:\n",
    "            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n",
    "        else:\n",
    "            x = shifted_x\n",
    "        x = x.view(B, H * W, C)\n",
    "\n",
    "        # FFN\n",
    "        x = shortcut + self.drop_path(x)\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"dim={self.dim}, input_resolution={self.input_resolution}, num_heads={self.num_heads}, \" \\\n",
    "               f\"window_size={self.window_size}, shift_size={self.shift_size}, mlp_ratio={self.mlp_ratio}\"\n",
    "\n",
    "    def flops(self):\n",
    "        flops = 0\n",
    "        H, W = self.input_resolution\n",
    "        # norm1\n",
    "        flops += self.dim * H * W\n",
    "        # W-MSA/SW-MSA\n",
    "        nW = H * W / self.window_size / self.window_size\n",
    "        flops += nW * self.attn.flops(self.window_size * self.window_size)\n",
    "        # mlp\n",
    "        flops += 2 * H * W * self.dim * self.dim * self.mlp_ratio\n",
    "        # norm2\n",
    "        flops += self.dim * H * W\n",
    "        return flops\n",
    "\n",
    "\n",
    "class PatchMerging(nn.Module):\n",
    "    r\"\"\" Patch Merging Layer.\n",
    "\n",
    "    Args:\n",
    "        input_resolution (tuple[int]): Resolution of input feature.\n",
    "        dim (int): Number of input channels.\n",
    "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_resolution, dim, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.input_resolution = input_resolution\n",
    "        self.dim = dim\n",
    "        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n",
    "        self.norm = norm_layer(4 * dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: B, H*W, C\n",
    "        \"\"\"\n",
    "        H, W = self.input_resolution\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "        assert H % 2 == 0 and W % 2 == 0, f\"x size ({H}*{W}) are not even.\"\n",
    "\n",
    "        x = x.view(B, H, W, C)\n",
    "\n",
    "        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C\n",
    "        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C\n",
    "        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C\n",
    "        x = x.view(B, -1, 4 * C)  # B H/2*W/2 4*C\n",
    "\n",
    "        x = self.norm(x)\n",
    "        x = self.reduction(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"input_resolution={self.input_resolution}, dim={self.dim}\"\n",
    "\n",
    "    def flops(self):\n",
    "        H, W = self.input_resolution\n",
    "        flops = H * W * self.dim\n",
    "        flops += (H // 2) * (W // 2) * 4 * self.dim * 2 * self.dim\n",
    "        return flops\n",
    "\n",
    "\n",
    "class BasicLayer(nn.Module):\n",
    "    \"\"\" A basic Swin Transformer layer for one stage.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        input_resolution (tuple[int]): Input resolution.\n",
    "        depth (int): Number of blocks.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        window_size (int): Local window size.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
    "        drop (float, optional): Dropout rate. Default: 0.0\n",
    "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
    "        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n",
    "        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n",
    "        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n",
    "        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, input_resolution, depth, num_heads, window_size,\n",
    "                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., norm_layer=nn.LayerNorm, downsample=None, use_checkpoint=False,\n",
    "                 focusing_factor=3, kernel_size=5, attn_type='L'):\n",
    "\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.input_resolution = input_resolution\n",
    "        self.depth = depth\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "\n",
    "        # build blocks\n",
    "        attn_types = [(attn_type if attn_type[0] != 'M' else ('L' if i < int(attn_type[1:]) else 'S')) for i in range(depth)]\n",
    "        window_sizes = [(window_size if attn_types[i] == 'L' else (7 if window_size <= 56 else 12)) for i in range(depth)]\n",
    "        self.blocks = nn.ModuleList([\n",
    "            SwinTransformerBlock(dim=dim, input_resolution=input_resolution,\n",
    "                                 num_heads=num_heads, window_size=window_sizes[i],\n",
    "                                 shift_size=0 if (i % 2 == 0) else window_sizes[i] // 2,\n",
    "                                 mlp_ratio=mlp_ratio,\n",
    "                                 qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                                 drop=drop, attn_drop=attn_drop,\n",
    "                                 drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n",
    "                                 norm_layer=norm_layer,\n",
    "                                 focusing_factor=focusing_factor,\n",
    "                                 kernel_size=kernel_size,\n",
    "                                 attn_type=attn_types[i])\n",
    "            for i in range(depth)])\n",
    "\n",
    "        # patch merging layer\n",
    "        if downsample is not None:\n",
    "            self.downsample = downsample(input_resolution, dim=dim, norm_layer=norm_layer)\n",
    "        else:\n",
    "            self.downsample = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        for blk in self.blocks:\n",
    "            if self.use_checkpoint:\n",
    "                x = checkpoint.checkpoint(blk, x)\n",
    "            else:\n",
    "                x = blk(x)\n",
    "        if self.downsample is not None:\n",
    "            x = self.downsample(x)\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"dim={self.dim}, input_resolution={self.input_resolution}, depth={self.depth}\"\n",
    "\n",
    "    def flops(self):\n",
    "        flops = 0\n",
    "        for blk in self.blocks:\n",
    "            flops += blk.flops()\n",
    "        if self.downsample is not None:\n",
    "            flops += self.downsample.flops()\n",
    "        return flops\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    r\"\"\" Image to Patch Embedding\n",
    "\n",
    "    Args:\n",
    "        img_size (int): Image size.  Default: 224.\n",
    "        patch_size (int): Patch token size. Default: 4.\n",
    "        in_chans (int): Number of input image channels. Default: 3.\n",
    "        embed_dim (int): Number of linear projection output channels. Default: 96.\n",
    "        norm_layer (nn.Module, optional): Normalization layer. Default: None\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):\n",
    "        super().__init__()\n",
    "        img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.patches_resolution = patches_resolution\n",
    "        self.num_patches = patches_resolution[0] * patches_resolution[1]\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        if norm_layer is not None:\n",
    "            self.norm = norm_layer(embed_dim)\n",
    "        else:\n",
    "            self.norm = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        # FIXME look at relaxing size constraints\n",
    "        assert H == self.img_size[0] and W == self.img_size[1], \\\n",
    "            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)  # B Ph*Pw C\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "    def flops(self):\n",
    "        Ho, Wo = self.patches_resolution\n",
    "        flops = Ho * Wo * self.embed_dim * self.in_chans * (self.patch_size[0] * self.patch_size[1])\n",
    "        if self.norm is not None:\n",
    "            flops += Ho * Wo * self.embed_dim\n",
    "        return flops\n",
    "\n",
    "\n",
    "class FLattenSwinTransformer(nn.Module):\n",
    "    r\"\"\" Swin Transformer\n",
    "        A PyTorch impl of : `Swin Transformer: Hierarchical Vision Transformer using Shifted Windows`  -\n",
    "          https://arxiv.org/pdf/2103.14030\n",
    "\n",
    "    Args:\n",
    "        img_size (int | tuple(int)): Input image size. Default 224\n",
    "        patch_size (int | tuple(int)): Patch size. Default: 4\n",
    "        in_chans (int): Number of input image channels. Default: 3\n",
    "        num_classes (int): Number of classes for classification head. Default: 1000\n",
    "        embed_dim (int): Patch embedding dimension. Default: 96\n",
    "        depths (tuple(int)): Depth of each Swin Transformer layer.\n",
    "        num_heads (tuple(int)): Number of attention heads in different layers.\n",
    "        window_size (int): Window size. Default: 7\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4\n",
    "        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float): Override default qk scale of head_dim ** -0.5 if set. Default: None\n",
    "        drop_rate (float): Dropout rate. Default: 0\n",
    "        attn_drop_rate (float): Attention dropout rate. Default: 0\n",
    "        drop_path_rate (float): Stochastic depth rate. Default: 0.1\n",
    "        norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.\n",
    "        ape (bool): If True, add absolute position embedding to the patch embedding. Default: False\n",
    "        patch_norm (bool): If True, add normalization after patch embedding. Default: True\n",
    "        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size=224, patch_size=4, in_chans=3, num_classes=1000,\n",
    "                 embed_dim=96, depths=[2, 2, 6, 2], num_heads=[3, 6, 12, 24],\n",
    "                 window_size=7, mlp_ratio=4., qkv_bias=True, qk_scale=None,\n",
    "                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,\n",
    "                 norm_layer=nn.LayerNorm, ape=False, patch_norm=True,\n",
    "                 use_checkpoint=False,\n",
    "                 focusing_factor=3, kernel_size=5, attn_type='LLLL', **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.num_layers = len(depths)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.ape = ape\n",
    "        self.patch_norm = patch_norm\n",
    "        self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "\n",
    "        # split image into non-overlapping patches\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim,\n",
    "            norm_layer=norm_layer if self.patch_norm else None)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        patches_resolution = self.patch_embed.patches_resolution\n",
    "        self.patches_resolution = patches_resolution\n",
    "\n",
    "        # absolute position embedding\n",
    "        if self.ape:\n",
    "            self.absolute_pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n",
    "            trunc_normal_(self.absolute_pos_embed, std=.02)\n",
    "\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        # stochastic depth\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n",
    "\n",
    "        # build layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i_layer in range(self.num_layers):\n",
    "            layer = BasicLayer(dim=int(embed_dim * 2 ** i_layer),\n",
    "                               input_resolution=(patches_resolution[0] // (2 ** i_layer),\n",
    "                                                 patches_resolution[1] // (2 ** i_layer)),\n",
    "                               depth=depths[i_layer],\n",
    "                               num_heads=num_heads[i_layer],\n",
    "                               window_size=window_size,\n",
    "                               mlp_ratio=self.mlp_ratio,\n",
    "                               qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                               drop=drop_rate, attn_drop=attn_drop_rate,\n",
    "                               drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],\n",
    "                               norm_layer=norm_layer,\n",
    "                               downsample=PatchMerging if (i_layer < self.num_layers - 1) else None,\n",
    "                               use_checkpoint=use_checkpoint,\n",
    "                               focusing_factor=focusing_factor,\n",
    "                               kernel_size=kernel_size,\n",
    "                               attn_type=attn_type[i_layer] + (attn_type[self.num_layers:] if attn_type[i_layer] == 'M' else ''))\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        self.norm = norm_layer(self.num_features)\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'absolute_pos_embed'}\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay_keywords(self):\n",
    "        return {'relative_position_bias_table'}\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        if self.ape:\n",
    "            x = x + self.absolute_pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        x = self.norm(x)  # B L C\n",
    "        x = self.avgpool(x.transpose(1, 2))  # B C 1\n",
    "        x = torch.flatten(x, 1)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "    def flops(self):\n",
    "        flops = 0\n",
    "        flops += self.patch_embed.flops()\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            flops += layer.flops()\n",
    "        flops += self.num_features * self.patches_resolution[0] * self.patches_resolution[1] // (2 ** self.num_layers)\n",
    "        flops += self.num_features * self.num_classes\n",
    "        return flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ee7c900-d59c-4d85-aca2-6b03138e4a3b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9ee7c900-d59c-4d85-aca2-6b03138e4a3b",
    "outputId": "d1b5a201-e322-40a7-a0fa-e0b5e644fb3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 5601434019688778708\n",
      "xla_global_id: -1\n",
      "]\n",
      "2.4.1+cu118\n",
      "11.8\n",
      "True\n",
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "print(device_lib.list_local_devices())\n",
    "print(torch.__version__)  # This will show the version of PyTorch\n",
    "print(torch.version.cuda)  # This will show the version of CUDA PyTorch is linked against\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2de199c5-bc80-4c82-8283-37196a217824",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2de199c5-bc80-4c82-8283-37196a217824",
    "outputId": "d58db472-499f-4423-e15b-b761ffb47ccb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Attention window(56, 56) f3 kernel5\n",
      "Linear Attention window(56, 56) f3 kernel5\n",
      "Linear Attention window(28, 28) f3 kernel5\n",
      "Linear Attention window(28, 28) f3 kernel5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\functional.py:513: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3610.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "# Model instantiation\n",
    "# param config of the official FLatten-Swin-Small\n",
    "\n",
    "model = FLattenSwinTransformer(\n",
    "    img_size=IMAGE_SIZE,\n",
    "    patch_size=4,\n",
    "    in_chans=3,\n",
    "    num_classes=37,\n",
    "    embed_dim=96, # 96\n",
    "    depths=[2, 2, 6, 2], # 2,2,6,2 in Swin-T\n",
    "    num_heads=[3, 6, 12, 24],\n",
    "    window_size=56, # FLatten-Swin-T config\n",
    "    mlp_ratio=4.,\n",
    "    qkv_bias=True,\n",
    "    drop_rate=0.1, \n",
    "    attn_drop_rate=0.1,\n",
    "    drop_path_rate=0.2, #0.2 in FLatten-Swin-T config\n",
    "    ape=False, #False\n",
    "    patch_norm=True,\n",
    "    use_checkpoint=False, #False\n",
    "    focusing_factor=3, # p=3\n",
    "    kernel_size=5,\n",
    "    attn_type='LLSS'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "271cb823-626e-4b0d-8011-45420420e0dc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "271cb823-626e-4b0d-8011-45420420e0dc",
    "outputId": "10a9ed52-be27-488a-9535-86bebfa5d2ef"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FLattenSwinTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
       "    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.1, inplace=False)\n",
       "  (layers): ModuleList(\n",
       "    (0): BasicLayer(\n",
       "      dim=96, input_resolution=(56, 56), depth=2\n",
       "      (blocks): ModuleList(\n",
       "        (0): SwinTransformerBlock(\n",
       "          dim=96, input_resolution=(56, 56), num_heads=3, window_size=56, shift_size=0, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): FocusedLinearAttention(\n",
       "            dim=96, window_size=(56, 56), num_heads=3\n",
       "            (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "            (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "            (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dwc): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=32)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          dim=96, input_resolution=(56, 56), num_heads=3, window_size=56, shift_size=0, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): FocusedLinearAttention(\n",
       "            dim=96, window_size=(56, 56), num_heads=3\n",
       "            (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "            (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "            (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dwc): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=32)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.018)\n",
       "          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (downsample): PatchMerging(\n",
       "        input_resolution=(56, 56), dim=96\n",
       "        (reduction): Linear(in_features=384, out_features=192, bias=False)\n",
       "        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicLayer(\n",
       "      dim=192, input_resolution=(28, 28), depth=2\n",
       "      (blocks): ModuleList(\n",
       "        (0): SwinTransformerBlock(\n",
       "          dim=192, input_resolution=(28, 28), num_heads=6, window_size=28, shift_size=0, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): FocusedLinearAttention(\n",
       "            dim=192, window_size=(28, 28), num_heads=6\n",
       "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "            (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dwc): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=32)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.036)\n",
       "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          dim=192, input_resolution=(28, 28), num_heads=6, window_size=28, shift_size=0, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): FocusedLinearAttention(\n",
       "            dim=192, window_size=(28, 28), num_heads=6\n",
       "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "            (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dwc): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=32)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.055)\n",
       "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (downsample): PatchMerging(\n",
       "        input_resolution=(28, 28), dim=192\n",
       "        (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (2): BasicLayer(\n",
       "      dim=384, input_resolution=(14, 14), depth=6\n",
       "      (blocks): ModuleList(\n",
       "        (0): SwinTransformerBlock(\n",
       "          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            dim=384, window_size=(7, 7), num_heads=12\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.073)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            dim=384, window_size=(7, 7), num_heads=12\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.091)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): SwinTransformerBlock(\n",
       "          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            dim=384, window_size=(7, 7), num_heads=12\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.109)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): SwinTransformerBlock(\n",
       "          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            dim=384, window_size=(7, 7), num_heads=12\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.127)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): SwinTransformerBlock(\n",
       "          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            dim=384, window_size=(7, 7), num_heads=12\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.145)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): SwinTransformerBlock(\n",
       "          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            dim=384, window_size=(7, 7), num_heads=12\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.164)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (downsample): PatchMerging(\n",
       "        input_resolution=(14, 14), dim=384\n",
       "        (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
       "        (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (3): BasicLayer(\n",
       "      dim=768, input_resolution=(7, 7), depth=2\n",
       "      (blocks): ModuleList(\n",
       "        (0): SwinTransformerBlock(\n",
       "          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            dim=768, window_size=(7, 7), num_heads=24\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.182)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            dim=768, window_size=(7, 7), num_heads=24\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.200)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (avgpool): AdaptiveAvgPool1d(output_size=1)\n",
       "  (head): Linear(in_features=768, out_features=37, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6963eef2-ca5d-40d7-9304-e130f942306e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 28,451,837\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c588f927-195d-48dd-a091-98ab1caa0b12",
   "metadata": {
    "id": "c588f927-195d-48dd-a091-98ab1caa0b12"
   },
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in tqdm(train_loader, desc=\"Training\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Train Loss: {epoch_loss:.4f}, Train Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0713241b-771b-4064-bd41-06b9c83f594b",
   "metadata": {
    "id": "0713241b-771b-4064-bd41-06b9c83f594b"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, criterion, device, phase=\"Validation\"):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(data_loader, desc=f\"{phase}\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(data_loader)\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"{phase} Loss: {epoch_loss:.4f}, {phase} Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86dee40c-94ae-459e-b05a-9b879bfce40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(model, learning_rate, weight_decay):\n",
    "    parameters_decay = []\n",
    "    parameters_no_decay = []\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if not param.requires_grad:\n",
    "            continue  # Skip frozen parameters\n",
    "        if \"bias\" in name or \"norm\" in name:\n",
    "            parameters_no_decay.append(param)\n",
    "        else:\n",
    "            parameters_decay.append(param)\n",
    "    \n",
    "    optim_groups = [\n",
    "        {\"params\": parameters_decay, \"weight_decay\": weight_decay},\n",
    "        {\"params\": parameters_no_decay, \"weight_decay\": 0.0},\n",
    "    ]\n",
    "    optimizer = optim.AdamW(optim_groups, lr=learning_rate)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c182c071-b773-43aa-ae99-8a4a0bfa791f",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-6\n",
    "WEIGHT_DECAY = 1e-2\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = get_optimizer(model, learning_rate=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "trainer = create_supervised_trainer(model, optimizer, loss, device=DEVICE)\n",
    "lr_scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=LEARNING_RATE, steps_per_epoch=len(train_loader), epochs=EPOCHS)\n",
    "\n",
    "trainer.add_event_handler(Events.ITERATION_COMPLETED, lambda engine: lr_scheduler.step());\n",
    "ignite.metrics.RunningAverage(output_transform=lambda x: x).attach(trainer, \"loss\")\n",
    "val_metrics = {\"accuracy\": ignite.metrics.Accuracy(), \"loss\": ignite.metrics.Loss(loss)}\n",
    "evaluator = create_supervised_evaluator(model, metrics=val_metrics, device=DEVICE)\n",
    "history = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5690854-f4b0-4f1d-844c-414b8c516a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_validation_results(engine):\n",
    "    train_state = engine.state\n",
    "    epoch = train_state.epoch\n",
    "    max_epochs = train_state.max_epochs\n",
    "    train_loss = train_state.metrics[\"loss\"]\n",
    "    history['train loss'].append(train_loss)\n",
    "    \n",
    "    evaluator.run(test_loader)\n",
    "    val_metrics = evaluator.state.metrics\n",
    "    val_loss = val_metrics[\"loss\"]\n",
    "    val_acc = val_metrics[\"accuracy\"]\n",
    "    history['val loss'].append(val_loss)\n",
    "    history['val acc'].append(val_acc)\n",
    "    \n",
    "    print(\"{}/{} - train: loss {:.3f}; val: loss {:.3f} accuracy {:.3f}\".format(\n",
    "        epoch, max_epochs, train_loss, val_loss, val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb472361-c1e4-4ba6-886d-af04d9eaf055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/50 - train: loss 3.640; val: loss 3.654 accuracy 0.023\n",
      "2/50 - train: loss 3.638; val: loss 3.648 accuracy 0.024\n",
      "3/50 - train: loss 3.640; val: loss 3.642 accuracy 0.024\n",
      "4/50 - train: loss 3.636; val: loss 3.635 accuracy 0.025\n",
      "5/50 - train: loss 3.621; val: loss 3.627 accuracy 0.024\n",
      "6/50 - train: loss 3.617; val: loss 3.619 accuracy 0.026\n",
      "7/50 - train: loss 3.607; val: loss 3.611 accuracy 0.029\n",
      "8/50 - train: loss 3.594; val: loss 3.603 accuracy 0.032\n",
      "9/50 - train: loss 3.587; val: loss 3.596 accuracy 0.034\n",
      "10/50 - train: loss 3.587; val: loss 3.589 accuracy 0.039\n",
      "11/50 - train: loss 3.574; val: loss 3.583 accuracy 0.041\n",
      "12/50 - train: loss 3.567; val: loss 3.578 accuracy 0.041\n",
      "13/50 - train: loss 3.559; val: loss 3.572 accuracy 0.046\n",
      "14/50 - train: loss 3.552; val: loss 3.567 accuracy 0.051\n",
      "15/50 - train: loss 3.544; val: loss 3.561 accuracy 0.054\n",
      "16/50 - train: loss 3.543; val: loss 3.556 accuracy 0.053\n",
      "17/50 - train: loss 3.531; val: loss 3.551 accuracy 0.057\n",
      "18/50 - train: loss 3.528; val: loss 3.546 accuracy 0.061\n",
      "19/50 - train: loss 3.512; val: loss 3.540 accuracy 0.065\n",
      "20/50 - train: loss 3.512; val: loss 3.536 accuracy 0.059\n",
      "21/50 - train: loss 3.503; val: loss 3.531 accuracy 0.065\n",
      "22/50 - train: loss 3.502; val: loss 3.529 accuracy 0.065\n",
      "23/50 - train: loss 3.500; val: loss 3.525 accuracy 0.064\n",
      "24/50 - train: loss 3.490; val: loss 3.519 accuracy 0.069\n",
      "25/50 - train: loss 3.485; val: loss 3.512 accuracy 0.073\n",
      "26/50 - train: loss 3.473; val: loss 3.507 accuracy 0.071\n",
      "27/50 - train: loss 3.468; val: loss 3.501 accuracy 0.082\n",
      "28/50 - train: loss 3.474; val: loss 3.496 accuracy 0.081\n",
      "29/50 - train: loss 3.453; val: loss 3.487 accuracy 0.087\n",
      "30/50 - train: loss 3.444; val: loss 3.480 accuracy 0.091\n",
      "31/50 - train: loss 3.435; val: loss 3.471 accuracy 0.087\n",
      "32/50 - train: loss 3.424; val: loss 3.462 accuracy 0.090\n",
      "33/50 - train: loss 3.427; val: loss 3.460 accuracy 0.092\n",
      "34/50 - train: loss 3.423; val: loss 3.450 accuracy 0.089\n",
      "35/50 - train: loss 3.406; val: loss 3.449 accuracy 0.091\n",
      "36/50 - train: loss 3.400; val: loss 3.441 accuracy 0.093\n",
      "37/50 - train: loss 3.395; val: loss 3.429 accuracy 0.094\n",
      "38/50 - train: loss 3.371; val: loss 3.419 accuracy 0.093\n",
      "39/50 - train: loss 3.370; val: loss 3.413 accuracy 0.098\n",
      "40/50 - train: loss 3.362; val: loss 3.401 accuracy 0.096\n",
      "41/50 - train: loss 3.362; val: loss 3.393 accuracy 0.101\n",
      "42/50 - train: loss 3.350; val: loss 3.391 accuracy 0.097\n",
      "43/50 - train: loss 3.341; val: loss 3.383 accuracy 0.102\n",
      "44/50 - train: loss 3.325; val: loss 3.380 accuracy 0.098\n",
      "45/50 - train: loss 3.309; val: loss 3.371 accuracy 0.103\n",
      "46/50 - train: loss 3.320; val: loss 3.370 accuracy 0.102\n",
      "47/50 - train: loss 3.314; val: loss 3.366 accuracy 0.104\n",
      "48/50 - train: loss 3.295; val: loss 3.362 accuracy 0.105\n",
      "49/50 - train: loss 3.289; val: loss 3.359 accuracy 0.103\n",
      "50/50 - train: loss 3.280; val: loss 3.358 accuracy 0.110\n"
     ]
    }
   ],
   "source": [
    "trainer.run(train_loader, max_epochs=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5722b069-00de-4e26-b294-5c052ae7d281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51/100 - train: loss 3.242; val: loss 3.322 accuracy 0.114\n",
      "52/100 - train: loss 3.228; val: loss 3.305 accuracy 0.115\n",
      "53/100 - train: loss 3.213; val: loss 3.318 accuracy 0.113\n",
      "54/100 - train: loss 3.226; val: loss 3.311 accuracy 0.116\n",
      "55/100 - train: loss 3.208; val: loss 3.313 accuracy 0.119\n",
      "56/100 - train: loss 3.218; val: loss 3.311 accuracy 0.117\n",
      "57/100 - train: loss 3.196; val: loss 3.294 accuracy 0.117\n",
      "58/100 - train: loss 3.194; val: loss 3.300 accuracy 0.121\n",
      "59/100 - train: loss 3.198; val: loss 3.294 accuracy 0.121\n",
      "60/100 - train: loss 3.176; val: loss 3.292 accuracy 0.120\n",
      "61/100 - train: loss 3.176; val: loss 3.292 accuracy 0.121\n",
      "62/100 - train: loss 3.179; val: loss 3.286 accuracy 0.119\n",
      "63/100 - train: loss 3.176; val: loss 3.296 accuracy 0.122\n",
      "64/100 - train: loss 3.155; val: loss 3.289 accuracy 0.123\n",
      "65/100 - train: loss 3.162; val: loss 3.288 accuracy 0.125\n",
      "66/100 - train: loss 3.180; val: loss 3.282 accuracy 0.124\n",
      "67/100 - train: loss 3.179; val: loss 3.294 accuracy 0.125\n",
      "68/100 - train: loss 3.158; val: loss 3.284 accuracy 0.121\n",
      "69/100 - train: loss 3.154; val: loss 3.283 accuracy 0.121\n",
      "70/100 - train: loss 3.156; val: loss 3.288 accuracy 0.123\n",
      "71/100 - train: loss 3.149; val: loss 3.281 accuracy 0.121\n",
      "72/100 - train: loss 3.149; val: loss 3.285 accuracy 0.124\n",
      "73/100 - train: loss 3.140; val: loss 3.289 accuracy 0.125\n",
      "74/100 - train: loss 3.154; val: loss 3.292 accuracy 0.121\n",
      "75/100 - train: loss 3.145; val: loss 3.291 accuracy 0.126\n",
      "76/100 - train: loss 3.140; val: loss 3.288 accuracy 0.125\n",
      "77/100 - train: loss 3.139; val: loss 3.282 accuracy 0.122\n",
      "78/100 - train: loss 3.145; val: loss 3.286 accuracy 0.121\n",
      "79/100 - train: loss 3.133; val: loss 3.281 accuracy 0.125\n",
      "80/100 - train: loss 3.134; val: loss 3.284 accuracy 0.127\n",
      "81/100 - train: loss 3.131; val: loss 3.279 accuracy 0.127\n",
      "82/100 - train: loss 3.139; val: loss 3.287 accuracy 0.124\n",
      "83/100 - train: loss 3.128; val: loss 3.286 accuracy 0.123\n",
      "84/100 - train: loss 3.141; val: loss 3.285 accuracy 0.125\n",
      "85/100 - train: loss 3.123; val: loss 3.285 accuracy 0.124\n",
      "86/100 - train: loss 3.119; val: loss 3.283 accuracy 0.125\n",
      "87/100 - train: loss 3.142; val: loss 3.282 accuracy 0.124\n",
      "88/100 - train: loss 3.120; val: loss 3.285 accuracy 0.124\n",
      "89/100 - train: loss 3.113; val: loss 3.282 accuracy 0.125\n",
      "90/100 - train: loss 3.123; val: loss 3.283 accuracy 0.125\n",
      "91/100 - train: loss 3.121; val: loss 3.281 accuracy 0.124\n",
      "92/100 - train: loss 3.139; val: loss 3.283 accuracy 0.126\n",
      "93/100 - train: loss 3.132; val: loss 3.282 accuracy 0.126\n",
      "94/100 - train: loss 3.125; val: loss 3.282 accuracy 0.126\n",
      "95/100 - train: loss 3.132; val: loss 3.282 accuracy 0.126\n",
      "96/100 - train: loss 3.121; val: loss 3.283 accuracy 0.126\n",
      "97/100 - train: loss 3.127; val: loss 3.282 accuracy 0.126\n",
      "98/100 - train: loss 3.121; val: loss 3.282 accuracy 0.126\n",
      "99/100 - train: loss 3.119; val: loss 3.282 accuracy 0.126\n",
      "100/100 - train: loss 3.116; val: loss 3.282 accuracy 0.126\n"
     ]
    }
   ],
   "source": [
    "trainer.run(train_loader, max_epochs=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c1f624fd-3b8d-4f7b-b59e-9e26f7b03a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current run is terminating due to exception: Tried to step 16102 times. The specified number of total steps is 16100\n",
      "Engine run is terminating due to exception: Tried to step 16102 times. The specified number of total steps is 16100\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Tried to step 16102 times. The specified number of total steps is 16100",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m;\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ignite\\engine\\engine.py:889\u001b[0m, in \u001b[0;36mEngine.run\u001b[1;34m(self, data, max_epochs, epoch_length)\u001b[0m\n\u001b[0;32m    886\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mdataloader \u001b[38;5;241m=\u001b[39m data\n\u001b[0;32m    888\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterrupt_resume_enabled:\n\u001b[1;32m--> 889\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_internal_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    890\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    891\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_run_legacy()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ignite\\engine\\engine.py:932\u001b[0m, in \u001b[0;36mEngine._internal_run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    930\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_run_generator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_run_as_gen()\n\u001b[0;32m    931\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 932\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_internal_run_generator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    933\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m out:\n\u001b[0;32m    934\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_run_generator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ignite\\engine\\engine.py:990\u001b[0m, in \u001b[0;36mEngine._internal_run_as_gen\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    988\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataloader_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    989\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine run is terminating due to exception: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 990\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    992\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataloader_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    993\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ignite\\engine\\engine.py:644\u001b[0m, in \u001b[0;36mEngine._handle_exception\u001b[1;34m(self, e)\u001b[0m\n\u001b[0;32m    642\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fire_event(Events\u001b[38;5;241m.\u001b[39mEXCEPTION_RAISED, e)\n\u001b[0;32m    643\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 644\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ignite\\engine\\engine.py:956\u001b[0m, in \u001b[0;36mEngine._internal_run_as_gen\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    953\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataloader_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    954\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setup_engine()\n\u001b[1;32m--> 956\u001b[0m epoch_time_taken \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_once_on_dataset_as_gen()\n\u001b[0;32m    958\u001b[0m \u001b[38;5;66;03m# time is available for handlers but must be updated after fire\u001b[39;00m\n\u001b[0;32m    959\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mtimes[Events\u001b[38;5;241m.\u001b[39mEPOCH_COMPLETED\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;241m=\u001b[39m epoch_time_taken\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ignite\\engine\\engine.py:1096\u001b[0m, in \u001b[0;36mEngine._run_once_on_dataset_as_gen\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1094\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1095\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrent run is terminating due to exception: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1096\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ignite\\engine\\engine.py:644\u001b[0m, in \u001b[0;36mEngine._handle_exception\u001b[1;34m(self, e)\u001b[0m\n\u001b[0;32m    642\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fire_event(Events\u001b[38;5;241m.\u001b[39mEXCEPTION_RAISED, e)\n\u001b[0;32m    643\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 644\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ignite\\engine\\engine.py:1078\u001b[0m, in \u001b[0;36mEngine._run_once_on_dataset_as_gen\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1075\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_terminate_or_interrupt()\n\u001b[0;32m   1077\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39moutput \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_function(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mbatch)\n\u001b[1;32m-> 1078\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fire_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEvents\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mITERATION_COMPLETED\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1079\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_terminate_or_interrupt()\n\u001b[0;32m   1081\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mepoch_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m iter_counter \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mepoch_length:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ignite\\engine\\engine.py:431\u001b[0m, in \u001b[0;36mEngine._fire_event\u001b[1;34m(self, event_name, *event_args, **event_kwargs)\u001b[0m\n\u001b[0;32m    429\u001b[0m kwargs\u001b[38;5;241m.\u001b[39mupdate(event_kwargs)\n\u001b[0;32m    430\u001b[0m first, others \u001b[38;5;241m=\u001b[39m ((args[\u001b[38;5;241m0\u001b[39m],), args[\u001b[38;5;241m1\u001b[39m:]) \u001b[38;5;28;01mif\u001b[39;00m (args \u001b[38;5;129;01mand\u001b[39;00m args[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m ((), args)\n\u001b[1;32m--> 431\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfirst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mevent_args\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mothers\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[13], line 9\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(engine)\u001b[0m\n\u001b[0;32m      6\u001b[0m trainer \u001b[38;5;241m=\u001b[39m create_supervised_trainer(model, optimizer, loss, device\u001b[38;5;241m=\u001b[39mDEVICE)\n\u001b[0;32m      7\u001b[0m lr_scheduler \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mOneCycleLR(optimizer, max_lr\u001b[38;5;241m=\u001b[39mLEARNING_RATE, steps_per_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader), epochs\u001b[38;5;241m=\u001b[39mEPOCHS)\n\u001b[1;32m----> 9\u001b[0m trainer\u001b[38;5;241m.\u001b[39madd_event_handler(Events\u001b[38;5;241m.\u001b[39mITERATION_COMPLETED, \u001b[38;5;28;01mlambda\u001b[39;00m engine: \u001b[43mlr_scheduler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m);\n\u001b[0;32m     10\u001b[0m ignite\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mRunningAverage(output_transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x)\u001b[38;5;241m.\u001b[39mattach(trainer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m val_metrics \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m: ignite\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mAccuracy(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m: ignite\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mLoss(loss)}\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:230\u001b[0m, in \u001b[0;36mLRScheduler.step\u001b[1;34m(self, epoch)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_epoch \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 230\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_lr\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    232\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(EPOCH_DEPRECATION_WARNING, \u001b[38;5;167;01mUserWarning\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:2087\u001b[0m, in \u001b[0;36mOneCycleLR.get_lr\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2084\u001b[0m step_num \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_epoch\n\u001b[0;32m   2086\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step_num \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_steps:\n\u001b[1;32m-> 2087\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2088\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTried to step \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m times. The specified number of total steps is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_steps\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# noqa: UP032\u001b[39;00m\n\u001b[0;32m   2089\u001b[0m     )\n\u001b[0;32m   2091\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mparam_groups:\n\u001b[0;32m   2092\u001b[0m     start_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n",
      "\u001b[1;31mValueError\u001b[0m: Tried to step 16102 times. The specified number of total steps is 16100"
     ]
    }
   ],
   "source": [
    "trainer.run(train_loader, max_epochs=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7b282176-4f2f-4bd0-9451-d79a219e1922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAGwCAYAAABVdURTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABsGklEQVR4nO3deXhU1f3H8fed7CEkEMISAiEQdhARRI24gaKCIi5tqdKCWm37U6vYWgWtrUsR2rrRqtQq1qXiwuKCgsgiIAgICBgW2UMQwhICk5Cdmfv7Y5hJZsk+mcnyeT0PD5l7zz1z5hiTL+d8zzmGaZomIiIiIk2EJdgNEBEREfEnBTciIiLSpCi4ERERkSZFwY2IiIg0KQpuREREpElRcCMiIiJNioIbERERaVJCg92AQLPb7Rw+fJiWLVtiGEawmyMiIiLVYJomeXl5dOzYEYul8rGZZhfcHD58mM6dOwe7GSIiIlILBw8epFOnTpWWaXbBTcuWLQFH58TGxta6ntLSUr788kuuvvpqwsLC/NU88UF9HTjq68BSfweO+jpw6quvc3Nz6dy5s+v3eGWaXXDjnIqKjY2tc3ATHR1NbGys/kepZ+rrwFFfB5b6O3DU14FT331dnZQSJRSLiIhIk6LgRkRERJoUBTciIiLSpDS7nBsREZH6ZLPZKC0tDXYzgqa0tJTQ0FCKioqw2Ww1ejY8PLzKZd7VoeBGRETED0zT5MiRI5w6dSrYTQkq0zTp0KEDBw8erPF+chaLha5duxIeHl6nNii4ERER8QNnYNOuXTuio6Ob7Uaxdrud06dPExMTU6NRGOcmu1lZWSQnJ9ep/xTciIiI1JHNZnMFNm3atAl2c4LKbrdTUlJCZGRkjaeY2rZty+HDhzlz5kydlpEroVhERKSOnDk20dHRQW5J4+acjqppro4nBTciIiJ+0lynovzFX/2n4EZERESaFAU3IiIi0qQouPEn6yHYv9Lxt4iISDOTkpLC9OnTg90MrZbym+/ehvn3g2mCYYHR02HQ+GC3SkREpFJXXHEFAwcO5MUXX6xzXevXrycqKoozZ87UvWF1oJEbf7AegvkPOAIbANMO8ydqBEdERGosy1rIN3uzybIWBrspgGNTvuoGK23btm0QK8YU3PhDzl5HQFOeaYOD32qaSkSkGTJNk4KSMzX+886aDIZOW8Ztr61j6LRlvLMmo8Z1mM5/aFfD7bffzooVK5g+fTqGYWAYBm+++SaGYbBw4UIGDx5MREQEq1atYu/evYwZM4b27dsTExPDkCFDWLJkiVt9ntNShmHw+uuvc9NNNxEdHU2PHj349NNP/dbPFdG0lD/EpzqmojwDnDm3O/7WNJWISLNSWGqj758X1akOuwmPf7KNxz/ZVqPntj91DdHh1fv1Pn36dHbt2kX//v156qmnANi2zfF+kyZN4tlnn6Vbt260bt2agwcPMmrUKKZMmUJERARvv/02o0ePZufOnSQnJ1f4Hk8++SR///vf+cc//sG//vUvxo0bx4EDB4iPj6/R56oJjdz4Q1ySI3gxQnzf1zSViIg0QHFxcYSHhxMdHU2HDh3o0KEDISGO32VPPfUUI0aMIDU1lfj4eM4991x+85vf0L9/f3r06MHTTz9NampqlSMxt99+O7feeivdu3fnmWee4fTp03z77bf1+rk0cuMvg8ZD6pWQsw/yj8OcO9zvmzbHvbik4LRPREQCJioshO1PXVOjZ45Yi7jq+RXYy80qWQxY8vvL6RAXWaP39ofzzz/f7fXp06d54okn+Pzzz8nKyuLMmTMUFhaSmZlZaT0DBgxwfd2iRQtiY2M5duyYX9pYEQU3/hSX5PhjPeR7murUAbB2c+ToxKcq0BERaaIMw6j21JBTt7YxTL35HB6dtxWbaRJiGDxzc3+6tY2pp1ZWrkWLFm6vH3roIRYvXsyzzz5L9+7diYqK4ic/+QklJSWV1uN5RpRhGNjt9gpK+4eCGz/KshayPzufrgnxJI6e7piKMsudj/HJvYABaLm4iIh4Gzskmct6tiUju4CUhGgS46Lq/T3Dw8OrdZbT6tWruf3227npppsAx0hORkZGPbeudhTc+MkH6zOZPC8du+kYRpx68xWMnZjumIpqlQxLn4KtcwCP5eKpV2oER0REXBLjogIS1DilpKSwbt06MjIyiImJqXBUpUePHsybN4/Ro0djGAaPP/54vY/A1JYSiv0gy1roCmzAkeE+eV46W3Kj+cbehyxLO98jNM48HBERkSB56KGHCAkJoW/fvrRt27bCHJrnn3+e1q1bc/HFFzN69GiuueYaBg0aFODWVo9Gbvxgf3a+WwIYOAKcMS9/Azgmos6JPc1HpkGIUVbQNOGHgpac3JtN14QWAY3URUREAHr27MmaNWvcrt1+++1e5VJSUli2bJnbtXvvvdftdUZGBna7ndzcXACfe+6cOnWqbg2uBo3c+EHXhBZYKjml3QS+z41h8pm7OGM6utw0wTDA+v5veHXmq9wybTYfrK8841xERESqpuDGDxLjoph68zmEGI4Ip6I4Z7ZtGJcUT+fnJX/i/tJ7KTZDuSjkB94K/ztfh9/Ppo//1WC22xYREWmsNC3lJ+Uz3KPDLdz0yjduU1UhhsHDI3vx94U7OWJvQ0dOEIat3H2TKaGvs/nABBIH9A/CJxAREWkaFNz4UfkMd197FYwdkswN53YkI7uAttlrsSx0n4sMMeykcAhQcCMiIlJbCm7qSUV7FbgCoISBmAstGLgvo4vf/hb0HwEWzRiKiIjUhn6D1qPEuCjSUtv4XgUVl4Rxw3TMs+dR2UyDM6aB8cPn8Mk9sG+FzqISERGpBY3cBNOg8Rhnz6OavvEMGd8t4cXwl7BseQ+2vKddjEVERGpBIzfBFpcEXS/lthFpbDL6gFlurZVOExcREakxBTcNRIe4SO7sY8NieGx4pF2MRUSkAUtJSeHFF18MdjPcKLhpQEZdcQk2032XHBPgTFFQ2iMiItIYKbhpQOwtO3rvYgzYPr4XDq6H/Ss1RSUiIlIFBTcNyP7sfD4st4vxiOK/84O9MyH5RzFnXgVvjcZ8sT9893awmyoiIvXFeihg/5j9z3/+Q8eOHb1O9x4zZgx33nkne/fuZcyYMbRv356YmBiGDBnCkiVL6r1ddaXgpgFxnlF1hDastfdlD514uORu1wgOgGHasc9/QCM4IiINmWlCSX7N/3z7GrzYH94a7fj729dqXoePwyor8tOf/pQTJ07w1Vdfua7l5OTwxRdfMG7cOE6fPs2oUaNYunQpmzZt4tprr2X06NEVnhzeUGgpeAPiPKPKubOxxYCkaDuGzb2cxbRz4uAO2sQlBaehIiJSudICeKZj3eow7bDgIcefmnj0MIS3qFbR1q1bM3LkSGbNmsWVV14JwJw5c0hISGDYsGFYLBbOPfdcV/mnn36ajz76iE8//ZT77ruvZu0KII3cNDBjhySzatIw3rv7IlZPGs6vbrjKK8nYbkKGrW2QWigiIk3JuHHjmDt3LsXFxQC8++67/PznP8disXD69Gkeeugh+vTpQ6tWrYiJiWHHjh0auZGaK39GFSndeezMXfw1dCahhh3TBIsBfbe/AC3vgDbdHXvliIhIwxEW7RhBqYncw/DyBY4RGycjBO5dB7E1GAUKi67R244ePRrTNPn8888ZMmQIX3/9NS+88AIADz30EIsXL+bZZ5+le/fuREVF8ZOf/ISSkpIavUegKbhp4BLjojjvxvu5fN4AOhtH6cphpoT/l6idH8HOj7SLsYhIQ2QY1Z4ackno4fh5Pn+iY48zIwRGv+i4Xo8iIyO5+eabeffdd9mzZw+9evVi0KBBAKxevZrbb7+dm266CYDTp0+TkZFRr+3xBwU3jYDjEM6fsjHjJM/P+wrM/5ZlGDt3MU69UiM4IiKN3aDxjp/nOfsgvlvAfq6PGzeO66+/nm3btvGLX/zCdb1Hjx7MmzeP0aNHYxgGjz/+uNfKqoZIwU0jkRgXxfXnRhF3NBLLNxXsYqzgRkSk8YtLCvjP8+HDhxMfH8/OnTu57bbbXNeff/557rzzTi6++GISEhJ45JFHyM3NDWjbakPBTSPTo8+52FYbhJQ7psE04dTh3bTuemkQWyYiIo2VxWLh8GHvHKGUlBSWLVvmdu3ee+91e90Qp6m0WqqR2VfSym0XY7vpmNqNWzYJtn+iXYxFRKTZC2pwM2PGDAYMGEBsbCyxsbGkpaWxcOHCSp85deoU9957L4mJiURERNCzZ08WLFgQoBYHX9eEFsyxl+1ifGnxdBbbBmOxFcOH48s2ftIuxiIi0kwFNbjp1KkT06ZNY+PGjWzYsIHhw4czZswYtm3b5rN8SUkJI0aMICMjgzlz5rBz505ee+01kpKaT66Jc6O/44ZjF+NDtGV+h3vcCzmTjDWCIyIizVBQc25Gjx7t9nrKlCnMmDGDtWvX0q9fP6/yb7zxBjk5OXzzzTeEhYUBjvnAyhQXF7s2JgJciVClpaWUlpbWuu3OZ+tSR23dPDCRtK6tWbT9CFMW7CL70D4I9yhk2jhzfBdmdLuAt8/fgtnXzY36OrDU34FT331dWlqKaZrY7fZGsZqoPplnj39w9kdN2O12TNOktLSUkJAQt3s1+W9nmGYNDqGoRzabjdmzZzNhwgQ2bdpE3759vcqMGjWK+Ph4oqOj+eSTT2jbti233XYbjzzyiFcnOD3xxBM8+eSTXtdnzZpFdHTNNjpqiN7YaeFozklWR95PCOWSjIH1KfdQGhrL6YgOFIXHB6+RIiJNXGhoKB06dKBTp05EREQEuzmNVklJCQcPHuTIkSOcOXPG7V5BQQG33XYbVquV2NjYSusJenCTnp5OWloaRUVFxMTEMGvWLEaNGuWzbO/evcnIyGDcuHHcc8897Nmzh3vuuYf777+fv/zlLz6f8TVy07lzZ7Kzs6vsnMqUlpayePFiRowY4RpFCobMnAKu/edqbmIZ08JmYsGOiWMbHNffhgXbqOcxB/6i8soaqIbS182B+jqw1N+BU999bbPZ2LdvH23btqVNmzZ+r78xMU2TvLw8WrZsiWEYVT9QTm5uLocPH6Zr166EhoZ63UtISKhWcBP0peC9evVi8+bNWK1W5syZw4QJE1ixYoXPkRu73U67du34z3/+Q0hICIMHD+bQoUP84x//qDC4iYiI8BlFh4WF+eUb3F/11FZq+zjuvKQrr64YxkrbAFIsRwkzz/B2xDS3k8RDF/wBel7dqPfCCXZfNyfq68BSfwdOffV1WFgYrVu3Jjs7G4vFQnR0dI1/sTcVdrudkpISiouLsViqn9prt9vJzs6mRYsWREZGevVfTf67BT24CQ8Pp3v37gAMHjyY9evXM336dF599VWvsomJiYSFhblNQfXp04cjR45QUlJCeLhn4knz8JNBnXh1xT6O0IYj9jakWbbh9b+UNvoTEalXHTp0AODYsWNBbklwmaZJYWEhUVFRNQ7wLBYLycnJdQ4Mgx7ceLLb7W7TSOUNHTqUWbNmYbfbXdHgrl27SExMbLaBDcDx0+79td/eAZvpvtEfANm7Hdt55+yF+FQFOiIifmQYBomJibRr165ZJ4mXlpaycuVKLrvsshqPkoWHh9dotKciQQ1uJk+ezMiRI0lOTiYvL49Zs2axfPlyFi1aBMD48eNJSkpi6tSpAPzf//0fL730Eg888AC/+93v2L17N8888wz3339/MD9G0HVNaIHFcGzoB3CENjx65i6mhb+BYdpwZeB8/iB8fvZrHbgpIlIvQkJCKlzk0hyEhIRw5swZIiMjgzbdGtTg5tixY4wfP56srCzi4uIYMGAAixYtYsSIEQBkZma6RXCdO3dm0aJFPPjggwwYMICkpCQeeOABHnnkkWB9hAbBuffNo/PSsZ0NcD6wDaP3BTcyJPYUbTv3pP3G52HLe+BcUaUDN0VEpIkKanAzc+bMSu8vX77c61paWhpr166tpxY1Xo6Tw9uSkZ3PJ5sP8/76gzy5wgoYGOxmUu8L+A3vuT+kPBwREWmCdLZUE5IYF0VaagL3X9ndLaHYBP77Qyg20z1ByzQh58RRnUclIiJNioKbJijjRAGemxcdoY3bgZvm2QM3W3/2K51HJSIiTUqDWy0ldeeZYAyOKHaOfRgrix174YTbi3kr4h+4VtspB0dERJoIjdw0Qc4E45CzkUuIYTD1lnPOHriZwFp7X0ot4XhtI2Da4NB3jikqTVWJiEgjpZGbJqoswbiAlIRoEuOiALisZ1vW7j3Bc3NO+N4L5+PfQkk+Wi4uIiKNlUZumjBHgnEbV2DjvHbToE5cd+kQJp+5C9vZbwHTsEB0Oyg5jddycY3giIhII6KRm2bqd8N7cNHaEawscuTgZJrtmXp+CJevv8e9oJaLi4hII6PgppnKKyolv/gMp8+eRwXw6KocVkVaMEy7e+F9XzlGcdp0V5AjIiINnqalmqn92fley8UPmfHsu3AKGB7bhn/9HLx9g5aLi4hIo6DgpplyLhf3lN1zLExMhwmfwa+WQPntAE07zH9AOTgiItKgKbhppjyXizvdO2sTK4+G8429Dyesp8BzfMe0w5qXwfqjlouLiEiDpJybZqz8cvHYqFD+OPt7tmflMv6NbwHoaJxgdYQFA48cnLUvO/6AlouLiEiDo5GbZs65XLxfxzhe/PlAt3uHzTZMLr0L05mDY4RA9xHuFWi5uIiINDAauRGX7NPFXtfet13BT2+awOCWJyG+G+TshT2L3QuZNsd1cPwdn6pVVSIiEjQKbsTF15lUAGGtO0Hn/mUXDItjxKa8LybDse2O65qqEhGRINK0lLhUlGR8//ub+HLbEbKshY4RmdHTyy0XP1v26NaygEdTVSIiEkQauRE35ZOMI0INbv/vejJOFPDrdzZiMWDqzecwdsh4x+nhOfscU1U/fA4L/+hekWmDg99CThtNU4mISEApuBEviXFRJMZFkWUt5HTxGdd1uwmT56VzWc+2JMYllQUsva+DLx7xnqqacwc6gFNERAJN01JSof3Z+V75N3YTth/Odb/oNVXlpAM4RUQk8DRyIxWqKMH4H4t20ql1FCfyS+ia0MJx6vigclNVJ/fDp79zf0gHcIqISIAouJEKOROMH523FZtpYjEgKiyEH47kcc2LXwOUy8NJdgQucUmOPBxfK6rOFDlGb7RcXERE6pGCG6lU+QTjlIRo9h3PZ9zr61z37SY8Om/r2TycKMdF5zTV/ImOERun924F+xmUhyMiIvVJwY1UyZlgDI48HE820yQju6AsuAH3aaroNrDwYcj4uuy+Mw8n9UqN4IiIiF8puJEaqSgPJ6lVpHfh8iuqLn3IPbgBLRcXEZF6odVSUiMVbfT37Je7OHSygG/2Zjs2+/OU0MMxFeVpzh3w1mh4sT9893Y9tVpERJoTjdxIjZXPw8myFvLwnO/5dMth5m85jElZkvFlPduyPzv/7IqqCvJwPJeLa5pKRETqSMGN1Er5PJy8olL+8ul2Z5iC3YRH5qZjgFuw47azcf7xs5v8leM8gFPBjYiI1IGCG6mzHu1b+rxePtgpW1F1Ng/Hesj3cvEVz0JUayg8qTwcERGpFeXcSJ05k4wr41xR5eLrAE4jFDJWwL8vUR6OiIjUmoIbqTPPJGMLrrPCXQwDUhKi3S8OGg8T02HCZ/DgNhg32/2+aYf5D8Dh70jI2w65h+vtM4iISNOhaSnxC8/N/lbuOu7a2RggPMSCxfAxvFN+uXjOXu/7pp3Q/17DUEzMl/6ujf9ERKRKGrkRv0mMiyIttQ2JcVGMHZLMqknDePeuC+nToSXFZ+w89dn2yiuIT/W5XNw4m71j6ABOERGpBgU3Um8S46IY2j2BZ392LiEWg8+/z2Ludz9WvBeOVx6Oj29P0waHNzsCnP0rFeiIiIgXTUtJvevXMY47Lk7h9VX7+cOHWwCPAzfLK39sQ1g0zLzKe0XV3F85DuEsf0ZV6pU6kFNERAAFNxIgt16QzOur9rteO5aHp9O7Q0vyS2xnN/ord/CmM0AZPR1z/kQM04ZpWDCi20L+0bKKTTt8er8jY9m060BOERFRcCOBcTSvyOuazYQbX/7GY6M/75GcM10uZ93C97hw5K2EndoL79zkUZMJpnY6FhERB+XcSEBUtBeO50Z/PnNxYjtyomUfiO0ICb18n1HlVqkNDn2nvBwRkWZKwY0EhOdeOL72/PPa6M8Xn0nHPmqb+yt4oZ82AxQRaYY0LSUBU34vnOhwCze98g12s+y+xddGf76UTzqO7wZ7l5YdyGlYIKY95GWVlddUlYhIs6LgRgKq/IGbU28+h0fnpWM7G+CM6Nu+LKm4KuWTjj2Dnezd8M4Y9/LOQzlBq6pERJo4BTcSNM6RnHfWHOCV5XvZlHmK4jM2IkJDqn7YU/lgB3wfyrnoUTi6TauqRESaOOXcSFAlxkUx8aqetI+N4FheMZ9u9sP5Ub4O5QQ4kl4W8Gi3YxGRJkvBjQRdeKiF2y/uCsDMVfsxTbOKJ6rB81DOkX/3LmPaHFNZIiLSpCi4kQbhtguSiQ4P4YcjeXy9O9s/lcYlQddLHX/3vt57CblhceToiIhIk6LgRhqEuOgwxg7pDMBLy3a7nT+VZS1it9Ugy+q9EWD138BzqgqwhMLBddoLR0SkiVFCsTQYdw7typurM/g24yS3vbYOiwE3nZfER5sOYTdDeGXHSt+7GFeXc1XVsR3w9bOQuQbm3OG4pwRjEZEmQyM30mCEhhiUz7axmzD3u0OuvXAq3cW4uuKSoMdVMOYV9+tKMBYRaTIU3EiDsT87v8oy1drFuDpyf/S+Ztpgz1Id2yAi0shpWkoaDOf5U/YqFkslx1dzo7/KxKf63gtn/gM4TrwyNVUlItJIaeRGGgzP86dCDINbBiV5Hbj59toDdX8zzwRjwwIJvQE7ruM8NVUlItIoaeRGGpTy50+lJESTGBfFA8NT+XDBV7RO6cfTC37g1RX7SGgRTr+kOLomtCAxLoosayH7s/Ndr6vF89iGE3vg7Rvcyzj3wtFRDSIijUZQg5sZM2YwY8YMMjIyAOjXrx9//vOfGTlypM/yb775JnfccYfbtYiICIqK6rBEWBqc8udPOV5H0iPOZFRaMtaiM/xz2R6mLPgBcBy2eeN5SXy8yZF4bDFwraiqVsBTnWMbDqwBTJ1HJSLSSAQ1uOnUqRPTpk2jR48emKbJW2+9xZgxY9i0aRP9+vXz+UxsbCw7d+50vTYMw2c5aZrGDunMP5ftcb22mzDvu0NuryfPS+dIbhHTl+z2Cngq5Zyqcp4w7rR8iuNv5eCIiDQKQQ1uRo8e7fZ6ypQpzJgxg7Vr11YY3BiGQYcOHQLRPGmADuRUvVLKbsILi3e7vX503lYu69m26imr8lNVoREw82q8cnBSr9QIjohIA9Zgcm5sNhuzZ88mPz+ftLS0CsudPn2aLl26YLfbGTRoEM8880yFgRBAcXExxcXFrte5ubkAlJaWUlpaWuv2Op+tSx1SPeX7ulNcRLVWVHmymSZ7j+aSEF2Nb/nodhDdDiPja0LxeCPTxpldizG7DcPI2YsZnwqxHWvWmAZM39eBpf4OHPV14NRXX9ekPsP0yymFtZeenk5aWhpFRUXExMQwa9YsRo0a5bPsmjVr2L17NwMGDMBqtfLss8+ycuVKtm3bRqdOnXw+88QTT/Dkk096XZ81axbR0dF+/SwSGGuOGnywz4KJgYHJ+QkmG7IN1+vRyXbmZzruOxmYPDHIRquI6r9PZEkOV297EMMjwLERggU7BiYmBpuT7ySzzeX++ngiIuJDQUEBt912G1arldjY2ErLBj24KSkpITMzE6vVypw5c3j99ddZsWIFffv2rfLZ0tJS+vTpw6233srTTz/ts4yvkZvOnTuTnZ1dZedU9d6LFy9mxIgRhIWF1boeqZqvvs6yFpGZU0ByfDSJcZFer2dv/JE/fbLdNcKTHB/FovuHEhpSs90PjM3/I2TBHzBMG6ZhwWzVBcvJ/W5lTCOEM/dtahIjOPq+Diz1d+CorwOnvvo6NzeXhISEagU3QZ+WCg8Pp3v37gAMHjyY9evXM336dF599dUqnw0LC+O8885jz549FZaJiIggIsL7n+thYWF+6XR/1SNVK9/XyQlhJCe0dN3zfH3bRV0Z1qcD6/fnMHleOpk5hby2OpP7r+xRszcdcgf0vBpy9mHEd8PI3gXv3OhWxDBthOVmQpsutf5sDY2+rwNL/R046uvA8Xdf16SuoAc3nux2u9tIS2VsNhvp6ekVTmNJ85YYF8UNA5OwmSYPfrCF6Ut30yexJS0iQmu2H05Vy8UNi2OfHBERaRCCGtxMnjyZkSNHkpycTF5eHrNmzWL58uUsWrQIgPHjx5OUlMTUqVMBeOqpp7jooovo3r07p06d4h//+AcHDhzgrrvuCubHkAbuxoFJLPvhOPO3HObutzcCNVge7snXcvGQCCjO82+jRUSk1oJ6/MKxY8cYP348vXr14sorr2T9+vUsWrSIESNGAJCZmUlWVpar/MmTJ7n77rvp06cPo0aNIjc3l2+++aZa+TnSfBmGwe+Gd3e7VqcTxgeNh4np8It5kHgenCmE/90E2z/VUQ0iIg1AUEduZs6cWen95cuXu71+4YUXeOGFF+qxRdJUZZ/2nuq0mSYbM3KIj4mo2TQVlE1VJQ6Ely+A3MPw4S+10Z+ISAPQ4HJuROpDRSeO3/feZqAO01RniqAwp+y1NvoTEQk6nQouzYLnieOeaj1NlbPX+ywq0wZHt9WypSIiUlcauZFmo/yJ4yfyi7lv1ia3+zbTJCO7oGbTU/Gpvg/b/OoZaNUF8o/qwE0RkQBTcCPNivPE8Sxrodc0lWFASkINd632XD1lWByrp7I2wSsXnK1YeTgiIoGkaSlplnxNU0WFhRBWwx2MgbLVUxM+g4lb4Wdvu9935uFoJZWISEAouJFma+yQZFZNGsb/fnUB3dvFUFBi48n522tXWVwSdL3U8XdYpPd90+Y4aVxEROqdghtp1hLjorikR1teHDuQEIvB/C2H+WD9Qb7Zm11pcnGWtbDiMs48HDcGxHf1b+NFRMQn5dyIAP2T4rj70m78e8VeHpn7PVDx8vAP1mcyeV46drOCMr52McaE796GlEuUYCwiUs80ciNy1s+HdHZ77Wt5+OFTBUyam+5KRK5wCXn5PJxLH3JcW/E3eGs0vNjfEeiIiEi9UHAjctZhH1NMzuXhACVn7DwyNx2zkjJunHk4598BlNtfRwnGIiL1StNSImdVtIvxD0dyySsq5ZXle9l88JTXcyGGUfkS8px94BkSOROMNT0lIuJ3GrkROauiXYyfnL+dX7+zkc0HTxERauHOS7pSvsiUm/pXvvGfzwRjoHUXP7VcRETKU3AjUo5zefh7d1/E7N9e5HW/1Gbn7ku7svT3lxMR6vjfp2eHlpVX6kwwNkLcr29611/NFhGRchTciHhIjIsiLbUNpTbP7BrHlFVGdgHd2sYwsn8HAD7ZVI3cmfIJxtdOc1xb8TfY8gHsX6n8GxERP1JwI1IBZw5OeeXza8YMdOTLfPZ9FqU2u+fj3pwJxhf9Hwy+AzDho197r6CyHlLAIyJSBwpuRCrgmYMTYhg8c3NZfs0lPRKIbxHOifwSVu/JrlnlF9/v/tq0w/wHYNWLjkBHS8ZFRGpNq6VEKlH+JPGUhGi3xOGwEAvXD0jk7TUH+GTzYa7o1a76Fef+6H3NtMOSv7i/nj8RUq/UqioRkRrQyI1IFZw5OL5WRDmnphZtO0JByZnqV1rRCipPOpNKRKTGFNyI1MGg5FZ0jo+ioMTG4u1Hq/+g5woqIwSG/wW3zf6c1+O7+a29IiLNgaalROrAMAzGnJvES1/t4c3VGVzQNb7yPW/KGzTeMeWUs88RwMQlQUyCI/fGPJugPHi8pqRERGpIIzcidRQR6hht2XTwFEOnLeOD9ZnVf9i5gsoZwAwaDxO3wjk/c7xOnwcnD/i5xSIiTZuCG5E6yLIW8sKS3a7XFR6kWRNxSXDjK9BpCBRb4YNfwt5l7kvDtVxcRKRCmpYSqYP92fleZ1E5D9Ks9vSULyFhcMtMePlCOLIF3rnJkYA87FEIawFfPuaYujIsjtydQePr9kFERJoQBTcidVDRYZtFpba6V24JhTNFZa9NOyz7q3sZLRcXEfGiaSmROqjosM0/fbyV7YetfLM3u/ZTVDl78TpN3BctFxcRcaORG5E6Kr/RX5uYcH7zzkb2Z+cz6p+rALAYMPXmcxg7JLlmFTv3wjHLH+1gcawWd7tmaLm4iEg5GrkR8QPnRn8927dkyo393e7VOsnY1144N0z3ccK4CZveUYKxiMhZGrkR8TfD+1Ktk4x97YUDZdd2LoS1L8PyqWffWwnGIiIauRHxM1+niVsMXKeJ15jnXjjlr6Xdg1s05Uww1giOiDRjCm5E/MyZZFw+wImLDiMqLKTih2orZx9eScdKMBaRZk7BjUg9GDskmdWThvPa+MF0bBXJyfxSfv32BlbtrsPqKV98HsCpBGMRad4U3IjUk8S4KEb07cDMCUMICzH4NuMkv5i5ruZHNFTGM+kYICS8eieOi4g0UfoJKFLPWkWHccZWNnXklyMayhs0Hiamw/j50GEA2IodOxiLiDRTCm5E6tn+7Hyvrficq6f8Ji4Jul0GY15yjNpsnQvpc7Q8XESaJQU3IvXM1+opgzqsnqpM4rkw5G7H13N/BW+Nhhf7w3dv+/+9REQaKAU3IvXM1xENJrA585TP8lnWwrod23D+ne6vtTxcRJoZbeInEgDlj2hYtPUIb67J4PFPtnJhtzbEtwh3lXtj1X6e/mw7JnU4tiH/mPc15/JwHa4pIs2AghuRAEmMiyIxLopBXVrxzb5sdh09zR9nb+FXl3YloUUE732byX+/yXCVdyQep3NZz7Y129nY15lURoiWh4tIs6HgRiTAIkJDePan5zLm5dUs/eEYS3/wMdJyls2EL9Kz6JUYS9eEFtULcpzLw+c/UBbgdLtcozYi0mwouBEJgrYtI7w2FoazB357XHvysx1ADaepnGdSbXkPlj0Ne7+C/V87jmwQEWnilFAsEgS+locD3H1pN1fisef5mzXeHycuCS57CAZNAEyYexf8sECJxSLS5GnkRiQInMvD7eUinBDD4I5LUrjjkhQysgs4kV/MfbM2uT1Xq9PFr3kGdn4Op4/A+7fq5HARafI0ciMSBJ7Lw0MMg2du7u9KOk5LbcPgLq39c7p4kRUKcspea2m4iDRxGrkRCZLyy8NTEqK9RmOcAdCj87ZiMx1DPIOTW9ds1AYgZ6/7yilwLA0/tl1JxiLSJCm4EQki50hNRZwB0JfbjvKXT7ex5Ucrx/KKaNcysvpv4mtpOMCXf4Ko1lBa4CijQEdEmghNS4k0cIlxUUy4OIXBXVpTYrPz9jcHalaB58nhhgXCY+D4D/D6lTqiQUSaHAU3Io3E3Zc6NuF7Z+0BCkrO1Oxh58nhEz6DiVvhlx+533fm4eQe9k9jRUSCSMGNSCMxom97UtpEYy0sZfaGH2teQVySY5+buCQ4U+R937RhnNxX94aKiASZghuRRiLEYvCrs6M3/165l1W763C4pjMPpzzDgtlaRzSISOOn4EakEfnJoE5Eh4eQdaqIX8xcx9Bpy/hgfWbNK/LMwwFo0RZaJPivsSIiQaLgRqQROVVYQmGJzfW6xrsWl+fMw7n1PYiKh9NHsayb4cfWiogER1CDmxkzZjBgwABiY2OJjY0lLS2NhQsXVuvZ999/H8MwuPHGG+u3kSINiK9jG5y7FtdKXBL0GgXXTgXAsvIfdMxZo8RiEWnUghrcdOrUiWnTprFx40Y2bNjA8OHDGTNmDNu2bav0uYyMDB566CEuvVSHAErz4jy2wdOC9Cy+2VOHHJwBY6FNDwxbEUMOzCD0pYFaGi4ijVZQg5vRo0czatQoevToQc+ePZkyZQoxMTGsXbu2wmdsNhvjxo3jySefpFs3JT9K8+J5bIMzznln7QFue70OOTi5hx07GZ9l6IgGEWnEGswOxTabjdmzZ5Ofn09aWlqF5Z566inatWvHr371K77++usq6y0uLqa4uNj1Ojc3F4DS0lJKS0tr3V7ns3WpQ6pHfe3u5oGJpHVtTWZOAZFhIfz01XWuqSq7CZPnpZPWtTWJcdXfxdg4tpNQH0c0nDm+CzO6nf8aL270vR046uvAqa++rkl9QQ9u0tPTSUtLo6ioiJiYGD766CP69u3rs+yqVauYOXMmmzdvrnb9U6dO5cknn/S6/uWXXxIdXcMDCH1YvHhxneuQ6lFfe9ttNTAJcbtmN+FvH3zFBW1NjhcZtI00aRVReT2RJTlcjYFRLqPHBNZ/8zXHtuXWQ8ulPH1vB476OnD83dcFBdXPLTRM0/TMT6zSW2+9RUJCAtdddx0ADz/8MP/5z3/o27cv7733Hl26dKl2XSUlJWRmZmK1WpkzZw6vv/46K1as8Apw8vLyGDBgAK+88gojR44E4Pbbb+fUqVN8/PHHFdbva+Smc+fOZGdnExsbW4NP7a60tJTFixczYsQIwsLCal2PVE19XbEsaxFXPLcSu4//iw0cAYrFgL+O6ctPB3eqtC5j8/8IWfAHDNOG6Xy+RVvO3PwGhv0MZnwqxHash0/RfOl7O3DU14FTX32dm5tLQkICVqu1yt/ftRq5eeaZZ5gxw7FkdM2aNbz88su88MILfPbZZzz44IPMmzev2nWFh4fTvXt3AAYPHsz69euZPn06r776qlu5vXv3kpGRwejRo13X7HbHMHpoaCg7d+4kNTXVq/6IiAgiIrz/2RoWFuaXTvdXPVI19bW35IQwt5PDLQYM7tKa9Rkn3aaqHv9kB8P6dKj8RPEhd1DabTjrFr7HhVeOJuyT32AcSSfsnbP/zxkWx944g8Y7cnFy9urATT/R93bgqK8Dx999XZO6ahXcHDx40BWQfPzxx9xyyy38+te/ZujQoVxxxRW1qdLFbre7jbQ49e7dm/T0dLdrf/rTn8jLy2P69Ol07ty5Tu8r0lg5Tw7PyC4gJSGa/dn53PbaOrcyzuXilQY3ALEdOdGyDyT0hBtehv9cVnbPtMOn98OPG2HT247X5QMeEZEGolbBTUxMDCdOnCA5OZkvv/yS3//+9wBERkZSWFj9paiTJ09m5MiRJCcnk5eXx6xZs1i+fDmLFi0CYPz48SQlJTF16lQiIyPp37+/2/OtWrUC8Lou0twkxkW5BS4WA7epqhDDICWhhjlmxVYfF0347s1yL8+uqkq9UiM4ItJg1Cq4GTFiBHfddRfnnXceu3btYtSoUQBs27aNlJSUatdz7Ngxxo8fT1ZWFnFxcQwYMIBFixYxYsQIADIzM7FYtImySE04l4tPmpvumpp65ub+VY/aeHKeP+W5isqTaYOcfQpuRKTBqFVw8/LLL/OnP/2JgwcPMnfuXNq0aQPAxo0bufXWW6tdz8yZMyu9v3z58krvv/nmm9V+L5HmZOyQZDrERjLhv+uJCDW44dxaBB7O86fmT3QEMEYIXPUXWPKEe8BjWCBee06JSMNRq+CmVatWvPTSS17XfS25FpHguKxnWzq1juLHk4Ws2HWca/t3qHklg8Y7ppxy9jkCmLgkiGpdFvAAhMdA3hElGItIg1GrOZ8vvviCVatWuV6//PLLDBw4kNtuu42TJ0/6rXEiUnuGYXBtP0dAs2jbkdpXFJcEXS8tC1pcB25+CG16QHEuvD4c3hoNL/bXsQ0iEnS1Cm7++Mc/unb6TU9P5w9/+AOjRo1i//79ruRiEQm+a86O1izdcZRSWxW5MzURlwS9roExL7tfL39sg/UQ7F+pIxxEJOBqNS21f/9+1yZ7c+fO5frrr+eZZ57hu+++cyUXi0jwDUpuTUJMONmnS1i77wSX9mjr3zeweW/bgGmD2bfDoQ1aLi4iQVGrkZvw8HDXNshLlizh6quvBiA+Pt41oiMiwRdiMRjRtz1Qx6mpijhXVHn68duypGMdwikiAVar4OaSSy7h97//PU8//TTffvut6xiGXbt20alT5Vu8i0hgXX027+bLbUex+zqnoS6cK6qMs+dbGSHQ/Wrvcs7l4iIiAVCr4Oall14iNDSUOXPmMGPGDJKSHImGCxcu5Nprr/VrA0Wkbi5ObUNMRCjH8orZdPCU/9/AmWA84TPH36Nf9B7N0XJxEQmgWuXcJCcn89lnn3ldf+GFF+rcIBHxr4jQEIb3bsenWw7z39X76dgqsuYb+lUlLsl9CXj5/XEAouMhIsa/7ykiUoFaBTcANpuNjz/+mB07dgDQr18/brjhBkJCQvzWOBHxj5aRjv/VP/s+iwXpWUy9+RzGDkkmy1rI/ux8uia08G/A49wfJ2sLfP57yMuCOXfCxfdDm+5lgZAO4BSRelCr4GbPnj2MGjWKQ4cO0atXLwCmTp1K586d+fzzz32ezi0iwZFlLeS9bzNdr+0mTJ6XzvG8Ep5fvBO76TiLaurN53DzwET/vbFzNCemPcwcAXuWOP4YFhjxNBgGfPknragSEb+rVc7N/fffT2pqKgcPHuS7777ju+++IzMzk65du3L//ff7u40iUgf7s/PxzCO2m/Dslztd1+0mPDpvK1nWIv83oGUH9+MaTDt8+RgselQrqkSkXtRq5GbFihWsXbuW+Ph417U2bdowbdo0hg4d6rfGiUjddU1o4XVKuC820yQzp8D/DcjZC1RjlZZpgx3zoX1fTVOJSJ3UauQmIiKCvLw8r+unT58mPDy8zo0SEf9xnhIeYhgAhBgGk6/tjcVwL2cxIDk+2v8N8LkXjsX3/jhfPKJjHESkzmo1cnP99dfz61//mpkzZ3LBBRcAsG7dOn77299yww03+LWBIlJ3Y4ckc1nPtmRkF5CSEE1iXBStWoTx6Lx0bGcHVQwMNh08xW6rQZa1iOSEMIC6Jx37Ol189IuOe64VVQZuozumHeY/AKnDHfeUdCwiNVCr4Oaf//wnEyZMIC0tjbAwxw/A0tJSxowZw4svvujP9omInyTGRbkFJ86AZ9/xfF5buY/lu47zwAffAyG8smMlU28+B3AkH5dPOh47JLnmb+7rdHEou5Z/HObc4f6MaYc3roHcw0o6FpEaqVVw06pVKz755BP27NnjWgrep08funfv7tfGiUj9cgY8Sa2iuOLZ5a7rdhMemZvuVtaZdHxZz7a1H8HxHHlxXrMecgQvpsfhntYfy752Jh236wel+RrJEZEKVTu4qeq076+++sr19fPPP1/7FolIwB22FlarnM00ycguqJ9NAD2nrgbeBpvecS9n2uD14Y6vy4/kaL8cESmn2sHNpk2bqlXOMIyqC4lIg1LRiiqPTBgsBqQk1EPSMXhPXQFsftd7NMfJtMOnv4NtH8O+ZWCamroSEaAGwU35kRkRaVqcK6o882ugLOcG4IKu8f4ftSmv0mMcLICPQGfv0rKvnVNXqVdqBEekGav18Qsi0rSMHZJMWtfWfLjgK342ahjJCS0BuKxnWz7fksVfF+xg7b4cvtmbzcWpCYFpVPnRnLBomHlVxSM5Ts4TyBXciDRbtdrnRkSapsS4SHrEmSTGRZa7FsVdl3Xjtgsdq6QmzU1n3/HTfLM3m6xq5urUSVwSdL0UOg12jOQYZ8+vM0JgxFM+TiAP0QnkIs2cRm5EpFomj+zNVz8cIzOngCufW4FJHZeH14avJeVRreHT+3FkBxmOPXQ0aiPSrGnkRkSqpWVkGH+8xnFQrjPJuOxMqgCM4Dg5R3KcAcyg8TDqWcfX7fspmVhEFNyISPV1KDdd5eRcHh5U3a90/J29C86UBLctIhJ0Cm5EpNqcS8bLq9fl4dXVOsUxPWUrgWPbgtsWEQk6BTciUm3OJePlA5zzklvTIdZ7RCegDAM6nuf4+nD19uQSkaZLwY2I1MjYIcmsnjScR0f1xgA2HjjJ9CW7A7d6qiIKbkTkLK2WEpEaS4yL4teXpWIxDP76+Q5eXLoblgZh9VR5zuDmkIIbkeZOIzciUmujzung9jooq6ecnMHNse1QGsQRJBEJOgU3IlJrGSe8V0kFbfVUbBK0aOfYofjI1sC/v4g0GApuRKTWGtTqKSUVi8hZCm5EpNZ8rZ762fmd6/dwzcq4gpvvgvP+ItIgKLgRkTpxrp762fmdANielYtpmlU8VU80ciMiKLgRET9IjIvikWt7ExFq4fsfrWw8cDI4DXEGN8d3QvHp4LRBRIJOwY2I+EWbmAhuOs9x3tMbq/cHpxEt2zsSizHhyPfBaYOIBJ2CGxHxmzuGdgXgi61H+PFkkM6bcu13o7wbkeZKwY2I+E2vDi25pHsCdhNe/mpPcHYt7jjQ8bfybkSaLQU3IuJXd16SAsB73x7kttfWMXTaMj5Ynxm4BiipWKTZU3AjIn7Vq31Lt9cB37W44yDH3zl74YcFYD0UmPcVkQZDwY2I+NWBnOrvWpxlLfT/1FV0PETFO75+/1Z4sT9897b/6heRBk8HZ4qIXzl3LbaX2+omxDC8di3+YH0mk+elYzf9fOCm9RAU5pS9Nu0wfyKkXglxSXWvX0QaPI3ciIhfOXctLn8qw5Sb+rvtWpxlLXQFNuDnqaucvd7XTBvk7PO+bj0E+1dq6kqkiVFwIyJ+N3ZIMosmXkZkqONHTLvYCLf7e4/lu43sgB8P3IxPBcPjR5sRAvHd3K9997Zjyuqt0Zq6EmliFNyISL3o2aEl4y9OAeCVr9xHUzYeyPHxBBzIPl33HJy4JBg9HcqPHY1+0X1KynoIPr3fMWUFZVNXvkZwNLoj0ugouBGRevOrS7oSHmJhw4GTfLvfEdAczCng3yscU0SGx4nikz7a6p/l44PGw+2fl71OTnO/n7UF8Bg68jV1pdEdkUZJwY2I1Jv2sZHcMthxoOYry/dgmiaPfpROYamNC7rGs+rhYbx390V88OuL3J7zSw5OylDoNcrx9dpX3O/98Ll3eYDsPWWjNCcPwPwHvEd3ftyokRyRBk6rpUSkXv3msm58sD6T5TuPc++73/H17mzCQy1Mu/kcklpHk9Q6mm/2Zns958zBKZ+IXGNp98LOBbD5PRj2J2jRBo5ugy3vOe4blrLgBeDziWVfh0S43wPH6M7rw8ueHT3dMUokIg2KRm5EpF6lJLSgf1IcAAu2HgFgeO92dGsb4yrjXD5ensXAa/l4jXUZCokD4UwhbHgDTBMWPOwIUvrcABO3woTP4I6F3s/aiiuvu7I8HREJKgU3IlKvsqyFpB+yul37ctsRtykn5/LxkHJJONcP6Fi3URtwJPWk3ef4eu0MWPIEHFgFoVFwzTOOJOOul4L9jO/nL7rPsdIK8PnjsqIl5iISVApuRKRe7c/Ox/TI3bWbeC37HjskmVWThjEhrYvrOb/odyNExEHhCVj9ouNa9+HQqnNZmYqWj6fdAxPTHaM7dy3xUcbivcRcRIJOwY2I1CtfU06+diwGxwjOA1f1JCzEIP2QlV1H8+regNPHoDjX/drOL9ynk5zLx52jNEZI2fJx5+hOp8HuZQA6X6hdj0UaoKAGNzNmzGDAgAHExsYSGxtLWloaCxf6mPs+a968eZx//vm0atWKFi1aMHDgQN55550AtlhEaspzyinEMHjm5v4VTjnFtwhnWK92AMzd+GPdG5Czl2ot+x40vmyUZmK670RhZ5lr/+Z4fXgzFJ6sextFxK+CulqqU6dOTJs2jR49emCaJm+99RZjxoxh06ZN9OvXz6t8fHw8jz32GL179yY8PJzPPvuMO+64g3bt2nHNNdcE4ROISHWMHZLMZT3bkpFdQEpCdJW5NLcM7sSX248yb9Mh/nhNL0JD6vDvMOeUU/mVT752LIaykZrKxCXBhb9x7HlzbJtjJVbaPe5lrIccQVV8qkZ2RIIgqCM3o0ePZtSoUfTo0YOePXsyZcoUYmJiWLt2rc/yV1xxBTfddBN9+vQhNTWVBx54gAEDBrBq1aoAt1xEaioxLoq01DbVShIe1qsdraPDOJ5XzNd7vJeJ10hlU061ZRhwwV2Or9e/DvZygZM2/hMJugazz43NZmP27Nnk5+eTlpZWZXnTNFm2bBk7d+7kb3/7W4XliouLKS4uW9KZm+uYey8tLaW0tLTW7XU+W5c6pHrU14HTUPraAEYPSOTttZnMWX+QS7q1rluF59wKXS7HOLkPs3U3iO0Idf2MfW4idPGfMXL2cmb3UsxuV0DuYULnP4BRbuM/c/4DnOlyueM9PdSqv3MPY+TsxYxP9Vmn+NZQvrebg/rq65rUZ5im5zqGwEpPTyctLY2ioiJiYmKYNWsWo0aNqrC81WolKSmJ4uJiQkJCeOWVV7jzzjsrLP/EE0/w5JNPel2fNWsW0dF13ENDROrNwdPwbHooIZjc0ctO5xYmrSKqfi6QzvnxHbodX0xW3CC+7TaRzidWMSjzP17lNnW+k2OxA4gpPsLpiA4UhcfX6v2ST6xgYOYbGJiYGGxOvpPMNpfXqq7Ikpw6t0ckkAoKCrjtttuwWq3ExsZWWjbowU1JSQmZmZlYrVbmzJnD66+/zooVK+jbt6/P8na7nX379nH69GmWLl3K008/zccff8wVV1zhs7yvkZvOnTuTnZ1dZedUprS0lMWLFzNixAjCwsJqXY9UTX0dOA2pr03T5NJ/rORonuP/X4sBfx3Tl5+ePc6hQcjeRdirF2NiYE+7H8vGmRglp72KmUYomDZHUGJYsI16HnPgL7z7u7JRmdzDhL40sGxUCDCNEM7ct6nGIzjG5v8RsuD3GKbdrT1NWUP63m7q6quvc3NzSUhIqFZwE/RpqfDwcLp37w7A4MGDWb9+PdOnT+fVV1/1Wd5isbjKDxw4kB07djB16tQKg5uIiAgiIrz/uRcWFuaXTvdXPVI19XXgNIS+zrIWciyv7B8mdhMe/2QHw/p0qPvmfv6S2A8SemBk7yZkzXTHtegEKMxxJDAbIdA6BSOn7FR0w7QTuuAP0PNqOHOGhLzthBUOJOyHFWVnWfk62iH3gNdxEIZpIyw3E9p0qX6brYdgwe9ddbm1J5DJz76SrgOQiN0QvrebC3/3dU3qCnpw48lut7uNtPi7vIg0Dvuz8z0XcPvnvCl/sh5yHLZZXuFJ+NUSKC1wrMjK3g3vjHEvY9rgf7cQevwHhmJi/muax/2zRzukXln2S76iQfajWx3vU92gIGev7zOzcvYFLrj57m33QG7UsxASVnlwJ1IDQQ1uJk+ezMiRI0lOTiYvL49Zs2axfPlyFi1aBMD48eNJSkpi6tSpAEydOpXzzz+f1NRUiouLWbBgAe+88w4zZswI5scQkXrg3PzP7vE7femOI5imSde2LYIf5FS0h05pgWPjPyfPpegAx3fg3NvQY4/DsnqcAceZEvjyMY8ChuO9v5gEX0x2fO0MClKvrDjYiU8te7Z8XdXdabm2oyvWQ45A7/gOR5tdn9MOn//evawzuGvXD0rztaReaiyowc2xY8cYP348WVlZxMXFMWDAABYtWsSIESMAyMzMxGIpW62en5/PPffcw48//khUVBS9e/fmf//7H2PHjg3WRxCReuLc/O/ReVuxmabr1/HrqzJ4fVUGFgOm3nwOY4ckB6+R1dlDx7kUff5ER8BihEDfG2DbR1XXH93G8ffyqXAkHaLi4ZcfO3Zcbp0Cy5+BzbNwBSqmHT6937FUvaIREFsxWEK8z9PKPVR1AOE54lLd0ZW1M8oCsOoybfD6lbgFbRW9l7+mszzr0X5FjVZQg5uZM2dWen/58uVur//617/y17/+tR5bJCINSfnN/6LCDG56ZY3r16PdhMnz0rmsZ9vgjeD4Clx87aEzaPzZ0ZR9ZYHP9k88RnOMsqDEafbtMOQuWPWC4/Xo6dDx3LL7A35+NrgpzyybwvKc3jJN+PwhR2DT+SIY/hisexV++Azm/gp+uwoi43x/VuuhssDGV92eZXP2QstE+O4d+GZ6JZ0Iji3XTLyDnwo+R3m1Dbg8edYz4Ofw/fve9forAFLgVK8aXM6NiEh5iXFRJMZF8c3ebK9ffc4DOIM6PeUZuFT0i8pz9+PR0zHnT8QwbZhGCMboF8vqsZXCJ/dC9k5Y+MeyZ4pOudfZprvvKa/yTJvjl2hcEmybB3uXQkg4jHkZErpD4rnw7+/hVCZ8fI9j92XPJN8Te2DLB9XL1SkfJFTG2W5nQAhlQaLXtNnZ9zqxx/29Th10jFRVJwjKPUxC3nbIHeidgO0rcNtSLmh0jojt/ersiJvpaGPXy2D/SrxGl6oKXPwVkDnbriDJi4IbEWkUKsrB2XbYSlpqm+A0yqk6xzZ4GjSeM10uZ93C97hw5K2EOX/hOusZ+87ZaZlyPH9xe44cVTQCsnYGhLeEz/7geH3pHxyBDThGam6ZCTOvdozg/PCZ4xfudc87pq+qClTys8t+wYZGugcbLh7BihECv1pclnTt/DzO4C4sGmZe5f2+K/8BLdpCQTYUnIRlT3m/l2mDg99CTpuyX/gb3yT0swcZatoxX/q7dzCx+8uqgzFMR3BY/vX+FeVe2uHT38GeJbBjfsUjPmHRMP9+36NrUPWoUPlre5f6DpKqM7pUnWu1LVNZIBkgCm5EpFGoKAfn71/spHPrKFpGhdE1oQEkGddEbEdOtOzje5+a0gLva75GSjxHjvYudR8BMQzYucDxx6lFO492eARmph0+m+i7zZ4jRfPuAruNSvNpLv4drHnZfequ02DvcuWDRLfpPovjT8bXMKPqHeyZcweu0ZTOF0LmmrLkbc/RnazvYclfqq6zurZ/Uva1aXcEMlnpsOH1igMo0wZvXH32pPqzo0KJAxxtc74e9qjjv9vnD56tx/mJPPKttn8Ce5aWPZc6HPZ9VRYAXf6IY1ry6+fK6jnvF3CmGNJnlz3X8Tw4vKns9bljAQtsea/sWucL4eC6stcDxwEQuvldxypAX4FkgAR9E79Ay83NJS4urlqbAFWmtLSUBQsWMGrUKO2ZUM/U14HTGPo6y1pIRnYByfFRPPXZdhZtO+q61yCSjGug0v62HnKcTeWZrDwxvepRIuuhsmDn0Eb48Jfu9z3r2b/ScRZWdfzkTWiR4AiIPnsQ9i+vvLzzvaDqqbvKPsex7fDuTzwrh8sfhpXPng3mauCGlxx9u/jPjum+1imOaS5nADZgLHz/Qdnrq/4CS56oxgiPuKnu92w11OT3t0ZuRKRRcebgADxybW+34MZuwqPztgY3ydhfqpusXNGzznLlNhB08RwB8rXqy3NkABxt6HxB2XOXPOA7uPHMpyk/jVYTVX0OTEi5FAZNcHwe64/w8W99lPORw/PpfWVft+oCv1kJxafdA7Dhf3J/HdXa/b9HXQKg8n008DbY9E61uqTRCfQeSmcpuBGRRutIbpHXtQa30V9dVDdZuTK1Xa7umeTrK7hK6OW7bl/5NHVV2edwBkHWQ77LXPUE5pInHMnbGBiegY71R0dg45k75fna13+PqgIgXwGPZx8BbH63iqDIcjZGq2CFXbWDK1/1+LpWneeqUcbzey1AFNyISKPlK8k4xDBISWhCh+LWJlnZ8/naLFf3TPL1FahUVLevfJq6qs7nqKjMoPGc6T2GdQvf46JzUgn96C73umsyulCbAMgz4PHVR57t9hwVqijYrOq9qluP57XqPFdBGa9VgEFYxaXgRkQaLWeS8aS56c6URp65uX/TGLXxp9ouV6/oWm3q9ofqvFdFZc4mb5udBlY9klVX1Ql4qtNuz1Eh8F1PTUeXKqqnNs/5KONzFWCAKbgRkUZt7JBkTuSX8PcvdjIkpXWjSSYOuLqOAAWr7tq8V2VlYjvWPpepLmrT7toEm3WppzbP+SpT2SrAAFFwIyKN3sWpCcBO9h7PxzRNDMPnaU0iDoEcbZKgUHAjIo1e7w4tCbEYnMgv4UhukaalpGqBHG2SgLNUXUREpGGLDAuhR7sYALYeyg1ya0Qk2BTciEiT0LejY1OvbYetQW6JiASbghsRaRL6d3ScZq2RGxFRcCMiTUI/jdyIyFkKbkSkSXBOS2VZizhxujjIrRGRYFJwIyJNQstIx6ngANsOa2pKpDlTcCMiTYZz9GarpqZEmjUFNyLSZDiTijVyI9K8KbgRkSajf9LZpOJDGrkRac4U3IhIk9Hv7MhNxokCcotKg9waEQkWBTci0mTEtwinY1wkADs0NSXSbCm4EZEmpV/S2c38FNyINFsKbkSkSdFmfiKi4EZEmhTXiikdwyDSbCm4EZEmpd/ZFVN7jp+mqNQW5NaISDAouBGRJqVDbCRtWoRjs5vM3nCQLGthsJskIgGm4EZEmhTDMIiPCQfg8U+2MXTaMj5YnxnkVolIICm4EZEmJctayJ6jp12v7SY8Om+rRnBEmhEFNyLSpOzPzsf0uGYzTTKyC4LSHhEJPAU3ItKkdE1ogcVwvxZiGKQkRAenQSIScApuRKRJSYyLYurN57gFOHdf1pXEuKjgNUpEAkrBjYg0OWOHJLN60nCG924HwKbMU5im52SViDRVCm5EpElKjIvirzf2JzzEwrr9OazecyLYTRKRAFFwIyJNVsdWUdx2YTIAzyzYwTd7srVqSqQZUHAjIk3aPcNSCQ0x2J6Vy22vr9O+NyLNgIIbEWnSbHYTm60s36ayfW+yrIV8s1ejOyKNXWiwGyAiUp8q2/em/AqqD9ZnMnleOnYTLAZMvfkcxg5JDmxjRcQvNHIjIk2a731vcNv3Jsta6ApsQLsaizR2Cm5EpEnzte/NLy7q4jZqsz873xXYOGlXY5HGS8GNiDR5zn1vRp/bEYBNB933vWkdHe71jMVjdEdEGg8FNyLSLCTGRfHE6L5EhYXw/Y9Wlu867rr37roDXuVH9k/0uauxko5FGj4FNyLSbLSJieCXaV0AmL5kN6Zp8sORXGatcywNf/m287j70q4A7MjK9drV+IP1mQydtozbXtOScpGGTMGNiDQrd1/ajYhQC5sPnmLl7mye/mw7dhNG9u/AdQM68sBVPWkRHsK+7HzW7c9xPaekY5HGQ8GNiDQrbVtGMO5Cx+jNgx9sZvWeE4SFGDw6qg8AMRGh3DAwCYD3vi0bmVHSsUjjoeBGRJqd31zejRCLQU5+CQBnbCbf7M123b/tAsf+NgvTj3DybJmsU0Ve9SjpWKRhUnAjIs2O3TSxlxuGMXGfYjqnUxz9k2IpsdmZ+92P5OSXMO2LHwAov2XO9QN8Jx2LSHApuBGRZqeyXYudbj07evPOmgz+738bOJ5XTPd2MXz10BX88iLHtFaW1Xs0R0SCT8GNiDQ7vnctNtymmG44tyNhIQYHcgpZt/8k4Eg6TklowT3DUgFYn3FSCcUiDZCCGxFpdpy7FocYjggnxDB45ub+blNMp4vPUGpzH9955au9ZFkLSYyLYkhKawA+/z4rcA0XkWrRwZki0iyNHZLMZT3bkpFdQEpCtFfuzP7sfK9nyh+4ed05iazPOMnn6VncdWm3QDVbRKohqCM3M2bMYMCAAcTGxhIbG0taWhoLFy6ssPxrr73GpZdeSuvWrWndujVXXXUV3377bQBbLCJNSWJcFGmpbXwmBVc1dTXqnEQMAzZlnuLHk1oOLtKQBDW46dSpE9OmTWPjxo1s2LCB4cOHM2bMGLZt2+az/PLly7n11lv56quvWLNmDZ07d+bqq6/m0KFDAW65iDR1VU1dtYuN5IKUeAAWpGtqSqQhCeq01OjRo91eT5kyhRkzZrB27Vr69evnVf7dd991e/36668zd+5cli5dyvjx4+u1rSLS/FQ1dXX9uR1Ztz+Hz77P4teXpVZaV5a1kP3Z+XRNaKHl4yL1rMHk3NhsNmbPnk1+fj5paWnVeqagoIDS0lLi4+MrLFNcXExxcbHrdW5uLgClpaWUlpbWur3OZ+tSh1SP+jpw1NfeEqJDSUiOBbz7ZUSvNvzFgO9/tDJnwwEuSIknMS7Sq47ZG3/kT584jnmwGPDXMX356eBO6u8AUl8HTn31dU3qM0zPk+ECLD09nbS0NIqKioiJiWHWrFmMGjWqWs/ec889LFq0iG3bthEZ6f0DBeCJJ57gySef9Lo+a9YsoqO1s6iI1M0zmy0cLXTM8BuYjO1mp08rk+NFBm0jTUwTntwUgllu+z8DkycG2WgVEaxWizQ+BQUF3HbbbVitVmJjYystG/TgpqSkhMzMTKxWK3PmzOH1119nxYoV9O3bt9Lnpk2bxt///neWL1/OgAEDKizna+Smc+fOZGdnV9k5lSktLWXx4sWMGDGCsLCwWtcjVVNfB476umayrEVc/uxKtw0BDcAwHAdrGkCbmDCyT3v/i/N/d57PoE4t1d8Bou/twKmvvs7NzSUhIaFawU3Qp6XCw8Pp3r07AIMHD2b9+vVMnz6dV199tcJnnn32WaZNm8aSJUsqDWwAIiIiiIjw/udRWFiYXzrdX/VI1dTXgaO+rp4frVavnY5NwPlPRhN8BjYhhkFq+1jCwhw/gtXfgaO+Dhx/93VN6mpwm/jZ7Xa3kRZPf//733n66af54osvOP/88wPYMhERd76Wi/vyy4uS3cr99ab+SioWqUdBDW4mT57MypUrycjIID09ncmTJ7N8+XLGjRsHwPjx45k8ebKr/N/+9jcef/xx3njjDVJSUjhy5AhHjhzh9OnTwfoIItKMeS4Xt+B+sCY4RmnuGdad5Q9dQXSY40du14QWgW2oSDMT1GmpY8eOMX78eLKysoiLi2PAgAEsWrSIESNGAJCZmYnFUhZ/zZgxg5KSEn7yk5+41fOXv/yFJ554IpBNFxEBvJeLr9x1nEfnbcVmml5741x/bkc+3PAj87cc5qJubYLccpGmK6jBzcyZMyu9v3z5crfXGRkZ9dcYEZFaSoyLcgUwle2NM/pscLNw6xGevMF7Ly8R8Y+gJxSLiDQ15YOd8tK6taFNi3BO5Jfwzd4TpHVtFfjGiTQDDS6hWESkqQoNsTDynA4AzN9yOMitEWm6FNyIiATQ6AEdAfhi2xGKz9gD9r5Z1kK+2ZtNlrUwYO8pEiyalhIRCaAhKfG0j43gaG4xq/ZkV1neH2dSfbA+k8nz0l3HP0y9+Rwu69lWZ11Jk6XgRkQkgCwWg+vO6cgbq/czZ+MhelsMsqxFJCd4b1DmKygZOyS5Ru+XZS101QGOnZMfmZuOgWOTwdrWK9KQaVpKRCTArj83EYAlPxznpe0hXPHcSj5Yn+maOjqYk8976w7wyFz3oGTyvHS2HDxZo+ml/dn5rjrKc16ym/DovK2arpImRSM3IiIB1iHW/UgYz9GUithNGPPyN0D1R1xS2lR9QLDNNMnILtD0lDQZGrkREQmwjBMFPq97BjaVnexQ3RGXjGz39/K1i7LFgJSEqoMgkcZCwY2ISIBV90yquy/t5na0gyfniEtl3l2XCcBN53XkvbsvYvXk4Uy7pezICIDrzknUqI00KQpuREQCzHkmlTPAMfB9JtUdl6SwatIw3rv7Ij6692KvgKiqEZfjecUs2nYEgLsvTSUttQ2JcVGMHZLMqknDmJDWBYDMnMoDJJHGRsGNiEgQjB2SzPI/XMZ9fW2seOgyt9GU8mdSJcZFkZbahnM7t3YLiADGp3WpdMTlww0HOWM3OS+5FX07xrrdS4yL4ndX9iDEYrDlRyv7s/Pr5XOKBIMSikVEgiQxLpIecSaJcZGVnknl5Czz5PxtfLH1KNbCMxXWbbebvPetY0pq3IVdfJZJiIngku4JrNh1nI83HeLBET3988FEgkwjNyIiDYRzlKay0ZjEuCjuvjQVgC+3HaGo1Oaz3Mrdx/nxZCGxkaFcPyCxwvpuPM+xY/Inmw9hmpWt1RJpPBTciIg0MoOSW5HUKor8EhvLdx7zup9lLeRfy/YAcMvgTkSGhVRY19V9OxAVFkLGiQK2/GittzaLBJKCGxGRRsYwDNdozPwtWW73PlifydBpy9h44CQA8dHeOx+X1yIilBF92wPw8aZD9dBakcBTcCMi0giNPtcxnbT0h6PkFztybzyPWgB4ccmeKvfCcU5Nffb9Yc7YAneYp0h9UXAjItII9esYS0qbaIpK7SzZcRTwfdRCdfbCubRHW+JbhJN9uoTXV+3XUQzS6Cm4ERFphAzDcI3eOKemdh3J8yoXYhhV7j4cFmKhZ/sYAKYt/IGh05bxwfpMn2Wd518pAJKGTEvBRUQaqesHdORfy/awYucx/vr5dmZ+vR/AdUZV+f1yKpNlLWTd/hzXa+chnZf1bAs4RoS6JrRg5a7jdT6lXCQQFNyIiDRSvTq0pH1sBEdzi3n9bGAzuEtr/vnzgWTmFFa4X46n/dn5eK4Ct5vwx9lb+GbvCZ+nijvPtrqsZ9taHd2QZS10BU06+kH8TcGNiEgjlWUt5Fhusdu1TZknsVgM0lLbVLse51lXnkHMqj0nKn2utqeJf7A+UyNAUq+UcyMi0kjtz873OkncbnqfBF4V51lXrkM6Dbg4Nb5az0aEep8AWllejueKruqebi5SExq5ERFppHyNuFQngdgXz+MfAIZOW+ZWtwEYHu/3+w+38K9bzyOv+Ey18nIqW9Gl6SnxFwU3IiKNlHPE5dF5W7GZZrUTiCurr/yzvup2BkDR4RbunbWJjBMFjH5ptc/6fOXldGrtu21bD52q0VSaSGUU3IiINGLVOXDT33U7/35x7EB+8u81ldbhOSqzfOdxt/vOlV3TvthJXFQ4neKjapxkrORk8aTgRkSkkfMccQlU3SXV2M3YANc0V15RKdOX7AbgoWt6MTi5NcnxUTy3eBfzvjvEw3O/B2qWZKzkZPFFCcUiIlIrzpyf8gxwu2YCP2Q5Nhd8dcU+TuSX0C2hBb+5rBtpqW1Iah3NxKt6uNVR3STjQycLmDS3cSQna/PDwNLIjYiI1EpFOT/OqawP1mfy8ebD3P/+Jp65qT+vrtwLwMPX9iYspOzf1j+e9P6FbzNN1u07QbvYSLfpJucUVGxkGJPnfe+1Wqy+k5NrMwU2a90BHvt4K6ZGlwJGwY2IiNRaZXk5g7u05seThWw4cJLfvbfZ9cypghK3OiraZ2fiB1uAsoAA8DoY1FNtV4tB1YHLB+szmTQvvUZBSpa1kMc+2uoKwuq6+WFN29xcaVpKRETqJDEuirTUNl6/XMNDLfzlhr5e5R/7yH3qyHOfHc+dc+wmPDI3nUfmegc2d1/a1W0a7O7Lutbql/wH6zMZOm0Zt722zufZWlnWQldg42xTdabAfO1FVJ3DTP3R5uZMwY2IiNSbvKIzXtd8/XIfOySZVZOG8d7dF/Gv286rdv3De7dn9aThDO/tOAfrqx+OU1qNROfysqyFVebu7Dl22uuIiuoEKRbDe5PDuowulW+z92aI6crpOUvBjYiI1BtfSccV/XJ3jgAN7tLa6xnwHtFx1pMYF8VzPx1IfItwdh7N483VGV7PZlmL2G01yLIWed2bv/lwlaMrWw6e8nquOkHKpkz35wyo015ETr43Q4S9x07Xqd6mQsGNiIjUG88pp+psNOjrmb/dcg7Tbqm4ntYtwpk0sjcAzy3eyWffHybLWkipzc4Li3dx+bMreWl7CFc8t9Jt+mZT5kmeX7LLqw0Wo2wJe25RKW+cDZjKB1gTr+peZZDy5fYjAFyQ0hqA9rGR/Oz8zpU+Ux1dE1p4BXsAM5bvZf/x/CpXZjX11VtKKBYRkXpVm40GK3qmsnp+MqgTLy/bw4GcAu6btQlw/Au+/CSV3XQkJffu0JKdR0/z18+2U1Rqp0e7GPYeP+0aDUlp04L2LSMBePmrPeTkl5DatgX/vWMIE9/fwneZJzlZ4D3lVt7R3CLXyM0/fnouI6d/zZHcItIPWRnQqVWVfVCZ1tHhRIaHUFhic3xOwzEFtnrvCYY9t9x1zVfSc3PYG0gjNyIiUu8qSjqu6TOV1XM0r4jMk+45ML6yb+wmjHn5Gx6e8z25RWfo3DqKj+8dyupJw3lx7LlEhlrYl53P/9Yd4GBOAf9dlQHAY9f1ITm+Bb8b3h2Aud/9SFGprcL2L95+FIDzklvRpU0LhvVuB8CC9CPV7oOKfP59FoUlNjrERvLuXReyetJwXvLIVfKVO9RcDi5VcCMiIk3C/ux8r6RfcBz2WZlDpwrJLSolMS6KG8/r5JreeubzHfzmnY2U2Oxc0j2BYb0cwcllPduS1CoKa2EpC9KzKqx30TZHEHN13w4AjOzv+Hvh1ixMXw2tgXfWHgDgl2ldGNo9gcS4KGKjwrzKeeYOfXfgZIUHl3pqzFNXCm5ERKRJqCh5edLI3q7rvuIcu4nbL/fxaSl0TWhB0Rk727NyATi/S2sMZ76PxeDWCxx5M++u87382lpYypq9JwC4pl97AIb1akdEqIUDJwpc9dbG1kNWNh88RViI4Za/4+vzA7RtGQHAEWsRUxfs8LpfPr/IqbEvM1dwIyIiTUJFycu/uSyV5X+4jPv62pj9mwurXL11NK+IAyfy3cr8a9ketxGMn53fmVCLwcYDJ9l5JM+rLct3HuOM3aR7uxi6tY0BoEVEKFf0cixZX1iNqamKRk7eXecYtbm2f6IrcPH1+Z3+8OFm5m48yE9mrObHU0W0jg5z64POraNd+UXO923sU1dKKBYRkSaj4h2TI+kRZ3JupzifR0aUz+Hxvcza/ViHdrGRXNWnPV9sO8JzX+7kyTH93Or4cpsj38Y5auM06pxEFm07yoL0LP5wdU/XaJCnipJ+c4tK+XjTYQB+caF3EnD5z19UauO3/9vIlh+t/GG241DS1tFhfHrfJYSGGGzIOMnDc7ZwIKeA/607wPi0FAB2Hsmr8vM3dApuRESkSanqlPSqVm/5Og7C1542Sa0dz325/ShLdhx1BSBFpTaW7zwGlOXbOA3v3Y7wEEfC8q6jp+nVoaVX+3yNnEyel85lPdvy4fqDFJba6JoQzQVd4yv9/FnWQq+T262FpYSGGCTGRTH63ChOFpTw50+28beFP3BOUhw/nizkhSU7fdabV1TaaI57UHAjIiLNTmUBUEUHgpYvn2Ut5L+r97telw9AVu/JJr/ERtuYCM5JinOru2VkGJf1TGDJjmO8umIvf7y2l1c7fI0c2U34xevr2HvcMV2WkV3AhxsOVrqE21eCtTO/yPmev7iwC59uPsyGAye56ZVvXOXCQwzO2E23dvzf/zZiNx0nvTtHky7r2bZBBjsKbkRERDxUNbpTUQBy8yurybIWA5B9upjZG70DkPgW4QDM23SIjzcf8tpnpmMFQYIzsAFHgFHVAZzVGYGyWAz+cHUvbn1trduzZ+wmH91zMQUldtq2jGDKgu189cNxt886aW46xtn6G9p+OUooFhER8aGyPXUqWpnkDGygLADx3GdmzsYfXa99nQn19poDbnWGGDDm3I5e71XV2VbV3R3a9Dp8wtGughI7aalt6N4uhrsv6eZVxgSvqbMsa2GlR10EikZuREREasjX1NXPhnTivW8PupXzTMSt6Eyo/cfzSYyLYvWebN44O9317E8HkNQq2jXSMv/7w1XmAXmqzu7Q1Rnh6drWu4wn59TZvuP5mITwyo6VQRvNUXAjIiJSC56BA8AH6w9WHiT4CCQA3l+fSXyLcO6b9R0A4y5M5ieD3c+gqioPqCJVJVhXJ8fIs4wFx8iNZ6xTfurMuYS8sqmz+qLgRkREpJY8A4eaBgmGAZjw6ZYsPt1Stttxbx+rqGpzRld1VaduzzIrdx0v91nh+gEd+WTLYbdngrWEXMGNiIiIn9QmSFi64xh/+nirW5knPt3OVX3bez1f1ShMXVSn7vJlfI1c1WbqrD4ouBEREfGjmgYJ3dq28LrfWDbN8zVyVX7zwepOnfmbghsREZEgqu6mgY3B2CHJpHVtzYcLvuJno4aRnOA9vRYIWgouIiISRNVdst1YOI+6SIyLrLpwPdHIjYiISJDVZ7JwcxTUkZsZM2YwYMAAYmNjiY2NJS0tjYULF1ZYftu2bdxyyy2kpKRgGAYvvvhi4BorIiJSjyrbNFBqJqjBTadOnZg2bRobN25kw4YNDB8+nDFjxrBt2zaf5QsKCujWrRvTpk2jQ4cOPsuIiIhI8xbUaanRo0e7vZ4yZQozZsxg7dq19OvXz6v8kCFDGDJkCACTJk0KSBtFRESkcWkwOTc2m43Zs2eTn59PWlqa3+otLi6muLjsrI/c3FwASktLKS0trXW9zmfrUodUj/o6cNTXgaX+Dhz1deDUV1/XpL6gBzfp6emkpaVRVFRETEwMH330EX379vVb/VOnTuXJJ5/0uv7ll18SHV33ZXaLFy+ucx1SPerrwFFfB5b6O3DU14Hj774uKKj4kFBPhmmalRyDVf9KSkrIzMzEarUyZ84cXn/9dVasWFFlgJOSksLEiROZOHFipeV8jdx07tyZ7OxsYmNja93u0tJSFi9ezIgRIwgLC6t1PVI19XXgqK8DS/0dOOrrwKmvvs7NzSUhIQGr1Vrl7++gj9yEh4fTvXt3AAYPHsz69euZPn06r776ql/qj4iIICIiwut6WFiYXzrdX/VI1dTXgaO+Diz1d+CorwPH331dk7oa3CZ+drvdbaRFREREpCaCOnIzefJkRo4cSXJyMnl5ecyaNYvly5ezaNEiAMaPH09SUhJTp04FHFNY27dvd3196NAhNm/eTExMjGv0R0RERJq3oAY3x44dY/z48WRlZREXF8eAAQNYtGgRI0aMACAzMxOLpWxw6fDhw5x33nmu188++yzPPvssl19+OcuXLw9080VERKQBCmpwM3PmzErvewYsKSkpBDn/WURERBq4BpdzIyIiIlIXQV8tFWjOkR/nZn61VVpaSkFBAbm5ucq8r2fq68BRXweW+jtw1NeBU1997fy9XZ0ZnGYX3OTl5QHQuXPnILdEREREaiovL4+4uLhKywR9E79As9vtHD58mJYtW2IYRq3rcW4GePDgwTptBihVU18Hjvo6sNTfgaO+Dpz66mvTNMnLy6Njx45ui418aXYjNxaLhU6dOvmtvtjYWP2PEiDq68BRXweW+jtw1NeBUx99XdWIjZMSikVERKRJUXAjIiIiTYqCm1qKiIjgL3/5i89zq8S/1NeBo74OLPV34KivA6ch9HWzSygWERGRpk0jNyIiItKkKLgRERGRJkXBjYiIiDQpCm5ERESkSVFwU0svv/wyKSkpREZGcuGFF/Ltt98Gu0mN3tSpUxkyZAgtW7akXbt23HjjjezcudOtTFFREffeey9t2rQhJiaGW265haNHjwapxU3DtGnTMAyDiRMnuq6pn/3r0KFD/OIXv6BNmzZERUVxzjnnsGHDBtd90zT585//TGJiIlFRUVx11VXs3r07iC1unGw2G48//jhdu3YlKiqK1NRUnn76abeziNTXtbNy5UpGjx5Nx44dMQyDjz/+2O1+dfo1JyeHcePGERsbS6tWrfjVr37F6dOn66fBptTY+++/b4aHh5tvvPGGuW3bNvPuu+82W7VqZR49ejTYTWvUrrnmGvO///2vuXXrVnPz5s3mqFGjzOTkZPP06dOuMr/97W/Nzp07m0uXLjU3bNhgXnTRRebFF18cxFY3bt9++62ZkpJiDhgwwHzggQdc19XP/pOTk2N26dLFvP32281169aZ+/btMxctWmTu2bPHVWbatGlmXFyc+fHHH5tbtmwxb7jhBrNr165mYWFhEFve+EyZMsVs06aN+dlnn5n79+83Z8+ebcbExJjTp093lVFf186CBQvMxx57zJw3b54JmB999JHb/er067XXXmuee+655tq1a82vv/7a7N69u3nrrbfWS3sV3NTCBRdcYN57772u1zabzezYsaM5derUILaq6Tl27JgJmCtWrDBN0zRPnTplhoWFmbNnz3aV2bFjhwmYa9asCVYzG628vDyzR48e5uLFi83LL7/cFdyon/3rkUceMS+55JIK79vtdrNDhw7mP/7xD9e1U6dOmREREeZ7770XiCY2Gdddd5155513ul27+eabzXHjxpmmqb72F8/gpjr9un37dhMw169f7yqzcOFC0zAM89ChQ35vo6alaqikpISNGzdy1VVXua5ZLBauuuoq1qxZE8SWNT1WqxWA+Ph4ADZu3Ehpaalb3/fu3Zvk5GT1fS3ce++9XHfddW79Cepnf/v00085//zz+elPf0q7du0477zzeO2111z39+/fz5EjR9z6Oy4ujgsvvFD9XUMXX3wxS5cuZdeuXQBs2bKFVatWMXLkSEB9XV+q069r1qyhVatWnH/++a4yV111FRaLhXXr1vm9Tc3u4My6ys7Oxmaz0b59e7fr7du354cffghSq5oeu93OxIkTGTp0KP379wfgyJEjhIeH06pVK7ey7du358iRI0FoZeP1/vvv891337F+/Xqve+pn/9q3bx8zZszg97//PY8++ijr16/n/vvvJzw8nAkTJrj61NfPFPV3zUyaNInc3Fx69+5NSEgINpuNKVOmMG7cOAD1dT2pTr8eOXKEdu3aud0PDQ0lPj6+XvpewY00SPfeey9bt25l1apVwW5Kk3Pw4EEeeOABFi9eTGRkZLCb0+TZ7XbOP/98nnnmGQDOO+88tm7dyr///W8mTJgQ5NY1LR9++CHvvvsus2bNol+/fmzevJmJEyfSsWNH9XUzo2mpGkpISCAkJMRr5cjRo0fp0KFDkFrVtNx333189tlnfPXVV3Tq1Ml1vUOHDpSUlHDq1Cm38ur7mtm4cSPHjh1j0KBBhIaGEhoayooVK/jnP/9JaGgo7du3Vz/7UWJiIn379nW71qdPHzIzMwFcfaqfKXX3xz/+kUmTJvHzn/+cc845h1/+8pc8+OCDTJ06FVBf15fq9GuHDh04duyY2/0zZ86Qk5NTL32v4KaGwsPDGTx4MEuXLnVds9vtLF26lLS0tCC2rPEzTZP77ruPjz76iGXLltG1a1e3+4MHDyYsLMyt73fu3ElmZqb6vgauvPJK0tPT2bx5s+vP+eefz7hx41xfq5/9Z+jQoV5bGuzatYsuXboA0LVrVzp06ODW37m5uaxbt079XUMFBQVYLO6/1kJCQrDb7YD6ur5Up1/T0tI4deoUGzdudJVZtmwZdrudCy+80P+N8nuKcjPw/vvvmxEREeabb75pbt++3fz1r39ttmrVyjxy5Eiwm9ao/d///Z8ZFxdnLl++3MzKynL9KSgocJX57W9/ayYnJ5vLli0zN2zYYKalpZlpaWlBbHXTUH61lGmqn/3p22+/NUNDQ80pU6aYu3fvNt99910zOjra/N///ucqM23aNLNVq1bmJ598Yn7//ffmmDFjtDy5FiZMmGAmJSW5loLPmzfPTEhIMB9++GFXGfV17eTl5ZmbNm0yN23aZALm888/b27atMk8cOCAaZrV69drr73WPO+888x169aZq1atMnv06KGl4A3Nv/71LzM5OdkMDw83L7jgAnPt2rXBblKjB/j889///tdVprCw0LznnnvM1q1bm9HR0eZNN91kZmVlBa/RTYRncKN+9q/58+eb/fv3NyMiIszevXub//nPf9zu2+128/HHHzfbt29vRkREmFdeeaW5c+fOILW28crNzTUfeOABMzk52YyMjDS7detmPvbYY2ZxcbGrjPq6dr766iufP58nTJhgmmb1+vXEiRPmrbfeasbExJixsbHmHXfcYebl5dVLew3TLLd1o4iIiEgjp5wbERERaVIU3IiIiEiTouBGREREmhQFNyIiItKkKLgRERGRJkXBjYiIiDQpCm5ERESkSVFwIyIiIk2KghsRafaWL1+OYRheh4WKSOOk4EZERESaFAU3IiIi0qQouBGRoLPb7UydOpWuXbsSFRXFueeey5w5c4CyKaPPP/+cAQMGEBkZyUUXXcTWrVvd6pg7dy79+vUjIiKClJQUnnvuObf7xcXFPPLII3Tu3JmIiAi6d+/OzJkz3cps3LiR888/n+joaC6++GJ27txZvx9cROqFghsRCbqpU6fy9ttv8+9//5tt27bx4IMP8otf/IIVK1a4yvzxj3/kueeeY/369bRt25bRo0dTWloKOIKSn/3sZ/z85z8nPT2dJ554gscff5w333zT9fz48eN57733+Oc//8mOHTt49dVXiYmJcWvHY489xnPPPceGDRsIDQ3lzjvvDMjnFxH/0qngIhJUxcXFxMfHs2TJEtLS0lzX77rrLgoKCvj1r3/NsGHDeP/99xk7diwAOTk5dOrUiTfffJOf/exnjBs3juPHj/Pll1+6nn/44Yf5/PPP2bZtG7t27aJXr14sXryYq666yqsNy5cvZ9iwYSxZsoQrr7wSgAULFnDddddRWFhIZGRkPfeCiPiTRm5EJKj27NlDQUEBI0aMICYmxvXn7bffZu/eva5y5QOf+Ph4evXqxY4dOwDYsWMHQ4cOdat36NCh7N69G5vNxubNmwkJCeHyyy+vtC0DBgxwfZ2YmAjAsWPH6vwZRSSwQoPdABFp3k6fPg3A559/TlJSktu9iIgItwCntqKioqpVLiwszPW1YRiAIx9IRBoXjdyISFD17duXiIgIMjMz6d69u9ufzp07u8qtXbvW9fXJkyfZtWsXffr0AaBPnz6sXr3ard7Vq1fTs2dPQkJCOOecc7Db7W45PCLSdGnkRkSCqmXLljz00EM8+OCD2O12LrnkEqxWK6tXryY2NpYuXboA8NRTT9GmTRvat2/PY489RkJCAjfeeCMAf/jDHxgyZAhPP/00Y8eOZc2aNbz00ku88sorAKSkpDBhwgTuvPNO/vnPf3Luuedy4MABjh07xs9+9rNgfXQRqScKbkQk6J5++mnatm3L1KlT2bdvH61atWLQoEE8+uijrmmhadOm8cADD7B7924GDhzI/PnzCQ8PB2DQoEF8+OGH/PnPf+bpp58mMTGRp556ittvv931HjNmzODRRx/lnnvu4cSJEyQnJ/Poo48G4+OKSD3TaikRadCcK5lOnjxJq1atgt0cEWkElHMjIiIiTYqCGxEREWlSNC0lIiIiTYpGbkRERKRJUXAjIiIiTYqCGxEREWlSFNyIiIhIk6LgRkRERJoUBTciIiLSpCi4ERERkSZFwY2IiIg0Kf8PNVwH+NuNuI4AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "xs = np.arange(1, len(history['train loss']) + 1)\n",
    "ax.plot(xs, history['train loss'], '.-', label='train')\n",
    "ax.plot(xs, history['val loss'], '.-', label='val')\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_ylabel('loss')\n",
    "ax.legend()\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f01662-dae9-4fe7-aab5-0270c9937d85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0c8c38-c5d5-449d-8a7c-a3953ed07b5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
