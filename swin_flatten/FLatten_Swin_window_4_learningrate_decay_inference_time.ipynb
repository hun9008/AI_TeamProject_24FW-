{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bEMpr27c0RCi",
    "outputId": "d86b304b-23be-4240-f2b5-1c77b82e8d9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu112\n",
      "Requirement already satisfied: torch in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.4.1+cu118)\n",
      "Requirement already satisfied: torchvision in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.19.1+cu118)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.4.1+cu118)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: timm in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.0.9)\n",
      "Requirement already satisfied: pytorch-ignite in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.5.1)\n",
      "Requirement already satisfied: einops in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.8.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: torch in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from timm) (2.4.1+cu118)\n",
      "Requirement already satisfied: torchvision in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from timm) (0.19.1+cu118)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from timm) (6.0.2)\n",
      "Requirement already satisfied: huggingface_hub in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from timm) (0.25.0)\n",
      "Requirement already satisfied: safetensors in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from timm) (0.4.5)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pytorch-ignite) (24.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->timm) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->timm) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->timm) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->timm) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->timm) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->timm) (2024.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->timm) (75.1.0)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface_hub->timm) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface_hub->timm) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub->timm) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch->timm) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface_hub->timm) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface_hub->timm) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface_hub->timm) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface_hub->timm) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->torch->timm) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ignite\\handlers\\checkpoint.py:16: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.\n",
      "  from torch.distributed.optim import ZeroRedundancyOptimizer\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------\n",
    "# Swin Transformer\n",
    "# Copyright (c) 2021 Microsoft\n",
    "# Licensed under The MIT License [see LICENSE for details]\n",
    "# Written by Ze Liu\n",
    "# --------------------------------------------------------\n",
    "\n",
    "\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu112\n",
    "!pip3 install timm pytorch-ignite einops matplotlib\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn as nn\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "\n",
    "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
    "from einops import rearrange\n",
    "\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
    "import ignite.metrics\n",
    "import ignite.contrib.handlers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "cxGT3QPy0q0H"
   },
   "outputs": [],
   "source": [
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "wncXpbHk09rc"
   },
   "outputs": [],
   "source": [
    "def window_partition(x, window_size):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x: (B, H, W, C)\n",
    "        window_size (int): window size\n",
    "\n",
    "    Returns:\n",
    "        windows: (num_windows*B, window_size, window_size, C)\n",
    "    \"\"\"\n",
    "    B, H, W, C = x.shape\n",
    "    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n",
    "    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n",
    "    return windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "uvsOSiH-1DdL"
   },
   "outputs": [],
   "source": [
    "def window_reverse(windows, window_size, H, W):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        windows: (num_windows*B, window_size, window_size, C)\n",
    "        window_size (int): Window size\n",
    "        H (int): Height of image\n",
    "        W (int): Width of image\n",
    "\n",
    "    Returns:\n",
    "        x: (B, H, W, C)\n",
    "    \"\"\"\n",
    "    B = int(windows.shape[0] / (H * W / window_size / window_size))\n",
    "    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n",
    "    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Fp57DIJ41EBj"
   },
   "outputs": [],
   "source": [
    "class WindowAttention(nn.Module):\n",
    "    r\"\"\" Window based multi-head self attention (W-MSA) module with relative position bias.\n",
    "    It supports both of shifted and non-shifted window.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        window_size (tuple[int]): The height and width of the window.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n",
    "        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n",
    "        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.window_size = window_size  # Wh, Ww\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        # define a parameter table of relative position bias\n",
    "        self.relative_position_bias_table = nn.Parameter(\n",
    "            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n",
    "\n",
    "        # get pair-wise relative position index for each token inside the window\n",
    "        coords_h = torch.arange(self.window_size[0])\n",
    "        coords_w = torch.arange(self.window_size[1])\n",
    "        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n",
    "        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n",
    "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n",
    "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
    "        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n",
    "        relative_coords[:, :, 1] += self.window_size[1] - 1\n",
    "        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n",
    "        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n",
    "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "        trunc_normal_(self.relative_position_bias_table, std=.02)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input features with shape of (num_windows*B, N, C)\n",
    "            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None\n",
    "        \"\"\"\n",
    "        B_, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "        q = q * self.scale\n",
    "        attn = (q @ k.transpose(-2, -1))\n",
    "\n",
    "        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n",
    "            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)  # Wh*Ww,Wh*Ww,nH\n",
    "        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n",
    "        attn = attn + relative_position_bias.unsqueeze(0)\n",
    "\n",
    "        if mask is not None:\n",
    "            nW = mask.shape[0]\n",
    "            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n",
    "            attn = attn.view(-1, self.num_heads, N, N)\n",
    "            attn = self.softmax(attn)\n",
    "        else:\n",
    "            attn = self.softmax(attn)\n",
    "\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f'dim={self.dim}, window_size={self.window_size}, num_heads={self.num_heads}'\n",
    "\n",
    "    def flops(self, N):\n",
    "        # calculate flops for 1 window with token length of N\n",
    "        flops = 0\n",
    "        # qkv = self.qkv(x)\n",
    "        flops += N * self.dim * 3 * self.dim\n",
    "        # attn = (q @ k.transpose(-2, -1))\n",
    "        flops += self.num_heads * N * (self.dim // self.num_heads) * N\n",
    "        #  x = (attn @ v)\n",
    "        flops += self.num_heads * N * N * (self.dim // self.num_heads)\n",
    "        # x = self.proj(x)\n",
    "        flops += N * self.dim * self.dim\n",
    "        return flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "e_UlHriB1I6l"
   },
   "outputs": [],
   "source": [
    "class FocusedLinearAttention(nn.Module):\n",
    "    r\"\"\" Window based multi-head self attention (W-MSA) module with relative position bias.\n",
    "    It supports both of shifted and non-shifted window.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        window_size (tuple[int]): The height and width of the window.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n",
    "        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n",
    "        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.,\n",
    "                 focusing_factor=3, kernel_size=5):\n",
    "\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.window_size = window_size  # Wh, Ww\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "\n",
    "        self.focusing_factor = focusing_factor  # Used to sharpen attention distribution\n",
    "\n",
    "    # Linear layer to project input to query, key, and value\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)  # Output projection\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "        # Depth-wise convolution for capturing local spatial information\n",
    "        self.dwc = nn.Conv2d(in_channels=head_dim, out_channels=head_dim, kernel_size=kernel_size,\n",
    "                            groups=head_dim, padding=kernel_size // 2)\n",
    "\n",
    "        # Learnable scale parameter\n",
    "        self.scale = nn.Parameter(torch.zeros(size=(1, 1, dim)))\n",
    "\n",
    "        # Learnable positional encoding\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(size=(1, window_size[0] * window_size[1], dim)))\n",
    "\n",
    "        print('Linear Attention window{} f{} kernel{}'.\n",
    "              format(window_size, focusing_factor, kernel_size))\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        B, N, C = x.shape\n",
    "\n",
    "        # Project input to query, key, and value\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, C).permute(2, 0, 1, 3)\n",
    "        q, k, v = qkv.unbind(0)\n",
    "\n",
    "        # Add positional encoding to keys\n",
    "        k = k + self.positional_encoding\n",
    "\n",
    "        focusing_factor = self.focusing_factor\n",
    "        kernel_function = nn.ReLU()\n",
    "\n",
    "        # Apply ReLU and add small epsilon to avoid zero values\n",
    "        q = kernel_function(q) + 1e-6\n",
    "        k = kernel_function(k) + 1e-6\n",
    "\n",
    "        # Compute scale using Softplus for stability\n",
    "        scale = nn.Softplus()(self.scale)\n",
    "        q = q / scale\n",
    "        k = k / scale\n",
    "\n",
    "        # Store original norms\n",
    "        q_norm = q.norm(dim=-1, keepdim=True)\n",
    "        k_norm = k.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # Apply focusing factor\n",
    "        q = q ** focusing_factor\n",
    "        k = k ** focusing_factor\n",
    "\n",
    "        # Renormalize to original norms\n",
    "        q = (q / q.norm(dim=-1, keepdim=True)) * q_norm\n",
    "        k = (k / k.norm(dim=-1, keepdim=True)) * k_norm\n",
    "\n",
    "        # Reshape for multi-head attention\n",
    "        q = q.reshape(B, N, self.num_heads, -1).permute(0, 2, 1, 3)\n",
    "        k = k.reshape(B, N, self.num_heads, -1).permute(0, 2, 1, 3)\n",
    "        v = v.reshape(B, N, self.num_heads, -1).permute(0, 2, 1, 3)\n",
    "\n",
    "        # Compute linear attention\n",
    "        z = 1 / (q @ k.mean(dim=-2, keepdim=True).transpose(-2, -1) + 1e-6)\n",
    "        kv = (k.transpose(-2, -1) * (N ** -0.5)) @ (v * (N ** -0.5))\n",
    "        x = q @ kv * z\n",
    "\n",
    "        # Reshape output\n",
    "        H = W = int(N ** 0.5)\n",
    "        x = x.transpose(1, 2).reshape(B, N, C)\n",
    "\n",
    "        # Apply depth-wise convolution to capture local spatial information\n",
    "        v = v.reshape(B * self.num_heads, H, W, -1).permute(0, 3, 1, 2)\n",
    "        x = x + self.dwc(v).reshape(B, C, N).permute(0, 2, 1)\n",
    "\n",
    "        # Final projection and dropout\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "    def eval(self):\n",
    "        super(FocusedLinearAttention, self).eval()\n",
    "        print('eval')\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f'dim={self.dim}, window_size={self.window_size}, num_heads={self.num_heads}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "DIR8e1tu1O5O"
   },
   "outputs": [],
   "source": [
    "class SwinTransformerBlock(nn.Module):\n",
    "    r\"\"\" Swin Transformer Block.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        input_resolution (tuple[int]): Input resulotion.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        window_size (int): Window size.\n",
    "        shift_size (int): Shift size for SW-MSA.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
    "        drop (float, optional): Dropout rate. Default: 0.0\n",
    "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
    "        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n",
    "        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU\n",
    "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, input_resolution, num_heads, window_size=7, shift_size=0,\n",
    "                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.,\n",
    "                 act_layer=nn.GELU, norm_layer=nn.LayerNorm,\n",
    "                 focusing_factor=3, kernel_size=5, attn_type='L'):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.input_resolution = input_resolution\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        if min(self.input_resolution) <= self.window_size:\n",
    "            # if window size is larger than input resolution, we don't partition windows\n",
    "            self.shift_size = 0\n",
    "            self.window_size = min(self.input_resolution)\n",
    "        assert 0 <= self.shift_size < self.window_size, \"shift_size must in 0-window_size\"\n",
    "\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        assert attn_type in ['L', 'S']\n",
    "        if attn_type == 'L':\n",
    "            self.attn = FocusedLinearAttention(\n",
    "                dim, window_size=to_2tuple(self.window_size), num_heads=num_heads,\n",
    "                qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop,\n",
    "                focusing_factor=focusing_factor, kernel_size=kernel_size)\n",
    "        else:\n",
    "            self.attn = WindowAttention(\n",
    "                dim, window_size=to_2tuple(self.window_size), num_heads=num_heads,\n",
    "                qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "        if self.shift_size > 0:\n",
    "            # calculate attention mask for SW-MSA\n",
    "            H, W = self.input_resolution\n",
    "            img_mask = torch.zeros((1, H, W, 1))  # 1 H W 1\n",
    "            h_slices = (slice(0, -self.window_size),\n",
    "                        slice(-self.window_size, -self.shift_size),\n",
    "                        slice(-self.shift_size, None))\n",
    "            w_slices = (slice(0, -self.window_size),\n",
    "                        slice(-self.window_size, -self.shift_size),\n",
    "                        slice(-self.shift_size, None))\n",
    "            cnt = 0\n",
    "            for h in h_slices:\n",
    "                for w in w_slices:\n",
    "                    img_mask[:, h, w, :] = cnt\n",
    "                    cnt += 1\n",
    "\n",
    "            mask_windows = window_partition(img_mask, self.window_size)  # nW, window_size, window_size, 1\n",
    "            mask_windows = mask_windows.view(-1, self.window_size * self.window_size)\n",
    "            attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n",
    "            attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n",
    "        else:\n",
    "            attn_mask = None\n",
    "\n",
    "        self.register_buffer(\"attn_mask\", attn_mask)\n",
    "\n",
    "    def forward(self, x):\n",
    "        H, W = self.input_resolution\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = x.view(B, H, W, C)\n",
    "\n",
    "        # cyclic shift\n",
    "        if self.shift_size > 0:\n",
    "            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n",
    "        else:\n",
    "            shifted_x = x\n",
    "\n",
    "        # partition windows\n",
    "        x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, window_size, C\n",
    "        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)  # nW*B, window_size*window_size, C\n",
    "\n",
    "        # W-MSA/SW-MSA\n",
    "        attn_windows = self.attn(x_windows, mask=self.attn_mask)  # nW*B, window_size*window_size, C\n",
    "\n",
    "        # merge windows\n",
    "        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)\n",
    "        shifted_x = window_reverse(attn_windows, self.window_size, H, W)  # B H' W' C\n",
    "\n",
    "        # reverse cyclic shift\n",
    "        if self.shift_size > 0:\n",
    "            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n",
    "        else:\n",
    "            x = shifted_x\n",
    "        x = x.view(B, H * W, C)\n",
    "\n",
    "        # FFN\n",
    "        x = shortcut + self.drop_path(x)\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"dim={self.dim}, input_resolution={self.input_resolution}, num_heads={self.num_heads}, \" \\\n",
    "               f\"window_size={self.window_size}, shift_size={self.shift_size}, mlp_ratio={self.mlp_ratio}\"\n",
    "\n",
    "    def flops(self):\n",
    "        flops = 0\n",
    "        H, W = self.input_resolution\n",
    "        # norm1\n",
    "        flops += self.dim * H * W\n",
    "        # W-MSA/SW-MSA\n",
    "        nW = H * W / self.window_size / self.window_size\n",
    "        flops += nW * self.attn.flops(self.window_size * self.window_size)\n",
    "        # mlp\n",
    "        flops += 2 * H * W * self.dim * self.dim * self.mlp_ratio\n",
    "        # norm2\n",
    "        flops += self.dim * H * W\n",
    "        return flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "blyYs6kZ1TeT"
   },
   "outputs": [],
   "source": [
    "class PatchMerging(nn.Module):\n",
    "    r\"\"\" Patch Merging Layer.\n",
    "\n",
    "    Args:\n",
    "        input_resolution (tuple[int]): Resolution of input feature.\n",
    "        dim (int): Number of input channels.\n",
    "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_resolution, dim, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.input_resolution = input_resolution\n",
    "        self.dim = dim\n",
    "        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n",
    "        self.norm = norm_layer(4 * dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: B, H*W, C\n",
    "        \"\"\"\n",
    "        H, W = self.input_resolution\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "        assert H % 2 == 0 and W % 2 == 0, f\"x size ({H}*{W}) are not even.\"\n",
    "\n",
    "        x = x.view(B, H, W, C)\n",
    "\n",
    "        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C\n",
    "        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C\n",
    "        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C\n",
    "        x = x.view(B, -1, 4 * C)  # B H/2*W/2 4*C\n",
    "\n",
    "        x = self.norm(x)\n",
    "        x = self.reduction(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"input_resolution={self.input_resolution}, dim={self.dim}\"\n",
    "\n",
    "    def flops(self):\n",
    "        H, W = self.input_resolution\n",
    "        flops = H * W * self.dim\n",
    "        flops += (H // 2) * (W // 2) * 4 * self.dim * 2 * self.dim\n",
    "        return flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "XNJN4Z_01WBE"
   },
   "outputs": [],
   "source": [
    "class BasicLayer(nn.Module):\n",
    "    \"\"\" A basic Swin Transformer layer for one stage.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        input_resolution (tuple[int]): Input resolution.\n",
    "        depth (int): Number of blocks.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        window_size (int): Local window size.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
    "        drop (float, optional): Dropout rate. Default: 0.0\n",
    "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
    "        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n",
    "        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n",
    "        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n",
    "        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, input_resolution, depth, num_heads, window_size,\n",
    "                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., norm_layer=nn.LayerNorm, downsample=None, use_checkpoint=False,\n",
    "                 focusing_factor=3, kernel_size=5, attn_type='L'):\n",
    "\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.input_resolution = input_resolution\n",
    "        self.depth = depth\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "\n",
    "        # build blocks\n",
    "        attn_types = [(attn_type if attn_type[0] != 'M' else ('L' if i < int(attn_type[1:]) else 'S')) for i in range(depth)]\n",
    "        window_sizes = [(window_size if attn_types[i] == 'L' else (7 if window_size <= 56 else 12)) for i in range(depth)]\n",
    "        self.blocks = nn.ModuleList([\n",
    "            SwinTransformerBlock(dim=dim, input_resolution=input_resolution,\n",
    "                                 num_heads=num_heads, window_size=window_sizes[i],\n",
    "                                 shift_size=0 if (i % 2 == 0) else window_sizes[i] // 2,\n",
    "                                 mlp_ratio=mlp_ratio,\n",
    "                                 qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                                 drop=drop, attn_drop=attn_drop,\n",
    "                                 drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n",
    "                                 norm_layer=norm_layer,\n",
    "                                 focusing_factor=focusing_factor,\n",
    "                                 kernel_size=kernel_size,\n",
    "                                 attn_type=attn_types[i])\n",
    "            for i in range(depth)])\n",
    "\n",
    "        # patch merging layer\n",
    "        if downsample is not None:\n",
    "            self.downsample = downsample(input_resolution, dim=dim, norm_layer=norm_layer)\n",
    "        else:\n",
    "            self.downsample = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        for blk in self.blocks:\n",
    "            if self.use_checkpoint:\n",
    "                x = checkpoint.checkpoint(blk, x)\n",
    "            else:\n",
    "                x = blk(x)\n",
    "        if self.downsample is not None:\n",
    "            x = self.downsample(x)\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"dim={self.dim}, input_resolution={self.input_resolution}, depth={self.depth}\"\n",
    "\n",
    "    def flops(self):\n",
    "        flops = 0\n",
    "        for blk in self.blocks:\n",
    "            flops += blk.flops()\n",
    "        if self.downsample is not None:\n",
    "            flops += self.downsample.flops()\n",
    "        return flops\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    r\"\"\" Image to Patch Embedding\n",
    "\n",
    "    Args:\n",
    "        img_size (int): Image size.  Default: 224.\n",
    "        patch_size (int): Patch token size. Default: 4.\n",
    "        in_chans (int): Number of input image channels. Default: 3.\n",
    "        embed_dim (int): Number of linear projection output channels. Default: 96.\n",
    "        norm_layer (nn.Module, optional): Normalization layer. Default: None\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):\n",
    "        super().__init__()\n",
    "        img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.patches_resolution = patches_resolution\n",
    "        self.num_patches = patches_resolution[0] * patches_resolution[1]\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        if norm_layer is not None:\n",
    "            self.norm = norm_layer(embed_dim)\n",
    "        else:\n",
    "            self.norm = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        # FIXME look at relaxing size constraints\n",
    "        assert H == self.img_size[0] and W == self.img_size[1], \\\n",
    "            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)  # B Ph*Pw C\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "    def flops(self):\n",
    "        Ho, Wo = self.patches_resolution\n",
    "        flops = Ho * Wo * self.embed_dim * self.in_chans * (self.patch_size[0] * self.patch_size[1])\n",
    "        if self.norm is not None:\n",
    "            flops += Ho * Wo * self.embed_dim\n",
    "        return flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "LuascylH1a-6"
   },
   "outputs": [],
   "source": [
    "class FLattenSwinTransformer(nn.Module):\n",
    "    r\"\"\" Swin Transformer\n",
    "        A PyTorch impl of : `Swin Transformer: Hierarchical Vision Transformer using Shifted Windows`  -\n",
    "          https://arxiv.org/pdf/2103.14030\n",
    "\n",
    "    Args:\n",
    "        img_size (int | tuple(int)): Input image size. Default 224\n",
    "        patch_size (int | tuple(int)): Patch size. Default: 4\n",
    "        in_chans (int): Number of input image channels. Default: 3\n",
    "        num_classes (int): Number of classes for classification head. Default: 1000\n",
    "        embed_dim (int): Patch embedding dimension. Default: 96\n",
    "        depths (tuple(int)): Depth of each Swin Transformer layer.\n",
    "        num_heads (tuple(int)): Number of attention heads in different layers.\n",
    "        window_size (int): Window size. Default: 7\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4\n",
    "        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float): Override default qk scale of head_dim ** -0.5 if set. Default: None\n",
    "        drop_rate (float): Dropout rate. Default: 0\n",
    "        attn_drop_rate (float): Attention dropout rate. Default: 0\n",
    "        drop_path_rate (float): Stochastic depth rate. Default: 0.1\n",
    "        norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.\n",
    "        ape (bool): If True, add absolute position embedding to the patch embedding. Default: False\n",
    "        patch_norm (bool): If True, add normalization after patch embedding. Default: True\n",
    "        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size=32, patch_size=4, in_chans=3, num_classes=1000,\n",
    "                 embed_dim=96, depths=[2, 2, 6, 2], num_heads=[3, 6, 12, 24],\n",
    "                 window_size=7, mlp_ratio=4., qkv_bias=True, qk_scale=None,\n",
    "                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,\n",
    "                 norm_layer=nn.LayerNorm, ape=False, patch_norm=True,\n",
    "                 use_checkpoint=False,\n",
    "                 focusing_factor=3, kernel_size=5, attn_type='LLLL', **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.num_layers = len(depths)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.ape = ape\n",
    "        self.patch_norm = patch_norm\n",
    "        self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "\n",
    "        # split image into non-overlapping patches\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim,\n",
    "            norm_layer=norm_layer if self.patch_norm else None)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        patches_resolution = self.patch_embed.patches_resolution\n",
    "        self.patches_resolution = patches_resolution\n",
    "\n",
    "        # absolute position embedding\n",
    "        if self.ape:\n",
    "            self.absolute_pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n",
    "            trunc_normal_(self.absolute_pos_embed, std=.02)\n",
    "\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        # stochastic depth\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n",
    "\n",
    "        # build layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i_layer in range(self.num_layers):\n",
    "            layer = BasicLayer(dim=int(embed_dim * 2 ** i_layer),\n",
    "                               input_resolution=(patches_resolution[0] // (2 ** i_layer),\n",
    "                                                 patches_resolution[1] // (2 ** i_layer)),\n",
    "                               depth=depths[i_layer],\n",
    "                               num_heads=num_heads[i_layer],\n",
    "                               window_size=window_size,\n",
    "                               mlp_ratio=self.mlp_ratio,\n",
    "                               qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                               drop=drop_rate, attn_drop=attn_drop_rate,\n",
    "                               drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],\n",
    "                               norm_layer=norm_layer,\n",
    "                               downsample=PatchMerging if (i_layer < self.num_layers - 1) else None,\n",
    "                               use_checkpoint=use_checkpoint,\n",
    "                               focusing_factor=focusing_factor,\n",
    "                               kernel_size=kernel_size,\n",
    "                               attn_type=attn_type[i_layer] + (attn_type[self.num_layers:] if attn_type[i_layer] == 'M' else ''))\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        self.norm = norm_layer(self.num_features)\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'absolute_pos_embed'}\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay_keywords(self):\n",
    "        return {'relative_position_bias_table'}\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        if self.ape:\n",
    "            x = x + self.absolute_pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        x = self.norm(x)  # B L C\n",
    "        x = self.avgpool(x.transpose(1, 2))  # B C 1\n",
    "        x = torch.flatten(x, 1)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "    def flops(self):\n",
    "        flops = 0\n",
    "        flops += self.patch_embed.flops()\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            flops += layer.flops()\n",
    "        flops += self.num_features * self.patches_resolution[0] * self.patches_resolution[1] // (2 ** self.num_layers)\n",
    "        flops += self.num_features * self.num_classes\n",
    "        return flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "CH3oe5YA1en5"
   },
   "outputs": [],
   "source": [
    "DATA_DIR='./data'\n",
    "\n",
    "IMAGE_SIZE = 32\n",
    "\n",
    "NUM_CLASSES = 10\n",
    "NUM_WORKERS = 8\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CIG2Ai-N1rHK",
    "outputId": "4d641b02-cebf-4dd2-da65-46ad48144db5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 11154889500136310049\n",
      "xla_global_id: -1\n",
      "]\n",
      "2.4.1+cu118\n",
      "11.8\n",
      "True\n",
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)  # This will show the version of PyTorch\n",
    "print(torch.version.cuda)  # This will show the version of CUDA PyTorch is linked against\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c73u1cDN2wC4",
    "outputId": "ef5015da-5064-4dd4-d2fa-79d4631d2d7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.PILToTensor(),\n",
    "    transforms.ConvertImageDtype(torch.float)\n",
    "])\n",
    "\n",
    "train_dset = datasets.CIFAR10(root=DATA_DIR, train=True, download=True, transform=train_transform)\n",
    "test_dset = datasets.CIFAR10(root=DATA_DIR, train=False, download=True, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "KKpPWqWt2xoz"
   },
   "outputs": [],
   "source": [
    "def dataset_show_image(dset, idx):\n",
    "    X, Y = dset[idx]\n",
    "    title = \"Ground truth: {}\".format(dset.classes[Y])\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(np.moveaxis(X.numpy(), 0, -1))\n",
    "    ax.set_title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "id": "nL_w38GL3LeA",
    "outputId": "d173c5f6-0fe9-42f0-bd1f-98849ac90963"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_show_image(test_dset, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "IGs1A1fy3O3h"
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                                           num_workers=NUM_WORKERS, pin_memory=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                                          num_workers=NUM_WORKERS, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zWEj9CuI3Rjj",
    "outputId": "34daaffb-66fe-47a1-af42-101d1b24671e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Linear Attention window(4, 4) f3 kernel5\n",
      "Linear Attention window(4, 4) f3 kernel5\n",
      "Linear Attention window(4, 4) f3 kernel5\n",
      "Linear Attention window(4, 4) f3 kernel5\n",
      "Linear Attention window(2, 2) f3 kernel5\n",
      "Linear Attention window(2, 2) f3 kernel5\n",
      "Linear Attention window(2, 2) f3 kernel5\n",
      "Linear Attention window(2, 2) f3 kernel5\n",
      "Linear Attention window(2, 2) f3 kernel5\n",
      "Linear Attention window(2, 2) f3 kernel5\n",
      "Linear Attention window(1, 1) f3 kernel5\n",
      "Linear Attention window(1, 1) f3 kernel5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FLattenSwinTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
       "    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.1, inplace=False)\n",
       "  (layers): ModuleList(\n",
       "    (0): BasicLayer(\n",
       "      dim=96, input_resolution=(8, 8), depth=2\n",
       "      (blocks): ModuleList(\n",
       "        (0): SwinTransformerBlock(\n",
       "          dim=96, input_resolution=(8, 8), num_heads=3, window_size=4, shift_size=0, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): FocusedLinearAttention(\n",
       "            dim=96, window_size=(4, 4), num_heads=3\n",
       "            (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "            (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "            (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dwc): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=32)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          dim=96, input_resolution=(8, 8), num_heads=3, window_size=4, shift_size=2, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): FocusedLinearAttention(\n",
       "            dim=96, window_size=(4, 4), num_heads=3\n",
       "            (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "            (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "            (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dwc): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=32)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.009)\n",
       "          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (downsample): PatchMerging(\n",
       "        input_resolution=(8, 8), dim=96\n",
       "        (reduction): Linear(in_features=384, out_features=192, bias=False)\n",
       "        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicLayer(\n",
       "      dim=192, input_resolution=(4, 4), depth=2\n",
       "      (blocks): ModuleList(\n",
       "        (0): SwinTransformerBlock(\n",
       "          dim=192, input_resolution=(4, 4), num_heads=6, window_size=4, shift_size=0, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): FocusedLinearAttention(\n",
       "            dim=192, window_size=(4, 4), num_heads=6\n",
       "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "            (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dwc): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=32)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.018)\n",
       "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          dim=192, input_resolution=(4, 4), num_heads=6, window_size=4, shift_size=0, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): FocusedLinearAttention(\n",
       "            dim=192, window_size=(4, 4), num_heads=6\n",
       "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "            (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dwc): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=32)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.027)\n",
       "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (downsample): PatchMerging(\n",
       "        input_resolution=(4, 4), dim=192\n",
       "        (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (2): BasicLayer(\n",
       "      dim=384, input_resolution=(2, 2), depth=6\n",
       "      (blocks): ModuleList(\n",
       "        (0): SwinTransformerBlock(\n",
       "          dim=384, input_resolution=(2, 2), num_heads=12, window_size=2, shift_size=0, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): FocusedLinearAttention(\n",
       "            dim=384, window_size=(2, 2), num_heads=12\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dwc): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=32)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.036)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          dim=384, input_resolution=(2, 2), num_heads=12, window_size=2, shift_size=0, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): FocusedLinearAttention(\n",
       "            dim=384, window_size=(2, 2), num_heads=12\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dwc): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=32)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.045)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): SwinTransformerBlock(\n",
       "          dim=384, input_resolution=(2, 2), num_heads=12, window_size=2, shift_size=0, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): FocusedLinearAttention(\n",
       "            dim=384, window_size=(2, 2), num_heads=12\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dwc): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=32)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.055)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): SwinTransformerBlock(\n",
       "          dim=384, input_resolution=(2, 2), num_heads=12, window_size=2, shift_size=0, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): FocusedLinearAttention(\n",
       "            dim=384, window_size=(2, 2), num_heads=12\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dwc): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=32)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.064)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): SwinTransformerBlock(\n",
       "          dim=384, input_resolution=(2, 2), num_heads=12, window_size=2, shift_size=0, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): FocusedLinearAttention(\n",
       "            dim=384, window_size=(2, 2), num_heads=12\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dwc): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=32)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.073)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): SwinTransformerBlock(\n",
       "          dim=384, input_resolution=(2, 2), num_heads=12, window_size=2, shift_size=0, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): FocusedLinearAttention(\n",
       "            dim=384, window_size=(2, 2), num_heads=12\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dwc): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=32)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.082)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (downsample): PatchMerging(\n",
       "        input_resolution=(2, 2), dim=384\n",
       "        (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
       "        (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (3): BasicLayer(\n",
       "      dim=768, input_resolution=(1, 1), depth=2\n",
       "      (blocks): ModuleList(\n",
       "        (0): SwinTransformerBlock(\n",
       "          dim=768, input_resolution=(1, 1), num_heads=24, window_size=1, shift_size=0, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): FocusedLinearAttention(\n",
       "            dim=768, window_size=(1, 1), num_heads=24\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dwc): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=32)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.091)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          dim=768, input_resolution=(1, 1), num_heads=24, window_size=1, shift_size=0, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): FocusedLinearAttention(\n",
       "            dim=768, window_size=(1, 1), num_heads=24\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dwc): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=32)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.100)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (avgpool): AdaptiveAvgPool1d(output_size=1)\n",
       "  (head): Linear(in_features=768, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Data loading\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(32),  # Swin Transformer expects 224x224 input\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "# Model instantiation\n",
    "model = FLattenSwinTransformer(\n",
    "    img_size=32,\n",
    "    patch_size=4,\n",
    "    in_chans=3,\n",
    "    num_classes=10,  # CIFAR-10 has 10 classes\n",
    "    embed_dim=96,\n",
    "    depths=[2, 2, 6, 2],\n",
    "    num_heads=[3, 6, 12, 24],\n",
    "    window_size=4,\n",
    "    mlp_ratio=4.,\n",
    "    qkv_bias=True,\n",
    "    drop_rate=0.1,\n",
    "    attn_drop_rate=0.1,\n",
    "    drop_path_rate=0.1,\n",
    "    ape=False,\n",
    "    patch_norm=True,\n",
    "    use_checkpoint=False,\n",
    "    focusing_factor=3,\n",
    "    kernel_size=5,\n",
    "    attn_type='LLLL'\n",
    ")\n",
    "\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rqpdKvMZ6RWm",
    "outputId": "21ff8c31-b326-4f89-f11a-e2633d8435fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 27,538,090\n"
     ]
    }
   ],
   "source": [
    "# Optimizer and loss function\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Initialize params\n",
    "best_val_loss = 2.0\n",
    "train_losses = []\n",
    "val_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "4PmtkvvX7z53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Training Loss: 1.5630, Accuracy: 43.48%\n",
      "Test Loss: 1.3653, Test Accuracy: 51.23%\n",
      "Validation Loss: 1.3653, Best Validation Loss: 2.0000\n",
      "Validation loss improved from 2.0000 to 1.3653\n",
      "Epoch [2/50], Training Loss: 1.2610, Accuracy: 54.25%\n",
      "Test Loss: 1.2133, Test Accuracy: 56.55%\n",
      "Validation Loss: 1.2133, Best Validation Loss: 1.3653\n",
      "Validation loss improved from 1.3653 to 1.2133\n",
      "Epoch [3/50], Training Loss: 1.1024, Accuracy: 60.31%\n",
      "Test Loss: 1.1053, Test Accuracy: 59.72%\n",
      "Validation Loss: 1.1053, Best Validation Loss: 1.2133\n",
      "Validation loss improved from 1.2133 to 1.1053\n",
      "Epoch [4/50], Training Loss: 0.9670, Accuracy: 65.29%\n",
      "Test Loss: 1.0612, Test Accuracy: 62.32%\n",
      "Validation Loss: 1.0612, Best Validation Loss: 1.1053\n",
      "Validation loss improved from 1.1053 to 1.0612\n",
      "Epoch [5/50], Training Loss: 0.8445, Accuracy: 69.87%\n",
      "Test Loss: 1.0176, Test Accuracy: 64.59%\n",
      "Validation Loss: 1.0176, Best Validation Loss: 1.0612\n",
      "Validation loss improved from 1.0612 to 1.0176\n",
      "Epoch [6/50], Training Loss: 0.7294, Accuracy: 73.85%\n",
      "Test Loss: 1.0218, Test Accuracy: 64.98%\n",
      "Validation Loss: 1.0218, Best Validation Loss: 1.0176\n",
      "Validation loss did not improve. Patience counter: 1\n",
      "Epoch [7/50], Training Loss: 0.6218, Accuracy: 77.80%\n",
      "Test Loss: 1.0447, Test Accuracy: 64.70%\n",
      "Validation Loss: 1.0447, Best Validation Loss: 1.0176\n",
      "Validation loss did not improve. Patience counter: 2\n",
      "Learning rate decayed\n",
      "Epoch [8/50], Training Loss: 0.5330, Accuracy: 80.95%\n",
      "Test Loss: 1.0371, Test Accuracy: 66.07%\n",
      "Validation Loss: 1.0371, Best Validation Loss: 1.0176\n",
      "Validation loss did not improve. Patience counter: 1\n",
      "Epoch [9/50], Training Loss: 0.4399, Accuracy: 84.02%\n",
      "Test Loss: 1.1840, Test Accuracy: 64.45%\n",
      "Validation Loss: 1.1840, Best Validation Loss: 1.0176\n",
      "Validation loss did not improve. Patience counter: 2\n",
      "Learning rate decayed\n",
      "Epoch [10/50], Training Loss: 0.3717, Accuracy: 87.03%\n",
      "Test Loss: 1.2347, Test Accuracy: 65.90%\n",
      "Validation Loss: 1.2347, Best Validation Loss: 1.0176\n",
      "Validation loss did not improve. Patience counter: 1\n",
      "Epoch [11/50], Training Loss: 0.3108, Accuracy: 88.94%\n",
      "Test Loss: 1.2777, Test Accuracy: 66.01%\n",
      "Validation Loss: 1.2777, Best Validation Loss: 1.0176\n",
      "Validation loss did not improve. Patience counter: 2\n",
      "Learning rate decayed\n",
      "Epoch [12/50], Training Loss: 0.2641, Accuracy: 90.54%\n",
      "Test Loss: 1.2699, Test Accuracy: 66.31%\n",
      "Validation Loss: 1.2699, Best Validation Loss: 1.0176\n",
      "Validation loss did not improve. Patience counter: 1\n",
      "Epoch [13/50], Training Loss: 0.2293, Accuracy: 91.90%\n",
      "Test Loss: 1.3437, Test Accuracy: 66.02%\n",
      "Validation Loss: 1.3437, Best Validation Loss: 1.0176\n",
      "Validation loss did not improve. Patience counter: 2\n",
      "Learning rate decayed\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[0;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[1;32m---> 15\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     18\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(15):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        #_, predicted = outputs.max(1)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    train_accuracy = 100. * correct / total\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)  # Store average training loss\n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}], Training Loss: {avg_train_loss:.4f}, \"\n",
    "          f\"Accuracy: {train_accuracy:.2f}%\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            #_, predicted = outputs.max(1)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    avg_val_loss = test_loss / len(test_loader)\n",
    "    val_losses.append(avg_val_loss)  # Store average validation loss\n",
    "    test_accuracy = 100. * correct / total\n",
    "    print(f\"Test Loss: {avg_val_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n",
    "\n",
    "        # Early Stopping with Learning Rate Decay\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}, Best Validation Loss: {best_val_loss:.4f}\")\n",
    "    \n",
    "    if avg_val_loss < best_val_loss:\n",
    "        print(f\"Validation loss improved from {best_val_loss:.4f} to {avg_val_loss:.4f}\")\n",
    "        best_val_loss = avg_val_loss\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"Validation loss did not improve. Patience counter: {patience_counter}\")\n",
    "        if patience_counter >= 2:\n",
    "            print(\"Learning rate decayed\")\n",
    "            # Reduce the learning rate\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] * 0.1  # Halve the learning rate\n",
    "            patience_counter = 0  # Reset the counter after decaying the LR\n",
    "    \n",
    "    # Update learning rate using scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "print(\"Training finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Training Loss', color='blue', marker='o')\n",
    "plt.plot(val_losses, label='Validation Loss', color='orange', marker='o')\n",
    "plt.title('Training and Validation Loss per Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total inference time: 3.7579 seconds\n",
      "Average inference time per batch: 0.0120 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Ensure model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Initialize a variable to store the total inference time\n",
    "total_inference_time = 0\n",
    "\n",
    "# Disable gradient calculation for inference\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "\n",
    "        # Record the start time\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Perform inference (forward pass)\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Record the end time\n",
    "        end_time = time.time()\n",
    "\n",
    "        # Calculate the inference time for this batch\n",
    "        inference_time = end_time - start_time\n",
    "\n",
    "        # Accumulate total inference time\n",
    "        total_inference_time += inference_time\n",
    "\n",
    "# Calculate average inference time per batch\n",
    "average_inference_time = total_inference_time / len(test_loader)\n",
    "\n",
    "print(f\"Total inference time: {total_inference_time:.4f} seconds\")\n",
    "print(f\"Average inference time per batch: {average_inference_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
