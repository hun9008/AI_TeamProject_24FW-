{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bEMpr27c0RCi",
    "outputId": "d86b304b-23be-4240-f2b5-1c77b82e8d9c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ignite\\handlers\\checkpoint.py:16: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.\n",
      "  from torch.distributed.optim import ZeroRedundancyOptimizer\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------\n",
    "# Swin Transformer\n",
    "# Copyright (c) 2021 Microsoft\n",
    "# Licensed under The MIT License [see LICENSE for details]\n",
    "# Written by Ze Liu\n",
    "# --------------------------------------------------------\n",
    "\n",
    "# install these if needed\n",
    "#!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu112\n",
    "#!pip3 install timm pytorch-ignite einops matplotlib\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn as nn\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "\n",
    "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
    "from einops import rearrange\n",
    "\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
    "import ignite.metrics\n",
    "import ignite.contrib.handlers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "cxGT3QPy0q0H"
   },
   "outputs": [],
   "source": [
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "wncXpbHk09rc"
   },
   "outputs": [],
   "source": [
    "def window_partition(x, window_size):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x: (B, H, W, C)\n",
    "        window_size (int): window size\n",
    "\n",
    "    Returns:\n",
    "        windows: (num_windows*B, window_size, window_size, C)\n",
    "    \"\"\"\n",
    "    B, H, W, C = x.shape\n",
    "    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n",
    "    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n",
    "    return windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "uvsOSiH-1DdL"
   },
   "outputs": [],
   "source": [
    "def window_reverse(windows, window_size, H, W):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        windows: (num_windows*B, window_size, window_size, C)\n",
    "        window_size (int): Window size\n",
    "        H (int): Height of image\n",
    "        W (int): Width of image\n",
    "\n",
    "    Returns:\n",
    "        x: (B, H, W, C)\n",
    "    \"\"\"\n",
    "    B = int(windows.shape[0] / (H * W / window_size / window_size))\n",
    "    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n",
    "    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Fp57DIJ41EBj"
   },
   "outputs": [],
   "source": [
    "class WindowAttention(nn.Module):\n",
    "    r\"\"\" Window based multi-head self attention (W-MSA) module with relative position bias.\n",
    "    It supports both of shifted and non-shifted window.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        window_size (tuple[int]): The height and width of the window.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n",
    "        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n",
    "        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.window_size = window_size  # Wh, Ww\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        # define a parameter table of relative position bias\n",
    "        self.relative_position_bias_table = nn.Parameter(\n",
    "            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n",
    "\n",
    "        # get pair-wise relative position index for each token inside the window\n",
    "        coords_h = torch.arange(self.window_size[0])\n",
    "        coords_w = torch.arange(self.window_size[1])\n",
    "        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n",
    "        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n",
    "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n",
    "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
    "        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n",
    "        relative_coords[:, :, 1] += self.window_size[1] - 1\n",
    "        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n",
    "        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n",
    "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "        trunc_normal_(self.relative_position_bias_table, std=.02)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input features with shape of (num_windows*B, N, C)\n",
    "            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None\n",
    "        \"\"\"\n",
    "        B_, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "        q = q * self.scale\n",
    "        attn = (q @ k.transpose(-2, -1))\n",
    "\n",
    "        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n",
    "            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)  # Wh*Ww,Wh*Ww,nH\n",
    "        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n",
    "        attn = attn + relative_position_bias.unsqueeze(0)\n",
    "\n",
    "        if mask is not None:\n",
    "            nW = mask.shape[0]\n",
    "            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n",
    "            attn = attn.view(-1, self.num_heads, N, N)\n",
    "            attn = self.softmax(attn)\n",
    "        else:\n",
    "            attn = self.softmax(attn)\n",
    "\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f'dim={self.dim}, window_size={self.window_size}, num_heads={self.num_heads}'\n",
    "\n",
    "    def flops(self, N):\n",
    "        # calculate flops for 1 window with token length of N\n",
    "        flops = 0\n",
    "        # qkv = self.qkv(x)\n",
    "        flops += N * self.dim * 3 * self.dim\n",
    "        # attn = (q @ k.transpose(-2, -1))\n",
    "        flops += self.num_heads * N * (self.dim // self.num_heads) * N\n",
    "        #  x = (attn @ v)\n",
    "        flops += self.num_heads * N * N * (self.dim // self.num_heads)\n",
    "        # x = self.proj(x)\n",
    "        flops += N * self.dim * self.dim\n",
    "        return flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "e_UlHriB1I6l"
   },
   "outputs": [],
   "source": [
    "class FocusedLinearAttention(nn.Module):\n",
    "    r\"\"\" Window based multi-head self attention (W-MSA) module with relative position bias.\n",
    "    It supports both of shifted and non-shifted window.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        window_size (tuple[int]): The height and width of the window.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n",
    "        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n",
    "        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.,\n",
    "                 focusing_factor=3, kernel_size=5):\n",
    "\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.window_size = window_size  # Wh, Ww\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "\n",
    "        self.focusing_factor = focusing_factor  # Used to sharpen attention distribution\n",
    "\n",
    "    # Linear layer to project input to query, key, and value\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)  # Output projection\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "        # Depth-wise convolution for capturing local spatial information\n",
    "        self.dwc = nn.Conv2d(in_channels=head_dim, out_channels=head_dim, kernel_size=kernel_size,\n",
    "                            groups=head_dim, padding=kernel_size // 2)\n",
    "\n",
    "        # Learnable scale parameter\n",
    "        self.scale = nn.Parameter(torch.zeros(size=(1, 1, dim)))\n",
    "\n",
    "        # Learnable positional encoding\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(size=(1, window_size[0] * window_size[1], dim)))\n",
    "\n",
    "        print('Linear Attention window{} f{} kernel{}'.\n",
    "              format(window_size, focusing_factor, kernel_size))\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        B, N, C = x.shape\n",
    "\n",
    "        # Project input to query, key, and value\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, C).permute(2, 0, 1, 3)\n",
    "        q, k, v = qkv.unbind(0)\n",
    "\n",
    "        # Add positional encoding to keys\n",
    "        k = k + self.positional_encoding\n",
    "\n",
    "        focusing_factor = self.focusing_factor\n",
    "        kernel_function = nn.ReLU()\n",
    "\n",
    "        # Apply ReLU and add small epsilon to avoid zero values\n",
    "        q = kernel_function(q) + 1e-6\n",
    "        k = kernel_function(k) + 1e-6\n",
    "\n",
    "        # Compute scale using Softplus for stability\n",
    "        scale = nn.Softplus()(self.scale)\n",
    "        q = q / scale\n",
    "        k = k / scale\n",
    "\n",
    "        # Store original norms\n",
    "        q_norm = q.norm(dim=-1, keepdim=True)\n",
    "        k_norm = k.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # Apply focusing factor\n",
    "        q = q ** focusing_factor\n",
    "        k = k ** focusing_factor\n",
    "\n",
    "        # Renormalize to original norms\n",
    "        q = (q / q.norm(dim=-1, keepdim=True)) * q_norm\n",
    "        k = (k / k.norm(dim=-1, keepdim=True)) * k_norm\n",
    "\n",
    "        # Reshape for multi-head attention\n",
    "        q = q.reshape(B, N, self.num_heads, -1).permute(0, 2, 1, 3)\n",
    "        k = k.reshape(B, N, self.num_heads, -1).permute(0, 2, 1, 3)\n",
    "        v = v.reshape(B, N, self.num_heads, -1).permute(0, 2, 1, 3)\n",
    "\n",
    "        # Compute linear attention\n",
    "        z = 1 / (q @ k.mean(dim=-2, keepdim=True).transpose(-2, -1) + 1e-6)\n",
    "        kv = (k.transpose(-2, -1) * (N ** -0.5)) @ (v * (N ** -0.5))\n",
    "        x = q @ kv * z\n",
    "\n",
    "        # Reshape output\n",
    "        H = W = int(N ** 0.5)\n",
    "        x = x.transpose(1, 2).reshape(B, N, C)\n",
    "\n",
    "        # Apply depth-wise convolution to capture local spatial information\n",
    "        v = v.reshape(B * self.num_heads, H, W, -1).permute(0, 3, 1, 2)\n",
    "        x = x + self.dwc(v).reshape(B, C, N).permute(0, 2, 1)\n",
    "\n",
    "        # Final projection and dropout\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "    def eval(self):\n",
    "        super(FocusedLinearAttention, self).eval()\n",
    "        print('eval')\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f'dim={self.dim}, window_size={self.window_size}, num_heads={self.num_heads}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "DIR8e1tu1O5O"
   },
   "outputs": [],
   "source": [
    "class SwinTransformerBlock(nn.Module):\n",
    "    r\"\"\" Swin Transformer Block.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        input_resolution (tuple[int]): Input resulotion.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        window_size (int): Window size.\n",
    "        shift_size (int): Shift size for SW-MSA.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
    "        drop (float, optional): Dropout rate. Default: 0.0\n",
    "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
    "        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n",
    "        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU\n",
    "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, input_resolution, num_heads, window_size=7, shift_size=0,\n",
    "                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.,\n",
    "                 act_layer=nn.GELU, norm_layer=nn.LayerNorm,\n",
    "                 focusing_factor=3, kernel_size=5, attn_type='L'):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.input_resolution = input_resolution\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        if min(self.input_resolution) <= self.window_size:\n",
    "            # if window size is larger than input resolution, we don't partition windows\n",
    "            self.shift_size = 0\n",
    "            self.window_size = min(self.input_resolution)\n",
    "        assert 0 <= self.shift_size < self.window_size, \"shift_size must in 0-window_size\"\n",
    "\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        assert attn_type in ['L', 'S']\n",
    "        if attn_type == 'L':\n",
    "            self.attn = FocusedLinearAttention(\n",
    "                dim, window_size=to_2tuple(self.window_size), num_heads=num_heads,\n",
    "                qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop,\n",
    "                focusing_factor=focusing_factor, kernel_size=kernel_size)\n",
    "        else:\n",
    "            self.attn = WindowAttention(\n",
    "                dim, window_size=to_2tuple(self.window_size), num_heads=num_heads,\n",
    "                qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "        if self.shift_size > 0:\n",
    "            # calculate attention mask for SW-MSA\n",
    "            H, W = self.input_resolution\n",
    "            img_mask = torch.zeros((1, H, W, 1))  # 1 H W 1\n",
    "            h_slices = (slice(0, -self.window_size),\n",
    "                        slice(-self.window_size, -self.shift_size),\n",
    "                        slice(-self.shift_size, None))\n",
    "            w_slices = (slice(0, -self.window_size),\n",
    "                        slice(-self.window_size, -self.shift_size),\n",
    "                        slice(-self.shift_size, None))\n",
    "            cnt = 0\n",
    "            for h in h_slices:\n",
    "                for w in w_slices:\n",
    "                    img_mask[:, h, w, :] = cnt\n",
    "                    cnt += 1\n",
    "\n",
    "            mask_windows = window_partition(img_mask, self.window_size)  # nW, window_size, window_size, 1\n",
    "            mask_windows = mask_windows.view(-1, self.window_size * self.window_size)\n",
    "            attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n",
    "            attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n",
    "        else:\n",
    "            attn_mask = None\n",
    "\n",
    "        self.register_buffer(\"attn_mask\", attn_mask)\n",
    "\n",
    "    def forward(self, x):\n",
    "        H, W = self.input_resolution\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = x.view(B, H, W, C)\n",
    "\n",
    "        # cyclic shift\n",
    "        if self.shift_size > 0:\n",
    "            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n",
    "        else:\n",
    "            shifted_x = x\n",
    "\n",
    "        # partition windows\n",
    "        x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, window_size, C\n",
    "        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)  # nW*B, window_size*window_size, C\n",
    "\n",
    "        # W-MSA/SW-MSA\n",
    "        attn_windows = self.attn(x_windows, mask=self.attn_mask)  # nW*B, window_size*window_size, C\n",
    "\n",
    "        # merge windows\n",
    "        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)\n",
    "        shifted_x = window_reverse(attn_windows, self.window_size, H, W)  # B H' W' C\n",
    "\n",
    "        # reverse cyclic shift\n",
    "        if self.shift_size > 0:\n",
    "            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n",
    "        else:\n",
    "            x = shifted_x\n",
    "        x = x.view(B, H * W, C)\n",
    "\n",
    "        # FFN\n",
    "        x = shortcut + self.drop_path(x)\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"dim={self.dim}, input_resolution={self.input_resolution}, num_heads={self.num_heads}, \" \\\n",
    "               f\"window_size={self.window_size}, shift_size={self.shift_size}, mlp_ratio={self.mlp_ratio}\"\n",
    "\n",
    "    def flops(self):\n",
    "        flops = 0\n",
    "        H, W = self.input_resolution\n",
    "        # norm1\n",
    "        flops += self.dim * H * W\n",
    "        # W-MSA/SW-MSA\n",
    "        nW = H * W / self.window_size / self.window_size\n",
    "        flops += nW * self.attn.flops(self.window_size * self.window_size)\n",
    "        # mlp\n",
    "        flops += 2 * H * W * self.dim * self.dim * self.mlp_ratio\n",
    "        # norm2\n",
    "        flops += self.dim * H * W\n",
    "        return flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "blyYs6kZ1TeT"
   },
   "outputs": [],
   "source": [
    "class PatchMerging(nn.Module):\n",
    "    r\"\"\" Patch Merging Layer.\n",
    "\n",
    "    Args:\n",
    "        input_resolution (tuple[int]): Resolution of input feature.\n",
    "        dim (int): Number of input channels.\n",
    "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_resolution, dim, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.input_resolution = input_resolution\n",
    "        self.dim = dim\n",
    "        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n",
    "        self.norm = norm_layer(4 * dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: B, H*W, C\n",
    "        \"\"\"\n",
    "        H, W = self.input_resolution\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "        assert H % 2 == 0 and W % 2 == 0, f\"x size ({H}*{W}) are not even.\"\n",
    "\n",
    "        x = x.view(B, H, W, C)\n",
    "\n",
    "        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C\n",
    "        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C\n",
    "        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C\n",
    "        x = x.view(B, -1, 4 * C)  # B H/2*W/2 4*C\n",
    "\n",
    "        x = self.norm(x)\n",
    "        x = self.reduction(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"input_resolution={self.input_resolution}, dim={self.dim}\"\n",
    "\n",
    "    def flops(self):\n",
    "        H, W = self.input_resolution\n",
    "        flops = H * W * self.dim\n",
    "        flops += (H // 2) * (W // 2) * 4 * self.dim * 2 * self.dim\n",
    "        return flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "XNJN4Z_01WBE"
   },
   "outputs": [],
   "source": [
    "class BasicLayer(nn.Module):\n",
    "    \"\"\" A basic Swin Transformer layer for one stage.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        input_resolution (tuple[int]): Input resolution.\n",
    "        depth (int): Number of blocks.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        window_size (int): Local window size.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
    "        drop (float, optional): Dropout rate. Default: 0.0\n",
    "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
    "        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n",
    "        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n",
    "        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n",
    "        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, input_resolution, depth, num_heads, window_size,\n",
    "                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., norm_layer=nn.LayerNorm, downsample=None, use_checkpoint=False,\n",
    "                 focusing_factor=3, kernel_size=5, attn_type='L'):\n",
    "\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.input_resolution = input_resolution\n",
    "        self.depth = depth\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "\n",
    "        # build blocks\n",
    "        attn_types = [(attn_type if attn_type[0] != 'M' else ('L' if i < int(attn_type[1:]) else 'S')) for i in range(depth)]\n",
    "        window_sizes = [(window_size if attn_types[i] == 'L' else (7 if window_size <= 56 else 12)) for i in range(depth)]\n",
    "        self.blocks = nn.ModuleList([\n",
    "            SwinTransformerBlock(dim=dim, input_resolution=input_resolution,\n",
    "                                 num_heads=num_heads, window_size=window_sizes[i],\n",
    "                                 shift_size=0 if (i % 2 == 0) else window_sizes[i] // 2,\n",
    "                                 mlp_ratio=mlp_ratio,\n",
    "                                 qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                                 drop=drop, attn_drop=attn_drop,\n",
    "                                 drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n",
    "                                 norm_layer=norm_layer,\n",
    "                                 focusing_factor=focusing_factor,\n",
    "                                 kernel_size=kernel_size,\n",
    "                                 attn_type=attn_types[i])\n",
    "            for i in range(depth)])\n",
    "\n",
    "        # patch merging layer\n",
    "        if downsample is not None:\n",
    "            self.downsample = downsample(input_resolution, dim=dim, norm_layer=norm_layer)\n",
    "        else:\n",
    "            self.downsample = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        for blk in self.blocks:\n",
    "            if self.use_checkpoint:\n",
    "                x = checkpoint.checkpoint(blk, x)\n",
    "            else:\n",
    "                x = blk(x)\n",
    "        if self.downsample is not None:\n",
    "            x = self.downsample(x)\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"dim={self.dim}, input_resolution={self.input_resolution}, depth={self.depth}\"\n",
    "\n",
    "    def flops(self):\n",
    "        flops = 0\n",
    "        for blk in self.blocks:\n",
    "            flops += blk.flops()\n",
    "        if self.downsample is not None:\n",
    "            flops += self.downsample.flops()\n",
    "        return flops\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    r\"\"\" Image to Patch Embedding\n",
    "\n",
    "    Args:\n",
    "        img_size (int): Image size.  Default: 224.\n",
    "        patch_size (int): Patch token size. Default: 4.\n",
    "        in_chans (int): Number of input image channels. Default: 3.\n",
    "        embed_dim (int): Number of linear projection output channels. Default: 96.\n",
    "        norm_layer (nn.Module, optional): Normalization layer. Default: None\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):\n",
    "        super().__init__()\n",
    "        img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.patches_resolution = patches_resolution\n",
    "        self.num_patches = patches_resolution[0] * patches_resolution[1]\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        if norm_layer is not None:\n",
    "            self.norm = norm_layer(embed_dim)\n",
    "        else:\n",
    "            self.norm = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        # FIXME look at relaxing size constraints\n",
    "        assert H == self.img_size[0] and W == self.img_size[1], \\\n",
    "            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)  # B Ph*Pw C\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "    def flops(self):\n",
    "        Ho, Wo = self.patches_resolution\n",
    "        flops = Ho * Wo * self.embed_dim * self.in_chans * (self.patch_size[0] * self.patch_size[1])\n",
    "        if self.norm is not None:\n",
    "            flops += Ho * Wo * self.embed_dim\n",
    "        return flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "LuascylH1a-6"
   },
   "outputs": [],
   "source": [
    "class FLattenSwinTransformer(nn.Module):\n",
    "    r\"\"\" Swin Transformer\n",
    "        A PyTorch impl of : `Swin Transformer: Hierarchical Vision Transformer using Shifted Windows`  -\n",
    "          https://arxiv.org/pdf/2103.14030\n",
    "\n",
    "    Args:\n",
    "        img_size (int | tuple(int)): Input image size. Default 224\n",
    "        patch_size (int | tuple(int)): Patch size. Default: 4\n",
    "        in_chans (int): Number of input image channels. Default: 3\n",
    "        num_classes (int): Number of classes for classification head. Default: 1000\n",
    "        embed_dim (int): Patch embedding dimension. Default: 96\n",
    "        depths (tuple(int)): Depth of each Swin Transformer layer.\n",
    "        num_heads (tuple(int)): Number of attention heads in different layers.\n",
    "        window_size (int): Window size. Default: 7\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4\n",
    "        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float): Override default qk scale of head_dim ** -0.5 if set. Default: None\n",
    "        drop_rate (float): Dropout rate. Default: 0\n",
    "        attn_drop_rate (float): Attention dropout rate. Default: 0\n",
    "        drop_path_rate (float): Stochastic depth rate. Default: 0.1\n",
    "        norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.\n",
    "        ape (bool): If True, add absolute position embedding to the patch embedding. Default: False\n",
    "        patch_norm (bool): If True, add normalization after patch embedding. Default: True\n",
    "        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size=32, patch_size=4, in_chans=3, num_classes=1000,\n",
    "                 embed_dim=96, depths=[2, 2, 6, 2], num_heads=[3, 6, 12, 24],\n",
    "                 window_size=7, mlp_ratio=4., qkv_bias=True, qk_scale=None,\n",
    "                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,\n",
    "                 norm_layer=nn.LayerNorm, ape=False, patch_norm=True,\n",
    "                 use_checkpoint=False,\n",
    "                 focusing_factor=3, kernel_size=5, attn_type='LLLL', **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.num_layers = len(depths)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.ape = ape\n",
    "        self.patch_norm = patch_norm\n",
    "        self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "\n",
    "        # split image into non-overlapping patches\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim,\n",
    "            norm_layer=norm_layer if self.patch_norm else None)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        patches_resolution = self.patch_embed.patches_resolution\n",
    "        self.patches_resolution = patches_resolution\n",
    "\n",
    "        # absolute position embedding\n",
    "        if self.ape:\n",
    "            self.absolute_pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n",
    "            trunc_normal_(self.absolute_pos_embed, std=.02)\n",
    "\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        # stochastic depth\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n",
    "\n",
    "        # build layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i_layer in range(self.num_layers):\n",
    "            layer = BasicLayer(dim=int(embed_dim * 2 ** i_layer),\n",
    "                               input_resolution=(patches_resolution[0] // (2 ** i_layer),\n",
    "                                                 patches_resolution[1] // (2 ** i_layer)),\n",
    "                               depth=depths[i_layer],\n",
    "                               num_heads=num_heads[i_layer],\n",
    "                               window_size=window_size,\n",
    "                               mlp_ratio=self.mlp_ratio,\n",
    "                               qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                               drop=drop_rate, attn_drop=attn_drop_rate,\n",
    "                               drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],\n",
    "                               norm_layer=norm_layer,\n",
    "                               downsample=PatchMerging if (i_layer < self.num_layers - 1) else None,\n",
    "                               use_checkpoint=use_checkpoint,\n",
    "                               focusing_factor=focusing_factor,\n",
    "                               kernel_size=kernel_size,\n",
    "                               attn_type=attn_type[i_layer] + (attn_type[self.num_layers:] if attn_type[i_layer] == 'M' else ''))\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        self.norm = norm_layer(self.num_features)\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'absolute_pos_embed'}\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay_keywords(self):\n",
    "        return {'relative_position_bias_table'}\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        if self.ape:\n",
    "            x = x + self.absolute_pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        x = self.norm(x)  # B L C\n",
    "        x = self.avgpool(x.transpose(1, 2))  # B C 1\n",
    "        x = torch.flatten(x, 1)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "    def flops(self):\n",
    "        flops = 0\n",
    "        flops += self.patch_embed.flops()\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            flops += layer.flops()\n",
    "        flops += self.num_features * self.patches_resolution[0] * self.patches_resolution[1] // (2 ** self.num_layers)\n",
    "        flops += self.num_features * self.num_classes\n",
    "        return flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "CH3oe5YA1en5"
   },
   "outputs": [],
   "source": [
    "DATA_DIR='./data'\n",
    "\n",
    "IMAGE_SIZE = 32\n",
    "\n",
    "NUM_CLASSES = 10\n",
    "NUM_WORKERS = 8\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100\n",
    "\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CIG2Ai-N1rHK",
    "outputId": "4d641b02-cebf-4dd2-da65-46ad48144db5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 9945000647723539534\n",
      "xla_global_id: -1\n",
      "]\n",
      "2.4.1+cu118\n",
      "11.8\n",
      "True\n",
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)  # This will show the version of PyTorch\n",
    "print(torch.version.cuda)  # This will show the version of CUDA PyTorch is linked against\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zWEj9CuI3Rjj",
    "outputId": "34daaffb-66fe-47a1-af42-101d1b24671e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Data loading\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(32),  # Swin Transformer expects 224x224 input\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Attention window(4, 4) f2 kernel5\n",
      "Linear Attention window(4, 4) f2 kernel5\n",
      "Linear Attention window(4, 4) f2 kernel5\n",
      "Linear Attention window(4, 4) f2 kernel5\n",
      "Linear Attention window(2, 2) f2 kernel5\n",
      "Linear Attention window(2, 2) f2 kernel5\n",
      "Linear Attention window(2, 2) f2 kernel5\n",
      "Linear Attention window(2, 2) f2 kernel5\n",
      "Linear Attention window(2, 2) f2 kernel5\n",
      "Linear Attention window(2, 2) f2 kernel5\n",
      "Linear Attention window(1, 1) f2 kernel5\n",
      "Linear Attention window(1, 1) f2 kernel5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FLattenSwinTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
       "    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.1, inplace=False)\n",
       "  (layers): ModuleList(\n",
       "    (0): BasicLayer(\n",
       "      dim=96, input_resolution=(8, 8), depth=2\n",
       "      (blocks): ModuleList(\n",
       "        (0): SwinTransformerBlock(\n",
       "          dim=96, input_resolution=(8, 8), num_heads=3, window_size=4, shift_size=0, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): FocusedLinearAttention(\n",
       "            dim=96, window_size=(4, 4), num_heads=3\n",
       "            (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "            (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "            (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dwc): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=32)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          dim=96, input_resolution=(8, 8), num_heads=3, window_size=4, shift_size=2, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): FocusedLinearAttention(\n",
       "            dim=96, window_size=(4, 4), num_heads=3\n",
       "            (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "            (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "            (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dwc): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=32)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.009)\n",
       "          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (downsample): PatchMerging(\n",
       "        input_resolution=(8, 8), dim=96\n",
       "        (reduction): Linear(in_features=384, out_features=192, bias=False)\n",
       "        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicLayer(\n",
       "      dim=192, input_resolution=(4, 4), depth=2\n",
       "      (blocks): ModuleList(\n",
       "        (0): SwinTransformerBlock(\n",
       "          dim=192, input_resolution=(4, 4), num_heads=6, window_size=4, shift_size=0, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): FocusedLinearAttention(\n",
       "            dim=192, window_size=(4, 4), num_heads=6\n",
       "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "            (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dwc): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=32)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.018)\n",
       "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          dim=192, input_resolution=(4, 4), num_heads=6, window_size=4, shift_size=0, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): FocusedLinearAttention(\n",
       "            dim=192, window_size=(4, 4), num_heads=6\n",
       "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "            (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dwc): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=32)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.027)\n",
       "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (downsample): PatchMerging(\n",
       "        input_resolution=(4, 4), dim=192\n",
       "        (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (2): BasicLayer(\n",
       "      dim=384, input_resolution=(2, 2), depth=6\n",
       "      (blocks): ModuleList(\n",
       "        (0): SwinTransformerBlock(\n",
       "          dim=384, input_resolution=(2, 2), num_heads=12, window_size=2, shift_size=0, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): FocusedLinearAttention(\n",
       "            dim=384, window_size=(2, 2), num_heads=12\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dwc): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=32)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.036)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          dim=384, input_resolution=(2, 2), num_heads=12, window_size=2, shift_size=0, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): FocusedLinearAttention(\n",
       "            dim=384, window_size=(2, 2), num_heads=12\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dwc): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=32)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.045)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): SwinTransformerBlock(\n",
       "          dim=384, input_resolution=(2, 2), num_heads=12, window_size=2, shift_size=0, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): FocusedLinearAttention(\n",
       "            dim=384, window_size=(2, 2), num_heads=12\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dwc): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=32)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.055)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): SwinTransformerBlock(\n",
       "          dim=384, input_resolution=(2, 2), num_heads=12, window_size=2, shift_size=0, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): FocusedLinearAttention(\n",
       "            dim=384, window_size=(2, 2), num_heads=12\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dwc): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=32)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.064)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): SwinTransformerBlock(\n",
       "          dim=384, input_resolution=(2, 2), num_heads=12, window_size=2, shift_size=0, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): FocusedLinearAttention(\n",
       "            dim=384, window_size=(2, 2), num_heads=12\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dwc): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=32)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.073)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): SwinTransformerBlock(\n",
       "          dim=384, input_resolution=(2, 2), num_heads=12, window_size=2, shift_size=0, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): FocusedLinearAttention(\n",
       "            dim=384, window_size=(2, 2), num_heads=12\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dwc): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=32)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.082)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (downsample): PatchMerging(\n",
       "        input_resolution=(2, 2), dim=384\n",
       "        (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
       "        (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (3): BasicLayer(\n",
       "      dim=768, input_resolution=(1, 1), depth=2\n",
       "      (blocks): ModuleList(\n",
       "        (0): SwinTransformerBlock(\n",
       "          dim=768, input_resolution=(1, 1), num_heads=24, window_size=1, shift_size=0, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): FocusedLinearAttention(\n",
       "            dim=768, window_size=(1, 1), num_heads=24\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dwc): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=32)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.091)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          dim=768, input_resolution=(1, 1), num_heads=24, window_size=1, shift_size=0, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): FocusedLinearAttention(\n",
       "            dim=768, window_size=(1, 1), num_heads=24\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dwc): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=32)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.100)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (avgpool): AdaptiveAvgPool1d(output_size=1)\n",
       "  (head): Linear(in_features=768, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model instantiation\n",
    "model = FLattenSwinTransformer(\n",
    "    img_size=32,\n",
    "    patch_size=4,\n",
    "    in_chans=3,\n",
    "    num_classes=10,  # CIFAR-10 has 10 classes\n",
    "    embed_dim=96,\n",
    "    depths=[2, 2, 6, 2], # 2,2,6,2\n",
    "    num_heads=[3, 6, 12, 24],\n",
    "    window_size=4, #4\n",
    "    mlp_ratio=4.,\n",
    "    qkv_bias=True,\n",
    "    drop_rate=0.1,\n",
    "    attn_drop_rate=0.1,\n",
    "    drop_path_rate=0.1,\n",
    "    ape=False,\n",
    "    patch_norm=True,\n",
    "    use_checkpoint=False,\n",
    "    focusing_factor=2, # p=3\n",
    "    kernel_size=5,\n",
    "    attn_type='LLLL'\n",
    ")\n",
    "\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rqpdKvMZ6RWm",
    "outputId": "21ff8c31-b326-4f89-f11a-e2633d8435fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 27,547,306\n"
     ]
    }
   ],
   "source": [
    "# Optimizer and loss function\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Initialize lists to store losses\n",
    "train_losses = []\n",
    "val_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true,
    "id": "4PmtkvvX7z53",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Training Loss: 2.3419, Accuracy: 10.82%\n",
      "Test Loss: 2.3096, Test Accuracy: 10.00%\n",
      "Epoch [2/100], Training Loss: 2.3073, Accuracy: 10.09%\n",
      "Test Loss: 2.3035, Test Accuracy: 10.00%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m inputs, targets \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(DEVICE), targets\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m     11\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 12\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[0;32m     14\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[11], line 121\u001b[0m, in \u001b[0;36mFLattenSwinTransformer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m--> 121\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead(x)\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "Cell \u001b[1;32mIn[11], line 113\u001b[0m, in \u001b[0;36mFLattenSwinTransformer.forward_features\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    110\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_drop(x)\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m--> 113\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)  \u001b[38;5;66;03m# B L C\u001b[39;00m\n\u001b[0;32m    116\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavgpool(x\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m))  \u001b[38;5;66;03m# B C 1\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[10], line 60\u001b[0m, in \u001b[0;36mBasicLayer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     58\u001b[0m         x \u001b[38;5;241m=\u001b[39m checkpoint\u001b[38;5;241m.\u001b[39mcheckpoint(blk, x)\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 60\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample(x)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[8], line 99\u001b[0m, in \u001b[0;36mSwinTransformerBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     96\u001b[0m x_windows \u001b[38;5;241m=\u001b[39m x_windows\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size, C)  \u001b[38;5;66;03m# nW*B, window_size*window_size, C\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# W-MSA/SW-MSA\u001b[39;00m\n\u001b[1;32m---> 99\u001b[0m attn_windows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_windows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# nW*B, window_size*window_size, C\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m# merge windows\u001b[39;00m\n\u001b[0;32m    102\u001b[0m attn_windows \u001b[38;5;241m=\u001b[39m attn_windows\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size, C)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[7], line 79\u001b[0m, in \u001b[0;36mFocusedLinearAttention.forward\u001b[1;34m(self, x, mask)\u001b[0m\n\u001b[0;32m     76\u001b[0m k \u001b[38;5;241m=\u001b[39m k \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m focusing_factor\n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# Renormalize to original norms\u001b[39;00m\n\u001b[1;32m---> 79\u001b[0m q \u001b[38;5;241m=\u001b[39m (q \u001b[38;5;241m/\u001b[39m \u001b[43mq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m) \u001b[38;5;241m*\u001b[39m q_norm\n\u001b[0;32m     80\u001b[0m k \u001b[38;5;241m=\u001b[39m (k \u001b[38;5;241m/\u001b[39m k\u001b[38;5;241m.\u001b[39mnorm(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)) \u001b[38;5;241m*\u001b[39m k_norm\n\u001b[0;32m     82\u001b[0m \u001b[38;5;66;03m# Reshape for multi-head attention\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_tensor.py:761\u001b[0m, in \u001b[0;36mTensor.norm\u001b[1;34m(self, p, dim, keepdim, dtype)\u001b[0m\n\u001b[0;32m    757\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    759\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mnorm, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, p\u001b[38;5;241m=\u001b[39mp, dim\u001b[38;5;241m=\u001b[39mdim, keepdim\u001b[38;5;241m=\u001b[39mkeepdim, dtype\u001b[38;5;241m=\u001b[39mdtype\n\u001b[0;32m    760\u001b[0m     )\n\u001b[1;32m--> 761\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\functional.py:1616\u001b[0m, in \u001b[0;36mnorm\u001b[1;34m(input, p, dim, keepdim, out, dtype)\u001b[0m\n\u001b[0;32m   1614\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfro\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m (dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dim, (\u001b[38;5;28mint\u001b[39m, torch\u001b[38;5;241m.\u001b[39mSymInt)) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(dim) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m):\n\u001b[0;32m   1615\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1616\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvector_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1618\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mvector_norm(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m2\u001b[39m, _dim, keepdim, dtype\u001b[38;5;241m=\u001b[39mdtype, out\u001b[38;5;241m=\u001b[39mout)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        #_, predicted = outputs.max(1)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    train_accuracy = 100. * correct / total\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)  # Store average training loss\n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}], Training Loss: {avg_train_loss:.4f}, \"\n",
    "          f\"Accuracy: {train_accuracy:.2f}%\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            #_, predicted = outputs.max(1)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    avg_val_loss = test_loss / len(test_loader)\n",
    "    val_losses.append(avg_val_loss)  # Store average validation loss\n",
    "    test_accuracy = 100. * correct / total\n",
    "    print(f\"Test Loss: {avg_val_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n",
    "\n",
    "    # Update learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "print(\"Training finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Training Loss', color='blue', marker='o')\n",
    "plt.plot(val_losses, label='Validation Loss', color='orange', marker='o')\n",
    "plt.title('Training and Validation Loss / FLatten+Swin (window: 8, p=2)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
