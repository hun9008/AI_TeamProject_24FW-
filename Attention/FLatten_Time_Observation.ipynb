{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Wa8mdzSwvzMe"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import time\n",
        "\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wHeuFDCPyuLf"
      },
      "outputs": [],
      "source": [
        "class FocusedLinearAttention(nn.Module):\n",
        "    def __init__(self, dim, num_heads, qkv_bias=True, attn_drop=0., proj_drop=0., focusing_factor=3, kernel_size=5):\n",
        "        super(FocusedLinearAttention, self).__init__()\n",
        "\n",
        "        self.dim = dim\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "\n",
        "        self.focusing_factor = focusing_factor\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "        self.dwc = nn.Conv2d(in_channels=head_dim, out_channels=head_dim, kernel_size=kernel_size, groups=head_dim, padding=kernel_size // 2)\n",
        "        self.scale = nn.Parameter(torch.zeros(size=(1, 1, dim)))\n",
        "\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, x, mask=None, measure_time=False):\n",
        "        start_time = time.time()\n",
        "\n",
        "        B, N, C = x.shape\n",
        "        print(f\"Input shape: {x.shape}\")\n",
        "        H = W = int(N ** 0.5)\n",
        "        assert H * W == N, f\"Input does not correspond to a square grid. Got N={N}, H={H}, W={W}\"\n",
        "        time_records = {}\n",
        "\n",
        "\n",
        "        # 1. QKV 연산\n",
        "        t1 = time.time()\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, C).permute(2, 0, 1, 3)\n",
        "        q, k, v = qkv.unbind(0)\n",
        "        t2 = time.time()\n",
        "        time_records[\"QKV computation\"] = (t2 - t1) * 1000\n",
        "\n",
        "        # Dynamically generate positional encoding\n",
        "        t3 = time.time()\n",
        "        positional_encoding = self.generate_positional_encoding(N, C, H, W, x.device)\n",
        "        t4 = time.time()\n",
        "        time_records[\"Positional encoding generation\"] = (t4 - t3) * 1000\n",
        "\n",
        "        # Add positional encoding to k\n",
        "        t5 = time.time()\n",
        "        k = k + positional_encoding\n",
        "        t6 = time.time()\n",
        "        time_records[\"Positional encoding addition\"] = (t6 - t5) * 1000\n",
        "\n",
        "        # Apply kernel function and focusing mechanism\n",
        "        t7 = time.time()\n",
        "        kernel_function = nn.ReLU()\n",
        "        q = kernel_function(q) + 1e-6\n",
        "        k = kernel_function(k) + 1e-6\n",
        "        t8 = time.time()\n",
        "        time_records[\"Kernel function application\"] = (t8 - t7) * 1000\n",
        "\n",
        "        # Apply scaling using Softplus\n",
        "        t9 = time.time()\n",
        "        scale = nn.Softplus()(self.scale)\n",
        "        q = (q / scale) ** self.focusing_factor\n",
        "        k = (k / scale) ** self.focusing_factor\n",
        "        t10 = time.time()\n",
        "        time_records[\"QK Scaling and focusing\"] = (t10 - t9) * 1000\n",
        "\n",
        "        # Normalize q and k\n",
        "        t11 = time.time()\n",
        "        q = q / q.norm(dim=-1, keepdim=True)\n",
        "        k = k / k.norm(dim=-1, keepdim=True)\n",
        "        t12 = time.time()\n",
        "        time_records[\"QK Normalization\"] = (t12 - t11) * 1000\n",
        "\n",
        "        # Multi-head attention reshaping\n",
        "        t13 = time.time()\n",
        "        q = q.reshape(B, N, self.num_heads, -1).permute(0, 2, 1, 3)\n",
        "        k = k.reshape(B, N, self.num_heads, -1).permute(0, 2, 1, 3)\n",
        "        v = v.reshape(B, N, self.num_heads, -1).permute(0, 2, 1, 3)\n",
        "        t14 = time.time()\n",
        "        time_records[\"QK Reshaping\"] = (t14 - t13) * 1000\n",
        "\n",
        "        # Compute attention\n",
        "        t15 = time.time()\n",
        "        attn = (q @ k.transpose(-2, -1)) * (N ** -0.5)\n",
        "        attn = self.softmax(attn)\n",
        "        t16 = time.time()\n",
        "        time_records[\"QK Attention computation\"] = (t16 - t15) * 1000\n",
        "\n",
        "        # Apply attention to v\n",
        "        t17 = time.time()\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        t18 = time.time()\n",
        "        time_records[\"Apply Attention to V\"] = (t18 - t17) * 1000\n",
        "\n",
        "        # Post-attention processing with depthwise convolution\n",
        "        t19 = time.time()\n",
        "        v = v.reshape(B * self.num_heads, H, W, -1).permute(0, 3, 1, 2)\n",
        "        x = x + self.dwc(v).reshape(B, C, N).permute(0, 2, 1)\n",
        "        t20 = time.time()\n",
        "        time_records[\"Reshape V and apply DWC\"] = (t20 - t19) * 1000\n",
        "\n",
        "        # Final projection and dropout\n",
        "        t21 = time.time()\n",
        "        x = self.proj(x)\n",
        "        final_output = self.proj_drop(x)\n",
        "        t22 = time.time()\n",
        "        time_records[\"Final projection and dropout\"] = (t22 - t21) * 1000\n",
        "\n",
        "        total_time = time.time() - start_time\n",
        "        total_time_ms = total_time * 1000\n",
        "\n",
        "        return final_output, time_records if measure_time else final_output\n",
        "\n",
        "    def generate_positional_encoding(self, N, C, H, W, device):\n",
        "        # Directly create a normalized positional encoding grid\n",
        "        grid = torch.linspace(0, 1, H, device=device).view(H, 1).expand(H, W)\n",
        "        positional_encoding = torch.stack((grid, grid.T), dim=-1).reshape(1, N, 2)  # Shape: (1, N, 2)\n",
        "\n",
        "        # Expand positional encoding to match the embedding dimension\n",
        "        positional_encoding = positional_encoding.repeat(1, 1, C // 2)  # Shape: (1, N, C)\n",
        "\n",
        "        return positional_encoding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1kuSSo-B-JVp"
      },
      "outputs": [],
      "source": [
        "B, N, C = 32, 144, 256  # Batch, Sequence Length, Embedding Dim\n",
        "num_heads = 8\n",
        "num_runs = 5  # 실행 횟수\n",
        "\n",
        "def measure_execution_time(model, x, device):\n",
        "    stage_times = defaultdict(list)\n",
        "\n",
        "    for i in range(num_runs):\n",
        "        if device == \"cuda\":\n",
        "            torch.cuda.synchronize()\n",
        "\n",
        "        start_time = time.time()\n",
        "        _, time_records = model(x, measure_time=True)  # 실행 및 시간 측정\n",
        "        if device == \"cuda\":\n",
        "            torch.cuda.synchronize()\n",
        "\n",
        "        total_time = (time.time() - start_time) * 1000  # ms 변환\n",
        "        stage_times[\"Total forward pass\"].append(total_time)\n",
        "\n",
        "        if i > 0:  # 첫 실행 제외\n",
        "            for stage, t in time_records.items():\n",
        "                stage_times[stage].append(t)\n",
        "\n",
        "    # 평균 계산\n",
        "    avg_stage_times = {stage: sum(times) / len(times) for stage, times in stage_times.items()}\n",
        "\n",
        "    print(f\"\\nAverage Execution Time on {device.upper()} (excluding first run)\")\n",
        "    print(\"=\" * 65)\n",
        "    print(f\"{'Stage':<35}{'Time (ms)':<15}\")\n",
        "    print(\"=\" * 65)\n",
        "    total_avg_time = avg_stage_times[\"Total forward pass\"]\n",
        "    for stage, avg_time in avg_stage_times.items():\n",
        "        percentage = (avg_time / total_avg_time) * 100\n",
        "        print(f\"{stage:<35}{avg_time:<15.3f}\")\n",
        "\n",
        "    print(\"||\" * 65)\n",
        "    print(f\"{'Total':<35}{total_avg_time:<15.3f}\")\n",
        "    qk_attn = avg_stage_times[\"QK Attention computation\"]\n",
        "    print(f\"{'QK attn':<35}{qk_attn:<15.3f}\")\n",
        "    attn_percentage = (avg_stage_times[\"QK Attention computation\"] / total_avg_time) * 100\n",
        "    print(f\"{'QK attn percent':<35}{attn_percentage:<15.3f}\")\n",
        "    v_attn = avg_stage_times[\"Apply Attention to V\"]\n",
        "    print(f\"{'V attn':<35}{v_attn:<15.3f}\")\n",
        "    v_percentage = (avg_stage_times[\"Apply Attention to V\"] / total_avg_time) * 100\n",
        "    print(f\"{'V attn percent':<35}{v_percentage:<15.3f}\")\n",
        "    print(\"||\" * 65)\n",
        "\n",
        "    return avg_stage_times"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    print(\"\\nRunning on CUDA...\")\n",
        "    x_cuda = torch.randn(B, N, C).cuda()\n",
        "    attn_cuda = FocusedLinearAttention(dim=C, num_heads=num_heads).cuda()\n",
        "    measure_execution_time(attn_cuda, x_cuda, \"cuda\")\n",
        "\n",
        "# CPU 실행\n",
        "print(\"\\nRunning on CPU...\")\n",
        "x_cpu = torch.randn(B, N, C)\n",
        "attn_cpu = FocusedLinearAttention(dim=C, num_heads=num_heads)\n",
        "measure_execution_time(attn_cpu, x_cpu, \"cpu\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIjIomaa2yki",
        "outputId": "b104438c-4120-49f2-f16c-61f008283417"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running on CUDA...\n",
            "Input shape: torch.Size([32, 144, 256])\n",
            "Input shape: torch.Size([32, 144, 256])\n",
            "Input shape: torch.Size([32, 144, 256])\n",
            "Input shape: torch.Size([32, 144, 256])\n",
            "Input shape: torch.Size([32, 144, 256])\n",
            "\n",
            "Average Execution Time on CUDA (excluding first run)\n",
            "=================================================================\n",
            "Stage                              Time (ms)      \n",
            "=================================================================\n",
            "Total forward pass                 1.851          \n",
            "QKV computation                    0.334          \n",
            "Positional encoding generation     0.141          \n",
            "Positional encoding addition       0.023          \n",
            "Kernel function application        0.139          \n",
            "QK Scaling and focusing            0.132          \n",
            "QK Normalization                   0.109          \n",
            "QK Reshaping                       0.056          \n",
            "QK Attention computation           0.197          \n",
            "Apply Attention to V               0.113          \n",
            "Reshape V and apply DWC            0.215          \n",
            "Final projection and dropout       0.096          \n",
            "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
            "Total                              1.851          \n",
            "QK attn                            0.197          \n",
            "QK attn percent                    10.617         \n",
            "V attn                             0.113          \n",
            "V attn percent                     6.089          \n",
            "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
            "\n",
            "Running on CPU...\n",
            "Input shape: torch.Size([32, 144, 256])\n",
            "Input shape: torch.Size([32, 144, 256])\n",
            "Input shape: torch.Size([32, 144, 256])\n",
            "Input shape: torch.Size([32, 144, 256])\n",
            "Input shape: torch.Size([32, 144, 256])\n",
            "\n",
            "Average Execution Time on CPU (excluding first run)\n",
            "=================================================================\n",
            "Stage                              Time (ms)      \n",
            "=================================================================\n",
            "Total forward pass                 27.669         \n",
            "QKV computation                    5.036          \n",
            "Positional encoding generation     0.285          \n",
            "Positional encoding addition       0.276          \n",
            "Kernel function application        1.312          \n",
            "QK Scaling and focusing            1.420          \n",
            "QK Normalization                   1.014          \n",
            "QK Reshaping                       0.091          \n",
            "QK Attention computation           10.399         \n",
            "Apply Attention to V               1.741          \n",
            "Reshape V and apply DWC            2.008          \n",
            "Final projection and dropout       1.625          \n",
            "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
            "Total                              27.669         \n",
            "QK attn                            10.399         \n",
            "QK attn percent                    37.585         \n",
            "V attn                             1.741          \n",
            "V attn percent                     6.292          \n",
            "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Total forward pass': 27.66885757446289,\n",
              " 'QKV computation': 5.03617525100708,\n",
              " 'Positional encoding generation': 0.28526782989501953,\n",
              " 'Positional encoding addition': 0.27567148208618164,\n",
              " 'Kernel function application': 1.3124346733093262,\n",
              " 'QK Scaling and focusing': 1.4196038246154785,\n",
              " 'QK Normalization': 1.01393461227417,\n",
              " 'QK Reshaping': 0.09119510650634766,\n",
              " 'QK Attention computation': 10.399460792541504,\n",
              " 'Apply Attention to V': 1.7408132553100586,\n",
              " 'Reshape V and apply DWC': 2.0082592964172363,\n",
              " 'Final projection and dropout': 1.624763011932373}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3jh_QIAt4WSb"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}