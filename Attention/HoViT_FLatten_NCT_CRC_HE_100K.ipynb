{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!wget -O NCT-CRC-HE-100K.zip https://zenodo.org/record/1214456/files/NCT-CRC-HE-100K.zip\n",
        "!unzip -qq NCT-CRC-HE-100K.zip -d train"
      ],
      "metadata": {
        "id": "NYpt3QcVCl83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "Wa8mdzSwvzMe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe0c9a2c-2661-4d5a-964f-80c9a657215d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchinfo in /usr/local/lib/python3.11/dist-packages (1.8.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchinfo\n",
        "\n",
        "import timm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, ConcatDataset\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "import itertools\n",
        "from torchinfo import summary\n",
        "from sklearn.metrics import accuracy_score, balanced_accuracy_score, classification_report, confusion_matrix\n",
        "from PIL import Image\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import random\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Subset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvNorm(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=2, padding=1):\n",
        "        super(ConvNorm, self).__init__()\n",
        "        self.linear = nn.Conv2d(\n",
        "            in_channels, out_channels, kernel_size=kernel_size,\n",
        "            stride=stride, padding=padding, bias=False\n",
        "        )\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear(x)\n",
        "        x = self.bn(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "4FLtpbS_Vrd7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "zhaJu1oZyp0D"
      },
      "outputs": [],
      "source": [
        "class Stem16(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Stem16, self).__init__()\n",
        "        self.conv1 = ConvNorm(3, 32)\n",
        "        self.act1 = nn.Hardswish()\n",
        "        self.conv2 = ConvNorm(32, 64)\n",
        "        self.act2 = nn.Hardswish()\n",
        "        self.conv3 = ConvNorm(64, 128)\n",
        "        self.act3 = nn.Hardswish()\n",
        "        self.conv4 = ConvNorm(128, 256)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.act1(self.conv1(x))\n",
        "        x = self.act2(self.conv2(x))\n",
        "        x = self.act3(self.conv3(x))\n",
        "        x = self.conv4(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "W1Xh6-wOyskM"
      },
      "outputs": [],
      "source": [
        "class LinearNorm(nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super(LinearNorm, self).__init__()\n",
        "        self.linear = nn.Linear(in_features, out_features, bias=False)\n",
        "        self.bn = nn.BatchNorm1d(out_features)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        if x.dim() == 3:\n",
        "            B, N, C = x.shape\n",
        "            x = x.reshape(B * N, C)\n",
        "            x = self.bn(self.linear(x))\n",
        "            x = x.reshape(B, N, -1)\n",
        "        else:\n",
        "            x = self.bn(self.linear(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "wHeuFDCPyuLf"
      },
      "outputs": [],
      "source": [
        "class FocusedLinearAttention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, height=224, width=224, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.,\n",
        "                 sr_ratio=1, focusing_factor=3, kernel_size=5):\n",
        "        super().__init__()\n",
        "        assert dim % num_heads == 0, f\"dim {dim} should be divisible by num_heads {num_heads}.\"\n",
        "\n",
        "        self.dim = dim\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "\n",
        "        # Store height and width as attributes\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "\n",
        "        # Query, Key-Value projection layers\n",
        "        self.q = nn.Linear(dim, dim, bias=qkv_bias)\n",
        "        self.kv = nn.Linear(dim, dim * 2, bias=qkv_bias)\n",
        "\n",
        "        # Dropout and projection layers\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "        # Spatial reduction setup\n",
        "        self.sr_ratio = sr_ratio\n",
        "        if sr_ratio > 1:\n",
        "            self.sr = nn.Conv2d(dim, dim, kernel_size=sr_ratio, stride=sr_ratio)\n",
        "            self.norm = nn.LayerNorm(dim)\n",
        "\n",
        "        # Focus control\n",
        "        self.focusing_factor = focusing_factor\n",
        "        self.kernel_size = kernel_size\n",
        "\n",
        "    def forward(self, x, H, W):\n",
        "        B, N, C = x.shape\n",
        "        H = self.height # Access the stored height attribute\n",
        "        W = self.width # Access the stored width attribute\n",
        "\n",
        "        # Compute query projection\n",
        "        q = self.q(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
        "\n",
        "        # Spatial reduction if sr_ratio > 1\n",
        "        if self.sr_ratio > 1:\n",
        "            x_ = x.permute(0, 2, 1).reshape(B, C, H, W)  # Reshape for spatial reduction\n",
        "            x_ = self.sr(x_).reshape(B, C, -1).permute(0, 2, 1)  # Apply Conv2d and reshape back\n",
        "            x_ = self.norm(x_)  # Normalize the reduced features\n",
        "            kv = self.kv(x_).reshape(B, -1, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        else:\n",
        "            kv = self.kv(x).reshape(B, -1, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "\n",
        "        k, v = kv[0], kv[1]\n",
        "\n",
        "        # Attention computation with scaled dot product\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = F.softmax(attn, dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        # Focused output projection\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xvyY-FeUywyR"
      },
      "outputs": [],
      "source": [
        "class LevitMlp(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features, out_features):\n",
        "        super(LevitMlp, self).__init__()\n",
        "        self.ln1 = LinearNorm(in_features, hidden_features)\n",
        "        self.act = nn.Hardswish()\n",
        "        self.drop = nn.Dropout(p=0.5, inplace=False)#dropout 적용\n",
        "        self.ln2 = LinearNorm(hidden_features, out_features)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.ln1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.ln2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Ubl233osyyHQ"
      },
      "outputs": [],
      "source": [
        "class HoViTBlock(nn.Module):\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=2, sr_ratio=1):\n",
        "        super(HoViTBlock, self).__init__()\n",
        "        self.attn = FocusedLinearAttention(dim, num_heads, sr_ratio=sr_ratio)\n",
        "        self.drop_path1 = nn.Identity()\n",
        "        self.mlp = LevitMlp(dim, dim * mlp_ratio, dim)\n",
        "        self.drop_path2 = nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        H = int(N**0.5)\n",
        "        W = H\n",
        "\n",
        "        x = x + self.drop_path1(self.attn(x, H, W))\n",
        "        x = x + self.drop_path2(self.mlp(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "cjBHkWJYyzfc"
      },
      "outputs": [],
      "source": [
        "class CNNDownsample(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(CNNDownsample, self).__init__()\n",
        "        self.out_channels = out_channels\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1)\n",
        "        self.act = nn.Hardswish()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # print(x.shape)\n",
        "        B, N, C = x.shape # (B, N, C)  N=H*W (16 * 16 = 196)\n",
        "        H = int(np.sqrt(N))\n",
        "        x = x.view(B, H, H, C).permute(0, 3, 1, 2)\n",
        "        x = self.conv(x)\n",
        "        x = self.act(x)\n",
        "        x = x.permute(0, 2, 3, 1).view(B, -1, self.out_channels)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "jNpOjwVVy1Im"
      },
      "outputs": [],
      "source": [
        "class HoViTStage(nn.Module):\n",
        "    def __init__(self, dim, out_dim, num_heads, num_blocks, downsample=True):\n",
        "        super(HoViTStage, self).__init__()\n",
        "        self.downsample = CNNDownsample(dim, out_dim) if downsample else nn.Identity()\n",
        "        self.blocks = nn.Sequential(*[HoViTBlock(out_dim, num_heads) for _ in range(num_blocks)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.downsample(x)\n",
        "        x = self.blocks(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "FyFtflRsy2QE"
      },
      "outputs": [],
      "source": [
        "class ConvLevitStage(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, num_blocks, kernel_size, stride, padding):\n",
        "        super(ConvLevitStage, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            *[nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size, stride, padding)\n",
        "              for i in range(num_blocks)],\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "yKzCWv-Ny3j9"
      },
      "outputs": [],
      "source": [
        "class NormLinear(nn.Module):\n",
        "    def __init__(self, in_features, out_features, dropout_prob=0.5):#drop_out_0.5 적용\n",
        "        super(NormLinear, self).__init__()\n",
        "        self.bn = nn.BatchNorm1d(in_features)\n",
        "        self.drop = nn.Dropout(p=dropout_prob, inplace=False)\n",
        "        self.linear = nn.Linear(in_features, out_features, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.bn(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.linear(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "xG1A6osfy5M-"
      },
      "outputs": [],
      "source": [
        "class HoViT(nn.Module):\n",
        "    def __init__(self, num_classes=9):\n",
        "        super(HoViT, self).__init__()\n",
        "\n",
        "        self.stem = Stem16()\n",
        "\n",
        "        self.stage1 = HoViTStage(dim=256, out_dim=256, num_heads=4, num_blocks=2, downsample=False) # block 수 적용\n",
        "        self.stage2 = HoViTStage(dim=256, out_dim=384, num_heads=6, num_blocks=2, downsample=True)\n",
        "\n",
        "        self.conv1x1 = nn.Sequential(\n",
        "            nn.Conv2d(384, 512, kernel_size=1, stride=1, padding=0),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        self.head = NormLinear(in_features=512, out_features=num_classes, dropout_prob=0.0)\n",
        "        self.head_dist = NormLinear(in_features=512, out_features=num_classes, dropout_prob=0.0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.stem(x)\n",
        "\n",
        "        B, C, H, W = x.shape\n",
        "        x = x.view(B, C, -1).transpose(1, 2)\n",
        "        x = self.stage1(x)\n",
        "        x = self.stage2(x)\n",
        "\n",
        "        H = W = int(x.shape[1]**0.5)\n",
        "        x = x.transpose(1, 2).view(B, 384, H, W)\n",
        "\n",
        "        x = self.conv1x1(x)\n",
        "\n",
        "        x = torch.mean(x, dim=(2, 3))\n",
        "        out = self.head(x)\n",
        "        out_dist = self.head_dist(x)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "lbCq25bWzJSQ",
        "outputId": "5a9a4a3f-90ac-447a-ec3d-4f5dbb851bb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HoViT(\n",
            "  (stem): Stem16(\n",
            "    (conv1): ConvNorm(\n",
            "      (linear): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (act1): Hardswish()\n",
            "    (conv2): ConvNorm(\n",
            "      (linear): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (act2): Hardswish()\n",
            "    (conv3): ConvNorm(\n",
            "      (linear): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (act3): Hardswish()\n",
            "    (conv4): ConvNorm(\n",
            "      (linear): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (stage1): HoViTStage(\n",
            "    (downsample): Identity()\n",
            "    (blocks): Sequential(\n",
            "      (0): HoViTBlock(\n",
            "        (attn): FocusedLinearAttention(\n",
            "          (q): Linear(in_features=256, out_features=256, bias=False)\n",
            "          (kv): Linear(in_features=256, out_features=512, bias=False)\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path1): Identity()\n",
            "        (mlp): LevitMlp(\n",
            "          (ln1): LinearNorm(\n",
            "            (linear): Linear(in_features=256, out_features=512, bias=False)\n",
            "            (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (act): Hardswish()\n",
            "          (drop): Dropout(p=0.5, inplace=False)\n",
            "          (ln2): LinearNorm(\n",
            "            (linear): Linear(in_features=512, out_features=256, bias=False)\n",
            "            (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (drop_path2): Identity()\n",
            "      )\n",
            "      (1): HoViTBlock(\n",
            "        (attn): FocusedLinearAttention(\n",
            "          (q): Linear(in_features=256, out_features=256, bias=False)\n",
            "          (kv): Linear(in_features=256, out_features=512, bias=False)\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path1): Identity()\n",
            "        (mlp): LevitMlp(\n",
            "          (ln1): LinearNorm(\n",
            "            (linear): Linear(in_features=256, out_features=512, bias=False)\n",
            "            (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (act): Hardswish()\n",
            "          (drop): Dropout(p=0.5, inplace=False)\n",
            "          (ln2): LinearNorm(\n",
            "            (linear): Linear(in_features=512, out_features=256, bias=False)\n",
            "            (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (drop_path2): Identity()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (stage2): HoViTStage(\n",
            "    (downsample): CNNDownsample(\n",
            "      (conv): Conv2d(256, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "      (act): Hardswish()\n",
            "    )\n",
            "    (blocks): Sequential(\n",
            "      (0): HoViTBlock(\n",
            "        (attn): FocusedLinearAttention(\n",
            "          (q): Linear(in_features=384, out_features=384, bias=False)\n",
            "          (kv): Linear(in_features=384, out_features=768, bias=False)\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path1): Identity()\n",
            "        (mlp): LevitMlp(\n",
            "          (ln1): LinearNorm(\n",
            "            (linear): Linear(in_features=384, out_features=768, bias=False)\n",
            "            (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (act): Hardswish()\n",
            "          (drop): Dropout(p=0.5, inplace=False)\n",
            "          (ln2): LinearNorm(\n",
            "            (linear): Linear(in_features=768, out_features=384, bias=False)\n",
            "            (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (drop_path2): Identity()\n",
            "      )\n",
            "      (1): HoViTBlock(\n",
            "        (attn): FocusedLinearAttention(\n",
            "          (q): Linear(in_features=384, out_features=384, bias=False)\n",
            "          (kv): Linear(in_features=384, out_features=768, bias=False)\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path1): Identity()\n",
            "        (mlp): LevitMlp(\n",
            "          (ln1): LinearNorm(\n",
            "            (linear): Linear(in_features=384, out_features=768, bias=False)\n",
            "            (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (act): Hardswish()\n",
            "          (drop): Dropout(p=0.5, inplace=False)\n",
            "          (ln2): LinearNorm(\n",
            "            (linear): Linear(in_features=768, out_features=384, bias=False)\n",
            "            (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (drop_path2): Identity()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (conv1x1): Sequential(\n",
            "    (0): Conv2d(384, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "  )\n",
            "  (head): NormLinear(\n",
            "    (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (drop): Dropout(p=0.0, inplace=False)\n",
            "    (linear): Linear(in_features=512, out_features=9, bias=True)\n",
            "  )\n",
            "  (head_dist): NormLinear(\n",
            "    (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (drop): Dropout(p=0.0, inplace=False)\n",
            "    (linear): Linear(in_features=512, out_features=9, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model = HoViT()\n",
        "print(model)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "batch_size = 32\n",
        "learning_rate = 5e-4\n",
        "num_epochs = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "gNpMsNYAzLDF",
        "outputId": "14d157c6-bebc-45f6-e3c6-dcd837f2624c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "====================================================================================================\n",
            "Layer (type:depth-idx)                             Output Shape              Param #\n",
            "====================================================================================================\n",
            "HoViT                                              [32, 9]                   --\n",
            "├─Stem16: 1-1                                      [32, 256, 14, 14]         --\n",
            "│    └─ConvNorm: 2-1                               [32, 32, 112, 112]        --\n",
            "│    │    └─Conv2d: 3-1                            [32, 32, 112, 112]        864\n",
            "│    │    └─BatchNorm2d: 3-2                       [32, 32, 112, 112]        64\n",
            "│    └─Hardswish: 2-2                              [32, 32, 112, 112]        --\n",
            "│    └─ConvNorm: 2-3                               [32, 64, 56, 56]          --\n",
            "│    │    └─Conv2d: 3-3                            [32, 64, 56, 56]          18,432\n",
            "│    │    └─BatchNorm2d: 3-4                       [32, 64, 56, 56]          128\n",
            "│    └─Hardswish: 2-4                              [32, 64, 56, 56]          --\n",
            "│    └─ConvNorm: 2-5                               [32, 128, 28, 28]         --\n",
            "│    │    └─Conv2d: 3-5                            [32, 128, 28, 28]         73,728\n",
            "│    │    └─BatchNorm2d: 3-6                       [32, 128, 28, 28]         256\n",
            "│    └─Hardswish: 2-6                              [32, 128, 28, 28]         --\n",
            "│    └─ConvNorm: 2-7                               [32, 256, 14, 14]         --\n",
            "│    │    └─Conv2d: 3-7                            [32, 256, 14, 14]         294,912\n",
            "│    │    └─BatchNorm2d: 3-8                       [32, 256, 14, 14]         512\n",
            "├─HoViTStage: 1-2                                  [32, 196, 256]            --\n",
            "│    └─Identity: 2-8                               [32, 196, 256]            --\n",
            "│    └─Sequential: 2-9                             [32, 196, 256]            --\n",
            "│    │    └─HoViTBlock: 3-9                        [32, 196, 256]            526,080\n",
            "│    │    └─HoViTBlock: 3-10                       [32, 196, 256]            526,080\n",
            "├─HoViTStage: 1-3                                  [32, 49, 384]             --\n",
            "│    └─CNNDownsample: 2-10                         [32, 49, 384]             --\n",
            "│    │    └─Conv2d: 3-11                           [32, 384, 7, 7]           885,120\n",
            "│    │    └─Hardswish: 3-12                        [32, 384, 7, 7]           --\n",
            "│    └─Sequential: 2-11                            [32, 49, 384]             --\n",
            "│    │    └─HoViTBlock: 3-13                       [32, 49, 384]             1,182,336\n",
            "│    │    └─HoViTBlock: 3-14                       [32, 49, 384]             1,182,336\n",
            "├─Sequential: 1-4                                  [32, 512, 7, 7]           --\n",
            "│    └─Conv2d: 2-12                                [32, 512, 7, 7]           197,120\n",
            "│    └─BatchNorm2d: 2-13                           [32, 512, 7, 7]           1,024\n",
            "│    └─ReLU: 2-14                                  [32, 512, 7, 7]           --\n",
            "├─NormLinear: 1-5                                  [32, 9]                   --\n",
            "│    └─BatchNorm1d: 2-15                           [32, 512]                 1,024\n",
            "│    └─Dropout: 2-16                               [32, 512]                 --\n",
            "│    └─Linear: 2-17                                [32, 9]                   4,617\n",
            "├─NormLinear: 1-6                                  [32, 9]                   --\n",
            "│    └─BatchNorm1d: 2-18                           [32, 512]                 1,024\n",
            "│    └─Dropout: 2-19                               [32, 512]                 --\n",
            "│    └─Linear: 2-20                                [32, 9]                   4,617\n",
            "====================================================================================================\n",
            "Total params: 4,900,274\n",
            "Trainable params: 4,900,274\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (Units.GIGABYTES): 12.81\n",
            "====================================================================================================\n",
            "Input size (MB): 19.27\n",
            "Forward/backward pass size (MB): 756.52\n",
            "Params size (MB): 19.60\n",
            "Estimated Total Size (MB): 795.39\n",
            "====================================================================================================\n"
          ]
        }
      ],
      "source": [
        "print(summary(model, input_size=(32, 3, 224, 224)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "qDELjYy2zP90",
        "outputId": "1ba9997f-d9b6-43e7-f821-6d358df884cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "====================================================================================================\n",
            "Layer (type:depth-idx)                             Output Shape              Param #\n",
            "====================================================================================================\n",
            "HoViT                                              [32, 9]                   --\n",
            "├─Stem16: 1-1                                      [32, 256, 14, 14]         --\n",
            "│    └─conv1.linear.weight                                                   ├─864\n",
            "│    └─conv1.bn.weight                                                       ├─32\n",
            "│    └─conv1.bn.bias                                                         ├─32\n",
            "│    └─conv2.linear.weight                                                   ├─18,432\n",
            "│    └─conv2.bn.weight                                                       ├─64\n",
            "│    └─conv2.bn.bias                                                         ├─64\n",
            "│    └─conv3.linear.weight                                                   ├─73,728\n",
            "│    └─conv3.bn.weight                                                       ├─128\n",
            "│    └─conv3.bn.bias                                                         ├─128\n",
            "│    └─conv4.linear.weight                                                   ├─294,912\n",
            "│    └─conv4.bn.weight                                                       ├─256\n",
            "│    └─conv4.bn.bias                                                         └─256\n",
            "│    └─ConvNorm: 2-1                               [32, 32, 112, 112]        --\n",
            "│    │    └─linear.weight                                                    ├─864\n",
            "│    │    └─bn.weight                                                        ├─32\n",
            "│    │    └─bn.bias                                                          └─32\n",
            "│    │    └─Conv2d: 3-1                            [32, 32, 112, 112]        864\n",
            "│    │    │    └─weight                                                      └─864\n",
            "│    │    └─BatchNorm2d: 3-2                       [32, 32, 112, 112]        64\n",
            "│    │    │    └─weight                                                      ├─32\n",
            "│    │    │    └─bias                                                        └─32\n",
            "│    └─Hardswish: 2-2                              [32, 32, 112, 112]        --\n",
            "│    └─ConvNorm: 2-3                               [32, 64, 56, 56]          --\n",
            "│    │    └─linear.weight                                                    ├─18,432\n",
            "│    │    └─bn.weight                                                        ├─64\n",
            "│    │    └─bn.bias                                                          └─64\n",
            "│    │    └─Conv2d: 3-3                            [32, 64, 56, 56]          18,432\n",
            "│    │    │    └─weight                                                      └─18,432\n",
            "│    │    └─BatchNorm2d: 3-4                       [32, 64, 56, 56]          128\n",
            "│    │    │    └─weight                                                      ├─64\n",
            "│    │    │    └─bias                                                        └─64\n",
            "│    └─Hardswish: 2-4                              [32, 64, 56, 56]          --\n",
            "│    └─ConvNorm: 2-5                               [32, 128, 28, 28]         --\n",
            "│    │    └─linear.weight                                                    ├─73,728\n",
            "│    │    └─bn.weight                                                        ├─128\n",
            "│    │    └─bn.bias                                                          └─128\n",
            "│    │    └─Conv2d: 3-5                            [32, 128, 28, 28]         73,728\n",
            "│    │    │    └─weight                                                      └─73,728\n",
            "│    │    └─BatchNorm2d: 3-6                       [32, 128, 28, 28]         256\n",
            "│    │    │    └─weight                                                      ├─128\n",
            "│    │    │    └─bias                                                        └─128\n",
            "│    └─Hardswish: 2-6                              [32, 128, 28, 28]         --\n",
            "│    └─ConvNorm: 2-7                               [32, 256, 14, 14]         --\n",
            "│    │    └─linear.weight                                                    ├─294,912\n",
            "│    │    └─bn.weight                                                        ├─256\n",
            "│    │    └─bn.bias                                                          └─256\n",
            "│    │    └─Conv2d: 3-7                            [32, 256, 14, 14]         294,912\n",
            "│    │    │    └─weight                                                      └─294,912\n",
            "│    │    └─BatchNorm2d: 3-8                       [32, 256, 14, 14]         512\n",
            "│    │    │    └─weight                                                      ├─256\n",
            "│    │    │    └─bias                                                        └─256\n",
            "├─HoViTStage: 1-2                                  [32, 196, 256]            --\n",
            "│    └─blocks.0.attn.q.weight                                                ├─65,536\n",
            "│    └─blocks.0.attn.kv.weight                                               ├─131,072\n",
            "│    └─blocks.0.attn.proj.weight                                             ├─65,536\n",
            "│    └─blocks.0.attn.proj.bias                                               ├─256\n",
            "│    └─blocks.0.mlp.ln1.linear.weight                                        ├─131,072\n",
            "│    └─blocks.0.mlp.ln1.bn.weight                                            ├─512\n",
            "│    └─blocks.0.mlp.ln1.bn.bias                                              ├─512\n",
            "│    └─blocks.0.mlp.ln2.linear.weight                                        ├─131,072\n",
            "│    └─blocks.0.mlp.ln2.bn.weight                                            ├─256\n",
            "│    └─blocks.0.mlp.ln2.bn.bias                                              ├─256\n",
            "│    └─blocks.1.attn.q.weight                                                ├─65,536\n",
            "│    └─blocks.1.attn.kv.weight                                               ├─131,072\n",
            "│    └─blocks.1.attn.proj.weight                                             ├─65,536\n",
            "│    └─blocks.1.attn.proj.bias                                               ├─256\n",
            "│    └─blocks.1.mlp.ln1.linear.weight                                        ├─131,072\n",
            "│    └─blocks.1.mlp.ln1.bn.weight                                            ├─512\n",
            "│    └─blocks.1.mlp.ln1.bn.bias                                              ├─512\n",
            "│    └─blocks.1.mlp.ln2.linear.weight                                        ├─131,072\n",
            "│    └─blocks.1.mlp.ln2.bn.weight                                            ├─256\n",
            "│    └─blocks.1.mlp.ln2.bn.bias                                              └─256\n",
            "│    └─Identity: 2-8                               [32, 196, 256]            --\n",
            "│    └─Sequential: 2-9                             [32, 196, 256]            --\n",
            "│    │    └─0.attn.q.weight                                                  ├─65,536\n",
            "│    │    └─0.attn.kv.weight                                                 ├─131,072\n",
            "│    │    └─0.attn.proj.weight                                               ├─65,536\n",
            "│    │    └─0.attn.proj.bias                                                 ├─256\n",
            "│    │    └─0.mlp.ln1.linear.weight                                          ├─131,072\n",
            "│    │    └─0.mlp.ln1.bn.weight                                              ├─512\n",
            "│    │    └─0.mlp.ln1.bn.bias                                                ├─512\n",
            "│    │    └─0.mlp.ln2.linear.weight                                          ├─131,072\n",
            "│    │    └─0.mlp.ln2.bn.weight                                              ├─256\n",
            "│    │    └─0.mlp.ln2.bn.bias                                                ├─256\n",
            "│    │    └─1.attn.q.weight                                                  ├─65,536\n",
            "│    │    └─1.attn.kv.weight                                                 ├─131,072\n",
            "│    │    └─1.attn.proj.weight                                               ├─65,536\n",
            "│    │    └─1.attn.proj.bias                                                 ├─256\n",
            "│    │    └─1.mlp.ln1.linear.weight                                          ├─131,072\n",
            "│    │    └─1.mlp.ln1.bn.weight                                              ├─512\n",
            "│    │    └─1.mlp.ln1.bn.bias                                                ├─512\n",
            "│    │    └─1.mlp.ln2.linear.weight                                          ├─131,072\n",
            "│    │    └─1.mlp.ln2.bn.weight                                              ├─256\n",
            "│    │    └─1.mlp.ln2.bn.bias                                                └─256\n",
            "│    │    └─HoViTBlock: 3-9                        [32, 196, 256]            526,080\n",
            "│    │    │    └─attn.q.weight                                               ├─65,536\n",
            "│    │    │    └─attn.kv.weight                                              ├─131,072\n",
            "│    │    │    └─attn.proj.weight                                            ├─65,536\n",
            "│    │    │    └─attn.proj.bias                                              ├─256\n",
            "│    │    │    └─mlp.ln1.linear.weight                                       ├─131,072\n",
            "│    │    │    └─mlp.ln1.bn.weight                                           ├─512\n",
            "│    │    │    └─mlp.ln1.bn.bias                                             ├─512\n",
            "│    │    │    └─mlp.ln2.linear.weight                                       ├─131,072\n",
            "│    │    │    └─mlp.ln2.bn.weight                                           ├─256\n",
            "│    │    │    └─mlp.ln2.bn.bias                                             └─256\n",
            "│    │    └─HoViTBlock: 3-10                       [32, 196, 256]            526,080\n",
            "│    │    │    └─attn.q.weight                                               ├─65,536\n",
            "│    │    │    └─attn.kv.weight                                              ├─131,072\n",
            "│    │    │    └─attn.proj.weight                                            ├─65,536\n",
            "│    │    │    └─attn.proj.bias                                              ├─256\n",
            "│    │    │    └─mlp.ln1.linear.weight                                       ├─131,072\n",
            "│    │    │    └─mlp.ln1.bn.weight                                           ├─512\n",
            "│    │    │    └─mlp.ln1.bn.bias                                             ├─512\n",
            "│    │    │    └─mlp.ln2.linear.weight                                       ├─131,072\n",
            "│    │    │    └─mlp.ln2.bn.weight                                           ├─256\n",
            "│    │    │    └─mlp.ln2.bn.bias                                             └─256\n",
            "├─HoViTStage: 1-3                                  [32, 49, 384]             --\n",
            "│    └─downsample.conv.weight                                                ├─884,736\n",
            "│    └─downsample.conv.bias                                                  ├─384\n",
            "│    └─blocks.0.attn.q.weight                                                ├─147,456\n",
            "│    └─blocks.0.attn.kv.weight                                               ├─294,912\n",
            "│    └─blocks.0.attn.proj.weight                                             ├─147,456\n",
            "│    └─blocks.0.attn.proj.bias                                               ├─384\n",
            "│    └─blocks.0.mlp.ln1.linear.weight                                        ├─294,912\n",
            "│    └─blocks.0.mlp.ln1.bn.weight                                            ├─768\n",
            "│    └─blocks.0.mlp.ln1.bn.bias                                              ├─768\n",
            "│    └─blocks.0.mlp.ln2.linear.weight                                        ├─294,912\n",
            "│    └─blocks.0.mlp.ln2.bn.weight                                            ├─384\n",
            "│    └─blocks.0.mlp.ln2.bn.bias                                              ├─384\n",
            "│    └─blocks.1.attn.q.weight                                                ├─147,456\n",
            "│    └─blocks.1.attn.kv.weight                                               ├─294,912\n",
            "│    └─blocks.1.attn.proj.weight                                             ├─147,456\n",
            "│    └─blocks.1.attn.proj.bias                                               ├─384\n",
            "│    └─blocks.1.mlp.ln1.linear.weight                                        ├─294,912\n",
            "│    └─blocks.1.mlp.ln1.bn.weight                                            ├─768\n",
            "│    └─blocks.1.mlp.ln1.bn.bias                                              ├─768\n",
            "│    └─blocks.1.mlp.ln2.linear.weight                                        ├─294,912\n",
            "│    └─blocks.1.mlp.ln2.bn.weight                                            ├─384\n",
            "│    └─blocks.1.mlp.ln2.bn.bias                                              └─384\n",
            "│    └─CNNDownsample: 2-10                         [32, 49, 384]             --\n",
            "│    │    └─conv.weight                                                      ├─884,736\n",
            "│    │    └─conv.bias                                                        └─384\n",
            "│    │    └─Conv2d: 3-11                           [32, 384, 7, 7]           885,120\n",
            "│    │    │    └─weight                                                      ├─884,736\n",
            "│    │    │    └─bias                                                        └─384\n",
            "│    │    └─Hardswish: 3-12                        [32, 384, 7, 7]           --\n",
            "│    └─Sequential: 2-11                            [32, 49, 384]             --\n",
            "│    │    └─0.attn.q.weight                                                  ├─147,456\n",
            "│    │    └─0.attn.kv.weight                                                 ├─294,912\n",
            "│    │    └─0.attn.proj.weight                                               ├─147,456\n",
            "│    │    └─0.attn.proj.bias                                                 ├─384\n",
            "│    │    └─0.mlp.ln1.linear.weight                                          ├─294,912\n",
            "│    │    └─0.mlp.ln1.bn.weight                                              ├─768\n",
            "│    │    └─0.mlp.ln1.bn.bias                                                ├─768\n",
            "│    │    └─0.mlp.ln2.linear.weight                                          ├─294,912\n",
            "│    │    └─0.mlp.ln2.bn.weight                                              ├─384\n",
            "│    │    └─0.mlp.ln2.bn.bias                                                ├─384\n",
            "│    │    └─1.attn.q.weight                                                  ├─147,456\n",
            "│    │    └─1.attn.kv.weight                                                 ├─294,912\n",
            "│    │    └─1.attn.proj.weight                                               ├─147,456\n",
            "│    │    └─1.attn.proj.bias                                                 ├─384\n",
            "│    │    └─1.mlp.ln1.linear.weight                                          ├─294,912\n",
            "│    │    └─1.mlp.ln1.bn.weight                                              ├─768\n",
            "│    │    └─1.mlp.ln1.bn.bias                                                ├─768\n",
            "│    │    └─1.mlp.ln2.linear.weight                                          ├─294,912\n",
            "│    │    └─1.mlp.ln2.bn.weight                                              ├─384\n",
            "│    │    └─1.mlp.ln2.bn.bias                                                └─384\n",
            "│    │    └─HoViTBlock: 3-13                       [32, 49, 384]             1,182,336\n",
            "│    │    │    └─attn.q.weight                                               ├─147,456\n",
            "│    │    │    └─attn.kv.weight                                              ├─294,912\n",
            "│    │    │    └─attn.proj.weight                                            ├─147,456\n",
            "│    │    │    └─attn.proj.bias                                              ├─384\n",
            "│    │    │    └─mlp.ln1.linear.weight                                       ├─294,912\n",
            "│    │    │    └─mlp.ln1.bn.weight                                           ├─768\n",
            "│    │    │    └─mlp.ln1.bn.bias                                             ├─768\n",
            "│    │    │    └─mlp.ln2.linear.weight                                       ├─294,912\n",
            "│    │    │    └─mlp.ln2.bn.weight                                           ├─384\n",
            "│    │    │    └─mlp.ln2.bn.bias                                             └─384\n",
            "│    │    └─HoViTBlock: 3-14                       [32, 49, 384]             1,182,336\n",
            "│    │    │    └─attn.q.weight                                               ├─147,456\n",
            "│    │    │    └─attn.kv.weight                                              ├─294,912\n",
            "│    │    │    └─attn.proj.weight                                            ├─147,456\n",
            "│    │    │    └─attn.proj.bias                                              ├─384\n",
            "│    │    │    └─mlp.ln1.linear.weight                                       ├─294,912\n",
            "│    │    │    └─mlp.ln1.bn.weight                                           ├─768\n",
            "│    │    │    └─mlp.ln1.bn.bias                                             ├─768\n",
            "│    │    │    └─mlp.ln2.linear.weight                                       ├─294,912\n",
            "│    │    │    └─mlp.ln2.bn.weight                                           ├─384\n",
            "│    │    │    └─mlp.ln2.bn.bias                                             └─384\n",
            "├─Sequential: 1-4                                  [32, 512, 7, 7]           --\n",
            "│    └─0.weight                                                              ├─196,608\n",
            "│    └─0.bias                                                                ├─512\n",
            "│    └─1.weight                                                              ├─512\n",
            "│    └─1.bias                                                                └─512\n",
            "│    └─Conv2d: 2-12                                [32, 512, 7, 7]           197,120\n",
            "│    │    └─weight                                                           ├─196,608\n",
            "│    │    └─bias                                                             └─512\n",
            "│    └─BatchNorm2d: 2-13                           [32, 512, 7, 7]           1,024\n",
            "│    │    └─weight                                                           ├─512\n",
            "│    │    └─bias                                                             └─512\n",
            "│    └─ReLU: 2-14                                  [32, 512, 7, 7]           --\n",
            "├─NormLinear: 1-5                                  [32, 9]                   --\n",
            "│    └─bn.weight                                                             ├─512\n",
            "│    └─bn.bias                                                               ├─512\n",
            "│    └─linear.weight                                                         ├─4,608\n",
            "│    └─linear.bias                                                           └─9\n",
            "│    └─BatchNorm1d: 2-15                           [32, 512]                 1,024\n",
            "│    │    └─weight                                                           ├─512\n",
            "│    │    └─bias                                                             └─512\n",
            "│    └─Dropout: 2-16                               [32, 512]                 --\n",
            "│    └─Linear: 2-17                                [32, 9]                   4,617\n",
            "│    │    └─weight                                                           ├─4,608\n",
            "│    │    └─bias                                                             └─9\n",
            "├─NormLinear: 1-6                                  [32, 9]                   --\n",
            "│    └─bn.weight                                                             ├─512\n",
            "│    └─bn.bias                                                               ├─512\n",
            "│    └─linear.weight                                                         ├─4,608\n",
            "│    └─linear.bias                                                           └─9\n",
            "│    └─BatchNorm1d: 2-18                           [32, 512]                 1,024\n",
            "│    │    └─weight                                                           ├─512\n",
            "│    │    └─bias                                                             └─512\n",
            "│    └─Dropout: 2-19                               [32, 512]                 --\n",
            "│    └─Linear: 2-20                                [32, 9]                   4,617\n",
            "│    │    └─weight                                                           ├─4,608\n",
            "│    │    └─bias                                                             └─9\n",
            "====================================================================================================\n",
            "Total params: 4,900,274\n",
            "Trainable params: 4,900,274\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (Units.GIGABYTES): 12.81\n",
            "====================================================================================================\n",
            "Input size (MB): 19.27\n",
            "Forward/backward pass size (MB): 756.52\n",
            "Params size (MB): 19.60\n",
            "Estimated Total Size (MB): 795.39\n",
            "====================================================================================================\n",
            "====================================================================================================\n",
            "Layer (type:depth-idx)                             Output Shape              Param #\n",
            "====================================================================================================\n",
            "HoViT                                              [32, 9]                   --\n",
            "├─Stem16: 1-1                                      [32, 256, 14, 14]         --\n",
            "│    └─conv1.linear.weight                                                   ├─864\n",
            "│    └─conv1.bn.weight                                                       ├─32\n",
            "│    └─conv1.bn.bias                                                         ├─32\n",
            "│    └─conv2.linear.weight                                                   ├─18,432\n",
            "│    └─conv2.bn.weight                                                       ├─64\n",
            "│    └─conv2.bn.bias                                                         ├─64\n",
            "│    └─conv3.linear.weight                                                   ├─73,728\n",
            "│    └─conv3.bn.weight                                                       ├─128\n",
            "│    └─conv3.bn.bias                                                         ├─128\n",
            "│    └─conv4.linear.weight                                                   ├─294,912\n",
            "│    └─conv4.bn.weight                                                       ├─256\n",
            "│    └─conv4.bn.bias                                                         └─256\n",
            "│    └─ConvNorm: 2-1                               [32, 32, 112, 112]        --\n",
            "│    │    └─linear.weight                                                    ├─864\n",
            "│    │    └─bn.weight                                                        ├─32\n",
            "│    │    └─bn.bias                                                          └─32\n",
            "│    │    └─Conv2d: 3-1                            [32, 32, 112, 112]        864\n",
            "│    │    │    └─weight                                                      └─864\n",
            "│    │    └─BatchNorm2d: 3-2                       [32, 32, 112, 112]        64\n",
            "│    │    │    └─weight                                                      ├─32\n",
            "│    │    │    └─bias                                                        └─32\n",
            "│    └─Hardswish: 2-2                              [32, 32, 112, 112]        --\n",
            "│    └─ConvNorm: 2-3                               [32, 64, 56, 56]          --\n",
            "│    │    └─linear.weight                                                    ├─18,432\n",
            "│    │    └─bn.weight                                                        ├─64\n",
            "│    │    └─bn.bias                                                          └─64\n",
            "│    │    └─Conv2d: 3-3                            [32, 64, 56, 56]          18,432\n",
            "│    │    │    └─weight                                                      └─18,432\n",
            "│    │    └─BatchNorm2d: 3-4                       [32, 64, 56, 56]          128\n",
            "│    │    │    └─weight                                                      ├─64\n",
            "│    │    │    └─bias                                                        └─64\n",
            "│    └─Hardswish: 2-4                              [32, 64, 56, 56]          --\n",
            "│    └─ConvNorm: 2-5                               [32, 128, 28, 28]         --\n",
            "│    │    └─linear.weight                                                    ├─73,728\n",
            "│    │    └─bn.weight                                                        ├─128\n",
            "│    │    └─bn.bias                                                          └─128\n",
            "│    │    └─Conv2d: 3-5                            [32, 128, 28, 28]         73,728\n",
            "│    │    │    └─weight                                                      └─73,728\n",
            "│    │    └─BatchNorm2d: 3-6                       [32, 128, 28, 28]         256\n",
            "│    │    │    └─weight                                                      ├─128\n",
            "│    │    │    └─bias                                                        └─128\n",
            "│    └─Hardswish: 2-6                              [32, 128, 28, 28]         --\n",
            "│    └─ConvNorm: 2-7                               [32, 256, 14, 14]         --\n",
            "│    │    └─linear.weight                                                    ├─294,912\n",
            "│    │    └─bn.weight                                                        ├─256\n",
            "│    │    └─bn.bias                                                          └─256\n",
            "│    │    └─Conv2d: 3-7                            [32, 256, 14, 14]         294,912\n",
            "│    │    │    └─weight                                                      └─294,912\n",
            "│    │    └─BatchNorm2d: 3-8                       [32, 256, 14, 14]         512\n",
            "│    │    │    └─weight                                                      ├─256\n",
            "│    │    │    └─bias                                                        └─256\n",
            "├─HoViTStage: 1-2                                  [32, 196, 256]            --\n",
            "│    └─blocks.0.attn.q.weight                                                ├─65,536\n",
            "│    └─blocks.0.attn.kv.weight                                               ├─131,072\n",
            "│    └─blocks.0.attn.proj.weight                                             ├─65,536\n",
            "│    └─blocks.0.attn.proj.bias                                               ├─256\n",
            "│    └─blocks.0.mlp.ln1.linear.weight                                        ├─131,072\n",
            "│    └─blocks.0.mlp.ln1.bn.weight                                            ├─512\n",
            "│    └─blocks.0.mlp.ln1.bn.bias                                              ├─512\n",
            "│    └─blocks.0.mlp.ln2.linear.weight                                        ├─131,072\n",
            "│    └─blocks.0.mlp.ln2.bn.weight                                            ├─256\n",
            "│    └─blocks.0.mlp.ln2.bn.bias                                              ├─256\n",
            "│    └─blocks.1.attn.q.weight                                                ├─65,536\n",
            "│    └─blocks.1.attn.kv.weight                                               ├─131,072\n",
            "│    └─blocks.1.attn.proj.weight                                             ├─65,536\n",
            "│    └─blocks.1.attn.proj.bias                                               ├─256\n",
            "│    └─blocks.1.mlp.ln1.linear.weight                                        ├─131,072\n",
            "│    └─blocks.1.mlp.ln1.bn.weight                                            ├─512\n",
            "│    └─blocks.1.mlp.ln1.bn.bias                                              ├─512\n",
            "│    └─blocks.1.mlp.ln2.linear.weight                                        ├─131,072\n",
            "│    └─blocks.1.mlp.ln2.bn.weight                                            ├─256\n",
            "│    └─blocks.1.mlp.ln2.bn.bias                                              └─256\n",
            "│    └─Identity: 2-8                               [32, 196, 256]            --\n",
            "│    └─Sequential: 2-9                             [32, 196, 256]            --\n",
            "│    │    └─0.attn.q.weight                                                  ├─65,536\n",
            "│    │    └─0.attn.kv.weight                                                 ├─131,072\n",
            "│    │    └─0.attn.proj.weight                                               ├─65,536\n",
            "│    │    └─0.attn.proj.bias                                                 ├─256\n",
            "│    │    └─0.mlp.ln1.linear.weight                                          ├─131,072\n",
            "│    │    └─0.mlp.ln1.bn.weight                                              ├─512\n",
            "│    │    └─0.mlp.ln1.bn.bias                                                ├─512\n",
            "│    │    └─0.mlp.ln2.linear.weight                                          ├─131,072\n",
            "│    │    └─0.mlp.ln2.bn.weight                                              ├─256\n",
            "│    │    └─0.mlp.ln2.bn.bias                                                ├─256\n",
            "│    │    └─1.attn.q.weight                                                  ├─65,536\n",
            "│    │    └─1.attn.kv.weight                                                 ├─131,072\n",
            "│    │    └─1.attn.proj.weight                                               ├─65,536\n",
            "│    │    └─1.attn.proj.bias                                                 ├─256\n",
            "│    │    └─1.mlp.ln1.linear.weight                                          ├─131,072\n",
            "│    │    └─1.mlp.ln1.bn.weight                                              ├─512\n",
            "│    │    └─1.mlp.ln1.bn.bias                                                ├─512\n",
            "│    │    └─1.mlp.ln2.linear.weight                                          ├─131,072\n",
            "│    │    └─1.mlp.ln2.bn.weight                                              ├─256\n",
            "│    │    └─1.mlp.ln2.bn.bias                                                └─256\n",
            "│    │    └─HoViTBlock: 3-9                        [32, 196, 256]            526,080\n",
            "│    │    │    └─attn.q.weight                                               ├─65,536\n",
            "│    │    │    └─attn.kv.weight                                              ├─131,072\n",
            "│    │    │    └─attn.proj.weight                                            ├─65,536\n",
            "│    │    │    └─attn.proj.bias                                              ├─256\n",
            "│    │    │    └─mlp.ln1.linear.weight                                       ├─131,072\n",
            "│    │    │    └─mlp.ln1.bn.weight                                           ├─512\n",
            "│    │    │    └─mlp.ln1.bn.bias                                             ├─512\n",
            "│    │    │    └─mlp.ln2.linear.weight                                       ├─131,072\n",
            "│    │    │    └─mlp.ln2.bn.weight                                           ├─256\n",
            "│    │    │    └─mlp.ln2.bn.bias                                             └─256\n",
            "│    │    └─HoViTBlock: 3-10                       [32, 196, 256]            526,080\n",
            "│    │    │    └─attn.q.weight                                               ├─65,536\n",
            "│    │    │    └─attn.kv.weight                                              ├─131,072\n",
            "│    │    │    └─attn.proj.weight                                            ├─65,536\n",
            "│    │    │    └─attn.proj.bias                                              ├─256\n",
            "│    │    │    └─mlp.ln1.linear.weight                                       ├─131,072\n",
            "│    │    │    └─mlp.ln1.bn.weight                                           ├─512\n",
            "│    │    │    └─mlp.ln1.bn.bias                                             ├─512\n",
            "│    │    │    └─mlp.ln2.linear.weight                                       ├─131,072\n",
            "│    │    │    └─mlp.ln2.bn.weight                                           ├─256\n",
            "│    │    │    └─mlp.ln2.bn.bias                                             └─256\n",
            "├─HoViTStage: 1-3                                  [32, 49, 384]             --\n",
            "│    └─downsample.conv.weight                                                ├─884,736\n",
            "│    └─downsample.conv.bias                                                  ├─384\n",
            "│    └─blocks.0.attn.q.weight                                                ├─147,456\n",
            "│    └─blocks.0.attn.kv.weight                                               ├─294,912\n",
            "│    └─blocks.0.attn.proj.weight                                             ├─147,456\n",
            "│    └─blocks.0.attn.proj.bias                                               ├─384\n",
            "│    └─blocks.0.mlp.ln1.linear.weight                                        ├─294,912\n",
            "│    └─blocks.0.mlp.ln1.bn.weight                                            ├─768\n",
            "│    └─blocks.0.mlp.ln1.bn.bias                                              ├─768\n",
            "│    └─blocks.0.mlp.ln2.linear.weight                                        ├─294,912\n",
            "│    └─blocks.0.mlp.ln2.bn.weight                                            ├─384\n",
            "│    └─blocks.0.mlp.ln2.bn.bias                                              ├─384\n",
            "│    └─blocks.1.attn.q.weight                                                ├─147,456\n",
            "│    └─blocks.1.attn.kv.weight                                               ├─294,912\n",
            "│    └─blocks.1.attn.proj.weight                                             ├─147,456\n",
            "│    └─blocks.1.attn.proj.bias                                               ├─384\n",
            "│    └─blocks.1.mlp.ln1.linear.weight                                        ├─294,912\n",
            "│    └─blocks.1.mlp.ln1.bn.weight                                            ├─768\n",
            "│    └─blocks.1.mlp.ln1.bn.bias                                              ├─768\n",
            "│    └─blocks.1.mlp.ln2.linear.weight                                        ├─294,912\n",
            "│    └─blocks.1.mlp.ln2.bn.weight                                            ├─384\n",
            "│    └─blocks.1.mlp.ln2.bn.bias                                              └─384\n",
            "│    └─CNNDownsample: 2-10                         [32, 49, 384]             --\n",
            "│    │    └─conv.weight                                                      ├─884,736\n",
            "│    │    └─conv.bias                                                        └─384\n",
            "│    │    └─Conv2d: 3-11                           [32, 384, 7, 7]           885,120\n",
            "│    │    │    └─weight                                                      ├─884,736\n",
            "│    │    │    └─bias                                                        └─384\n",
            "│    │    └─Hardswish: 3-12                        [32, 384, 7, 7]           --\n",
            "│    └─Sequential: 2-11                            [32, 49, 384]             --\n",
            "│    │    └─0.attn.q.weight                                                  ├─147,456\n",
            "│    │    └─0.attn.kv.weight                                                 ├─294,912\n",
            "│    │    └─0.attn.proj.weight                                               ├─147,456\n",
            "│    │    └─0.attn.proj.bias                                                 ├─384\n",
            "│    │    └─0.mlp.ln1.linear.weight                                          ├─294,912\n",
            "│    │    └─0.mlp.ln1.bn.weight                                              ├─768\n",
            "│    │    └─0.mlp.ln1.bn.bias                                                ├─768\n",
            "│    │    └─0.mlp.ln2.linear.weight                                          ├─294,912\n",
            "│    │    └─0.mlp.ln2.bn.weight                                              ├─384\n",
            "│    │    └─0.mlp.ln2.bn.bias                                                ├─384\n",
            "│    │    └─1.attn.q.weight                                                  ├─147,456\n",
            "│    │    └─1.attn.kv.weight                                                 ├─294,912\n",
            "│    │    └─1.attn.proj.weight                                               ├─147,456\n",
            "│    │    └─1.attn.proj.bias                                                 ├─384\n",
            "│    │    └─1.mlp.ln1.linear.weight                                          ├─294,912\n",
            "│    │    └─1.mlp.ln1.bn.weight                                              ├─768\n",
            "│    │    └─1.mlp.ln1.bn.bias                                                ├─768\n",
            "│    │    └─1.mlp.ln2.linear.weight                                          ├─294,912\n",
            "│    │    └─1.mlp.ln2.bn.weight                                              ├─384\n",
            "│    │    └─1.mlp.ln2.bn.bias                                                └─384\n",
            "│    │    └─HoViTBlock: 3-13                       [32, 49, 384]             1,182,336\n",
            "│    │    │    └─attn.q.weight                                               ├─147,456\n",
            "│    │    │    └─attn.kv.weight                                              ├─294,912\n",
            "│    │    │    └─attn.proj.weight                                            ├─147,456\n",
            "│    │    │    └─attn.proj.bias                                              ├─384\n",
            "│    │    │    └─mlp.ln1.linear.weight                                       ├─294,912\n",
            "│    │    │    └─mlp.ln1.bn.weight                                           ├─768\n",
            "│    │    │    └─mlp.ln1.bn.bias                                             ├─768\n",
            "│    │    │    └─mlp.ln2.linear.weight                                       ├─294,912\n",
            "│    │    │    └─mlp.ln2.bn.weight                                           ├─384\n",
            "│    │    │    └─mlp.ln2.bn.bias                                             └─384\n",
            "│    │    └─HoViTBlock: 3-14                       [32, 49, 384]             1,182,336\n",
            "│    │    │    └─attn.q.weight                                               ├─147,456\n",
            "│    │    │    └─attn.kv.weight                                              ├─294,912\n",
            "│    │    │    └─attn.proj.weight                                            ├─147,456\n",
            "│    │    │    └─attn.proj.bias                                              ├─384\n",
            "│    │    │    └─mlp.ln1.linear.weight                                       ├─294,912\n",
            "│    │    │    └─mlp.ln1.bn.weight                                           ├─768\n",
            "│    │    │    └─mlp.ln1.bn.bias                                             ├─768\n",
            "│    │    │    └─mlp.ln2.linear.weight                                       ├─294,912\n",
            "│    │    │    └─mlp.ln2.bn.weight                                           ├─384\n",
            "│    │    │    └─mlp.ln2.bn.bias                                             └─384\n",
            "├─Sequential: 1-4                                  [32, 512, 7, 7]           --\n",
            "│    └─0.weight                                                              ├─196,608\n",
            "│    └─0.bias                                                                ├─512\n",
            "│    └─1.weight                                                              ├─512\n",
            "│    └─1.bias                                                                └─512\n",
            "│    └─Conv2d: 2-12                                [32, 512, 7, 7]           197,120\n",
            "│    │    └─weight                                                           ├─196,608\n",
            "│    │    └─bias                                                             └─512\n",
            "│    └─BatchNorm2d: 2-13                           [32, 512, 7, 7]           1,024\n",
            "│    │    └─weight                                                           ├─512\n",
            "│    │    └─bias                                                             └─512\n",
            "│    └─ReLU: 2-14                                  [32, 512, 7, 7]           --\n",
            "├─NormLinear: 1-5                                  [32, 9]                   --\n",
            "│    └─bn.weight                                                             ├─512\n",
            "│    └─bn.bias                                                               ├─512\n",
            "│    └─linear.weight                                                         ├─4,608\n",
            "│    └─linear.bias                                                           └─9\n",
            "│    └─BatchNorm1d: 2-15                           [32, 512]                 1,024\n",
            "│    │    └─weight                                                           ├─512\n",
            "│    │    └─bias                                                             └─512\n",
            "│    └─Dropout: 2-16                               [32, 512]                 --\n",
            "│    └─Linear: 2-17                                [32, 9]                   4,617\n",
            "│    │    └─weight                                                           ├─4,608\n",
            "│    │    └─bias                                                             └─9\n",
            "├─NormLinear: 1-6                                  [32, 9]                   --\n",
            "│    └─bn.weight                                                             ├─512\n",
            "│    └─bn.bias                                                               ├─512\n",
            "│    └─linear.weight                                                         ├─4,608\n",
            "│    └─linear.bias                                                           └─9\n",
            "│    └─BatchNorm1d: 2-18                           [32, 512]                 1,024\n",
            "│    │    └─weight                                                           ├─512\n",
            "│    │    └─bias                                                             └─512\n",
            "│    └─Dropout: 2-19                               [32, 512]                 --\n",
            "│    └─Linear: 2-20                                [32, 9]                   4,617\n",
            "│    │    └─weight                                                           ├─4,608\n",
            "│    │    └─bias                                                             └─9\n",
            "====================================================================================================\n",
            "Total params: 4,900,274\n",
            "Trainable params: 4,900,274\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (Units.GIGABYTES): 12.81\n",
            "====================================================================================================\n",
            "Input size (MB): 19.27\n",
            "Forward/backward pass size (MB): 756.52\n",
            "Params size (MB): 19.60\n",
            "Estimated Total Size (MB): 795.39\n",
            "====================================================================================================\n"
          ]
        }
      ],
      "source": [
        "print(summary(model, input_size=(32, 3, 224, 224), verbose=2))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.init as init\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)  # CUDA 연산 시 동일한 결과 보장\n",
        "    torch.cuda.manual_seed_all(seed)  # 멀티-GPU 환경에서 동일한 결과 보장\n",
        "    torch.backends.cudnn.deterministic = True  # CuDNN 연산을 deterministic하게 설정\n",
        "    torch.backends.cudnn.benchmark = False  # 연산 속도를 희생하고 일관된 연산을 수행\n",
        "\n",
        "def initialize_weights(m):\n",
        "    if isinstance(m, nn.Conv2d):  # Conv 레이어 초기화\n",
        "        init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "        if m.bias is not None:\n",
        "            init.constant_(m.bias, 0)\n",
        "    elif isinstance(m, nn.Linear):  # Linear 레이어 초기화\n",
        "        init.xavier_uniform_(m.weight)\n",
        "        if m.bias is not None:\n",
        "            init.constant_(m.bias, 0)\n",
        "    elif isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):  # BatchNorm 초기화\n",
        "        init.constant_(m.weight, 1)\n",
        "        init.constant_(m.bias, 0)\n",
        "\n",
        "set_seed(42)  # 랜덤 시드 고정\n",
        "model.apply(initialize_weights)"
      ],
      "metadata": {
        "id": "cg98su-daBUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = './train/NCT-CRC-HE-100K'"
      ],
      "metadata": {
        "id": "p5GDxFHzR-y7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])"
      ],
      "metadata": {
        "id": "mAsRwMpISMIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = datasets.ImageFolder(root=train_dir, transform=transform)"
      ],
      "metadata": {
        "id": "fnlxaQSs7NsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open(\"../data/index_dict.json\", \"r\") as f:\n",
        "    index_dict = json.load(f)\n",
        "load_train_idx = index_dict[\"train_idx\"]\n",
        "load_val_idx = index_dict[\"val_idx\"]\n",
        "load_test_idx = index_dict[\"test_idx\"]"
      ],
      "metadata": {
        "id": "_7jaLSYMaFZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = Subset(dataset, load_train_idx)\n",
        "val_data = Subset(dataset, load_val_idx)\n",
        "test_data = Subset(dataset, load_test_idx)"
      ],
      "metadata": {
        "id": "dHwuGFkvMc89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8inM-bBnztEN"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=False)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(f\"Train set size: {len(train_data)}\")\n",
        "print(f\"Validation set size: {len(val_data)}\")\n",
        "print(f\"Test set size: {len(test_data)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Show one example per class from the training dataset\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Prepare a dictionary to store one example per class\n",
        "class_examples = {}\n",
        "for images, labels in train_loader:\n",
        "    for img, label in zip(images, labels):\n",
        "        label = label.item()\n",
        "        if label not in class_examples:\n",
        "            class_examples[label] = img\n",
        "        # Stop if we already have one example for each class\n",
        "        if len(class_examples) == len(dataset.classes):\n",
        "            break\n",
        "    if len(class_examples) == len(dataset.classes):\n",
        "        break\n",
        "\n",
        "# Display the images and their labels\n",
        "figure = plt.figure(figsize=(8, 8))\n",
        "cols = 3  # You can adjust the number of columns as needed\n",
        "rows = (len(dataset.classes) + cols - 1) // cols  # Calculate required rows\n",
        "for i, (label, img) in enumerate(class_examples.items(), start=1):\n",
        "    figure.add_subplot(rows, cols, i)\n",
        "    plt.title(dataset.classes[label])\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(img.permute(1, 2, 0).numpy())  # Convert tensor to numpy for display\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CES7XdSPWrHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6a-XyqaXzurh"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses = []\n",
        "train_accuracies = []\n",
        "val_losses = []\n",
        "val_accuracies = []"
      ],
      "metadata": {
        "id": "1No-7jzGgzrV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yf0ZPGLtz_gB"
      },
      "outputs": [],
      "source": [
        "def train(model, train_loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for inputs, labels in tqdm(train_loader, desc=\"Training\"):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Train Loss: {epoch_loss:.4f}, Train Accuracy: {accuracy:.2f}%\")\n",
        "    train_losses.append(epoch_loss)\n",
        "    train_accuracies.append(accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rbdjXJ520Btp"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, data_loader, criterion, device, phase=\"Validation\"):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(data_loader, desc=f\"{phase}\"):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            # Save all labels and predictions for balanced accuracy\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "    epoch_loss = running_loss / len(data_loader)\n",
        "    accuracy = 100 * correct / total\n",
        "    balanced_acc = balanced_accuracy_score(all_labels, all_predictions)\n",
        "\n",
        "    print(f\"{phase} Loss: {epoch_loss:.4f}, {phase} Accuracy: {accuracy:.2f}%\")\n",
        "    print(f\"Balanced Accuracy: {balanced_acc:.4f}\")\n",
        "\n",
        "    val_losses.append(epoch_loss)\n",
        "    val_accuracies.append(accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GfBjePI-0DkY"
      },
      "outputs": [],
      "source": [
        "def measure_inference_time(model, data_loader, device):\n",
        "    model.eval()\n",
        "    times = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, _ in data_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            start_time = torch.cuda.Event(enable_timing=True)\n",
        "            end_time = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "            start_time.record()\n",
        "            _ = model(inputs)  # inference 수행\n",
        "            end_time.record()\n",
        "\n",
        "            # 시간 측정\n",
        "            torch.cuda.synchronize()  # CUDA에서 모든 커널이 완료될 때까지 대기\n",
        "            elapsed_time = start_time.elapsed_time(end_time)  # 밀리초 단위로 반환\n",
        "            times.append(elapsed_time)\n",
        "\n",
        "    # 통계량 계산\n",
        "    times_np = np.array(times)\n",
        "    total_inferences = len(times_np)\n",
        "    avg_time = np.mean(times_np)\n",
        "    std_dev = np.std(times_np)\n",
        "    max_time = np.max(times_np)\n",
        "    min_time = np.min(times_np)\n",
        "\n",
        "    # 결과 출력\n",
        "    print(f\"Inference Time Measurement Results:\")\n",
        "    print(f\"Total Inferences: {total_inferences}\")\n",
        "    print(f\"Average Time: {avg_time:.2f} ms\")\n",
        "    print(f\"Standard Deviation: {std_dev:.2f} ms\")\n",
        "    print(f\"Maximum Time: {max_time:.2f} ms\")\n",
        "    print(f\"Minimum Time: {min_time:.2f} ms\")\n",
        "\n",
        "    return times"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXsiOTOO0GGK"
      },
      "outputs": [],
      "source": [
        "for epoch in range(num_epochs):\n",
        "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "    train(model, train_loader, criterion, optimizer, device)\n",
        "    evaluate(model, val_loader, criterion, device, phase=\"Validation\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nFinal Test Evaluation\")\n",
        "evaluate(model, test_loader, criterion, device, phase=\"Test\")"
      ],
      "metadata": {
        "id": "yq3Yxnau6V3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"train_loss length : \", len(train_losses))\n",
        "print(\"train_acc length : \", len(train_accuracies))\n",
        "print(\"val_loss length : \", len(val_losses))\n",
        "print(\"val_acc length : \", len(val_accuracies))\n",
        "\n",
        "print(\"val_adj : \", len(val_losses[:num_epochs]))\n",
        "print(\"val_acc_adj : \", len(val_accuracies[:num_epochs]))"
      ],
      "metadata": {
        "id": "W4SESq4qtmw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the metrics\n",
        "epochs = range(1, num_epochs + 1)\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Plot training and validation loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs, train_losses, label=\"Train Loss\")\n",
        "plt.plot(epochs, val_losses[:num_epochs], label=\"Validation Loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training and Validation Loss\")\n",
        "plt.legend()\n",
        "\n",
        "# Plot training and validation accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs, train_accuracies, label=\"Train Accuracy\")\n",
        "plt.plot(epochs, val_accuracies[:num_epochs], label=\"Validation Accuracy\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Training and Validation Accuracy\")\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "I0f2EnimhD4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dlazeL-a0WTt"
      },
      "outputs": [],
      "source": [
        "times = measure_inference_time(model, test_loader, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c77rmtOz0XuA"
      },
      "outputs": [],
      "source": [
        "from torch import profiler\n",
        "\n",
        "dummy_input = torch.randn(32, 3, 224, 224).cuda()\n",
        "\n",
        "# Profiling inference\n",
        "with profiler.profile(\n",
        "    activities=[\n",
        "       profiler.ProfilerActivity.CPU,\n",
        "        profiler.ProfilerActivity.CUDA,  # Include if using GPU\n",
        "    ],\n",
        "    on_trace_ready=profiler.tensorboard_trace_handler(\"./logs\"),  # Optional logging\n",
        "    record_shapes=True,\n",
        "    with_stack=True\n",
        ") as prof:\n",
        "    with torch.no_grad():\n",
        "        model(dummy_input)\n",
        "\n",
        "\n",
        "# Print results\n",
        "print(prof.key_averages().table(sort_by=\"cuda_time_total\" if torch.cuda.is_available() else \"cpu_time_total\", row_limit=10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YiQQmg-d0aG7",
        "outputId": "5569d055-5aec-4a6e-bb33-824cc34eacb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model weights saved to HoViT_NCT-CRC-HE-100K.pth\n"
          ]
        }
      ],
      "source": [
        "save_path = \"HoViT_FLatten_NCT-CRC-HE-100K.pth\"\n",
        "torch.save(model.state_dict(), save_path)\n",
        "print(f\"Model weights saved to {save_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXvKsupvZYjZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score, recall_score, precision_score, roc_curve, auc\n",
        "\n",
        "def score_evaluate(model, data_loader, criterion, device, phase=\"Validation\"):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(data_loader, desc=f\"{phase}\"):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "\n",
        "    overall_f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
        "    overall_recall = recall_score(all_labels, all_preds, average=\"macro\")\n",
        "    overall_precision = precision_score(all_labels, all_preds, average=\"macro\")\n",
        "\n",
        "    f1_per_class = f1_score(all_labels, all_preds, average=None)\n",
        "    recall_per_class = recall_score(all_labels, all_preds, average=None)\n",
        "    precision_per_class = precision_score(all_labels, all_preds, average=None)\n",
        "    class_labels = sorted(set(all_labels))\n",
        "\n",
        "    epoch_loss = running_loss / len(data_loader)\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"{phase} Loss: {epoch_loss:.4f}, {phase} Accuracy: {accuracy:.2f}%\")\n",
        "    val_losses.append(epoch_loss)\n",
        "    val_accuracies.append(accuracy)\n",
        "\n",
        "    print(f\"Overall - F1: {overall_f1:.4f}, Recall: {overall_recall:.4f}, Precision: {overall_precision:.4f}\")\n",
        "    print(\"Per-Class Metrics:\")\n",
        "    for i, label in enumerate(class_labels):\n",
        "        print(f\"Class {label} - F1: {f1_per_class[i]:.4f}, Recall: {recall_per_class[i]:.4f}, Precision: {precision_per_class[i]:.4f}\")\n",
        "\n",
        "    return overall_f1, overall_recall, overall_precision, f1_per_class, recall_per_class, precision_per_class"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "def roc_auc(model, data_loader, device, num_classes):\n",
        "\n",
        "    y = [\"F1\", \"Precision\", \"Recall\"]\n",
        "\n",
        "    model.eval()\n",
        "    all_labels = []\n",
        "    all_scores = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(data_loader, desc=\"ROC AUC\"):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_scores.extend(outputs.cpu().numpy())\n",
        "\n",
        "    all_labels = label_binarize(all_labels, classes=list(range(num_classes)))\n",
        "    print(f\"Binarized all_labels shape: {all_labels.shape}\")\n",
        "    print(f\"All_scores shape: {np.array(all_scores).shape}\")\n",
        "\n",
        "    fpr, tpr, _ = roc_curve(all_labels.ravel(), np.array(all_scores).ravel())\n",
        "    roc_auc_value = auc(fpr, tpr)\n",
        "\n",
        "    plt.figure(figsize=(9, 7))\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc_value:0.4f})')\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"Overall ROC AUC: {roc_auc_value:.4f}\")\n",
        "\n",
        "    return fpr, tpr, roc_auc_value"
      ],
      "metadata": {
        "id": "oe9DO2tS-BpI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "overall_f1, overall_recall, overall_precision, f1_per_class, recall_per_class, precision_per_class = score_evaluate(model, test_loader, criterion, device, phase=\"Test\")"
      ],
      "metadata": {
        "id": "ubbBuFrU-GRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = [\"ADI\", \"BACK\", \"DEB\", \"LYM\", \"MUC\", \"MUS\", \"NORM\", 'STR', \"TUM\"]\n",
        "y = [\"F1\", \"Precision\", \"Recall\"]\n",
        "\n",
        "f1_score_list = []\n",
        "precision_score_list = []\n",
        "recall_score_list = []\n",
        "\n",
        "for i, class_name in enumerate(x):\n",
        "    f1_score_list.append(f1_per_class[i])\n",
        "    precision_score_list.append(precision_per_class[i])\n",
        "    recall_score_list.append(recall_per_class[i])\n",
        "\n",
        "x.append(\"Overall\")\n",
        "f1_score_list.append(overall_f1)\n",
        "precision_score_list.append(overall_precision)\n",
        "recall_score_list.append(overall_recall)\n",
        "\n",
        "score_list = [f1_score_list, precision_score_list, recall_score_list]\n",
        "score_list = np.array(score_list)\n",
        "plt.figure(figsize=(20, 12))\n",
        "score_cm = plt.matshow(score_list, cmap=plt.cm.Blues, alpha=0.3)\n",
        "plt.xticks(range(len(x)), x)\n",
        "plt.yticks(range(len(y)), y)\n",
        "for i in range(len(y)):\n",
        "    for j in range(len(x)):\n",
        "        # 행렬의 각각의 수치를 각 칸의 중앙에 넣어준다\n",
        "        plt.text(x=j, y=i,\n",
        "                     s=\"{:.2f}\".format(score_list[i, j]),\n",
        "                     va='center',\n",
        "                     ha='center',\n",
        "                     )"
      ],
      "metadata": {
        "id": "3qaLb5c--H0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1kuSSo-B-JVp"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}