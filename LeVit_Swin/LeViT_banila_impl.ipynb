{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeong-yonghun/.pyenv/versions/3.10.0/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split, ConcatDataset\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import itertools\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNorm(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=2, padding=1):\n",
    "        super(ConvNorm, self).__init__()\n",
    "        self.linear = nn.Conv2d(\n",
    "            in_channels, out_channels, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding, bias=False\n",
    "        )\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = self.bn(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stem16(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Stem16, self).__init__()\n",
    "        self.conv1 = ConvNorm(3, 32)\n",
    "        self.act1 = nn.Hardswish()\n",
    "        self.conv2 = ConvNorm(32, 64)\n",
    "        self.act2 = nn.Hardswish()\n",
    "        self.conv3 = ConvNorm(64, 128)\n",
    "        self.act3 = nn.Hardswish()\n",
    "        self.conv4 = ConvNorm(128, 256)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act1(self.conv1(x))\n",
    "        x = self.act2(self.conv2(x))\n",
    "        x = self.act3(self.conv3(x))\n",
    "        x = self.conv4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearNorm(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(LinearNorm, self).__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features, bias=False)\n",
    "        self.bn = nn.BatchNorm1d(out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flatten if the input is 3D to apply BatchNorm1d\n",
    "        if x.dim() == 3:\n",
    "            B, N, C = x.shape\n",
    "            x = x.reshape(B * N, C)  # view에서 reshape로 변경\n",
    "            x = self.bn(self.linear(x))\n",
    "            x = x.reshape(B, N, -1)  # 원래 모양으로 다시 reshape\n",
    "        else:\n",
    "            x = self.bn(self.linear(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads, attn_ratio=2):\n",
    "        super(Attention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "        inner_dim = head_dim * num_heads * 3  \n",
    "        self.qkv = LinearNorm(dim, inner_dim)\n",
    "        \n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Hardswish(),\n",
    "            LinearNorm(dim, dim)  \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x)\n",
    "        qkv = qkv.view(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        return self.proj(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LevitMlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features):\n",
    "        super(LevitMlp, self).__init__()\n",
    "        self.ln1 = LinearNorm(in_features, hidden_features)\n",
    "        self.act = nn.Hardswish()\n",
    "        self.drop = nn.Dropout(p=0.0, inplace=False)\n",
    "        self.ln2 = LinearNorm(hidden_features, out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.ln1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.ln2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LevitBlock(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=2):\n",
    "        super(LevitBlock, self).__init__()\n",
    "        self.attn = Attention(dim, num_heads)\n",
    "        self.drop_path1 = nn.Identity()\n",
    "        self.mlp = LevitMlp(dim, dim * mlp_ratio, dim)\n",
    "        self.drop_path2 = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.drop_path1(self.attn(x))\n",
    "        x = x + self.drop_path2(self.mlp(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionDownsample(nn.Module):\n",
    "    def __init__(self, dim, out_dim, num_heads, attn_ratio=2):\n",
    "        super(AttentionDownsample, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.scale = (dim // num_heads) ** -0.5\n",
    "        inner_dim = dim * attn_ratio * num_heads\n",
    "        self.kv = LinearNorm(dim, inner_dim)\n",
    "        \n",
    "        # Downsample 적용\n",
    "        self.q = nn.Sequential(\n",
    "            nn.Conv2d(dim, dim, kernel_size=2, stride=2),\n",
    "            nn.Flatten(start_dim=1)\n",
    "        )\n",
    "        \n",
    "        # proj 부분 수정\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Hardswish(),\n",
    "            LinearNorm(dim, out_dim)  # dim과 out_dim이 올바르게 설정되었는지 확인\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        H = W = int(N ** 0.5)  # N의 제곱근을 통해 H, W 결정 (e.g., N=196 -> H=W=14)\n",
    "        x = x.reshape(B, C, H, W)  # view에서 reshape로 변경하여 연속된 메모리 레이아웃 문제 해결\n",
    "\n",
    "        kv = self.kv(x.flatten(2).transpose(1, 2))  # kv 생성\n",
    "        q = self.q(x)  # 다운샘플링된 q 생성\n",
    "        \n",
    "        # 다운샘플링 후 적절한 차원으로 변환\n",
    "        q = q.reshape(B, -1, C)  # view에서 reshape로 변경\n",
    "        x = self.proj(q)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LevitDownsample(nn.Module):\n",
    "    def __init__(self, dim, out_dim, num_heads, attn_ratio=2):\n",
    "        super(LevitDownsample, self).__init__()\n",
    "        self.attn_downsample = AttentionDownsample(dim, out_dim, num_heads, attn_ratio)\n",
    "        self.mlp = LevitMlp(out_dim, out_dim * attn_ratio, out_dim)\n",
    "        self.drop_path = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.attn_downsample(x)\n",
    "        x = self.drop_path(self.mlp(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LevitStage(nn.Module):\n",
    "    def __init__(self, dim, out_dim, num_heads, num_blocks, downsample=True):\n",
    "        super(LevitStage, self).__init__()\n",
    "        self.downsample = LevitDownsample(dim, out_dim, num_heads) if downsample else nn.Identity()\n",
    "        self.blocks = nn.Sequential(*[LevitBlock(out_dim, num_heads) for _ in range(num_blocks)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.downsample(x)\n",
    "        x = self.blocks(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, dropout_prob=0.0):\n",
    "        super(NormLinear, self).__init__()\n",
    "        self.bn = nn.BatchNorm1d(in_features)\n",
    "        self.drop = nn.Dropout(p=dropout_prob, inplace=False)\n",
    "        self.linear = nn.Linear(in_features, out_features, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bn(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.linear(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LevitDistilled(nn.Module):\n",
    "    def __init__(self, num_classes=37):\n",
    "        super(LevitDistilled, self).__init__()\n",
    "        \n",
    "        # Stem layer\n",
    "        self.stem = Stem16()\n",
    "        \n",
    "        # Stages: LevitStage layers with the adjusted number of blocks and output dimensions\n",
    "        self.stages = nn.Sequential(\n",
    "            LevitStage(dim=256, out_dim=256, num_heads=4, num_blocks=3, downsample=False),\n",
    "            LevitStage(dim=256, out_dim=384, num_heads=6, num_blocks=3, downsample=True),\n",
    "            LevitStage(dim=384, out_dim=512, num_heads=8, num_blocks=2, downsample=True)\n",
    "        )\n",
    "        \n",
    "        # Output heads\n",
    "        self.head = NormLinear(in_features=512, out_features=num_classes, dropout_prob=0.0)\n",
    "        self.head_dist = NormLinear(in_features=512, out_features=num_classes, dropout_prob=0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        B, C, H, W = x.shape\n",
    "        x = x.view(B, C, -1).transpose(1, 2)\n",
    "        x = self.stages(x)\n",
    "        out = self.head(x.mean(dim=1))\n",
    "        out_dist = self.head_dist(x.mean(dim=1))\n",
    "        return out, out_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LevitDistilled(\n",
      "  (stem): Stem16(\n",
      "    (conv1): ConvNorm(\n",
      "      (linear): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (act1): Hardswish()\n",
      "    (conv2): ConvNorm(\n",
      "      (linear): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (act2): Hardswish()\n",
      "    (conv3): ConvNorm(\n",
      "      (linear): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (act3): Hardswish()\n",
      "    (conv4): ConvNorm(\n",
      "      (linear): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (stages): Sequential(\n",
      "    (0): LevitStage(\n",
      "      (downsample): Identity()\n",
      "      (blocks): Sequential(\n",
      "        (0): LevitBlock(\n",
      "          (attn): Attention(\n",
      "            (qkv): LinearNorm(\n",
      "              (linear): Linear(in_features=256, out_features=768, bias=False)\n",
      "              (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "            (proj): Sequential(\n",
      "              (0): Hardswish()\n",
      "              (1): LinearNorm(\n",
      "                (linear): Linear(in_features=256, out_features=256, bias=False)\n",
      "                (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (drop_path1): Identity()\n",
      "          (mlp): LevitMlp(\n",
      "            (ln1): LinearNorm(\n",
      "              (linear): Linear(in_features=256, out_features=512, bias=False)\n",
      "              (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "            (act): Hardswish()\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "            (ln2): LinearNorm(\n",
      "              (linear): Linear(in_features=512, out_features=256, bias=False)\n",
      "              (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (drop_path2): Identity()\n",
      "        )\n",
      "        (1): LevitBlock(\n",
      "          (attn): Attention(\n",
      "            (qkv): LinearNorm(\n",
      "              (linear): Linear(in_features=256, out_features=768, bias=False)\n",
      "              (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "            (proj): Sequential(\n",
      "              (0): Hardswish()\n",
      "              (1): LinearNorm(\n",
      "                (linear): Linear(in_features=256, out_features=256, bias=False)\n",
      "                (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (drop_path1): Identity()\n",
      "          (mlp): LevitMlp(\n",
      "            (ln1): LinearNorm(\n",
      "              (linear): Linear(in_features=256, out_features=512, bias=False)\n",
      "              (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "            (act): Hardswish()\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "            (ln2): LinearNorm(\n",
      "              (linear): Linear(in_features=512, out_features=256, bias=False)\n",
      "              (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (drop_path2): Identity()\n",
      "        )\n",
      "        (2): LevitBlock(\n",
      "          (attn): Attention(\n",
      "            (qkv): LinearNorm(\n",
      "              (linear): Linear(in_features=256, out_features=768, bias=False)\n",
      "              (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "            (proj): Sequential(\n",
      "              (0): Hardswish()\n",
      "              (1): LinearNorm(\n",
      "                (linear): Linear(in_features=256, out_features=256, bias=False)\n",
      "                (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (drop_path1): Identity()\n",
      "          (mlp): LevitMlp(\n",
      "            (ln1): LinearNorm(\n",
      "              (linear): Linear(in_features=256, out_features=512, bias=False)\n",
      "              (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "            (act): Hardswish()\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "            (ln2): LinearNorm(\n",
      "              (linear): Linear(in_features=512, out_features=256, bias=False)\n",
      "              (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (drop_path2): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): LevitStage(\n",
      "      (downsample): LevitDownsample(\n",
      "        (attn_downsample): AttentionDownsample(\n",
      "          (kv): LinearNorm(\n",
      "            (linear): Linear(in_features=256, out_features=3072, bias=False)\n",
      "            (bn): BatchNorm1d(3072, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (q): Sequential(\n",
      "            (0): Conv2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "            (1): Flatten(start_dim=1, end_dim=-1)\n",
      "          )\n",
      "          (proj): Sequential(\n",
      "            (0): Hardswish()\n",
      "            (1): LinearNorm(\n",
      "              (linear): Linear(in_features=256, out_features=384, bias=False)\n",
      "              (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (mlp): LevitMlp(\n",
      "          (ln1): LinearNorm(\n",
      "            (linear): Linear(in_features=384, out_features=768, bias=False)\n",
      "            (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (act): Hardswish()\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "          (ln2): LinearNorm(\n",
      "            (linear): Linear(in_features=768, out_features=384, bias=False)\n",
      "            (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "      )\n",
      "      (blocks): Sequential(\n",
      "        (0): LevitBlock(\n",
      "          (attn): Attention(\n",
      "            (qkv): LinearNorm(\n",
      "              (linear): Linear(in_features=384, out_features=1152, bias=False)\n",
      "              (bn): BatchNorm1d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "            (proj): Sequential(\n",
      "              (0): Hardswish()\n",
      "              (1): LinearNorm(\n",
      "                (linear): Linear(in_features=384, out_features=384, bias=False)\n",
      "                (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (drop_path1): Identity()\n",
      "          (mlp): LevitMlp(\n",
      "            (ln1): LinearNorm(\n",
      "              (linear): Linear(in_features=384, out_features=768, bias=False)\n",
      "              (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "            (act): Hardswish()\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "            (ln2): LinearNorm(\n",
      "              (linear): Linear(in_features=768, out_features=384, bias=False)\n",
      "              (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (drop_path2): Identity()\n",
      "        )\n",
      "        (1): LevitBlock(\n",
      "          (attn): Attention(\n",
      "            (qkv): LinearNorm(\n",
      "              (linear): Linear(in_features=384, out_features=1152, bias=False)\n",
      "              (bn): BatchNorm1d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "            (proj): Sequential(\n",
      "              (0): Hardswish()\n",
      "              (1): LinearNorm(\n",
      "                (linear): Linear(in_features=384, out_features=384, bias=False)\n",
      "                (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (drop_path1): Identity()\n",
      "          (mlp): LevitMlp(\n",
      "            (ln1): LinearNorm(\n",
      "              (linear): Linear(in_features=384, out_features=768, bias=False)\n",
      "              (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "            (act): Hardswish()\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "            (ln2): LinearNorm(\n",
      "              (linear): Linear(in_features=768, out_features=384, bias=False)\n",
      "              (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (drop_path2): Identity()\n",
      "        )\n",
      "        (2): LevitBlock(\n",
      "          (attn): Attention(\n",
      "            (qkv): LinearNorm(\n",
      "              (linear): Linear(in_features=384, out_features=1152, bias=False)\n",
      "              (bn): BatchNorm1d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "            (proj): Sequential(\n",
      "              (0): Hardswish()\n",
      "              (1): LinearNorm(\n",
      "                (linear): Linear(in_features=384, out_features=384, bias=False)\n",
      "                (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (drop_path1): Identity()\n",
      "          (mlp): LevitMlp(\n",
      "            (ln1): LinearNorm(\n",
      "              (linear): Linear(in_features=384, out_features=768, bias=False)\n",
      "              (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "            (act): Hardswish()\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "            (ln2): LinearNorm(\n",
      "              (linear): Linear(in_features=768, out_features=384, bias=False)\n",
      "              (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (drop_path2): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): LevitStage(\n",
      "      (downsample): LevitDownsample(\n",
      "        (attn_downsample): AttentionDownsample(\n",
      "          (kv): LinearNorm(\n",
      "            (linear): Linear(in_features=384, out_features=6144, bias=False)\n",
      "            (bn): BatchNorm1d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (q): Sequential(\n",
      "            (0): Conv2d(384, 384, kernel_size=(2, 2), stride=(2, 2))\n",
      "            (1): Flatten(start_dim=1, end_dim=-1)\n",
      "          )\n",
      "          (proj): Sequential(\n",
      "            (0): Hardswish()\n",
      "            (1): LinearNorm(\n",
      "              (linear): Linear(in_features=384, out_features=512, bias=False)\n",
      "              (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (mlp): LevitMlp(\n",
      "          (ln1): LinearNorm(\n",
      "            (linear): Linear(in_features=512, out_features=1024, bias=False)\n",
      "            (bn): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (act): Hardswish()\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "          (ln2): LinearNorm(\n",
      "            (linear): Linear(in_features=1024, out_features=512, bias=False)\n",
      "            (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "      )\n",
      "      (blocks): Sequential(\n",
      "        (0): LevitBlock(\n",
      "          (attn): Attention(\n",
      "            (qkv): LinearNorm(\n",
      "              (linear): Linear(in_features=512, out_features=1536, bias=False)\n",
      "              (bn): BatchNorm1d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "            (proj): Sequential(\n",
      "              (0): Hardswish()\n",
      "              (1): LinearNorm(\n",
      "                (linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (drop_path1): Identity()\n",
      "          (mlp): LevitMlp(\n",
      "            (ln1): LinearNorm(\n",
      "              (linear): Linear(in_features=512, out_features=1024, bias=False)\n",
      "              (bn): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "            (act): Hardswish()\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "            (ln2): LinearNorm(\n",
      "              (linear): Linear(in_features=1024, out_features=512, bias=False)\n",
      "              (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (drop_path2): Identity()\n",
      "        )\n",
      "        (1): LevitBlock(\n",
      "          (attn): Attention(\n",
      "            (qkv): LinearNorm(\n",
      "              (linear): Linear(in_features=512, out_features=1536, bias=False)\n",
      "              (bn): BatchNorm1d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "            (proj): Sequential(\n",
      "              (0): Hardswish()\n",
      "              (1): LinearNorm(\n",
      "                (linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (drop_path1): Identity()\n",
      "          (mlp): LevitMlp(\n",
      "            (ln1): LinearNorm(\n",
      "              (linear): Linear(in_features=512, out_features=1024, bias=False)\n",
      "              (bn): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "            (act): Hardswish()\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "            (ln2): LinearNorm(\n",
      "              (linear): Linear(in_features=1024, out_features=512, bias=False)\n",
      "              (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (drop_path2): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (head): NormLinear(\n",
      "    (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (linear): Linear(in_features=512, out_features=37, bias=True)\n",
      "  )\n",
      "  (head_dist): NormLinear(\n",
      "    (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (linear): Linear(in_features=512, out_features=37, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = LevitDistilled()\n",
    "print(model)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================================================================================\n",
      "Layer (type:depth-idx)                                       Output Shape              Param #\n",
      "==============================================================================================================\n",
      "LevitDistilled                                               [32, 37]                  --\n",
      "├─Stem16: 1-1                                                [32, 256, 14, 14]         --\n",
      "│    └─ConvNorm: 2-1                                         [32, 32, 112, 112]        --\n",
      "│    │    └─Conv2d: 3-1                                      [32, 32, 112, 112]        864\n",
      "│    │    └─BatchNorm2d: 3-2                                 [32, 32, 112, 112]        64\n",
      "│    └─Hardswish: 2-2                                        [32, 32, 112, 112]        --\n",
      "│    └─ConvNorm: 2-3                                         [32, 64, 56, 56]          --\n",
      "│    │    └─Conv2d: 3-3                                      [32, 64, 56, 56]          18,432\n",
      "│    │    └─BatchNorm2d: 3-4                                 [32, 64, 56, 56]          128\n",
      "│    └─Hardswish: 2-4                                        [32, 64, 56, 56]          --\n",
      "│    └─ConvNorm: 2-5                                         [32, 128, 28, 28]         --\n",
      "│    │    └─Conv2d: 3-5                                      [32, 128, 28, 28]         73,728\n",
      "│    │    └─BatchNorm2d: 3-6                                 [32, 128, 28, 28]         256\n",
      "│    └─Hardswish: 2-6                                        [32, 128, 28, 28]         --\n",
      "│    └─ConvNorm: 2-7                                         [32, 256, 14, 14]         --\n",
      "│    │    └─Conv2d: 3-7                                      [32, 256, 14, 14]         294,912\n",
      "│    │    └─BatchNorm2d: 3-8                                 [32, 256, 14, 14]         512\n",
      "├─Sequential: 1-2                                            [32, 9, 512]              --\n",
      "│    └─LevitStage: 2-8                                       [32, 196, 256]            --\n",
      "│    │    └─Identity: 3-9                                    [32, 196, 256]            --\n",
      "│    │    └─Sequential: 3-10                                 [32, 196, 256]            1,583,616\n",
      "│    └─LevitStage: 2-9                                       [32, 49, 384]             --\n",
      "│    │    └─LevitDownsample: 3-11                            [32, 49, 384]             1,746,176\n",
      "│    │    └─Sequential: 3-12                                 [32, 49, 384]             3,555,072\n",
      "│    └─LevitStage: 2-10                                      [32, 9, 512]              --\n",
      "│    │    └─LevitDownsample: 3-13                            [32, 9, 512]              4,211,072\n",
      "│    │    └─Sequential: 3-14                                 [32, 9, 512]              4,208,640\n",
      "├─NormLinear: 1-3                                            [32, 37]                  --\n",
      "│    └─BatchNorm1d: 2-11                                     [32, 512]                 1,024\n",
      "│    └─Dropout: 2-12                                         [32, 512]                 --\n",
      "│    └─Linear: 2-13                                          [32, 37]                  18,981\n",
      "├─NormLinear: 1-4                                            [32, 37]                  --\n",
      "│    └─BatchNorm1d: 2-14                                     [32, 512]                 1,024\n",
      "│    └─Dropout: 2-15                                         [32, 512]                 --\n",
      "│    └─Linear: 2-16                                          [32, 37]                  18,981\n",
      "==============================================================================================================\n",
      "Total params: 15,733,482\n",
      "Trainable params: 15,733,482\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 33.33\n",
      "==============================================================================================================\n",
      "Input size (MB): 19.27\n",
      "Forward/backward pass size (MB): 1674.96\n",
      "Params size (MB): 62.93\n",
      "Estimated Total Size (MB): 1757.16\n",
      "==============================================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(summary(model, input_size=(32, 3, 224, 224)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://thor.robots.ox.ac.uk/pets/images.tar.gz to data/oxford-iiit-pet/images.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 983040/791918971 [00:25<5:47:12, 37967.19it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainval_data \u001b[38;5;241m=\u001b[39m \u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOxfordIIITPet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrainval\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcategory\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m test_data \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mOxfordIIITPet(root\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m, target_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m\"\u001b[39m, download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, transform\u001b[38;5;241m=\u001b[39mtransform)\n\u001b[1;32m      3\u001b[0m combined_data \u001b[38;5;241m=\u001b[39m ConcatDataset([trainval_data, test_data])\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/torchvision/datasets/oxford_iiit_pet.py:64\u001b[0m, in \u001b[0;36mOxfordIIITPet.__init__\u001b[0;34m(self, root, split, target_types, transforms, transform, target_transform, download)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_segs_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_anns_folder \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrimaps\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m download:\n\u001b[0;32m---> 64\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_exists():\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset not found. You can use download=True to download it\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/torchvision/datasets/oxford_iiit_pet.py:132\u001b[0m, in \u001b[0;36mOxfordIIITPet._download\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m url, md5 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_RESOURCES:\n\u001b[0;32m--> 132\u001b[0m     \u001b[43mdownload_and_extract_archive\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_root\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_base_folder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmd5\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmd5\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/torchvision/datasets/utils.py:395\u001b[0m, in \u001b[0;36mdownload_and_extract_archive\u001b[0;34m(url, download_root, extract_root, filename, md5, remove_finished)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m filename:\n\u001b[1;32m    393\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(url)\n\u001b[0;32m--> 395\u001b[0m \u001b[43mdownload_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_root\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmd5\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    397\u001b[0m archive \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(download_root, filename)\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtracting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marchive\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mextract_root\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/torchvision/datasets/utils.py:132\u001b[0m, in \u001b[0;36mdownload_url\u001b[0;34m(url, root, filename, md5, max_redirect_hops)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m url \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m to \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m fpath)\n\u001b[0;32m--> 132\u001b[0m     \u001b[43m_urlretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (urllib\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mURLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m url[:\u001b[38;5;241m5\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/torchvision/datasets/utils.py:30\u001b[0m, in \u001b[0;36m_urlretrieve\u001b[0;34m(url, filename, chunk_size)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39murlopen(urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39mRequest(url, headers\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m\"\u001b[39m: USER_AGENT})) \u001b[38;5;28;01mas\u001b[39;00m response:\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fh, tqdm(total\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mlength) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m---> 30\u001b[0m         \u001b[38;5;28;01mwhile\u001b[39;00m chunk \u001b[38;5;241m:=\u001b[39m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     31\u001b[0m             fh\u001b[38;5;241m.\u001b[39mwrite(chunk)\n\u001b[1;32m     32\u001b[0m             pbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mlen\u001b[39m(chunk))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/http/client.py:464\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[1;32m    462\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[1;32m    463\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[0;32m--> 464\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[1;32m    466\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/ssl.py:1273\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1269\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1270\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1271\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1272\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1273\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1274\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1275\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/ssl.py:1129\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1129\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1131\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainval_data = datasets.OxfordIIITPet(root=\"data\", split=\"trainval\", target_types=\"category\", download=True, transform=transform)\n",
    "test_data = datasets.OxfordIIITPet(root=\"data\", split=\"test\", target_types=\"category\", download=True, transform=transform)\n",
    "combined_data = ConcatDataset([trainval_data, test_data])\n",
    "\n",
    "train_size = int(0.7 * len(combined_data))\n",
    "val_size = int(0.15 * len(combined_data))\n",
    "test_size = len(combined_data) - train_size - val_size\n",
    "train_data, val_data, test_data = random_split(combined_data, [train_size, val_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Train set size: {len(train_data)}\")\n",
    "print(f\"Validation set size: {len(val_data)}\")\n",
    "print(f\"Test set size: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in tqdm(train_loader, desc=\"Training\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Train Loss: {epoch_loss:.4f}, Train Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, criterion, device, phase=\"Validation\"):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(data_loader, desc=f\"{phase}\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(data_loader)\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"{phase} Loss: {epoch_loss:.4f}, {phase} Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    train(model, train_loader, criterion, optimizer, device)\n",
    "    evaluate(model, val_loader, criterion, device, phase=\"Validation\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
