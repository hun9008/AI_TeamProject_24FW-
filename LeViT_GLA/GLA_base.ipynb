{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from einops import rearrange\n",
        "\n",
        "#!pip3 install triton\n",
        "import triton\n",
        "import triton.language as tl\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "\n",
        "#!pip install -U git+https://github.com/sustcsonglin/flash-linear-attention\n",
        "from fla.ops.gla import fused_chunk_gla, chunk_gla, fused_recurrent_gla\n",
        "\n",
        "#import os\n",
        "#os.environ['TRITON_DISABLE_BF16'] = '1'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DtecONBocuqe",
        "outputId": "4a6da840-2b30-4e40-cc97-18a17ea351ad"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: triton in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton) (3.16.1)\n",
            "Collecting git+https://github.com/sustcsonglin/flash-linear-attention\n",
            "  Cloning https://github.com/sustcsonglin/flash-linear-attention to /tmp/pip-req-build-2drn0m_k\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/sustcsonglin/flash-linear-attention /tmp/pip-req-build-2drn0m_k\n",
            "  Resolved https://github.com/sustcsonglin/flash-linear-attention to commit 3a5bb44e8c45ea1fd0e70edfdeaa2c23025a04f0\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: triton>=2.2 in /usr/local/lib/python3.10/dist-packages (from fla==0.1) (3.1.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from fla==0.1) (4.44.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from fla==0.1) (3.1.0)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from fla==0.1) (0.8.0)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from fla==0.1) (1.11.1.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton>=2.2->fla==0.1) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets->fla==0.1) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->fla==0.1) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->fla==0.1) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->fla==0.1) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets->fla==0.1) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets->fla==0.1) (4.66.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->fla==0.1) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets->fla==0.1) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets->fla==0.1) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->fla==0.1) (3.10.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets->fla==0.1) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets->fla==0.1) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets->fla==0.1) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->fla==0.1) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers->fla==0.1) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers->fla==0.1) (0.19.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->fla==0.1) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->fla==0.1) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->fla==0.1) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->fla==0.1) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->fla==0.1) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->fla==0.1) (1.17.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->fla==0.1) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets->fla==0.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets->fla==0.1) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets->fla==0.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets->fla==0.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets->fla==0.1) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->fla==0.1) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->fla==0.1) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->fla==0.1) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->fla==0.1) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets->fla==0.1) (0.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "9lbVeLEgcrbN"
      },
      "outputs": [],
      "source": [
        "class GatedLinearAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.embed_dim = config.d_model\n",
        "        self.num_heads = config.n_head\n",
        "\n",
        "        self.gate_fn = nn.functional.silu\n",
        "        assert config.use_gk and not config.use_gv, \"Only use_gk is supported for simplicity.\"\n",
        "\n",
        "        self.q_proj = nn.Linear(self.embed_dim, self.embed_dim//2, bias=False)\n",
        "        self.k_proj = nn.Linear(self.embed_dim, self.embed_dim//2, bias=False)\n",
        "        self.k_gate =  nn.Sequential(nn.Linear(self.embed_dim, 16, bias=False), nn.Linear(16, self.embed_dim // 2))\n",
        "\n",
        "        self.v_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n",
        "        self.g_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=True)\n",
        "        self.out_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n",
        "\n",
        "        self.head_dim = self.embed_dim // self.num_heads\n",
        "        self.key_dim = self.embed_dim // self.num_heads\n",
        "        self.scaling = self.key_dim ** -0.5\n",
        "        self.group_norm = nn.LayerNorm(self.head_dim, eps=1e-5, elementwise_affine=False)\n",
        "\n",
        "        self.post_init()\n",
        "\n",
        "\n",
        "\n",
        "    def post_init(self):\n",
        "        nn.init.xavier_uniform_(self.q_proj.weight, gain=2 ** -2.5)\n",
        "        nn.init.xavier_uniform_(self.k_proj.weight, gain=2 ** -2.5)\n",
        "        if isinstance(self.k_gate, nn.Sequential):\n",
        "            nn.init.xavier_uniform_(self.k_gate[0].weight, gain=2 ** -2.5)\n",
        "            nn.init.xavier_uniform_(self.k_gate[1].weight, gain=2 ** -2.5)\n",
        "        else:\n",
        "            nn.init.xavier_uniform_(self.k_gate.weight, gain=2 ** -2.5)\n",
        "\n",
        "    def forward(self, x, hidden_states=None):\n",
        "        q = self.q_proj(x)\n",
        "        k = self.k_proj(x) * self.scaling\n",
        "        k_gate = self.k_gate(x)\n",
        "        v = self.v_proj(x)\n",
        "        g = self.g_proj(x)\n",
        "\n",
        "        output, new_hidden_states = self.gated_linear_attention(q, k, v, k_gate, hidden_states=hidden_states)\n",
        "        output = self.gate_fn(g) * output\n",
        "        output = self.out_proj(output)\n",
        "        return output, new_hidden_states\n",
        "\n",
        "\n",
        "    def gated_linear_attention(self, q, k, v, gk, normalizer=16, hidden_states=None):\n",
        "        q = rearrange(q, 'b l (h d) -> b h l d', h = self.num_heads).contiguous()\n",
        "        k = rearrange(k, 'b l (h d) -> b h l d', h = self.num_heads).contiguous()\n",
        "        v = rearrange(v, 'b l (h d) -> b h l d', h = self.num_heads).contiguous()\n",
        "        gk = rearrange(gk, 'b l (h d) -> b h l d', h = self.num_heads).contiguous()\n",
        "        gk = F.logsigmoid(gk) / normalizer\n",
        "\n",
        "        # for storing original dtype\n",
        "        original_dtype = q.dtype\n",
        "\n",
        "        if self.training:\n",
        "            # cast inputs to float32 if needed\n",
        "            if q.dtype == torch.bfloat16:\n",
        "                q, k, v, gk = q.float(), k.float(), v.float(), gk.float()\n",
        "\n",
        "            o, new_hidden_states = fused_chunk_gla(q, k, v, gk, initial_state=hidden_states, output_final_state=True)\n",
        "\n",
        "            # cast back to origianl dtype if needed\n",
        "            if o.dtype != original_dtype:\n",
        "              o = o.type(original_dtype)\n",
        "\n",
        "        else:\n",
        "            o = fused_recurrent_gla(q, k, v, gk)\n",
        "            new_hidden_states = None\n",
        "\n",
        "        o = self.group_norm(o)\n",
        "        o = rearrange(o, 'b h l d -> b l (h d)')\n",
        "        return o, new_hidden_states"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define seperate config object for GLA input\n",
        "class Config:\n",
        "    def __init__(self, d_model, n_head, use_gk=True, use_gv=False):\n",
        "        self.d_model = d_model\n",
        "        self.n_head = n_head\n",
        "        self.use_gk = use_gk\n",
        "        self.use_gv = use_gv"
      ],
      "metadata": {
        "id": "XlZzakcbdswK"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    BATCH, H, N_CTX, D_MODEL = 32, 4, 2048, 1024\n",
        "\n",
        "    config = Config(D_MODEL, H, use_gk=True, use_gv=False)\n",
        "    GLA = GatedLinearAttention(config).cuda().to(torch.bfloat16)\n",
        "\n",
        "    x = torch.randn((BATCH, N_CTX, D_MODEL), dtype=torch.bfloat16, device=\"cuda\", requires_grad=True)\n",
        "\n",
        "    y, _ = GLA(x)\n",
        "    print(y.shape)\n",
        "    y.sum().backward()\n",
        "    print(x.grad.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2oS6ifwXdmyZ",
        "outputId": "5654d753-0f6c-4253-81a9-da4d5758ef3a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 2048, 1024])\n",
            "torch.Size([32, 2048, 1024])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Du6QMgvzd6ZM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}