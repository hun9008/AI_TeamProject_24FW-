{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "NYaLHklOKVhx",
        "outputId": "2f135532-1300-4153-f927-9a8aea7d5dff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: triton in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: torchinfo in /usr/local/lib/python3.10/dist-packages (1.8.0)\n",
            "Requirement already satisfied: utils in /usr/local/lib/python3.10/dist-packages (1.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton) (3.16.1)\n",
            "Collecting git+https://github.com/sustcsonglin/flash-linear-attention\n",
            "  Cloning https://github.com/sustcsonglin/flash-linear-attention to /tmp/pip-req-build-8hy0ez1k\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/sustcsonglin/flash-linear-attention /tmp/pip-req-build-8hy0ez1k\n",
            "  Resolved https://github.com/sustcsonglin/flash-linear-attention to commit 311d037b814facf5c67934a5d98d8cdb26ecad75\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: triton>=2.2 in /usr/local/lib/python3.10/dist-packages (from fla==0.1) (3.1.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from fla==0.1) (4.44.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from fla==0.1) (3.1.0)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from fla==0.1) (0.8.0)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from fla==0.1) (1.11.1.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton>=2.2->fla==0.1) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets->fla==0.1) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->fla==0.1) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->fla==0.1) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->fla==0.1) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets->fla==0.1) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets->fla==0.1) (4.66.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->fla==0.1) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets->fla==0.1) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets->fla==0.1) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->fla==0.1) (3.10.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets->fla==0.1) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets->fla==0.1) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets->fla==0.1) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->fla==0.1) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers->fla==0.1) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers->fla==0.1) (0.19.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->fla==0.1) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->fla==0.1) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->fla==0.1) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->fla==0.1) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->fla==0.1) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->fla==0.1) (1.17.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->fla==0.1) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets->fla==0.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets->fla==0.1) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets->fla==0.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets->fla==0.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets->fla==0.1) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->fla==0.1) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->fla==0.1) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->fla==0.1) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->fla==0.1) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets->fla==0.1) (0.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install triton torchinfo utils\n",
        "!pip install -U git+https://github.com/sustcsonglin/flash-linear-attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": true,
        "id": "DtecONBocuqe"
      },
      "outputs": [],
      "source": [
        "#TODO: 11/11 merge GLA into LeViT_impl and then test model\n",
        "\n",
        "import torch\n",
        "from einops import rearrange\n",
        "import triton\n",
        "import triton.language as tl\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import itertools\n",
        "import utils\n",
        "import timm\n",
        "\n",
        "from fla.ops.gla import fused_chunk_gla, chunk_gla, fused_recurrent_gla\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split, ConcatDataset\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "from torchinfo import summary\n",
        "\n",
        "#import os\n",
        "#os.environ['TRITON_DISABLE_BF16'] = '1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "mQ7ZefYFmqa0"
      },
      "outputs": [],
      "source": [
        "class ConvNorm(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=2, padding=1):\n",
        "        super(ConvNorm, self).__init__()\n",
        "        self.linear = nn.Conv2d(\n",
        "            in_channels, out_channels, kernel_size=kernel_size,\n",
        "            stride=stride, padding=padding, bias=False\n",
        "        )\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear(x)\n",
        "        x = self.bn(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "vpSpWdFUlz46"
      },
      "outputs": [],
      "source": [
        "class Stem16(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Stem16, self).__init__()\n",
        "        self.conv1 = ConvNorm(3, 32)\n",
        "        self.act1 = nn.Hardswish()\n",
        "        self.conv2 = ConvNorm(32, 64)\n",
        "        self.act2 = nn.Hardswish()\n",
        "        self.conv3 = ConvNorm(64, 128)\n",
        "        self.act3 = nn.Hardswish()\n",
        "        self.conv4 = ConvNorm(128, 256)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.act1(self.conv1(x))\n",
        "        x = self.act2(self.conv2(x))\n",
        "        x = self.act3(self.conv3(x))\n",
        "        x = self.conv4(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Cx4hkNBpPcXV"
      },
      "outputs": [],
      "source": [
        "class LinearNorm(nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super(LinearNorm, self).__init__()\n",
        "        self.linear = nn.Linear(in_features, out_features, bias=False)\n",
        "        self.bn = nn.BatchNorm1d(out_features)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        if x.dim() == 3:\n",
        "            B, N, C = x.shape\n",
        "            x = x.reshape(B * N, C)\n",
        "            x = self.bn(self.linear(x))\n",
        "            x = x.reshape(B, N, -1)\n",
        "        else:\n",
        "            x = self.bn(self.linear(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "dy9qBYfrPdra"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, num_heads, attn_ratio=2):\n",
        "        super(Attention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = head_dim ** -0.5\n",
        "        inner_dim = head_dim * num_heads * 3\n",
        "        self.qkv = LinearNorm(dim, inner_dim)\n",
        "\n",
        "        self.proj = nn.Sequential(\n",
        "            nn.Hardswish(),\n",
        "            LinearNorm(dim, dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x)\n",
        "        qkv = qkv.view(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        return self.proj(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJyHnFqJeRaU"
      },
      "source": [
        "## GLA (Gated Linear Attention) Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "9lbVeLEgcrbN"
      },
      "outputs": [],
      "source": [
        "class GatedLinearAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.embed_dim = config.dim\n",
        "        self.num_heads = config.num_heads\n",
        "\n",
        "        self.gate_fn = nn.functional.silu\n",
        "        assert config.use_gk and not config.use_gv, \"Only use_gk is supported for simplicity.\"\n",
        "\n",
        "        self.q_proj = nn.Linear(self.embed_dim, self.embed_dim//2, bias=False)\n",
        "        self.k_proj = nn.Linear(self.embed_dim, self.embed_dim//2, bias=False)\n",
        "        self.k_gate =  nn.Sequential(nn.Linear(self.embed_dim, 16, bias=False), nn.Linear(16, self.embed_dim // 2))\n",
        "\n",
        "        self.v_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n",
        "        self.g_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=True)\n",
        "        self.out_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n",
        "\n",
        "        self.head_dim = self.embed_dim // self.num_heads\n",
        "        self.key_dim = self.embed_dim // self.num_heads\n",
        "        self.scaling = self.key_dim ** -0.5\n",
        "        self.group_norm = nn.LayerNorm(self.head_dim, eps=1e-5, elementwise_affine=False)\n",
        "\n",
        "        self.post_init()\n",
        "\n",
        "    def post_init(self):\n",
        "        nn.init.xavier_uniform_(self.q_proj.weight, gain=2 ** -2.5)\n",
        "        nn.init.xavier_uniform_(self.k_proj.weight, gain=2 ** -2.5)\n",
        "        if isinstance(self.k_gate, nn.Sequential):\n",
        "            nn.init.xavier_uniform_(self.k_gate[0].weight, gain=2 ** -2.5)\n",
        "            nn.init.xavier_uniform_(self.k_gate[1].weight, gain=2 ** -2.5)\n",
        "        else:\n",
        "            nn.init.xavier_uniform_(self.k_gate.weight, gain=2 ** -2.5)\n",
        "\n",
        "    def forward(self, x, hidden_states=None):\n",
        "        q = self.q_proj(x)\n",
        "        k = self.k_proj(x) * self.scaling\n",
        "        k_gate = self.k_gate(x)\n",
        "        v = self.v_proj(x)\n",
        "        g = self.g_proj(x)\n",
        "\n",
        "        output, new_hidden_states = self.gated_linear_attention(q, k, v, k_gate, hidden_states=hidden_states)\n",
        "        output = self.gate_fn(g) * output\n",
        "        output = self.out_proj(output)\n",
        "        #print(f\"Inside GatedLinearAttention forward: new_hidden_states shape = {new_hidden_states.shape if new_hidden_states is not None else 'None'}\")\n",
        "        return output, new_hidden_states # this needs to be tensor, not tuple\n",
        "\n",
        "\n",
        "    def gated_linear_attention(self, q, k, v, gk, normalizer=16, hidden_states=None):\n",
        "        q = rearrange(q, 'b l (h d) -> b h l d', h = self.num_heads).contiguous()\n",
        "        k = rearrange(k, 'b l (h d) -> b h l d', h = self.num_heads).contiguous()\n",
        "        v = rearrange(v, 'b l (h d) -> b h l d', h = self.num_heads).contiguous()\n",
        "        gk = rearrange(gk, 'b l (h d) -> b h l d', h = self.num_heads).contiguous()\n",
        "        gk = F.logsigmoid(gk) / normalizer\n",
        "\n",
        "        # for storing original dtype\n",
        "        original_dtype = q.dtype\n",
        "\n",
        "        if self.training:\n",
        "            # cast inputs to float32 if needed\n",
        "            if q.dtype == torch.bfloat16:\n",
        "                q, k, v, gk = q.float(), k.float(), v.float(), gk.float()\n",
        "            o, new_hidden_states = fused_chunk_gla(q, k, v, gk, initial_state=hidden_states, output_final_state=True)\n",
        "            # cast back to origianl dtype if needed\n",
        "            if o.dtype != original_dtype:\n",
        "              o = o.type(original_dtype)\n",
        "\n",
        "        else:\n",
        "            o = fused_recurrent_gla(q, k, v, gk)\n",
        "\n",
        "            new_hidden_states = None\n",
        "\n",
        "        #print(f\"Inside gated_linear_attention: new_hidden_states shape = {new_hidden_states.shape if new_hidden_states is not None else 'None'}\")\n",
        "\n",
        "        if isinstance(o, tuple):\n",
        "          o = o[0]\n",
        "\n",
        "        o = self.group_norm(o)\n",
        "        o = rearrange(o, 'b h l d -> b l (h d)')\n",
        "        return o, new_hidden_states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "XlZzakcbdswK"
      },
      "outputs": [],
      "source": [
        "# define seperate config object for GLA input\n",
        "class Config:\n",
        "    def __init__(self, dim, num_heads, use_gk=True, use_gv=False):\n",
        "        self.dim = dim\n",
        "        self.num_heads = num_heads\n",
        "        self.use_gk = use_gk\n",
        "        self.use_gv = use_gv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "yVtpR2thmUWp"
      },
      "outputs": [],
      "source": [
        "class LevitMlp(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features, out_features):\n",
        "        super(LevitMlp, self).__init__()\n",
        "        self.ln1 = LinearNorm(in_features, hidden_features)\n",
        "        self.act = nn.Hardswish()\n",
        "        self.drop = nn.Dropout(p=0.0, inplace=False)\n",
        "        self.ln2 = LinearNorm(hidden_features, out_features)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.ln1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.ln2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ZdJ9BSyUPkvh"
      },
      "outputs": [],
      "source": [
        "class LevitBlock(nn.Module):\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=2): # hidden state from GLA\n",
        "        super(LevitBlock, self).__init__()\n",
        "        #self.attn = Attention(dim, num_heads) # -> GLA, hidden state updates\n",
        "        self.attn = GatedLinearAttention(Config(dim, num_heads))\n",
        "        self.drop_path1 = nn.Identity()\n",
        "        self.mlp = LevitMlp(dim, dim * mlp_ratio, dim)\n",
        "        self.drop_path2 = nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        attn_output, hidden_state = self.attn(x) # GLA output is tuple (o, hidden_state)\n",
        "        x = x + self.drop_path1(attn_output) # assign tuple\n",
        "        x = x + self.drop_path2(self.mlp(x))\n",
        "        return x, hidden_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "gRXUrjsTPkmA"
      },
      "outputs": [],
      "source": [
        "class AttentionDownsample(nn.Module):\n",
        "    def __init__(self, dim, out_dim, num_heads, attn_ratio=2):\n",
        "        super(AttentionDownsample, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.scale = (dim // num_heads) ** -0.5\n",
        "        inner_dim = dim * attn_ratio * num_heads\n",
        "        self.kv = LinearNorm(dim, inner_dim)\n",
        "\n",
        "        self.q = nn.Sequential(\n",
        "            nn.Conv2d(dim, dim, kernel_size=2, stride=2),\n",
        "            nn.Flatten(start_dim=1)\n",
        "        )\n",
        "\n",
        "        self.proj = nn.Sequential(\n",
        "            nn.Hardswish(),\n",
        "            LinearNorm(dim, out_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        if isinstance(x, tuple):\n",
        "          x, hidden_state = x\n",
        "        else:\n",
        "          hidden_state = None\n",
        "\n",
        "        B, N, C = x.shape\n",
        "        H = W = int(N ** 0.5)\n",
        "        x = x.reshape(B, C, H, W)\n",
        "\n",
        "        kv = self.kv(x.flatten(2).transpose(1, 2))\n",
        "        q = self.q(x)\n",
        "\n",
        "        q = q.reshape(B, -1, C)\n",
        "        x = self.proj(q)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Uz5pU7AFPkbD"
      },
      "outputs": [],
      "source": [
        "class LevitDownsample(nn.Module):\n",
        "    def __init__(self, dim, out_dim, num_heads, attn_ratio=2):\n",
        "        super(LevitDownsample, self).__init__()\n",
        "        self.attn_downsample = AttentionDownsample(dim, out_dim, num_heads, attn_ratio)\n",
        "        self.mlp = LevitMlp(out_dim, out_dim * attn_ratio, out_dim)\n",
        "        self.drop_path = nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        if isinstance(x, tuple):\n",
        "          x, hidden_state = x\n",
        "        else:\n",
        "          hidden_state = None\n",
        "\n",
        "        x = self.attn_downsample(x)\n",
        "        x = self.drop_path(self.mlp(x))\n",
        "\n",
        "        if hidden_state is None:\n",
        "          return x\n",
        "        else:\n",
        "          return x, hidden_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ySowH5PKPum7"
      },
      "outputs": [],
      "source": [
        "class NormLinear(nn.Module):\n",
        "    def __init__(self, in_features, out_features, dropout_prob=0.0):\n",
        "        super(NormLinear, self).__init__()\n",
        "        self.bn = nn.BatchNorm1d(in_features)\n",
        "        self.drop = nn.Dropout(p=dropout_prob, inplace=False)\n",
        "        self.linear = nn.Linear(in_features, out_features, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.bn(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.linear(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "NRg0WFn7PrQR"
      },
      "outputs": [],
      "source": [
        "class LevitStage(nn.Module):\n",
        "    def __init__(self, dim, out_dim, num_heads, num_blocks, downsample=True):\n",
        "        super(LevitStage, self).__init__()\n",
        "        self.downsample = LevitDownsample(dim, out_dim, num_heads) if downsample else nn.Identity()\n",
        "        self.blocks = nn.Sequential(*[LevitBlock(out_dim, num_heads) for _ in range(num_blocks)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.downsample(x)\n",
        "        hidden_states = []\n",
        "        for block in self.blocks:\n",
        "            if isinstance(x, tuple):\n",
        "                x, hidden_state = x\n",
        "            else:\n",
        "                hidden_state = None\n",
        "            x, hidden_state_block = block(x) # use only x as input\n",
        "            hidden_states.append(hidden_state_block)\n",
        "            #print(f\"Inside LevitStage: Block hidden_state shape = {hidden_state_block.shape if hidden_state_block is not None else 'None'}\")  # 추가된 디버그 출력\n",
        "        return x, hidden_states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "M3cMGLOCPugr"
      },
      "outputs": [],
      "source": [
        "class GLALeViT(nn.Module):\n",
        "    def __init__(self, num_classes=37):\n",
        "        super(GLALeViT, self).__init__()\n",
        "\n",
        "        self.stem = Stem16()\n",
        "\n",
        "        self.stages = nn.Sequential(\n",
        "            LevitStage(dim=256, out_dim=256, num_heads=4, num_blocks=3, downsample=False),\n",
        "            LevitStage(dim=256, out_dim=384, num_heads=6, num_blocks=3, downsample=True),\n",
        "            LevitStage(dim=384, out_dim=512, num_heads=8, num_blocks=2, downsample=True)\n",
        "        )\n",
        "\n",
        "        self.head = NormLinear(in_features=512, out_features=num_classes, dropout_prob=0.0)\n",
        "        self.head_dist = NormLinear(in_features=512, out_features=num_classes, dropout_prob=0.0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.stem(x)\n",
        "        B, C, H, W = x.shape\n",
        "        x = x.view(B, C, -1).transpose(1, 2)\n",
        "\n",
        "        x, all_hidden_states = self.stages(x)\n",
        "        all_hidden_states = [state for state in all_hidden_states if state is not None]\n",
        "        out = self.head(x.mean(dim=1))\n",
        "        out_dist = self.head_dist(x.mean(dim=1))\n",
        "        return out, all_hidden_states\n",
        "\n",
        "        #x = self.stages(x)\n",
        "        #out = self.head(x.mean(dim=1))\n",
        "        #out_dist = self.head_dist(x.mean(dim=1))\n",
        "        #return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "2oS6ifwXdmyZ"
      },
      "outputs": [],
      "source": [
        "model = GLALeViT()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "batch_size = 32\n",
        "learning_rate = 0.001\n",
        "num_epochs = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1ElLXJNrYGo",
        "outputId": "18b77b60-0bdd-4654-92c7-d53a5d824b50"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GLALeViT(\n",
              "  (stem): Stem16(\n",
              "    (conv1): ConvNorm(\n",
              "      (linear): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (act1): Hardswish()\n",
              "    (conv2): ConvNorm(\n",
              "      (linear): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (act2): Hardswish()\n",
              "    (conv3): ConvNorm(\n",
              "      (linear): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (act3): Hardswish()\n",
              "    (conv4): ConvNorm(\n",
              "      (linear): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (stages): Sequential(\n",
              "    (0): LevitStage(\n",
              "      (downsample): Identity()\n",
              "      (blocks): Sequential(\n",
              "        (0): LevitBlock(\n",
              "          (attn): GatedLinearAttention(\n",
              "            (q_proj): Linear(in_features=256, out_features=128, bias=False)\n",
              "            (k_proj): Linear(in_features=256, out_features=128, bias=False)\n",
              "            (k_gate): Sequential(\n",
              "              (0): Linear(in_features=256, out_features=16, bias=False)\n",
              "              (1): Linear(in_features=16, out_features=128, bias=True)\n",
              "            )\n",
              "            (v_proj): Linear(in_features=256, out_features=256, bias=False)\n",
              "            (g_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (out_proj): Linear(in_features=256, out_features=256, bias=False)\n",
              "            (group_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=False)\n",
              "          )\n",
              "          (drop_path1): Identity()\n",
              "          (mlp): LevitMlp(\n",
              "            (ln1): LinearNorm(\n",
              "              (linear): Linear(in_features=256, out_features=512, bias=False)\n",
              "              (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "            (act): Hardswish()\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "            (ln2): LinearNorm(\n",
              "              (linear): Linear(in_features=512, out_features=256, bias=False)\n",
              "              (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "        (1): LevitBlock(\n",
              "          (attn): GatedLinearAttention(\n",
              "            (q_proj): Linear(in_features=256, out_features=128, bias=False)\n",
              "            (k_proj): Linear(in_features=256, out_features=128, bias=False)\n",
              "            (k_gate): Sequential(\n",
              "              (0): Linear(in_features=256, out_features=16, bias=False)\n",
              "              (1): Linear(in_features=16, out_features=128, bias=True)\n",
              "            )\n",
              "            (v_proj): Linear(in_features=256, out_features=256, bias=False)\n",
              "            (g_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (out_proj): Linear(in_features=256, out_features=256, bias=False)\n",
              "            (group_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=False)\n",
              "          )\n",
              "          (drop_path1): Identity()\n",
              "          (mlp): LevitMlp(\n",
              "            (ln1): LinearNorm(\n",
              "              (linear): Linear(in_features=256, out_features=512, bias=False)\n",
              "              (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "            (act): Hardswish()\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "            (ln2): LinearNorm(\n",
              "              (linear): Linear(in_features=512, out_features=256, bias=False)\n",
              "              (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "        (2): LevitBlock(\n",
              "          (attn): GatedLinearAttention(\n",
              "            (q_proj): Linear(in_features=256, out_features=128, bias=False)\n",
              "            (k_proj): Linear(in_features=256, out_features=128, bias=False)\n",
              "            (k_gate): Sequential(\n",
              "              (0): Linear(in_features=256, out_features=16, bias=False)\n",
              "              (1): Linear(in_features=16, out_features=128, bias=True)\n",
              "            )\n",
              "            (v_proj): Linear(in_features=256, out_features=256, bias=False)\n",
              "            (g_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (out_proj): Linear(in_features=256, out_features=256, bias=False)\n",
              "            (group_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=False)\n",
              "          )\n",
              "          (drop_path1): Identity()\n",
              "          (mlp): LevitMlp(\n",
              "            (ln1): LinearNorm(\n",
              "              (linear): Linear(in_features=256, out_features=512, bias=False)\n",
              "              (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "            (act): Hardswish()\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "            (ln2): LinearNorm(\n",
              "              (linear): Linear(in_features=512, out_features=256, bias=False)\n",
              "              (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (1): LevitStage(\n",
              "      (downsample): LevitDownsample(\n",
              "        (attn_downsample): AttentionDownsample(\n",
              "          (kv): LinearNorm(\n",
              "            (linear): Linear(in_features=256, out_features=3072, bias=False)\n",
              "            (bn): BatchNorm1d(3072, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "          (q): Sequential(\n",
              "            (0): Conv2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
              "            (1): Flatten(start_dim=1, end_dim=-1)\n",
              "          )\n",
              "          (proj): Sequential(\n",
              "            (0): Hardswish()\n",
              "            (1): LinearNorm(\n",
              "              (linear): Linear(in_features=256, out_features=384, bias=False)\n",
              "              (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (mlp): LevitMlp(\n",
              "          (ln1): LinearNorm(\n",
              "            (linear): Linear(in_features=384, out_features=768, bias=False)\n",
              "            (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "          (act): Hardswish()\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "          (ln2): LinearNorm(\n",
              "            (linear): Linear(in_features=768, out_features=384, bias=False)\n",
              "            (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "      )\n",
              "      (blocks): Sequential(\n",
              "        (0): LevitBlock(\n",
              "          (attn): GatedLinearAttention(\n",
              "            (q_proj): Linear(in_features=384, out_features=192, bias=False)\n",
              "            (k_proj): Linear(in_features=384, out_features=192, bias=False)\n",
              "            (k_gate): Sequential(\n",
              "              (0): Linear(in_features=384, out_features=16, bias=False)\n",
              "              (1): Linear(in_features=16, out_features=192, bias=True)\n",
              "            )\n",
              "            (v_proj): Linear(in_features=384, out_features=384, bias=False)\n",
              "            (g_proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "            (out_proj): Linear(in_features=384, out_features=384, bias=False)\n",
              "            (group_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=False)\n",
              "          )\n",
              "          (drop_path1): Identity()\n",
              "          (mlp): LevitMlp(\n",
              "            (ln1): LinearNorm(\n",
              "              (linear): Linear(in_features=384, out_features=768, bias=False)\n",
              "              (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "            (act): Hardswish()\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "            (ln2): LinearNorm(\n",
              "              (linear): Linear(in_features=768, out_features=384, bias=False)\n",
              "              (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "        (1): LevitBlock(\n",
              "          (attn): GatedLinearAttention(\n",
              "            (q_proj): Linear(in_features=384, out_features=192, bias=False)\n",
              "            (k_proj): Linear(in_features=384, out_features=192, bias=False)\n",
              "            (k_gate): Sequential(\n",
              "              (0): Linear(in_features=384, out_features=16, bias=False)\n",
              "              (1): Linear(in_features=16, out_features=192, bias=True)\n",
              "            )\n",
              "            (v_proj): Linear(in_features=384, out_features=384, bias=False)\n",
              "            (g_proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "            (out_proj): Linear(in_features=384, out_features=384, bias=False)\n",
              "            (group_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=False)\n",
              "          )\n",
              "          (drop_path1): Identity()\n",
              "          (mlp): LevitMlp(\n",
              "            (ln1): LinearNorm(\n",
              "              (linear): Linear(in_features=384, out_features=768, bias=False)\n",
              "              (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "            (act): Hardswish()\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "            (ln2): LinearNorm(\n",
              "              (linear): Linear(in_features=768, out_features=384, bias=False)\n",
              "              (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "        (2): LevitBlock(\n",
              "          (attn): GatedLinearAttention(\n",
              "            (q_proj): Linear(in_features=384, out_features=192, bias=False)\n",
              "            (k_proj): Linear(in_features=384, out_features=192, bias=False)\n",
              "            (k_gate): Sequential(\n",
              "              (0): Linear(in_features=384, out_features=16, bias=False)\n",
              "              (1): Linear(in_features=16, out_features=192, bias=True)\n",
              "            )\n",
              "            (v_proj): Linear(in_features=384, out_features=384, bias=False)\n",
              "            (g_proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "            (out_proj): Linear(in_features=384, out_features=384, bias=False)\n",
              "            (group_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=False)\n",
              "          )\n",
              "          (drop_path1): Identity()\n",
              "          (mlp): LevitMlp(\n",
              "            (ln1): LinearNorm(\n",
              "              (linear): Linear(in_features=384, out_features=768, bias=False)\n",
              "              (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "            (act): Hardswish()\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "            (ln2): LinearNorm(\n",
              "              (linear): Linear(in_features=768, out_features=384, bias=False)\n",
              "              (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (2): LevitStage(\n",
              "      (downsample): LevitDownsample(\n",
              "        (attn_downsample): AttentionDownsample(\n",
              "          (kv): LinearNorm(\n",
              "            (linear): Linear(in_features=384, out_features=6144, bias=False)\n",
              "            (bn): BatchNorm1d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "          (q): Sequential(\n",
              "            (0): Conv2d(384, 384, kernel_size=(2, 2), stride=(2, 2))\n",
              "            (1): Flatten(start_dim=1, end_dim=-1)\n",
              "          )\n",
              "          (proj): Sequential(\n",
              "            (0): Hardswish()\n",
              "            (1): LinearNorm(\n",
              "              (linear): Linear(in_features=384, out_features=512, bias=False)\n",
              "              (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (mlp): LevitMlp(\n",
              "          (ln1): LinearNorm(\n",
              "            (linear): Linear(in_features=512, out_features=1024, bias=False)\n",
              "            (bn): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "          (act): Hardswish()\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "          (ln2): LinearNorm(\n",
              "            (linear): Linear(in_features=1024, out_features=512, bias=False)\n",
              "            (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "      )\n",
              "      (blocks): Sequential(\n",
              "        (0): LevitBlock(\n",
              "          (attn): GatedLinearAttention(\n",
              "            (q_proj): Linear(in_features=512, out_features=256, bias=False)\n",
              "            (k_proj): Linear(in_features=512, out_features=256, bias=False)\n",
              "            (k_gate): Sequential(\n",
              "              (0): Linear(in_features=512, out_features=16, bias=False)\n",
              "              (1): Linear(in_features=16, out_features=256, bias=True)\n",
              "            )\n",
              "            (v_proj): Linear(in_features=512, out_features=512, bias=False)\n",
              "            (g_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): Linear(in_features=512, out_features=512, bias=False)\n",
              "            (group_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=False)\n",
              "          )\n",
              "          (drop_path1): Identity()\n",
              "          (mlp): LevitMlp(\n",
              "            (ln1): LinearNorm(\n",
              "              (linear): Linear(in_features=512, out_features=1024, bias=False)\n",
              "              (bn): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "            (act): Hardswish()\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "            (ln2): LinearNorm(\n",
              "              (linear): Linear(in_features=1024, out_features=512, bias=False)\n",
              "              (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "        (1): LevitBlock(\n",
              "          (attn): GatedLinearAttention(\n",
              "            (q_proj): Linear(in_features=512, out_features=256, bias=False)\n",
              "            (k_proj): Linear(in_features=512, out_features=256, bias=False)\n",
              "            (k_gate): Sequential(\n",
              "              (0): Linear(in_features=512, out_features=16, bias=False)\n",
              "              (1): Linear(in_features=16, out_features=256, bias=True)\n",
              "            )\n",
              "            (v_proj): Linear(in_features=512, out_features=512, bias=False)\n",
              "            (g_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): Linear(in_features=512, out_features=512, bias=False)\n",
              "            (group_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=False)\n",
              "          )\n",
              "          (drop_path1): Identity()\n",
              "          (mlp): LevitMlp(\n",
              "            (ln1): LinearNorm(\n",
              "              (linear): Linear(in_features=512, out_features=1024, bias=False)\n",
              "              (bn): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "            (act): Hardswish()\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "            (ln2): LinearNorm(\n",
              "              (linear): Linear(in_features=1024, out_features=512, bias=False)\n",
              "              (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (head): NormLinear(\n",
              "    (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (drop): Dropout(p=0.0, inplace=False)\n",
              "    (linear): Linear(in_features=512, out_features=37, bias=True)\n",
              "  )\n",
              "  (head_dist): NormLinear(\n",
              "    (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (drop): Dropout(p=0.0, inplace=False)\n",
              "    (linear): Linear(in_features=512, out_features=37, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4KGu5baqAyR",
        "outputId": "82e1ed0f-f0a2-479c-aaba-83b0a36d16d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=========================================================================================================\n",
            "Layer (type:depth-idx)                                  Output Shape              Param #\n",
            "=========================================================================================================\n",
            "GLALeViT                                                [32, 37]                  --\n",
            "├─Stem16: 1-1                                           [32, 256, 14, 14]         --\n",
            "│    └─ConvNorm: 2-1                                    [32, 32, 112, 112]        --\n",
            "│    │    └─Conv2d: 3-1                                 [32, 32, 112, 112]        864\n",
            "│    │    └─BatchNorm2d: 3-2                            [32, 32, 112, 112]        64\n",
            "│    └─Hardswish: 2-2                                   [32, 32, 112, 112]        --\n",
            "│    └─ConvNorm: 2-3                                    [32, 64, 56, 56]          --\n",
            "│    │    └─Conv2d: 3-3                                 [32, 64, 56, 56]          18,432\n",
            "│    │    └─BatchNorm2d: 3-4                            [32, 64, 56, 56]          128\n",
            "│    └─Hardswish: 2-4                                   [32, 64, 56, 56]          --\n",
            "│    └─ConvNorm: 2-5                                    [32, 128, 28, 28]         --\n",
            "│    │    └─Conv2d: 3-5                                 [32, 128, 28, 28]         73,728\n",
            "│    │    └─BatchNorm2d: 3-6                            [32, 128, 28, 28]         256\n",
            "│    └─Hardswish: 2-6                                   [32, 128, 28, 28]         --\n",
            "│    └─ConvNorm: 2-7                                    [32, 256, 14, 14]         --\n",
            "│    │    └─Conv2d: 3-7                                 [32, 256, 14, 14]         294,912\n",
            "│    │    └─BatchNorm2d: 3-8                            [32, 256, 14, 14]         512\n",
            "├─Sequential: 1-2                                       [32, 9, 512]              --\n",
            "│    └─LevitStage: 2-8                                  [32, 196, 256]            --\n",
            "│    │    └─Identity: 3-9                               [32, 196, 256]            --\n",
            "│    │    └─Sequential: 3-10                            --                        1,597,056\n",
            "│    └─LevitStage: 2-9                                  [32, 49, 384]             --\n",
            "│    │    └─LevitDownsample: 3-11                       [32, 49, 384]             1,746,176\n",
            "│    │    └─Sequential: 3-12                            --                        3,575,232\n",
            "│    └─LevitStage: 2-10                                 [32, 9, 512]              --\n",
            "│    │    └─LevitDownsample: 3-13                       [32, 9, 512]              4,211,072\n",
            "│    │    └─Sequential: 3-14                            --                        4,226,560\n",
            "├─NormLinear: 1-3                                       [32, 37]                  --\n",
            "│    └─BatchNorm1d: 2-11                                [32, 512]                 1,024\n",
            "│    └─Dropout: 2-12                                    [32, 512]                 --\n",
            "│    └─Linear: 2-13                                     [32, 37]                  18,981\n",
            "├─NormLinear: 1-4                                       [32, 37]                  --\n",
            "│    └─BatchNorm1d: 2-14                                [32, 512]                 1,024\n",
            "│    └─Dropout: 2-15                                    [32, 512]                 --\n",
            "│    └─Linear: 2-16                                     [32, 37]                  18,981\n",
            "=========================================================================================================\n",
            "Total params: 15,785,002\n",
            "Trainable params: 15,785,002\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (G): 25.12\n",
            "=========================================================================================================\n",
            "Input size (MB): 19.27\n",
            "Forward/backward pass size (MB): 1484.33\n",
            "Params size (MB): 63.14\n",
            "Estimated Total Size (MB): 1566.74\n",
            "=========================================================================================================\n"
          ]
        }
      ],
      "source": [
        "from torchinfo import summary\n",
        "print(summary(model, input_size=(32, 3, 224, 224)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "7K2FpPhprYDG"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "y8bImNeErX7v"
      },
      "outputs": [],
      "source": [
        "trainval_data = datasets.OxfordIIITPet(root=\"data\", split=\"trainval\", target_types=\"category\", download=True, transform=transform)\n",
        "test_data = datasets.OxfordIIITPet(root=\"data\", split=\"test\", target_types=\"category\", download=True, transform=transform)\n",
        "combined_data = ConcatDataset([trainval_data, test_data])\n",
        "\n",
        "train_size = int(0.7 * len(combined_data))\n",
        "val_size = int(0.15 * len(combined_data))\n",
        "test_size = len(combined_data) - train_size - val_size\n",
        "train_data, val_data, test_data = random_split(combined_data, [train_size, val_size, test_size])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-EJl1EDrqWL",
        "outputId": "4808802a-21ec-437c-f435-b8ad08abd12f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set size: 5144\n",
            "Validation set size: 1102\n",
            "Test set size: 1103\n"
          ]
        }
      ],
      "source": [
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(f\"Train set size: {len(train_data)}\")\n",
        "print(f\"Validation set size: {len(val_data)}\")\n",
        "print(f\"Test set size: {len(test_data)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Q3gXlU_5rqSx"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "HxQ-bTtSrqQJ"
      },
      "outputs": [],
      "source": [
        "def train(model, train_loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    hidden_states = None\n",
        "\n",
        "    num_batches = len(train_loader)\n",
        "    print_interval = num_batches // 10\n",
        "\n",
        "    for batch_idx, (inputs, labels) in enumerate(tqdm(train_loader, desc=\"Training\")):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # Check if 'outputs' is a tuple and extract the logits if necessary\n",
        "        if isinstance(outputs, tuple):\n",
        "            hidden_states = outputs[1]\n",
        "            outputs = outputs[0]  # Assuming the first element of the tuple contains the logits\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        if (batch_idx + 1) % print_interval == 0 and hidden_states is not None:\n",
        "            if isinstance(hidden_states, list):\n",
        "                print(f\"{(batch_idx + 1) / num_batches * 100:.0f}% 진행 - hidden states element shapes: {[h.shape for h in hidden_states]}\")\n",
        "\n",
        "            else:\n",
        "                print(f\"{(batch_idx + 1) / num_batches * 100:.0f}% 진행 - hidden states shape: {hidden_states.shape}\")\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    accuracy = 100 * correct / total\n",
        "    if torch.isnan(torch.tensor(epoch_loss)):\n",
        "      print(f\"Epoch {epoch+1}: NaN epoch loss detected!\")\n",
        "      print(f\"running loss: {running_loss}, train_loader length: {len(train_loader)}\")\n",
        "    print(f\"Epoch {epoch+1} Loss: {epoch_loss:.4f}\")\n",
        "    if hidden_states is not None:\n",
        "        if isinstance(hidden_states, list):\n",
        "\n",
        "            print(f\"Epoch 종료 시 hidden states length: {len(hidden_states)}\")\n",
        "\n",
        "            hidden_states = torch.stack(hidden_states)\n",
        "            print(f\"Epoch 종료 시 hidden states shape: {hidden_states.shape}\")\n",
        "\n",
        "        else:\n",
        "            print(f\"Epoch 종료 시 hidden states shape: {hidden_states.shape}\")\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "9AxFISArrXwV"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, data_loader, criterion, device, phase=\"Validation\"):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(data_loader, desc=f\"{phase}\"):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Check if 'outputs' is a tuple and extract the logits if necessary\n",
        "            if isinstance(outputs, tuple):\n",
        "                outputs = outputs[0]  # Assuming the first element of the tuple contains the logits\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(data_loader)\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"{phase} Loss: {epoch_loss:.4f}, {phase} Accuracy: {accuracy:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "J0mP8LecrvlA"
      },
      "outputs": [],
      "source": [
        "def measure_inference_time(model, data_loader, device):\n",
        "    model.eval()\n",
        "    times = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, _ in data_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            start_time = torch.cuda.Event(enable_timing=True)\n",
        "            end_time = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "            start_time.record()\n",
        "            _ = model(inputs)  # inference 수행\n",
        "            end_time.record()\n",
        "\n",
        "            # 시간 측정\n",
        "            torch.cuda.synchronize()  # CUDA에서 모든 커널이 완료될 때까지 대기\n",
        "            elapsed_time = start_time.elapsed_time(end_time)  # 밀리초 단위로 반환\n",
        "            times.append(elapsed_time)\n",
        "\n",
        "    # 통계량 계산\n",
        "    times_np = np.array(times)\n",
        "    total_inferences = len(times_np)\n",
        "    avg_time = np.mean(times_np)\n",
        "    std_dev = np.std(times_np)\n",
        "    max_time = np.max(times_np)\n",
        "    min_time = np.min(times_np)\n",
        "\n",
        "    # 결과 출력\n",
        "    print(f\"Inference Time Measurement Results:\")\n",
        "    print(f\"Total Inferences: {total_inferences}\")\n",
        "    print(f\"Average Time: {avg_time:.2f} ms\")\n",
        "    print(f\"Standard Deviation: {std_dev:.2f} ms\")\n",
        "    print(f\"Maximum Time: {max_time:.2f} ms\")\n",
        "    print(f\"Minimum Time: {min_time:.2f} ms\")\n",
        "\n",
        "    return times"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tFoP-FZKrvbD",
        "outputId": "544e769d-44ef-44cd-ca3e-e17e65dcc9ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  11%|█         | 17/161 [00:03<00:28,  5.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10% 진행 - hidden states element shapes: [torch.Size([32, 8, 32, 64]), torch.Size([32, 8, 32, 64])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  20%|█▉        | 32/161 [00:06<00:26,  4.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20% 진행 - hidden states element shapes: [torch.Size([32, 8, 32, 64]), torch.Size([32, 8, 32, 64])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  30%|██▉       | 48/161 [00:10<00:23,  4.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30% 진행 - hidden states element shapes: [torch.Size([32, 8, 32, 64]), torch.Size([32, 8, 32, 64])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  40%|████      | 65/161 [00:13<00:19,  4.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "40% 진행 - hidden states element shapes: [torch.Size([32, 8, 32, 64]), torch.Size([32, 8, 32, 64])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  50%|████▉     | 80/161 [00:16<00:16,  4.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50% 진행 - hidden states element shapes: [torch.Size([32, 8, 32, 64]), torch.Size([32, 8, 32, 64])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  60%|█████▉    | 96/161 [00:20<00:14,  4.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60% 진행 - hidden states element shapes: [torch.Size([32, 8, 32, 64]), torch.Size([32, 8, 32, 64])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  70%|██████▉   | 112/161 [00:23<00:10,  4.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "70% 진행 - hidden states element shapes: [torch.Size([32, 8, 32, 64]), torch.Size([32, 8, 32, 64])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  80%|███████▉  | 128/161 [00:26<00:06,  4.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "80% 진행 - hidden states element shapes: [torch.Size([32, 8, 32, 64]), torch.Size([32, 8, 32, 64])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  90%|█████████ | 145/161 [00:30<00:03,  4.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "89% 진행 - hidden states element shapes: [torch.Size([32, 8, 32, 64]), torch.Size([32, 8, 32, 64])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 161/161 [00:33<00:00,  4.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "99% 진행 - hidden states element shapes: [torch.Size([32, 8, 32, 64]), torch.Size([32, 8, 32, 64])]\n",
            "Epoch 1 Loss: 3.5110\n",
            "Epoch 종료 시 hidden states length: 2\n",
            "Epoch 종료 시 hidden states shape: torch.Size([2, 24, 8, 32, 64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 35/35 [00:05<00:00,  6.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 3.4959, Validation Accuracy: 4.54%\n",
            "\n",
            "Epoch 2/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  11%|█         | 17/161 [00:03<00:30,  4.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10% 진행 - hidden states element shapes: [torch.Size([32, 8, 32, 64]), torch.Size([32, 8, 32, 64])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  20%|█▉        | 32/161 [00:06<00:25,  5.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20% 진행 - hidden states element shapes: [torch.Size([32, 8, 32, 64]), torch.Size([32, 8, 32, 64])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  30%|██▉       | 48/161 [00:09<00:23,  4.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30% 진행 - hidden states element shapes: [torch.Size([32, 8, 32, 64]), torch.Size([32, 8, 32, 64])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  40%|███▉      | 64/161 [00:13<00:18,  5.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "40% 진행 - hidden states element shapes: [torch.Size([32, 8, 32, 64]), torch.Size([32, 8, 32, 64])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  50%|████▉     | 80/161 [00:16<00:16,  5.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50% 진행 - hidden states element shapes: [torch.Size([32, 8, 32, 64]), torch.Size([32, 8, 32, 64])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  60%|██████    | 97/161 [00:19<00:13,  4.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60% 진행 - hidden states element shapes: [torch.Size([32, 8, 32, 64]), torch.Size([32, 8, 32, 64])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  70%|██████▉   | 112/161 [00:22<00:09,  4.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "70% 진행 - hidden states element shapes: [torch.Size([32, 8, 32, 64]), torch.Size([32, 8, 32, 64])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  80%|███████▉  | 128/161 [00:26<00:06,  4.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "80% 진행 - hidden states element shapes: [torch.Size([32, 8, 32, 64]), torch.Size([32, 8, 32, 64])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  89%|████████▉ | 144/161 [00:29<00:03,  4.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "89% 진행 - hidden states element shapes: [torch.Size([32, 8, 32, 64]), torch.Size([32, 8, 32, 64])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 161/161 [00:32<00:00,  4.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "99% 진행 - hidden states element shapes: [torch.Size([32, 8, 32, 64]), torch.Size([32, 8, 32, 64])]\n",
            "Epoch 2 Loss: 3.5054\n",
            "Epoch 종료 시 hidden states length: 2\n",
            "Epoch 종료 시 hidden states shape: torch.Size([2, 24, 8, 32, 64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 35/35 [00:05<00:00,  5.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 3.6071, Validation Accuracy: 4.26%\n",
            "\n",
            "Epoch 3/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  10%|▉         | 16/161 [00:03<00:29,  4.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10% 진행 - hidden states element shapes: [torch.Size([32, 8, 32, 64]), torch.Size([32, 8, 32, 64])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  20%|█▉        | 32/161 [00:06<00:25,  5.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20% 진행 - hidden states element shapes: [torch.Size([32, 8, 32, 64]), torch.Size([32, 8, 32, 64])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  30%|███       | 49/161 [00:09<00:21,  5.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30% 진행 - hidden states element shapes: [torch.Size([32, 8, 32, 64]), torch.Size([32, 8, 32, 64])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  40%|████      | 65/161 [00:13<00:18,  5.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "40% 진행 - hidden states element shapes: [torch.Size([32, 8, 32, 64]), torch.Size([32, 8, 32, 64])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  50%|████▉     | 80/161 [00:16<00:15,  5.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50% 진행 - hidden states element shapes: [torch.Size([32, 8, 32, 64]), torch.Size([32, 8, 32, 64])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  60%|█████▉    | 96/161 [00:19<00:12,  5.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60% 진행 - hidden states element shapes: [torch.Size([32, 8, 32, 64]), torch.Size([32, 8, 32, 64])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  70%|███████   | 113/161 [00:22<00:10,  4.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "70% 진행 - hidden states element shapes: [torch.Size([32, 8, 32, 64]), torch.Size([32, 8, 32, 64])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  80%|███████▉  | 128/161 [00:25<00:06,  5.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "80% 진행 - hidden states element shapes: [torch.Size([32, 8, 32, 64]), torch.Size([32, 8, 32, 64])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  90%|█████████ | 145/161 [00:29<00:03,  4.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "89% 진행 - hidden states element shapes: [torch.Size([32, 8, 32, 64]), torch.Size([32, 8, 32, 64])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 161/161 [00:32<00:00,  4.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "99% 진행 - hidden states element shapes: [torch.Size([32, 8, 32, 64]), torch.Size([32, 8, 32, 64])]\n",
            "Epoch 3 Loss: 3.4997\n",
            "Epoch 종료 시 hidden states length: 2\n",
            "Epoch 종료 시 hidden states shape: torch.Size([2, 24, 8, 32, 64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 35/35 [00:05<00:00,  5.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 3.5063, Validation Accuracy: 4.63%\n",
            "\n",
            "Epoch 4/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  10%|▉         | 16/161 [00:03<00:28,  5.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10% 진행 - hidden states element shapes: [torch.Size([32, 8, 32, 64]), torch.Size([32, 8, 32, 64])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  20%|█▉        | 32/161 [00:06<00:26,  4.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20% 진행 - hidden states element shapes: [torch.Size([32, 8, 32, 64]), torch.Size([32, 8, 32, 64])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  30%|██▉       | 48/161 [00:09<00:22,  4.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30% 진행 - hidden states element shapes: [torch.Size([32, 8, 32, 64]), torch.Size([32, 8, 32, 64])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  40%|███▉      | 64/161 [00:13<00:20,  4.80it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "40% 진행 - hidden states element shapes: [torch.Size([32, 8, 32, 64]), torch.Size([32, 8, 32, 64])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  50%|████▉     | 80/161 [00:16<00:16,  4.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50% 진행 - hidden states element shapes: [torch.Size([32, 8, 32, 64]), torch.Size([32, 8, 32, 64])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  60%|█████▉    | 96/161 [00:19<00:13,  4.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60% 진행 - hidden states element shapes: [torch.Size([32, 8, 32, 64]), torch.Size([32, 8, 32, 64])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  60%|██████    | 97/161 [00:20<00:13,  4.82it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-729afcc235cf>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nEpoch {epoch+1}/{num_epochs}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Validation\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-35-6176b2201486>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint_interval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_batches\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__getitems__\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m             \u001b[0msample_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcumulative_sizes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_idx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/oxford_iiit_pet.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    993\u001b[0m             \u001b[0mdeprecate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    994\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 995\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    996\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    997\u001b[0m         \u001b[0mhas_transparency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"transparency\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m                             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m                             \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m                                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "for epoch in range(num_epochs):\n",
        "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "    train(model, train_loader, criterion, optimizer, device)\n",
        "    evaluate(model, val_loader, criterion, device, phase=\"Validation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Gc2zQDfrvXz",
        "outputId": "f31fd070-d102-4dd0-8da9-07329c964dc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Test Evaluation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Test: 100%|██████████| 35/35 [00:06<00:00,  5.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 3.5360, Test Accuracy: 5.08%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nFinal Test Evaluation\")\n",
        "evaluate(model, test_loader, criterion, device, phase=\"Test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FH6ZUi6UrvVK",
        "outputId": "8d47fbe6-7341-4aec-8ba0-b69b9fe0a0be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference Time Measurement Results:\n",
            "Total Inferences: 35\n",
            "Average Time: 14.30 ms\n",
            "Standard Deviation: 0.57 ms\n",
            "Maximum Time: 16.06 ms\n",
            "Minimum Time: 13.80 ms\n"
          ]
        }
      ],
      "source": [
        "times = measure_inference_time(model, test_loader, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tTWtuIp6qFXC",
        "outputId": "8d52ef5f-b6f3-4784-ca36-6f00d91f25e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Batch 1 - Hidden States:\n",
            "\n",
            "Batch 2 - Hidden States:\n",
            "\n",
            "Batch 3 - Hidden States:\n",
            "\n",
            "Batch 4 - Hidden States:\n",
            "\n",
            "Batch 5 - Hidden States:\n",
            "\n",
            "Batch 6 - Hidden States:\n",
            "\n",
            "Batch 7 - Hidden States:\n",
            "\n",
            "Batch 8 - Hidden States:\n",
            "\n",
            "Batch 9 - Hidden States:\n",
            "\n",
            "Batch 10 - Hidden States:\n",
            "\n",
            "Batch 11 - Hidden States:\n",
            "\n",
            "Batch 12 - Hidden States:\n",
            "\n",
            "Batch 13 - Hidden States:\n",
            "\n",
            "Batch 14 - Hidden States:\n",
            "\n",
            "Batch 15 - Hidden States:\n",
            "\n",
            "Batch 16 - Hidden States:\n",
            "\n",
            "Batch 17 - Hidden States:\n",
            "\n",
            "Batch 18 - Hidden States:\n",
            "\n",
            "Batch 19 - Hidden States:\n",
            "\n",
            "Batch 20 - Hidden States:\n",
            "\n",
            "Batch 21 - Hidden States:\n",
            "\n",
            "Batch 22 - Hidden States:\n",
            "\n",
            "Batch 23 - Hidden States:\n",
            "\n",
            "Batch 24 - Hidden States:\n",
            "\n",
            "Batch 25 - Hidden States:\n",
            "\n",
            "Batch 26 - Hidden States:\n",
            "\n",
            "Batch 27 - Hidden States:\n",
            "\n",
            "Batch 28 - Hidden States:\n",
            "\n",
            "Batch 29 - Hidden States:\n",
            "\n",
            "Batch 30 - Hidden States:\n",
            "\n",
            "Batch 31 - Hidden States:\n",
            "\n",
            "Batch 32 - Hidden States:\n",
            "\n",
            "Batch 33 - Hidden States:\n",
            "\n",
            "Batch 34 - Hidden States:\n",
            "\n",
            "Batch 35 - Hidden States:\n",
            "\n",
            "Batch 36 - Hidden States:\n",
            "\n",
            "Batch 37 - Hidden States:\n",
            "\n",
            "Batch 38 - Hidden States:\n",
            "\n",
            "Batch 39 - Hidden States:\n",
            "\n",
            "Batch 40 - Hidden States:\n",
            "\n",
            "Batch 41 - Hidden States:\n",
            "\n",
            "Batch 42 - Hidden States:\n",
            "\n",
            "Batch 43 - Hidden States:\n",
            "\n",
            "Batch 44 - Hidden States:\n",
            "\n",
            "Batch 45 - Hidden States:\n",
            "\n",
            "Batch 46 - Hidden States:\n",
            "\n",
            "Batch 47 - Hidden States:\n",
            "\n",
            "Batch 48 - Hidden States:\n",
            "\n",
            "Batch 49 - Hidden States:\n",
            "\n",
            "Batch 50 - Hidden States:\n",
            "\n",
            "Batch 51 - Hidden States:\n",
            "\n",
            "Batch 52 - Hidden States:\n",
            "\n",
            "Batch 53 - Hidden States:\n",
            "\n",
            "Batch 54 - Hidden States:\n",
            "\n",
            "Batch 55 - Hidden States:\n",
            "\n",
            "Batch 56 - Hidden States:\n",
            "\n",
            "Batch 57 - Hidden States:\n",
            "\n",
            "Batch 58 - Hidden States:\n",
            "\n",
            "Batch 59 - Hidden States:\n",
            "\n",
            "Batch 60 - Hidden States:\n",
            "\n",
            "Batch 61 - Hidden States:\n",
            "\n",
            "Batch 62 - Hidden States:\n",
            "\n",
            "Batch 63 - Hidden States:\n",
            "\n",
            "Batch 64 - Hidden States:\n",
            "\n",
            "Batch 65 - Hidden States:\n",
            "\n",
            "Batch 66 - Hidden States:\n",
            "\n",
            "Batch 67 - Hidden States:\n",
            "\n",
            "Batch 68 - Hidden States:\n",
            "\n",
            "Batch 69 - Hidden States:\n",
            "\n",
            "Batch 70 - Hidden States:\n",
            "\n",
            "Batch 71 - Hidden States:\n",
            "\n",
            "Batch 72 - Hidden States:\n",
            "\n",
            "Batch 73 - Hidden States:\n",
            "\n",
            "Batch 74 - Hidden States:\n",
            "\n",
            "Batch 75 - Hidden States:\n",
            "\n",
            "Batch 76 - Hidden States:\n",
            "\n",
            "Batch 77 - Hidden States:\n",
            "\n",
            "Batch 78 - Hidden States:\n",
            "\n",
            "Batch 79 - Hidden States:\n",
            "\n",
            "Batch 80 - Hidden States:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-e33e9bf0de20>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Forward pass to get the output and hidden states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 데이터를 장치로 이동 (GPU 또는 CPU)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__getitems__\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m             \u001b[0msample_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcumulative_sizes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_idx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/oxford_iiit_pet.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/vision.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0mPIL\u001b[0m \u001b[0mImage\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRescaled\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \"\"\"\n\u001b[0;32m--> 354\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mantialias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Anti-alias option is always applied for PIL Image input. Argument antialias is ignored.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0mpil_interpolation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_modes_mapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF_pil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpil_interpolation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mF_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mantialias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mantialias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/_functional_pil.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Got inappropriate size arg: {size}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[1;32m   2326\u001b[0m                 )\n\u001b[1;32m   2327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2328\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2330\u001b[0m     def reduce(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Assuming 'device' is defined and set to your CUDA device (e.g., 'cuda:0')\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Sample input for testing, moved to the device\n",
        "sample_input = torch.rand(1, 3, 224, 224).to(device)  # Move input to device\n",
        "\n",
        "# Initialize the model and move it to the device\n",
        "model = GLALeViT(num_classes=37).to(device)  # Move model to device\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval() # This line is added to set the model to evaluation mode\n",
        "\n",
        "# Forward pass to get the output and hidden states\n",
        "for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
        "    inputs = inputs.to(device)  # 데이터를 장치로 이동 (GPU 또는 CPU)\n",
        "\n",
        "    # 모델의 forward pass를 수행하여 output과 hidden states를 얻음\n",
        "    output, hidden_states = model(inputs)\n",
        "\n",
        "    # 각 배치에 대해 hidden state를 출력하고 시각화합니다.\n",
        "    print(f\"\\nBatch {batch_idx+1} - Hidden States:\")\n",
        "    for i, hidden_state in enumerate(hidden_states):\n",
        "        if hidden_state is not None:\n",
        "            print(f\"Hidden state {i+1} shape: {hidden_state.shape}\")\n",
        "\n",
        "            # 첫 번째 이미지를 시각화 (배치에서 첫 번째 예제)\n",
        "            hidden_state_np = hidden_state[0].detach().cpu().numpy()\n",
        "\n",
        "            if hidden_state_np.ndim > 1:\n",
        "                plt.imshow(hidden_state_np, cmap='gray')\n",
        "                plt.title(f'Hidden State {i+1} - Batch {batch_idx+1}')\n",
        "                plt.colorbar()\n",
        "                plt.show()\n",
        "        else:\n",
        "            print(f\"Hidden state {i+1} is None.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}