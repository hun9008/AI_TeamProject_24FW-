{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYpt3QcVCl83",
        "outputId": "7d78ee0a-3628-4076-82b4-89ec1edfe31c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9655 sha256=8418a33294d788f45a2e8edc6f14b04a7fed619d937785b0d75c778661ab14a0\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/b3/0f/a40dbd1c6861731779f62cc4babcb234387e11d697df70ee97\n",
            "Successfully built wget\n",
            "Installing collected packages: wget, torchinfo\n",
            "Successfully installed torchinfo-1.8.0 wget-3.2\n",
            "--2025-04-06 10:38:32--  https://zenodo.org/record/1214456/files/NCT-CRC-HE-100K.zip\n",
            "Resolving zenodo.org (zenodo.org)... 188.185.48.194, 188.185.45.92, 188.185.43.25, ...\n",
            "Connecting to zenodo.org (zenodo.org)|188.185.48.194|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 MOVED PERMANENTLY\n",
            "Location: /records/1214456/files/NCT-CRC-HE-100K.zip [following]\n",
            "--2025-04-06 10:38:32--  https://zenodo.org/records/1214456/files/NCT-CRC-HE-100K.zip\n",
            "Reusing existing connection to zenodo.org:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11690284003 (11G) [application/octet-stream]\n",
            "Saving to: ‘NCT-CRC-HE-100K.zip’\n",
            "\n",
            "NCT-CRC-HE-100K.zip 100%[===================>]  10.89G  9.22MB/s    in 26m 31s \n",
            "\n",
            "2025-04-06 11:05:04 (7.01 MB/s) - ‘NCT-CRC-HE-100K.zip’ saved [11690284003/11690284003]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!pip install wget torchinfo\n",
        "!wget -O NCT-CRC-HE-100K.zip https://zenodo.org/record/1214456/files/NCT-CRC-HE-100K.zip\n",
        "!unzip -qq NCT-CRC-HE-100K.zip -d train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Wa8mdzSwvzMe"
      },
      "outputs": [],
      "source": [
        "import timm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, ConcatDataset\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "import itertools\n",
        "from torchinfo import summary\n",
        "from sklearn.metrics import accuracy_score, balanced_accuracy_score, classification_report, confusion_matrix\n",
        "from PIL import Image\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import random\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Subset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4FLtpbS_Vrd7"
      },
      "outputs": [],
      "source": [
        "class ConvNorm(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=2, padding=1):\n",
        "        super(ConvNorm, self).__init__()\n",
        "        self.linear = nn.Conv2d(\n",
        "            in_channels, out_channels, kernel_size=kernel_size,\n",
        "            stride=stride, padding=padding, bias=False\n",
        "        )\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear(x)\n",
        "        x = self.bn(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zhaJu1oZyp0D"
      },
      "outputs": [],
      "source": [
        "class Stem16(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Stem16, self).__init__()\n",
        "        self.conv1 = ConvNorm(3, 32)\n",
        "        self.act1 = nn.Hardswish()\n",
        "        self.conv2 = ConvNorm(32, 64)\n",
        "        self.act2 = nn.Hardswish()\n",
        "        self.conv3 = ConvNorm(64, 128)\n",
        "        self.act3 = nn.Hardswish()\n",
        "        self.conv4 = ConvNorm(128, 256)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.act1(self.conv1(x))\n",
        "        x = self.act2(self.conv2(x))\n",
        "        x = self.act3(self.conv3(x))\n",
        "        x = self.conv4(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "W1Xh6-wOyskM"
      },
      "outputs": [],
      "source": [
        "class LinearNorm(nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super(LinearNorm, self).__init__()\n",
        "        self.linear = nn.Linear(in_features, out_features, bias=False)\n",
        "        self.bn = nn.BatchNorm1d(out_features)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        if x.dim() == 3:\n",
        "            B, N, C = x.shape\n",
        "            x = x.reshape(B * N, C)\n",
        "            x = self.bn(self.linear(x))\n",
        "            x = x.reshape(B, N, -1)\n",
        "        else:\n",
        "            x = self.bn(self.linear(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "wHeuFDCPyuLf"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, num_heads, attn_ratio=2):\n",
        "        super(Attention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = head_dim ** -0.5\n",
        "        inner_dim = head_dim * num_heads * 3\n",
        "        self.qkv = LinearNorm(dim, inner_dim)\n",
        "\n",
        "        self.proj = nn.Sequential(\n",
        "            nn.Hardswish(),\n",
        "            LinearNorm(dim, dim)\n",
        "        )\n",
        "\n",
        "        self.attention_biases = None\n",
        "        self.attention_bias_idxs = None\n",
        "\n",
        "    def compute_attention_bias(self, resolution):\n",
        "\n",
        "        points = list(itertools.product(range(resolution), range(resolution)))\n",
        "        N = len(points)\n",
        "\n",
        "        attention_offsets = {}\n",
        "        idxs = []\n",
        "\n",
        "        # if N = 196, then resolution = 14\n",
        "        for p1 in points:\n",
        "            for p2 in points:\n",
        "                offset = (abs(p1[0] - p2[0]), abs(p1[1] - p2[1]))\n",
        "                if offset not in attention_offsets:\n",
        "                    attention_offsets[offset] = len(attention_offsets)\n",
        "                idxs.append(attention_offsets[offset])\n",
        "\n",
        "        num_offsets = len(attention_offsets)\n",
        "\n",
        "        # 각 attention head에 대해 num_offsets 만큼의 학습 가능한 Bias를 생성\n",
        "        self.attention_biases = nn.Parameter(torch.zeros(self.num_heads, num_offsets).to(next(self.parameters()).device))\n",
        "        self.attention_bias_idxs = torch.LongTensor(idxs).view(N, N).to(next(self.parameters()).device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        resolution = int(N ** 0.5)\n",
        "\n",
        "        if self.attention_biases is None or self.attention_bias_idxs.shape[0] != N:\n",
        "            self.compute_attention_bias(resolution)\n",
        "\n",
        "        qkv = self.qkv(x)\n",
        "        qkv = qkv.view(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        # qkv: (3, B, num_heads, N, head_dim)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2] # q, k, v: (B, num_heads, N, head_dim)\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale # attn: (B, num_heads, N, N)\n",
        "        attn_bias = self.attention_biases[:, self.attention_bias_idxs].unsqueeze(0) # attn_bias: (1, num_heads, N, N)\n",
        "        attn = attn + attn_bias\n",
        "        attn = attn.softmax(dim=-1)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        return self.proj(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "xvyY-FeUywyR"
      },
      "outputs": [],
      "source": [
        "class LevitMlp(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features, out_features):\n",
        "        super(LevitMlp, self).__init__()\n",
        "        self.ln1 = LinearNorm(in_features, hidden_features)\n",
        "        self.act = nn.Hardswish()\n",
        "        self.drop = nn.Dropout(p=0.5, inplace=False)#dropout 적용\n",
        "        self.ln2 = LinearNorm(hidden_features, out_features)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.ln1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.ln2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Ubl233osyyHQ"
      },
      "outputs": [],
      "source": [
        "class LevitBlock(nn.Module):\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=2):\n",
        "        super(LevitBlock, self).__init__()\n",
        "        self.attn = Attention(dim, num_heads)\n",
        "        self.drop_path1 = nn.Identity()\n",
        "        self.mlp = LevitMlp(dim, dim * mlp_ratio, dim)\n",
        "        self.drop_path2 = nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.drop_path1(self.attn(x))\n",
        "        x = x + self.drop_path2(self.mlp(x))\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "cjBHkWJYyzfc"
      },
      "outputs": [],
      "source": [
        "class CNNDownsample(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(CNNDownsample, self).__init__()\n",
        "        self.out_channels = out_channels\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1)\n",
        "        self.act = nn.Hardswish()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # print(x.shape)\n",
        "        B, N, C = x.shape # (B, N, C)  N=H*W (16 * 16 = 196)\n",
        "        H = int(np.sqrt(N))\n",
        "        x = x.view(B, H, H, C).permute(0, 3, 1, 2)\n",
        "        x = self.conv(x)\n",
        "        x = self.act(x)\n",
        "        x = x.permute(0, 2, 3, 1).view(B, -1, self.out_channels)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "jNpOjwVVy1Im"
      },
      "outputs": [],
      "source": [
        "class LevitStage(nn.Module):\n",
        "    def __init__(self, dim, out_dim, num_heads, num_blocks, downsample=True):\n",
        "        super(LevitStage, self).__init__()\n",
        "        self.downsample = CNNDownsample(dim, out_dim) if downsample else nn.Identity()\n",
        "        self.blocks = nn.Sequential(*[LevitBlock(out_dim, num_heads) for _ in range(num_blocks)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.downsample(x)\n",
        "        x = self.blocks(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "FyFtflRsy2QE"
      },
      "outputs": [],
      "source": [
        "class ConvLevitStage(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, num_blocks, kernel_size, stride, padding):\n",
        "        super(ConvLevitStage, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            *[nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size, stride, padding)\n",
        "              for i in range(num_blocks)],\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "yKzCWv-Ny3j9"
      },
      "outputs": [],
      "source": [
        "class NormLinear(nn.Module):\n",
        "    def __init__(self, in_features, out_features, dropout_prob=0.5):#drop_out_0.5 적용\n",
        "        super(NormLinear, self).__init__()\n",
        "        self.bn = nn.BatchNorm1d(in_features)\n",
        "        self.drop = nn.Dropout(p=dropout_prob, inplace=False)\n",
        "        self.linear = nn.Linear(in_features, out_features, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.bn(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.linear(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "nS7iQ8-453Ru"
      },
      "outputs": [],
      "source": [
        "class LevitDistilled(nn.Module):\n",
        "    def __init__(self, num_classes=9):\n",
        "        super().__init__()\n",
        "        self.stem = Stem16()\n",
        "        self.stage1 = LevitStage(256, 256, 4, 2, downsample=False)\n",
        "        self.stage2 = LevitStage(256, 384, 6, 2, downsample=True)\n",
        "        self.conv1x1 = nn.Sequential(nn.Conv2d(384, 512, 1), nn.BatchNorm2d(512), nn.ReLU(True))\n",
        "        self.head = NormLinear(512, num_classes, dropout_prob=0.0)\n",
        "        self.head_dist = NormLinear(512, num_classes, dropout_prob=0.0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.stem(x)\n",
        "        B, C, H, W = x.shape\n",
        "        x = x.view(B, C, -1).transpose(1, 2)\n",
        "        x = self.stage1(x)\n",
        "        x = self.stage2(x)\n",
        "        H = W = int(x.shape[1] ** 0.5)\n",
        "        x = x.transpose(1, 2).view(B, 384, H, W)\n",
        "        x = self.conv1x1(x)\n",
        "        x = torch.mean(x, dim=(2, 3))\n",
        "        return self.head(x)\n",
        "\n",
        "    def extract_features(self, x):\n",
        "        x = self.stem(x)\n",
        "        B, C, H, W = x.shape\n",
        "        x = x.view(B, C, -1).transpose(1, 2)\n",
        "        x = self.stage1(x)\n",
        "        x = self.stage2(x)\n",
        "        H = W = int(x.shape[1] ** 0.5)\n",
        "        x = x.transpose(1, 2).view(B, 384, H, W)\n",
        "        x = self.conv1x1(x)\n",
        "        return torch.mean(x, dim=(2, 3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "lbCq25bWzJSQ",
        "outputId": "6eb87651-ef4a-4a6b-b442-22210b626e1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LevitDistilled(\n",
            "  (stem): Stem16(\n",
            "    (conv1): ConvNorm(\n",
            "      (linear): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (act1): Hardswish()\n",
            "    (conv2): ConvNorm(\n",
            "      (linear): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (act2): Hardswish()\n",
            "    (conv3): ConvNorm(\n",
            "      (linear): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (act3): Hardswish()\n",
            "    (conv4): ConvNorm(\n",
            "      (linear): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (stage1): LevitStage(\n",
            "    (downsample): Identity()\n",
            "    (blocks): Sequential(\n",
            "      (0): LevitBlock(\n",
            "        (attn): Attention(\n",
            "          (qkv): LinearNorm(\n",
            "            (linear): Linear(in_features=256, out_features=768, bias=False)\n",
            "            (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (proj): Sequential(\n",
            "            (0): Hardswish()\n",
            "            (1): LinearNorm(\n",
            "              (linear): Linear(in_features=256, out_features=256, bias=False)\n",
            "              (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (drop_path1): Identity()\n",
            "        (mlp): LevitMlp(\n",
            "          (ln1): LinearNorm(\n",
            "            (linear): Linear(in_features=256, out_features=512, bias=False)\n",
            "            (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (act): Hardswish()\n",
            "          (drop): Dropout(p=0.5, inplace=False)\n",
            "          (ln2): LinearNorm(\n",
            "            (linear): Linear(in_features=512, out_features=256, bias=False)\n",
            "            (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (drop_path2): Identity()\n",
            "      )\n",
            "      (1): LevitBlock(\n",
            "        (attn): Attention(\n",
            "          (qkv): LinearNorm(\n",
            "            (linear): Linear(in_features=256, out_features=768, bias=False)\n",
            "            (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (proj): Sequential(\n",
            "            (0): Hardswish()\n",
            "            (1): LinearNorm(\n",
            "              (linear): Linear(in_features=256, out_features=256, bias=False)\n",
            "              (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (drop_path1): Identity()\n",
            "        (mlp): LevitMlp(\n",
            "          (ln1): LinearNorm(\n",
            "            (linear): Linear(in_features=256, out_features=512, bias=False)\n",
            "            (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (act): Hardswish()\n",
            "          (drop): Dropout(p=0.5, inplace=False)\n",
            "          (ln2): LinearNorm(\n",
            "            (linear): Linear(in_features=512, out_features=256, bias=False)\n",
            "            (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (drop_path2): Identity()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (stage2): LevitStage(\n",
            "    (downsample): CNNDownsample(\n",
            "      (conv): Conv2d(256, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "      (act): Hardswish()\n",
            "    )\n",
            "    (blocks): Sequential(\n",
            "      (0): LevitBlock(\n",
            "        (attn): Attention(\n",
            "          (qkv): LinearNorm(\n",
            "            (linear): Linear(in_features=384, out_features=1152, bias=False)\n",
            "            (bn): BatchNorm1d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (proj): Sequential(\n",
            "            (0): Hardswish()\n",
            "            (1): LinearNorm(\n",
            "              (linear): Linear(in_features=384, out_features=384, bias=False)\n",
            "              (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (drop_path1): Identity()\n",
            "        (mlp): LevitMlp(\n",
            "          (ln1): LinearNorm(\n",
            "            (linear): Linear(in_features=384, out_features=768, bias=False)\n",
            "            (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (act): Hardswish()\n",
            "          (drop): Dropout(p=0.5, inplace=False)\n",
            "          (ln2): LinearNorm(\n",
            "            (linear): Linear(in_features=768, out_features=384, bias=False)\n",
            "            (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (drop_path2): Identity()\n",
            "      )\n",
            "      (1): LevitBlock(\n",
            "        (attn): Attention(\n",
            "          (qkv): LinearNorm(\n",
            "            (linear): Linear(in_features=384, out_features=1152, bias=False)\n",
            "            (bn): BatchNorm1d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (proj): Sequential(\n",
            "            (0): Hardswish()\n",
            "            (1): LinearNorm(\n",
            "              (linear): Linear(in_features=384, out_features=384, bias=False)\n",
            "              (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (drop_path1): Identity()\n",
            "        (mlp): LevitMlp(\n",
            "          (ln1): LinearNorm(\n",
            "            (linear): Linear(in_features=384, out_features=768, bias=False)\n",
            "            (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (act): Hardswish()\n",
            "          (drop): Dropout(p=0.5, inplace=False)\n",
            "          (ln2): LinearNorm(\n",
            "            (linear): Linear(in_features=768, out_features=384, bias=False)\n",
            "            (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (drop_path2): Identity()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (conv1x1): Sequential(\n",
            "    (0): Conv2d(384, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "  )\n",
            "  (head): NormLinear(\n",
            "    (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (drop): Dropout(p=0.0, inplace=False)\n",
            "    (linear): Linear(in_features=512, out_features=9, bias=True)\n",
            "  )\n",
            "  (head_dist): NormLinear(\n",
            "    (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (drop): Dropout(p=0.0, inplace=False)\n",
            "    (linear): Linear(in_features=512, out_features=9, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model = LevitDistilled(num_classes=9)\n",
        "print(model)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "batch_size = 32\n",
        "learning_rate = 5e-4\n",
        "num_epochs = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "gNpMsNYAzLDF",
        "outputId": "e6f7dd40-a072-42e0-e9de-f0fbdcdf75ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=========================================================================================================\n",
            "Layer (type:depth-idx)                                  Output Shape              Param #\n",
            "=========================================================================================================\n",
            "LevitDistilled                                          [32, 9]                   5,641\n",
            "├─Stem16: 1-1                                           [32, 256, 14, 14]         --\n",
            "│    └─ConvNorm: 2-1                                    [32, 32, 112, 112]        --\n",
            "│    │    └─Conv2d: 3-1                                 [32, 32, 112, 112]        864\n",
            "│    │    └─BatchNorm2d: 3-2                            [32, 32, 112, 112]        64\n",
            "│    └─Hardswish: 2-2                                   [32, 32, 112, 112]        --\n",
            "│    └─ConvNorm: 2-3                                    [32, 64, 56, 56]          --\n",
            "│    │    └─Conv2d: 3-3                                 [32, 64, 56, 56]          18,432\n",
            "│    │    └─BatchNorm2d: 3-4                            [32, 64, 56, 56]          128\n",
            "│    └─Hardswish: 2-4                                   [32, 64, 56, 56]          --\n",
            "│    └─ConvNorm: 2-5                                    [32, 128, 28, 28]         --\n",
            "│    │    └─Conv2d: 3-5                                 [32, 128, 28, 28]         73,728\n",
            "│    │    └─BatchNorm2d: 3-6                            [32, 128, 28, 28]         256\n",
            "│    └─Hardswish: 2-6                                   [32, 128, 28, 28]         --\n",
            "│    └─ConvNorm: 2-7                                    [32, 256, 14, 14]         --\n",
            "│    │    └─Conv2d: 3-7                                 [32, 256, 14, 14]         294,912\n",
            "│    │    └─BatchNorm2d: 3-8                            [32, 256, 14, 14]         512\n",
            "├─LevitStage: 1-2                                       [32, 196, 256]            --\n",
            "│    └─Identity: 2-8                                    [32, 196, 256]            --\n",
            "│    └─Sequential: 2-9                                  [32, 196, 256]            --\n",
            "│    │    └─LevitBlock: 3-9                             [32, 196, 256]            527,872\n",
            "│    │    └─LevitBlock: 3-10                            [32, 196, 256]            527,872\n",
            "├─LevitStage: 1-3                                       [32, 49, 384]             --\n",
            "│    └─CNNDownsample: 2-10                              [32, 49, 384]             --\n",
            "│    │    └─Conv2d: 3-11                                [32, 384, 7, 7]           885,120\n",
            "│    │    └─Hardswish: 3-12                             [32, 384, 7, 7]           --\n",
            "│    └─Sequential: 2-11                                 [32, 49, 384]             --\n",
            "│    │    └─LevitBlock: 3-13                            [32, 49, 384]             1,185,024\n",
            "│    │    └─LevitBlock: 3-14                            [32, 49, 384]             1,185,024\n",
            "├─Sequential: 1-4                                       [32, 512, 7, 7]           --\n",
            "│    └─Conv2d: 2-12                                     [32, 512, 7, 7]           197,120\n",
            "│    └─BatchNorm2d: 2-13                                [32, 512, 7, 7]           1,024\n",
            "│    └─ReLU: 2-14                                       [32, 512, 7, 7]           --\n",
            "├─NormLinear: 1-5                                       [32, 9]                   --\n",
            "│    └─BatchNorm1d: 2-15                                [32, 512]                 1,024\n",
            "│    └─Dropout: 2-16                                    [32, 512]                 --\n",
            "│    └─Linear: 2-17                                     [32, 9]                   4,617\n",
            "=========================================================================================================\n",
            "Total params: 4,909,234\n",
            "Trainable params: 4,909,234\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (Units.GIGABYTES): 17.93\n",
            "=========================================================================================================\n",
            "Input size (MB): 19.27\n",
            "Forward/backward pass size (MB): 897.68\n",
            "Params size (MB): 19.61\n",
            "Estimated Total Size (MB): 936.56\n",
            "=========================================================================================================\n"
          ]
        }
      ],
      "source": [
        "print(summary(model, input_size=(32, 3, 224, 224)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "qDELjYy2zP90",
        "outputId": "9f54db28-7b75-413f-e16c-dcc4f862ef6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=========================================================================================================\n",
            "Layer (type:depth-idx)                                  Output Shape              Param #\n",
            "=========================================================================================================\n",
            "LevitDistilled                                          [32, 9]                   5,641\n",
            "├─Stem16: 1-1                                           [32, 256, 14, 14]         --\n",
            "│    └─conv1.linear.weight                                                        ├─864\n",
            "│    └─conv1.bn.weight                                                            ├─32\n",
            "│    └─conv1.bn.bias                                                              ├─32\n",
            "│    └─conv2.linear.weight                                                        ├─18,432\n",
            "│    └─conv2.bn.weight                                                            ├─64\n",
            "│    └─conv2.bn.bias                                                              ├─64\n",
            "│    └─conv3.linear.weight                                                        ├─73,728\n",
            "│    └─conv3.bn.weight                                                            ├─128\n",
            "│    └─conv3.bn.bias                                                              ├─128\n",
            "│    └─conv4.linear.weight                                                        ├─294,912\n",
            "│    └─conv4.bn.weight                                                            ├─256\n",
            "│    └─conv4.bn.bias                                                              └─256\n",
            "│    └─ConvNorm: 2-1                                    [32, 32, 112, 112]        --\n",
            "│    │    └─linear.weight                                                         ├─864\n",
            "│    │    └─bn.weight                                                             ├─32\n",
            "│    │    └─bn.bias                                                               └─32\n",
            "│    │    └─Conv2d: 3-1                                 [32, 32, 112, 112]        864\n",
            "│    │    │    └─weight                                                           └─864\n",
            "│    │    └─BatchNorm2d: 3-2                            [32, 32, 112, 112]        64\n",
            "│    │    │    └─weight                                                           ├─32\n",
            "│    │    │    └─bias                                                             └─32\n",
            "│    └─Hardswish: 2-2                                   [32, 32, 112, 112]        --\n",
            "│    └─ConvNorm: 2-3                                    [32, 64, 56, 56]          --\n",
            "│    │    └─linear.weight                                                         ├─18,432\n",
            "│    │    └─bn.weight                                                             ├─64\n",
            "│    │    └─bn.bias                                                               └─64\n",
            "│    │    └─Conv2d: 3-3                                 [32, 64, 56, 56]          18,432\n",
            "│    │    │    └─weight                                                           └─18,432\n",
            "│    │    └─BatchNorm2d: 3-4                            [32, 64, 56, 56]          128\n",
            "│    │    │    └─weight                                                           ├─64\n",
            "│    │    │    └─bias                                                             └─64\n",
            "│    └─Hardswish: 2-4                                   [32, 64, 56, 56]          --\n",
            "│    └─ConvNorm: 2-5                                    [32, 128, 28, 28]         --\n",
            "│    │    └─linear.weight                                                         ├─73,728\n",
            "│    │    └─bn.weight                                                             ├─128\n",
            "│    │    └─bn.bias                                                               └─128\n",
            "│    │    └─Conv2d: 3-5                                 [32, 128, 28, 28]         73,728\n",
            "│    │    │    └─weight                                                           └─73,728\n",
            "│    │    └─BatchNorm2d: 3-6                            [32, 128, 28, 28]         256\n",
            "│    │    │    └─weight                                                           ├─128\n",
            "│    │    │    └─bias                                                             └─128\n",
            "│    └─Hardswish: 2-6                                   [32, 128, 28, 28]         --\n",
            "│    └─ConvNorm: 2-7                                    [32, 256, 14, 14]         --\n",
            "│    │    └─linear.weight                                                         ├─294,912\n",
            "│    │    └─bn.weight                                                             ├─256\n",
            "│    │    └─bn.bias                                                               └─256\n",
            "│    │    └─Conv2d: 3-7                                 [32, 256, 14, 14]         294,912\n",
            "│    │    │    └─weight                                                           └─294,912\n",
            "│    │    └─BatchNorm2d: 3-8                            [32, 256, 14, 14]         512\n",
            "│    │    │    └─weight                                                           ├─256\n",
            "│    │    │    └─bias                                                             └─256\n",
            "├─LevitStage: 1-2                                       [32, 196, 256]            --\n",
            "│    └─blocks.0.attn.attention_biases                                             ├─784\n",
            "│    └─blocks.0.attn.qkv.linear.weight                                            ├─196,608\n",
            "│    └─blocks.0.attn.qkv.bn.weight                                                ├─768\n",
            "│    └─blocks.0.attn.qkv.bn.bias                                                  ├─768\n",
            "│    └─blocks.0.attn.proj.1.linear.weight                                         ├─65,536\n",
            "│    └─blocks.0.attn.proj.1.bn.weight                                             ├─256\n",
            "│    └─blocks.0.attn.proj.1.bn.bias                                               ├─256\n",
            "│    └─blocks.0.mlp.ln1.linear.weight                                             ├─131,072\n",
            "│    └─blocks.0.mlp.ln1.bn.weight                                                 ├─512\n",
            "│    └─blocks.0.mlp.ln1.bn.bias                                                   ├─512\n",
            "│    └─blocks.0.mlp.ln2.linear.weight                                             ├─131,072\n",
            "│    └─blocks.0.mlp.ln2.bn.weight                                                 ├─256\n",
            "│    └─blocks.0.mlp.ln2.bn.bias                                                   ├─256\n",
            "│    └─blocks.1.attn.attention_biases                                             ├─784\n",
            "│    └─blocks.1.attn.qkv.linear.weight                                            ├─196,608\n",
            "│    └─blocks.1.attn.qkv.bn.weight                                                ├─768\n",
            "│    └─blocks.1.attn.qkv.bn.bias                                                  ├─768\n",
            "│    └─blocks.1.attn.proj.1.linear.weight                                         ├─65,536\n",
            "│    └─blocks.1.attn.proj.1.bn.weight                                             ├─256\n",
            "│    └─blocks.1.attn.proj.1.bn.bias                                               ├─256\n",
            "│    └─blocks.1.mlp.ln1.linear.weight                                             ├─131,072\n",
            "│    └─blocks.1.mlp.ln1.bn.weight                                                 ├─512\n",
            "│    └─blocks.1.mlp.ln1.bn.bias                                                   ├─512\n",
            "│    └─blocks.1.mlp.ln2.linear.weight                                             ├─131,072\n",
            "│    └─blocks.1.mlp.ln2.bn.weight                                                 ├─256\n",
            "│    └─blocks.1.mlp.ln2.bn.bias                                                   └─256\n",
            "│    └─Identity: 2-8                                    [32, 196, 256]            --\n",
            "│    └─Sequential: 2-9                                  [32, 196, 256]            --\n",
            "│    │    └─0.attn.attention_biases                                               ├─784\n",
            "│    │    └─0.attn.qkv.linear.weight                                              ├─196,608\n",
            "│    │    └─0.attn.qkv.bn.weight                                                  ├─768\n",
            "│    │    └─0.attn.qkv.bn.bias                                                    ├─768\n",
            "│    │    └─0.attn.proj.1.linear.weight                                           ├─65,536\n",
            "│    │    └─0.attn.proj.1.bn.weight                                               ├─256\n",
            "│    │    └─0.attn.proj.1.bn.bias                                                 ├─256\n",
            "│    │    └─0.mlp.ln1.linear.weight                                               ├─131,072\n",
            "│    │    └─0.mlp.ln1.bn.weight                                                   ├─512\n",
            "│    │    └─0.mlp.ln1.bn.bias                                                     ├─512\n",
            "│    │    └─0.mlp.ln2.linear.weight                                               ├─131,072\n",
            "│    │    └─0.mlp.ln2.bn.weight                                                   ├─256\n",
            "│    │    └─0.mlp.ln2.bn.bias                                                     ├─256\n",
            "│    │    └─1.attn.attention_biases                                               ├─784\n",
            "│    │    └─1.attn.qkv.linear.weight                                              ├─196,608\n",
            "│    │    └─1.attn.qkv.bn.weight                                                  ├─768\n",
            "│    │    └─1.attn.qkv.bn.bias                                                    ├─768\n",
            "│    │    └─1.attn.proj.1.linear.weight                                           ├─65,536\n",
            "│    │    └─1.attn.proj.1.bn.weight                                               ├─256\n",
            "│    │    └─1.attn.proj.1.bn.bias                                                 ├─256\n",
            "│    │    └─1.mlp.ln1.linear.weight                                               ├─131,072\n",
            "│    │    └─1.mlp.ln1.bn.weight                                                   ├─512\n",
            "│    │    └─1.mlp.ln1.bn.bias                                                     ├─512\n",
            "│    │    └─1.mlp.ln2.linear.weight                                               ├─131,072\n",
            "│    │    └─1.mlp.ln2.bn.weight                                                   ├─256\n",
            "│    │    └─1.mlp.ln2.bn.bias                                                     └─256\n",
            "│    │    └─LevitBlock: 3-9                             [32, 196, 256]            528,656\n",
            "│    │    │    └─attn.attention_biases                                            ├─784\n",
            "│    │    │    └─attn.qkv.linear.weight                                           ├─196,608\n",
            "│    │    │    └─attn.qkv.bn.weight                                               ├─768\n",
            "│    │    │    └─attn.qkv.bn.bias                                                 ├─768\n",
            "│    │    │    └─attn.proj.1.linear.weight                                        ├─65,536\n",
            "│    │    │    └─attn.proj.1.bn.weight                                            ├─256\n",
            "│    │    │    └─attn.proj.1.bn.bias                                              ├─256\n",
            "│    │    │    └─mlp.ln1.linear.weight                                            ├─131,072\n",
            "│    │    │    └─mlp.ln1.bn.weight                                                ├─512\n",
            "│    │    │    └─mlp.ln1.bn.bias                                                  ├─512\n",
            "│    │    │    └─mlp.ln2.linear.weight                                            ├─131,072\n",
            "│    │    │    └─mlp.ln2.bn.weight                                                ├─256\n",
            "│    │    │    └─mlp.ln2.bn.bias                                                  └─256\n",
            "│    │    └─LevitBlock: 3-10                            [32, 196, 256]            528,656\n",
            "│    │    │    └─attn.attention_biases                                            ├─784\n",
            "│    │    │    └─attn.qkv.linear.weight                                           ├─196,608\n",
            "│    │    │    └─attn.qkv.bn.weight                                               ├─768\n",
            "│    │    │    └─attn.qkv.bn.bias                                                 ├─768\n",
            "│    │    │    └─attn.proj.1.linear.weight                                        ├─65,536\n",
            "│    │    │    └─attn.proj.1.bn.weight                                            ├─256\n",
            "│    │    │    └─attn.proj.1.bn.bias                                              ├─256\n",
            "│    │    │    └─mlp.ln1.linear.weight                                            ├─131,072\n",
            "│    │    │    └─mlp.ln1.bn.weight                                                ├─512\n",
            "│    │    │    └─mlp.ln1.bn.bias                                                  ├─512\n",
            "│    │    │    └─mlp.ln2.linear.weight                                            ├─131,072\n",
            "│    │    │    └─mlp.ln2.bn.weight                                                ├─256\n",
            "│    │    │    └─mlp.ln2.bn.bias                                                  └─256\n",
            "├─LevitStage: 1-3                                       [32, 49, 384]             --\n",
            "│    └─downsample.conv.weight                                                     ├─884,736\n",
            "│    └─downsample.conv.bias                                                       ├─384\n",
            "│    └─blocks.0.attn.attention_biases                                             ├─294\n",
            "│    └─blocks.0.attn.qkv.linear.weight                                            ├─442,368\n",
            "│    └─blocks.0.attn.qkv.bn.weight                                                ├─1,152\n",
            "│    └─blocks.0.attn.qkv.bn.bias                                                  ├─1,152\n",
            "│    └─blocks.0.attn.proj.1.linear.weight                                         ├─147,456\n",
            "│    └─blocks.0.attn.proj.1.bn.weight                                             ├─384\n",
            "│    └─blocks.0.attn.proj.1.bn.bias                                               ├─384\n",
            "│    └─blocks.0.mlp.ln1.linear.weight                                             ├─294,912\n",
            "│    └─blocks.0.mlp.ln1.bn.weight                                                 ├─768\n",
            "│    └─blocks.0.mlp.ln1.bn.bias                                                   ├─768\n",
            "│    └─blocks.0.mlp.ln2.linear.weight                                             ├─294,912\n",
            "│    └─blocks.0.mlp.ln2.bn.weight                                                 ├─384\n",
            "│    └─blocks.0.mlp.ln2.bn.bias                                                   ├─384\n",
            "│    └─blocks.1.attn.attention_biases                                             ├─294\n",
            "│    └─blocks.1.attn.qkv.linear.weight                                            ├─442,368\n",
            "│    └─blocks.1.attn.qkv.bn.weight                                                ├─1,152\n",
            "│    └─blocks.1.attn.qkv.bn.bias                                                  ├─1,152\n",
            "│    └─blocks.1.attn.proj.1.linear.weight                                         ├─147,456\n",
            "│    └─blocks.1.attn.proj.1.bn.weight                                             ├─384\n",
            "│    └─blocks.1.attn.proj.1.bn.bias                                               ├─384\n",
            "│    └─blocks.1.mlp.ln1.linear.weight                                             ├─294,912\n",
            "│    └─blocks.1.mlp.ln1.bn.weight                                                 ├─768\n",
            "│    └─blocks.1.mlp.ln1.bn.bias                                                   ├─768\n",
            "│    └─blocks.1.mlp.ln2.linear.weight                                             ├─294,912\n",
            "│    └─blocks.1.mlp.ln2.bn.weight                                                 ├─384\n",
            "│    └─blocks.1.mlp.ln2.bn.bias                                                   └─384\n",
            "│    └─CNNDownsample: 2-10                              [32, 49, 384]             --\n",
            "│    │    └─conv.weight                                                           ├─884,736\n",
            "│    │    └─conv.bias                                                             └─384\n",
            "│    │    └─Conv2d: 3-11                                [32, 384, 7, 7]           885,120\n",
            "│    │    │    └─weight                                                           ├─884,736\n",
            "│    │    │    └─bias                                                             └─384\n",
            "│    │    └─Hardswish: 3-12                             [32, 384, 7, 7]           --\n",
            "│    └─Sequential: 2-11                                 [32, 49, 384]             --\n",
            "│    │    └─0.attn.attention_biases                                               ├─294\n",
            "│    │    └─0.attn.qkv.linear.weight                                              ├─442,368\n",
            "│    │    └─0.attn.qkv.bn.weight                                                  ├─1,152\n",
            "│    │    └─0.attn.qkv.bn.bias                                                    ├─1,152\n",
            "│    │    └─0.attn.proj.1.linear.weight                                           ├─147,456\n",
            "│    │    └─0.attn.proj.1.bn.weight                                               ├─384\n",
            "│    │    └─0.attn.proj.1.bn.bias                                                 ├─384\n",
            "│    │    └─0.mlp.ln1.linear.weight                                               ├─294,912\n",
            "│    │    └─0.mlp.ln1.bn.weight                                                   ├─768\n",
            "│    │    └─0.mlp.ln1.bn.bias                                                     ├─768\n",
            "│    │    └─0.mlp.ln2.linear.weight                                               ├─294,912\n",
            "│    │    └─0.mlp.ln2.bn.weight                                                   ├─384\n",
            "│    │    └─0.mlp.ln2.bn.bias                                                     ├─384\n",
            "│    │    └─1.attn.attention_biases                                               ├─294\n",
            "│    │    └─1.attn.qkv.linear.weight                                              ├─442,368\n",
            "│    │    └─1.attn.qkv.bn.weight                                                  ├─1,152\n",
            "│    │    └─1.attn.qkv.bn.bias                                                    ├─1,152\n",
            "│    │    └─1.attn.proj.1.linear.weight                                           ├─147,456\n",
            "│    │    └─1.attn.proj.1.bn.weight                                               ├─384\n",
            "│    │    └─1.attn.proj.1.bn.bias                                                 ├─384\n",
            "│    │    └─1.mlp.ln1.linear.weight                                               ├─294,912\n",
            "│    │    └─1.mlp.ln1.bn.weight                                                   ├─768\n",
            "│    │    └─1.mlp.ln1.bn.bias                                                     ├─768\n",
            "│    │    └─1.mlp.ln2.linear.weight                                               ├─294,912\n",
            "│    │    └─1.mlp.ln2.bn.weight                                                   ├─384\n",
            "│    │    └─1.mlp.ln2.bn.bias                                                     └─384\n",
            "│    │    └─LevitBlock: 3-13                            [32, 49, 384]             1,185,318\n",
            "│    │    │    └─attn.attention_biases                                            ├─294\n",
            "│    │    │    └─attn.qkv.linear.weight                                           ├─442,368\n",
            "│    │    │    └─attn.qkv.bn.weight                                               ├─1,152\n",
            "│    │    │    └─attn.qkv.bn.bias                                                 ├─1,152\n",
            "│    │    │    └─attn.proj.1.linear.weight                                        ├─147,456\n",
            "│    │    │    └─attn.proj.1.bn.weight                                            ├─384\n",
            "│    │    │    └─attn.proj.1.bn.bias                                              ├─384\n",
            "│    │    │    └─mlp.ln1.linear.weight                                            ├─294,912\n",
            "│    │    │    └─mlp.ln1.bn.weight                                                ├─768\n",
            "│    │    │    └─mlp.ln1.bn.bias                                                  ├─768\n",
            "│    │    │    └─mlp.ln2.linear.weight                                            ├─294,912\n",
            "│    │    │    └─mlp.ln2.bn.weight                                                ├─384\n",
            "│    │    │    └─mlp.ln2.bn.bias                                                  └─384\n",
            "│    │    └─LevitBlock: 3-14                            [32, 49, 384]             1,185,318\n",
            "│    │    │    └─attn.attention_biases                                            ├─294\n",
            "│    │    │    └─attn.qkv.linear.weight                                           ├─442,368\n",
            "│    │    │    └─attn.qkv.bn.weight                                               ├─1,152\n",
            "│    │    │    └─attn.qkv.bn.bias                                                 ├─1,152\n",
            "│    │    │    └─attn.proj.1.linear.weight                                        ├─147,456\n",
            "│    │    │    └─attn.proj.1.bn.weight                                            ├─384\n",
            "│    │    │    └─attn.proj.1.bn.bias                                              ├─384\n",
            "│    │    │    └─mlp.ln1.linear.weight                                            ├─294,912\n",
            "│    │    │    └─mlp.ln1.bn.weight                                                ├─768\n",
            "│    │    │    └─mlp.ln1.bn.bias                                                  ├─768\n",
            "│    │    │    └─mlp.ln2.linear.weight                                            ├─294,912\n",
            "│    │    │    └─mlp.ln2.bn.weight                                                ├─384\n",
            "│    │    │    └─mlp.ln2.bn.bias                                                  └─384\n",
            "├─Sequential: 1-4                                       [32, 512, 7, 7]           --\n",
            "│    └─0.weight                                                                   ├─196,608\n",
            "│    └─0.bias                                                                     ├─512\n",
            "│    └─1.weight                                                                   ├─512\n",
            "│    └─1.bias                                                                     └─512\n",
            "│    └─Conv2d: 2-12                                     [32, 512, 7, 7]           197,120\n",
            "│    │    └─weight                                                                ├─196,608\n",
            "│    │    └─bias                                                                  └─512\n",
            "│    └─BatchNorm2d: 2-13                                [32, 512, 7, 7]           1,024\n",
            "│    │    └─weight                                                                ├─512\n",
            "│    │    └─bias                                                                  └─512\n",
            "│    └─ReLU: 2-14                                       [32, 512, 7, 7]           --\n",
            "├─NormLinear: 1-5                                       [32, 9]                   --\n",
            "│    └─bn.weight                                                                  ├─512\n",
            "│    └─bn.bias                                                                    ├─512\n",
            "│    └─linear.weight                                                              ├─4,608\n",
            "│    └─linear.bias                                                                └─9\n",
            "│    └─BatchNorm1d: 2-15                                [32, 512]                 1,024\n",
            "│    │    └─weight                                                                ├─512\n",
            "│    │    └─bias                                                                  └─512\n",
            "│    └─Dropout: 2-16                                    [32, 512]                 --\n",
            "│    └─Linear: 2-17                                     [32, 9]                   4,617\n",
            "│    │    └─weight                                                                ├─4,608\n",
            "│    │    └─bias                                                                  └─9\n",
            "=========================================================================================================\n",
            "Total params: 4,911,390\n",
            "Trainable params: 4,911,390\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (Units.GIGABYTES): 17.93\n",
            "=========================================================================================================\n",
            "Input size (MB): 19.27\n",
            "Forward/backward pass size (MB): 897.68\n",
            "Params size (MB): 19.61\n",
            "Estimated Total Size (MB): 936.56\n",
            "=========================================================================================================\n",
            "=========================================================================================================\n",
            "Layer (type:depth-idx)                                  Output Shape              Param #\n",
            "=========================================================================================================\n",
            "LevitDistilled                                          [32, 9]                   5,641\n",
            "├─Stem16: 1-1                                           [32, 256, 14, 14]         --\n",
            "│    └─conv1.linear.weight                                                        ├─864\n",
            "│    └─conv1.bn.weight                                                            ├─32\n",
            "│    └─conv1.bn.bias                                                              ├─32\n",
            "│    └─conv2.linear.weight                                                        ├─18,432\n",
            "│    └─conv2.bn.weight                                                            ├─64\n",
            "│    └─conv2.bn.bias                                                              ├─64\n",
            "│    └─conv3.linear.weight                                                        ├─73,728\n",
            "│    └─conv3.bn.weight                                                            ├─128\n",
            "│    └─conv3.bn.bias                                                              ├─128\n",
            "│    └─conv4.linear.weight                                                        ├─294,912\n",
            "│    └─conv4.bn.weight                                                            ├─256\n",
            "│    └─conv4.bn.bias                                                              └─256\n",
            "│    └─ConvNorm: 2-1                                    [32, 32, 112, 112]        --\n",
            "│    │    └─linear.weight                                                         ├─864\n",
            "│    │    └─bn.weight                                                             ├─32\n",
            "│    │    └─bn.bias                                                               └─32\n",
            "│    │    └─Conv2d: 3-1                                 [32, 32, 112, 112]        864\n",
            "│    │    │    └─weight                                                           └─864\n",
            "│    │    └─BatchNorm2d: 3-2                            [32, 32, 112, 112]        64\n",
            "│    │    │    └─weight                                                           ├─32\n",
            "│    │    │    └─bias                                                             └─32\n",
            "│    └─Hardswish: 2-2                                   [32, 32, 112, 112]        --\n",
            "│    └─ConvNorm: 2-3                                    [32, 64, 56, 56]          --\n",
            "│    │    └─linear.weight                                                         ├─18,432\n",
            "│    │    └─bn.weight                                                             ├─64\n",
            "│    │    └─bn.bias                                                               └─64\n",
            "│    │    └─Conv2d: 3-3                                 [32, 64, 56, 56]          18,432\n",
            "│    │    │    └─weight                                                           └─18,432\n",
            "│    │    └─BatchNorm2d: 3-4                            [32, 64, 56, 56]          128\n",
            "│    │    │    └─weight                                                           ├─64\n",
            "│    │    │    └─bias                                                             └─64\n",
            "│    └─Hardswish: 2-4                                   [32, 64, 56, 56]          --\n",
            "│    └─ConvNorm: 2-5                                    [32, 128, 28, 28]         --\n",
            "│    │    └─linear.weight                                                         ├─73,728\n",
            "│    │    └─bn.weight                                                             ├─128\n",
            "│    │    └─bn.bias                                                               └─128\n",
            "│    │    └─Conv2d: 3-5                                 [32, 128, 28, 28]         73,728\n",
            "│    │    │    └─weight                                                           └─73,728\n",
            "│    │    └─BatchNorm2d: 3-6                            [32, 128, 28, 28]         256\n",
            "│    │    │    └─weight                                                           ├─128\n",
            "│    │    │    └─bias                                                             └─128\n",
            "│    └─Hardswish: 2-6                                   [32, 128, 28, 28]         --\n",
            "│    └─ConvNorm: 2-7                                    [32, 256, 14, 14]         --\n",
            "│    │    └─linear.weight                                                         ├─294,912\n",
            "│    │    └─bn.weight                                                             ├─256\n",
            "│    │    └─bn.bias                                                               └─256\n",
            "│    │    └─Conv2d: 3-7                                 [32, 256, 14, 14]         294,912\n",
            "│    │    │    └─weight                                                           └─294,912\n",
            "│    │    └─BatchNorm2d: 3-8                            [32, 256, 14, 14]         512\n",
            "│    │    │    └─weight                                                           ├─256\n",
            "│    │    │    └─bias                                                             └─256\n",
            "├─LevitStage: 1-2                                       [32, 196, 256]            --\n",
            "│    └─blocks.0.attn.attention_biases                                             ├─784\n",
            "│    └─blocks.0.attn.qkv.linear.weight                                            ├─196,608\n",
            "│    └─blocks.0.attn.qkv.bn.weight                                                ├─768\n",
            "│    └─blocks.0.attn.qkv.bn.bias                                                  ├─768\n",
            "│    └─blocks.0.attn.proj.1.linear.weight                                         ├─65,536\n",
            "│    └─blocks.0.attn.proj.1.bn.weight                                             ├─256\n",
            "│    └─blocks.0.attn.proj.1.bn.bias                                               ├─256\n",
            "│    └─blocks.0.mlp.ln1.linear.weight                                             ├─131,072\n",
            "│    └─blocks.0.mlp.ln1.bn.weight                                                 ├─512\n",
            "│    └─blocks.0.mlp.ln1.bn.bias                                                   ├─512\n",
            "│    └─blocks.0.mlp.ln2.linear.weight                                             ├─131,072\n",
            "│    └─blocks.0.mlp.ln2.bn.weight                                                 ├─256\n",
            "│    └─blocks.0.mlp.ln2.bn.bias                                                   ├─256\n",
            "│    └─blocks.1.attn.attention_biases                                             ├─784\n",
            "│    └─blocks.1.attn.qkv.linear.weight                                            ├─196,608\n",
            "│    └─blocks.1.attn.qkv.bn.weight                                                ├─768\n",
            "│    └─blocks.1.attn.qkv.bn.bias                                                  ├─768\n",
            "│    └─blocks.1.attn.proj.1.linear.weight                                         ├─65,536\n",
            "│    └─blocks.1.attn.proj.1.bn.weight                                             ├─256\n",
            "│    └─blocks.1.attn.proj.1.bn.bias                                               ├─256\n",
            "│    └─blocks.1.mlp.ln1.linear.weight                                             ├─131,072\n",
            "│    └─blocks.1.mlp.ln1.bn.weight                                                 ├─512\n",
            "│    └─blocks.1.mlp.ln1.bn.bias                                                   ├─512\n",
            "│    └─blocks.1.mlp.ln2.linear.weight                                             ├─131,072\n",
            "│    └─blocks.1.mlp.ln2.bn.weight                                                 ├─256\n",
            "│    └─blocks.1.mlp.ln2.bn.bias                                                   └─256\n",
            "│    └─Identity: 2-8                                    [32, 196, 256]            --\n",
            "│    └─Sequential: 2-9                                  [32, 196, 256]            --\n",
            "│    │    └─0.attn.attention_biases                                               ├─784\n",
            "│    │    └─0.attn.qkv.linear.weight                                              ├─196,608\n",
            "│    │    └─0.attn.qkv.bn.weight                                                  ├─768\n",
            "│    │    └─0.attn.qkv.bn.bias                                                    ├─768\n",
            "│    │    └─0.attn.proj.1.linear.weight                                           ├─65,536\n",
            "│    │    └─0.attn.proj.1.bn.weight                                               ├─256\n",
            "│    │    └─0.attn.proj.1.bn.bias                                                 ├─256\n",
            "│    │    └─0.mlp.ln1.linear.weight                                               ├─131,072\n",
            "│    │    └─0.mlp.ln1.bn.weight                                                   ├─512\n",
            "│    │    └─0.mlp.ln1.bn.bias                                                     ├─512\n",
            "│    │    └─0.mlp.ln2.linear.weight                                               ├─131,072\n",
            "│    │    └─0.mlp.ln2.bn.weight                                                   ├─256\n",
            "│    │    └─0.mlp.ln2.bn.bias                                                     ├─256\n",
            "│    │    └─1.attn.attention_biases                                               ├─784\n",
            "│    │    └─1.attn.qkv.linear.weight                                              ├─196,608\n",
            "│    │    └─1.attn.qkv.bn.weight                                                  ├─768\n",
            "│    │    └─1.attn.qkv.bn.bias                                                    ├─768\n",
            "│    │    └─1.attn.proj.1.linear.weight                                           ├─65,536\n",
            "│    │    └─1.attn.proj.1.bn.weight                                               ├─256\n",
            "│    │    └─1.attn.proj.1.bn.bias                                                 ├─256\n",
            "│    │    └─1.mlp.ln1.linear.weight                                               ├─131,072\n",
            "│    │    └─1.mlp.ln1.bn.weight                                                   ├─512\n",
            "│    │    └─1.mlp.ln1.bn.bias                                                     ├─512\n",
            "│    │    └─1.mlp.ln2.linear.weight                                               ├─131,072\n",
            "│    │    └─1.mlp.ln2.bn.weight                                                   ├─256\n",
            "│    │    └─1.mlp.ln2.bn.bias                                                     └─256\n",
            "│    │    └─LevitBlock: 3-9                             [32, 196, 256]            528,656\n",
            "│    │    │    └─attn.attention_biases                                            ├─784\n",
            "│    │    │    └─attn.qkv.linear.weight                                           ├─196,608\n",
            "│    │    │    └─attn.qkv.bn.weight                                               ├─768\n",
            "│    │    │    └─attn.qkv.bn.bias                                                 ├─768\n",
            "│    │    │    └─attn.proj.1.linear.weight                                        ├─65,536\n",
            "│    │    │    └─attn.proj.1.bn.weight                                            ├─256\n",
            "│    │    │    └─attn.proj.1.bn.bias                                              ├─256\n",
            "│    │    │    └─mlp.ln1.linear.weight                                            ├─131,072\n",
            "│    │    │    └─mlp.ln1.bn.weight                                                ├─512\n",
            "│    │    │    └─mlp.ln1.bn.bias                                                  ├─512\n",
            "│    │    │    └─mlp.ln2.linear.weight                                            ├─131,072\n",
            "│    │    │    └─mlp.ln2.bn.weight                                                ├─256\n",
            "│    │    │    └─mlp.ln2.bn.bias                                                  └─256\n",
            "│    │    └─LevitBlock: 3-10                            [32, 196, 256]            528,656\n",
            "│    │    │    └─attn.attention_biases                                            ├─784\n",
            "│    │    │    └─attn.qkv.linear.weight                                           ├─196,608\n",
            "│    │    │    └─attn.qkv.bn.weight                                               ├─768\n",
            "│    │    │    └─attn.qkv.bn.bias                                                 ├─768\n",
            "│    │    │    └─attn.proj.1.linear.weight                                        ├─65,536\n",
            "│    │    │    └─attn.proj.1.bn.weight                                            ├─256\n",
            "│    │    │    └─attn.proj.1.bn.bias                                              ├─256\n",
            "│    │    │    └─mlp.ln1.linear.weight                                            ├─131,072\n",
            "│    │    │    └─mlp.ln1.bn.weight                                                ├─512\n",
            "│    │    │    └─mlp.ln1.bn.bias                                                  ├─512\n",
            "│    │    │    └─mlp.ln2.linear.weight                                            ├─131,072\n",
            "│    │    │    └─mlp.ln2.bn.weight                                                ├─256\n",
            "│    │    │    └─mlp.ln2.bn.bias                                                  └─256\n",
            "├─LevitStage: 1-3                                       [32, 49, 384]             --\n",
            "│    └─downsample.conv.weight                                                     ├─884,736\n",
            "│    └─downsample.conv.bias                                                       ├─384\n",
            "│    └─blocks.0.attn.attention_biases                                             ├─294\n",
            "│    └─blocks.0.attn.qkv.linear.weight                                            ├─442,368\n",
            "│    └─blocks.0.attn.qkv.bn.weight                                                ├─1,152\n",
            "│    └─blocks.0.attn.qkv.bn.bias                                                  ├─1,152\n",
            "│    └─blocks.0.attn.proj.1.linear.weight                                         ├─147,456\n",
            "│    └─blocks.0.attn.proj.1.bn.weight                                             ├─384\n",
            "│    └─blocks.0.attn.proj.1.bn.bias                                               ├─384\n",
            "│    └─blocks.0.mlp.ln1.linear.weight                                             ├─294,912\n",
            "│    └─blocks.0.mlp.ln1.bn.weight                                                 ├─768\n",
            "│    └─blocks.0.mlp.ln1.bn.bias                                                   ├─768\n",
            "│    └─blocks.0.mlp.ln2.linear.weight                                             ├─294,912\n",
            "│    └─blocks.0.mlp.ln2.bn.weight                                                 ├─384\n",
            "│    └─blocks.0.mlp.ln2.bn.bias                                                   ├─384\n",
            "│    └─blocks.1.attn.attention_biases                                             ├─294\n",
            "│    └─blocks.1.attn.qkv.linear.weight                                            ├─442,368\n",
            "│    └─blocks.1.attn.qkv.bn.weight                                                ├─1,152\n",
            "│    └─blocks.1.attn.qkv.bn.bias                                                  ├─1,152\n",
            "│    └─blocks.1.attn.proj.1.linear.weight                                         ├─147,456\n",
            "│    └─blocks.1.attn.proj.1.bn.weight                                             ├─384\n",
            "│    └─blocks.1.attn.proj.1.bn.bias                                               ├─384\n",
            "│    └─blocks.1.mlp.ln1.linear.weight                                             ├─294,912\n",
            "│    └─blocks.1.mlp.ln1.bn.weight                                                 ├─768\n",
            "│    └─blocks.1.mlp.ln1.bn.bias                                                   ├─768\n",
            "│    └─blocks.1.mlp.ln2.linear.weight                                             ├─294,912\n",
            "│    └─blocks.1.mlp.ln2.bn.weight                                                 ├─384\n",
            "│    └─blocks.1.mlp.ln2.bn.bias                                                   └─384\n",
            "│    └─CNNDownsample: 2-10                              [32, 49, 384]             --\n",
            "│    │    └─conv.weight                                                           ├─884,736\n",
            "│    │    └─conv.bias                                                             └─384\n",
            "│    │    └─Conv2d: 3-11                                [32, 384, 7, 7]           885,120\n",
            "│    │    │    └─weight                                                           ├─884,736\n",
            "│    │    │    └─bias                                                             └─384\n",
            "│    │    └─Hardswish: 3-12                             [32, 384, 7, 7]           --\n",
            "│    └─Sequential: 2-11                                 [32, 49, 384]             --\n",
            "│    │    └─0.attn.attention_biases                                               ├─294\n",
            "│    │    └─0.attn.qkv.linear.weight                                              ├─442,368\n",
            "│    │    └─0.attn.qkv.bn.weight                                                  ├─1,152\n",
            "│    │    └─0.attn.qkv.bn.bias                                                    ├─1,152\n",
            "│    │    └─0.attn.proj.1.linear.weight                                           ├─147,456\n",
            "│    │    └─0.attn.proj.1.bn.weight                                               ├─384\n",
            "│    │    └─0.attn.proj.1.bn.bias                                                 ├─384\n",
            "│    │    └─0.mlp.ln1.linear.weight                                               ├─294,912\n",
            "│    │    └─0.mlp.ln1.bn.weight                                                   ├─768\n",
            "│    │    └─0.mlp.ln1.bn.bias                                                     ├─768\n",
            "│    │    └─0.mlp.ln2.linear.weight                                               ├─294,912\n",
            "│    │    └─0.mlp.ln2.bn.weight                                                   ├─384\n",
            "│    │    └─0.mlp.ln2.bn.bias                                                     ├─384\n",
            "│    │    └─1.attn.attention_biases                                               ├─294\n",
            "│    │    └─1.attn.qkv.linear.weight                                              ├─442,368\n",
            "│    │    └─1.attn.qkv.bn.weight                                                  ├─1,152\n",
            "│    │    └─1.attn.qkv.bn.bias                                                    ├─1,152\n",
            "│    │    └─1.attn.proj.1.linear.weight                                           ├─147,456\n",
            "│    │    └─1.attn.proj.1.bn.weight                                               ├─384\n",
            "│    │    └─1.attn.proj.1.bn.bias                                                 ├─384\n",
            "│    │    └─1.mlp.ln1.linear.weight                                               ├─294,912\n",
            "│    │    └─1.mlp.ln1.bn.weight                                                   ├─768\n",
            "│    │    └─1.mlp.ln1.bn.bias                                                     ├─768\n",
            "│    │    └─1.mlp.ln2.linear.weight                                               ├─294,912\n",
            "│    │    └─1.mlp.ln2.bn.weight                                                   ├─384\n",
            "│    │    └─1.mlp.ln2.bn.bias                                                     └─384\n",
            "│    │    └─LevitBlock: 3-13                            [32, 49, 384]             1,185,318\n",
            "│    │    │    └─attn.attention_biases                                            ├─294\n",
            "│    │    │    └─attn.qkv.linear.weight                                           ├─442,368\n",
            "│    │    │    └─attn.qkv.bn.weight                                               ├─1,152\n",
            "│    │    │    └─attn.qkv.bn.bias                                                 ├─1,152\n",
            "│    │    │    └─attn.proj.1.linear.weight                                        ├─147,456\n",
            "│    │    │    └─attn.proj.1.bn.weight                                            ├─384\n",
            "│    │    │    └─attn.proj.1.bn.bias                                              ├─384\n",
            "│    │    │    └─mlp.ln1.linear.weight                                            ├─294,912\n",
            "│    │    │    └─mlp.ln1.bn.weight                                                ├─768\n",
            "│    │    │    └─mlp.ln1.bn.bias                                                  ├─768\n",
            "│    │    │    └─mlp.ln2.linear.weight                                            ├─294,912\n",
            "│    │    │    └─mlp.ln2.bn.weight                                                ├─384\n",
            "│    │    │    └─mlp.ln2.bn.bias                                                  └─384\n",
            "│    │    └─LevitBlock: 3-14                            [32, 49, 384]             1,185,318\n",
            "│    │    │    └─attn.attention_biases                                            ├─294\n",
            "│    │    │    └─attn.qkv.linear.weight                                           ├─442,368\n",
            "│    │    │    └─attn.qkv.bn.weight                                               ├─1,152\n",
            "│    │    │    └─attn.qkv.bn.bias                                                 ├─1,152\n",
            "│    │    │    └─attn.proj.1.linear.weight                                        ├─147,456\n",
            "│    │    │    └─attn.proj.1.bn.weight                                            ├─384\n",
            "│    │    │    └─attn.proj.1.bn.bias                                              ├─384\n",
            "│    │    │    └─mlp.ln1.linear.weight                                            ├─294,912\n",
            "│    │    │    └─mlp.ln1.bn.weight                                                ├─768\n",
            "│    │    │    └─mlp.ln1.bn.bias                                                  ├─768\n",
            "│    │    │    └─mlp.ln2.linear.weight                                            ├─294,912\n",
            "│    │    │    └─mlp.ln2.bn.weight                                                ├─384\n",
            "│    │    │    └─mlp.ln2.bn.bias                                                  └─384\n",
            "├─Sequential: 1-4                                       [32, 512, 7, 7]           --\n",
            "│    └─0.weight                                                                   ├─196,608\n",
            "│    └─0.bias                                                                     ├─512\n",
            "│    └─1.weight                                                                   ├─512\n",
            "│    └─1.bias                                                                     └─512\n",
            "│    └─Conv2d: 2-12                                     [32, 512, 7, 7]           197,120\n",
            "│    │    └─weight                                                                ├─196,608\n",
            "│    │    └─bias                                                                  └─512\n",
            "│    └─BatchNorm2d: 2-13                                [32, 512, 7, 7]           1,024\n",
            "│    │    └─weight                                                                ├─512\n",
            "│    │    └─bias                                                                  └─512\n",
            "│    └─ReLU: 2-14                                       [32, 512, 7, 7]           --\n",
            "├─NormLinear: 1-5                                       [32, 9]                   --\n",
            "│    └─bn.weight                                                                  ├─512\n",
            "│    └─bn.bias                                                                    ├─512\n",
            "│    └─linear.weight                                                              ├─4,608\n",
            "│    └─linear.bias                                                                └─9\n",
            "│    └─BatchNorm1d: 2-15                                [32, 512]                 1,024\n",
            "│    │    └─weight                                                                ├─512\n",
            "│    │    └─bias                                                                  └─512\n",
            "│    └─Dropout: 2-16                                    [32, 512]                 --\n",
            "│    └─Linear: 2-17                                     [32, 9]                   4,617\n",
            "│    │    └─weight                                                                ├─4,608\n",
            "│    │    └─bias                                                                  └─9\n",
            "=========================================================================================================\n",
            "Total params: 4,911,390\n",
            "Trainable params: 4,911,390\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (Units.GIGABYTES): 17.93\n",
            "=========================================================================================================\n",
            "Input size (MB): 19.27\n",
            "Forward/backward pass size (MB): 897.68\n",
            "Params size (MB): 19.61\n",
            "Estimated Total Size (MB): 936.56\n",
            "=========================================================================================================\n"
          ]
        }
      ],
      "source": [
        "print(summary(model, input_size=(32, 3, 224, 224), verbose=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "LUiqsuSdOgj0"
      },
      "outputs": [],
      "source": [
        "class BSDALayer(nn.Module):\n",
        "    def __init__(self, feature_dim, bsda_lambda=0.8) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.feature_dim = feature_dim\n",
        "        self.bsda_lambda = bsda_lambda\n",
        "\n",
        "        self.logvar = nn.Sequential(\n",
        "            nn.Linear(self.feature_dim, self.feature_dim),\n",
        "        )\n",
        "\n",
        "        self.d = nn.Dropout(p=self.bsda_lambda)\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(self.feature_dim, self.feature_dim),\n",
        "            nn.BatchNorm1d(self.feature_dim),\n",
        "            nn.GELU(),\n",
        "\n",
        "            nn.Linear(self.feature_dim, self.feature_dim),\n",
        "            nn.BatchNorm1d(self.feature_dim),\n",
        "            nn.GELU(),\n",
        "        )\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(self.feature_dim, self.feature_dim),\n",
        "            nn.BatchNorm1d(self.feature_dim),\n",
        "            nn.GELU(),\n",
        "\n",
        "            nn.Linear(self.feature_dim, self.feature_dim),\n",
        "            nn.BatchNorm1d(self.feature_dim),\n",
        "            nn.GELU(),\n",
        "        )\n",
        "\n",
        "    def modified_indicator_function(self, x):\n",
        "        return torch.where(x >= 0, torch.sign(x), -torch.sign(x))\n",
        "\n",
        "    def calc_a_tilde(self, a, m, multi=1):\n",
        "        a = a.repeat(multi, 1)\n",
        "        return a + self.d(m) * self.modified_indicator_function(a)\n",
        "\n",
        "    def reparameterize(self, mu, logvar, multi=1):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        std = std.repeat(multi, 1)\n",
        "        eps = torch.randn_like(std, device=std.device)\n",
        "        mu = mu.repeat(multi, 1)\n",
        "        return eps * std + mu\n",
        "\n",
        "    def forward(self, a, multi=1):\n",
        "        \"\"\"\n",
        "            a: (batch_size, feature_dim)\n",
        "            m: (batch_size, feature_dim)\n",
        "            mu: (batch_size, feature_dim)\n",
        "            logvar: (batch_size, feature_dim)\n",
        "        \"\"\"\n",
        "        x = self.encoder(a)\n",
        "\n",
        "        logvar = self.logvar(x)\n",
        "        mu = torch.zeros_like(logvar, device=logvar.device)\n",
        "\n",
        "        m = self.reparameterize(mu, logvar, multi)\n",
        "        a_hat = self.decoder(m)\n",
        "\n",
        "        return m, mu, logvar, a_hat\n",
        "\n",
        "    def calc_kl_loss(self, mu, logvar):\n",
        "\n",
        "        kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "        return kl_loss\n",
        "\n",
        "    def calc_recon_loss(self, a, a_hat, multi=1):\n",
        "        recon_loss = torch.mean((a.repeat(multi, 1) - a_hat) ** 2) * 0.5\n",
        "        return recon_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2XJWJO0JTIMT",
        "outputId": "f01d1c65-ac1f-4d81-9d1b-9cd0cd80a646"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LevitDistilled(\n",
              "  (stem): Stem16(\n",
              "    (conv1): ConvNorm(\n",
              "      (linear): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (act1): Hardswish()\n",
              "    (conv2): ConvNorm(\n",
              "      (linear): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (act2): Hardswish()\n",
              "    (conv3): ConvNorm(\n",
              "      (linear): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (act3): Hardswish()\n",
              "    (conv4): ConvNorm(\n",
              "      (linear): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (stage1): LevitStage(\n",
              "    (downsample): Identity()\n",
              "    (blocks): Sequential(\n",
              "      (0): LevitBlock(\n",
              "        (attn): Attention(\n",
              "          (qkv): LinearNorm(\n",
              "            (linear): Linear(in_features=256, out_features=768, bias=False)\n",
              "            (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "          (proj): Sequential(\n",
              "            (0): Hardswish()\n",
              "            (1): LinearNorm(\n",
              "              (linear): Linear(in_features=256, out_features=256, bias=False)\n",
              "              (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (drop_path1): Identity()\n",
              "        (mlp): LevitMlp(\n",
              "          (ln1): LinearNorm(\n",
              "            (linear): Linear(in_features=256, out_features=512, bias=False)\n",
              "            (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "          (act): Hardswish()\n",
              "          (drop): Dropout(p=0.5, inplace=False)\n",
              "          (ln2): LinearNorm(\n",
              "            (linear): Linear(in_features=512, out_features=256, bias=False)\n",
              "            (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "      (1): LevitBlock(\n",
              "        (attn): Attention(\n",
              "          (qkv): LinearNorm(\n",
              "            (linear): Linear(in_features=256, out_features=768, bias=False)\n",
              "            (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "          (proj): Sequential(\n",
              "            (0): Hardswish()\n",
              "            (1): LinearNorm(\n",
              "              (linear): Linear(in_features=256, out_features=256, bias=False)\n",
              "              (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (drop_path1): Identity()\n",
              "        (mlp): LevitMlp(\n",
              "          (ln1): LinearNorm(\n",
              "            (linear): Linear(in_features=256, out_features=512, bias=False)\n",
              "            (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "          (act): Hardswish()\n",
              "          (drop): Dropout(p=0.5, inplace=False)\n",
              "          (ln2): LinearNorm(\n",
              "            (linear): Linear(in_features=512, out_features=256, bias=False)\n",
              "            (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (stage2): LevitStage(\n",
              "    (downsample): CNNDownsample(\n",
              "      (conv): Conv2d(256, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "      (act): Hardswish()\n",
              "    )\n",
              "    (blocks): Sequential(\n",
              "      (0): LevitBlock(\n",
              "        (attn): Attention(\n",
              "          (qkv): LinearNorm(\n",
              "            (linear): Linear(in_features=384, out_features=1152, bias=False)\n",
              "            (bn): BatchNorm1d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "          (proj): Sequential(\n",
              "            (0): Hardswish()\n",
              "            (1): LinearNorm(\n",
              "              (linear): Linear(in_features=384, out_features=384, bias=False)\n",
              "              (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (drop_path1): Identity()\n",
              "        (mlp): LevitMlp(\n",
              "          (ln1): LinearNorm(\n",
              "            (linear): Linear(in_features=384, out_features=768, bias=False)\n",
              "            (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "          (act): Hardswish()\n",
              "          (drop): Dropout(p=0.5, inplace=False)\n",
              "          (ln2): LinearNorm(\n",
              "            (linear): Linear(in_features=768, out_features=384, bias=False)\n",
              "            (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "      (1): LevitBlock(\n",
              "        (attn): Attention(\n",
              "          (qkv): LinearNorm(\n",
              "            (linear): Linear(in_features=384, out_features=1152, bias=False)\n",
              "            (bn): BatchNorm1d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "          (proj): Sequential(\n",
              "            (0): Hardswish()\n",
              "            (1): LinearNorm(\n",
              "              (linear): Linear(in_features=384, out_features=384, bias=False)\n",
              "              (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (drop_path1): Identity()\n",
              "        (mlp): LevitMlp(\n",
              "          (ln1): LinearNorm(\n",
              "            (linear): Linear(in_features=384, out_features=768, bias=False)\n",
              "            (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "          (act): Hardswish()\n",
              "          (drop): Dropout(p=0.5, inplace=False)\n",
              "          (ln2): LinearNorm(\n",
              "            (linear): Linear(in_features=768, out_features=384, bias=False)\n",
              "            (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (conv1x1): Sequential(\n",
              "    (0): Conv2d(384, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "  )\n",
              "  (head): NormLinear(\n",
              "    (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (drop): Dropout(p=0.0, inplace=False)\n",
              "    (linear): Linear(in_features=512, out_features=9, bias=True)\n",
              "  )\n",
              "  (head_dist): NormLinear(\n",
              "    (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (drop): Dropout(p=0.0, inplace=False)\n",
              "    (linear): Linear(in_features=512, out_features=9, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "import torch.nn.init as init\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)  # CUDA 연산 시 동일한 결과 보장\n",
        "    torch.cuda.manual_seed_all(seed)  # 멀티-GPU 환경에서 동일한 결과 보장\n",
        "    torch.backends.cudnn.deterministic = True  # CuDNN 연산을 deterministic하게 설정\n",
        "    torch.backends.cudnn.benchmark = False  # 연산 속도를 희생하고 일관된 연산을 수행\n",
        "\n",
        "def initialize_weights(m):\n",
        "    if isinstance(m, nn.Conv2d):  # Conv 레이어 초기화\n",
        "        init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "        if m.bias is not None:\n",
        "            init.constant_(m.bias, 0)\n",
        "    elif isinstance(m, nn.Linear):  # Linear 레이어 초기화\n",
        "        init.xavier_uniform_(m.weight)\n",
        "        if m.bias is not None:\n",
        "            init.constant_(m.bias, 0)\n",
        "    elif isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):  # BatchNorm 초기화\n",
        "        init.constant_(m.weight, 1)\n",
        "        init.constant_(m.bias, 0)\n",
        "\n",
        "set_seed(42)  # 랜덤 시드 고정\n",
        "model.apply(initialize_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "p5GDxFHzR-y7"
      },
      "outputs": [],
      "source": [
        "train_dir = './train/NCT-CRC-HE-100K'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "def elastic_transform(image, alpha, sigma):\n",
        "    \"\"\"탄성 변형 적용\"\"\"\n",
        "\n",
        "    if isinstance(image, torch.Tensor):\n",
        "        image_np = image.permute(1, 2, 0).numpy()\n",
        "    else:\n",
        "        image_np = np.array(image)\n",
        "\n",
        "    shape = image_np.shape[:2]\n",
        "\n",
        "    ksize = 2 * int(sigma) + 1\n",
        "\n",
        "    dx = cv2.GaussianBlur((np.random.rand(*shape) * 2 - 1), (ksize, ksize), sigma) * alpha\n",
        "    dy = cv2.GaussianBlur((np.random.rand(*shape) * 2 - 1), (ksize, ksize), sigma) * alpha\n",
        "    x, y = np.meshgrid(np.arange(shape[1]), np.arange(shape[0]))\n",
        "\n",
        "    indices = np.stack((y + dy, x + dx), axis=-1).astype(np.float32)\n",
        "    distorted_image = cv2.remap(image_np, indices, None, cv2.INTER_LINEAR)\n",
        "\n",
        "    return distorted_image"
      ],
      "metadata": {
        "id": "2qQFL1y8fp5K"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "mAsRwMpISMIh"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "aug_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    #transforms.RandomHorizontalFlip(p=0.75),\n",
        "    #transforms.RandomVerticalFlip(p=0.75),\n",
        "    transforms.Lambda(lambda x: elastic_transform(x, alpha=10, sigma=4)),\n",
        "    transforms.ToTensor(),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "fnlxaQSs7NsL"
      },
      "outputs": [],
      "source": [
        "dataset = datasets.ImageFolder(root=train_dir, transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "N0BkxEAYTMvj"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with open(\"default_index_dict.json\", \"r\") as f:\n",
        "    index_dict = json.load(f)\n",
        "load_train_idx = index_dict[\"train_idx\"]\n",
        "load_val_idx = index_dict[\"val_idx\"]\n",
        "load_test_idx = index_dict[\"test_idx\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "rEIH5EZxTN6n"
      },
      "outputs": [],
      "source": [
        "dataset_train = datasets.ImageFolder(root=train_dir, transform=aug_transform)\n",
        "dataset_eval = datasets.ImageFolder(root=train_dir, transform=transform)\n",
        "\n",
        "train_data = Subset(dataset_train, load_train_idx)\n",
        "val_data = Subset(dataset_eval, load_val_idx)\n",
        "test_data = Subset(dataset_eval, load_test_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8inM-bBnztEN",
        "outputId": "704dcea4-8cda-4975-f56c-9a1e33971403"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set size: 70000\n",
            "Validation set size: 15000\n",
            "Test set size: 15000\n"
          ]
        }
      ],
      "source": [
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=False)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(f\"Train set size: {len(train_data)}\")\n",
        "print(f\"Validation set size: {len(val_data)}\")\n",
        "print(f\"Test set size: {len(test_data)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "4TSObUGwcYxd"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(lr=learning_rate, params=model.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "1No-7jzGgzrV"
      },
      "outputs": [],
      "source": [
        "train_losses = []\n",
        "train_accuracies = []\n",
        "val_losses = []\n",
        "val_accuracies = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Yf0ZPGLtz_gB"
      },
      "outputs": [],
      "source": [
        "def train(model, train_loader, criterion, optimizer, device, epoch,\n",
        "          use_bsda=False, bsda_layer=None, multi=10, bsda_alpha=0.5,\n",
        "          kl_weight=8e-4, recon_weight=1.0, use_ori=True, num_epochs=30):\n",
        "\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    ratio = min(bsda_alpha * (epoch / (num_epochs // 2)), bsda_alpha) if use_bsda else 0\n",
        "\n",
        "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        features = model.extract_features(inputs)\n",
        "        y_hat = model.head(features)\n",
        "\n",
        "        if use_bsda:\n",
        "            m, mu, logvar, a_hat = bsda_layer(features, multi=multi)\n",
        "            a_tilde = bsda_layer.calc_a_tilde(features, m, multi=multi)\n",
        "            y_hat_tilde = model.head(a_tilde)\n",
        "\n",
        "            loss_task = criterion(y_hat, labels)\n",
        "            loss_task_tilde = criterion(y_hat_tilde, labels.repeat(multi,))\n",
        "            loss_kl = bsda_layer.calc_kl_loss(mu, logvar)\n",
        "            loss_recon = bsda_layer.calc_recon_loss(features, a_hat, multi)\n",
        "            loss_bsda = kl_weight * loss_kl + recon_weight * loss_recon\n",
        "            loss = loss_task_tilde + loss_bsda\n",
        "            if use_ori:\n",
        "                loss = loss * ratio + loss_task\n",
        "        else:\n",
        "            loss = criterion(y_hat, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(y_hat, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f\"Train Loss: {running_loss/len(train_loader):.4f}, Accuracy: {100*correct/total:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "rbdjXJ520Btp"
      },
      "outputs": [],
      "source": [
        "# Global variable to track the best validation loss\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "def evaluate(model, data_loader, criterion, device, epoch, phase=\"Validation\", save_path=\"best_model.pth\"):\n",
        "    global best_val_loss  # Reference global best validation loss\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, labels) in enumerate(tqdm(data_loader, desc=f\"{phase}\")):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            # Save all labels and predictions for balanced accuracy\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "    epoch_loss = running_loss / len(data_loader)\n",
        "    accuracy = 100 * correct / total\n",
        "    balanced_acc = balanced_accuracy_score(all_labels, all_predictions)\n",
        "\n",
        "    print(f\"{phase} Loss: {epoch_loss:.4f}, {phase} Accuracy: {accuracy:.2f}%\")\n",
        "    print(f\"Balanced Accuracy: {balanced_acc:.4f}\")\n",
        "\n",
        "    # Check if this is the best model so far (lowest validation loss)\n",
        "    if epoch_loss < best_val_loss:\n",
        "        best_val_loss = epoch_loss\n",
        "        torch.save(model.state_dict(), save_path)\n",
        "        print(f\"New best model saved with {phase} loss {epoch_loss:.4f} at {save_path}\")\n",
        "\n",
        "    #return epoch_loss, accuracy, balanced_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "GfBjePI-0DkY"
      },
      "outputs": [],
      "source": [
        "def measure_inference_time(model, data_loader, device):\n",
        "    model.eval()\n",
        "    times = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, _ in data_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            start_time = torch.cuda.Event(enable_timing=True)\n",
        "            end_time = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "            start_time.record()\n",
        "            _ = model(inputs)  # inference 수행\n",
        "            end_time.record()\n",
        "\n",
        "            # 시간 측정\n",
        "            torch.cuda.synchronize()  # CUDA에서 모든 커널이 완료될 때까지 대기\n",
        "            elapsed_time = start_time.elapsed_time(end_time)  # 밀리초 단위로 반환\n",
        "            times.append(elapsed_time)\n",
        "\n",
        "    # 통계량 계산\n",
        "    times_np = np.array(times)\n",
        "    total_inferences = len(times_np)\n",
        "    avg_time = np.mean(times_np)\n",
        "    std_dev = np.std(times_np)\n",
        "    max_time = np.max(times_np)\n",
        "    min_time = np.min(times_np)\n",
        "\n",
        "    # 결과 출력\n",
        "    print(f\"Inference Time Measurement Results:\")\n",
        "    print(f\"Total Inferences: {total_inferences}\")\n",
        "    print(f\"Average Time: {avg_time:.2f} ms\")\n",
        "    print(f\"Standard Deviation: {std_dev:.2f} ms\")\n",
        "    print(f\"Maximum Time: {max_time:.2f} ms\")\n",
        "    print(f\"Minimum Time: {min_time:.2f} ms\")\n",
        "\n",
        "    return times"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXsiOTOO0GGK",
        "outputId": "99e48d67-4cbb-4f44-d839-eeeb04e76a8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 2188/2188 [05:25<00:00,  6.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.5033, Accuracy: 82.22%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 469/469 [00:20<00:00, 22.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.6070, Validation Accuracy: 87.41%\n",
            "Balanced Accuracy: 0.8676\n",
            "New best model saved with Validation loss 0.6070 at best_model.pth\n",
            "\n",
            "Epoch 2/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 2188/2188 [05:23<00:00,  6.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.3026, Accuracy: 91.64%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 469/469 [00:20<00:00, 22.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.9684, Validation Accuracy: 66.43%\n",
            "Balanced Accuracy: 0.6359\n",
            "\n",
            "Epoch 3/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 2188/2188 [05:20<00:00,  6.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.2674, Accuracy: 94.08%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 469/469 [00:20<00:00, 22.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.6110, Validation Accuracy: 75.93%\n",
            "Balanced Accuracy: 0.7609\n",
            "\n",
            "Epoch 4/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|██████████| 2188/2188 [05:22<00:00,  6.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.2596, Accuracy: 95.31%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 469/469 [00:20<00:00, 23.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.9137, Validation Accuracy: 63.43%\n",
            "Balanced Accuracy: 0.6200\n",
            "\n",
            "Epoch 5/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|██████████| 2188/2188 [05:22<00:00,  6.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.2532, Accuracy: 96.31%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 469/469 [00:20<00:00, 23.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 1.5257, Validation Accuracy: 56.46%\n",
            "Balanced Accuracy: 0.5423\n",
            "\n",
            "Epoch 6/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6: 100%|██████████| 2188/2188 [05:22<00:00,  6.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.2520, Accuracy: 97.09%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 469/469 [00:20<00:00, 23.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.3823, Validation Accuracy: 85.62%\n",
            "Balanced Accuracy: 0.8639\n",
            "New best model saved with Validation loss 0.3823 at best_model.pth\n",
            "\n",
            "Epoch 7/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7: 100%|██████████| 2188/2188 [05:21<00:00,  6.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.2567, Accuracy: 97.45%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 469/469 [00:20<00:00, 23.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.2587, Validation Accuracy: 91.53%\n",
            "Balanced Accuracy: 0.9162\n",
            "New best model saved with Validation loss 0.2587 at best_model.pth\n",
            "\n",
            "Epoch 8/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8: 100%|██████████| 2188/2188 [05:22<00:00,  6.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.2570, Accuracy: 97.91%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 469/469 [00:20<00:00, 23.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.1259, Validation Accuracy: 95.75%\n",
            "Balanced Accuracy: 0.9557\n",
            "New best model saved with Validation loss 0.1259 at best_model.pth\n",
            "\n",
            "Epoch 9/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9: 100%|██████████| 2188/2188 [05:22<00:00,  6.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.2620, Accuracy: 98.21%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 469/469 [00:20<00:00, 22.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.2386, Validation Accuracy: 91.96%\n",
            "Balanced Accuracy: 0.9107\n",
            "\n",
            "Epoch 10/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10: 100%|██████████| 2188/2188 [05:22<00:00,  6.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.2687, Accuracy: 98.42%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 469/469 [00:20<00:00, 23.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.0961, Validation Accuracy: 96.81%\n",
            "Balanced Accuracy: 0.9670\n",
            "New best model saved with Validation loss 0.0961 at best_model.pth\n",
            "\n",
            "Epoch 11/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11: 100%|██████████| 2188/2188 [05:22<00:00,  6.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.2744, Accuracy: 98.65%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 469/469 [00:20<00:00, 22.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.1889, Validation Accuracy: 93.46%\n",
            "Balanced Accuracy: 0.9332\n",
            "\n",
            "Epoch 12/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12: 100%|██████████| 2188/2188 [05:22<00:00,  6.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.2829, Accuracy: 98.77%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 469/469 [00:20<00:00, 22.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.2187, Validation Accuracy: 92.98%\n",
            "Balanced Accuracy: 0.9277\n",
            "\n",
            "Epoch 13/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13: 100%|██████████| 2188/2188 [05:21<00:00,  6.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.2870, Accuracy: 99.02%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 469/469 [00:20<00:00, 23.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.8188, Validation Accuracy: 78.27%\n",
            "Balanced Accuracy: 0.7720\n",
            "\n",
            "Epoch 14/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14: 100%|██████████| 2188/2188 [05:21<00:00,  6.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.3018, Accuracy: 98.97%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 469/469 [00:20<00:00, 23.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.2169, Validation Accuracy: 93.38%\n",
            "Balanced Accuracy: 0.9319\n",
            "\n",
            "Epoch 15/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15: 100%|██████████| 2188/2188 [05:22<00:00,  6.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.3145, Accuracy: 99.08%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 469/469 [00:20<00:00, 23.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.1136, Validation Accuracy: 96.41%\n",
            "Balanced Accuracy: 0.9621\n",
            "\n",
            "Epoch 16/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16: 100%|██████████| 2188/2188 [05:23<00:00,  6.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.3130, Accuracy: 99.31%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 469/469 [00:20<00:00, 22.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.2293, Validation Accuracy: 93.44%\n",
            "Balanced Accuracy: 0.9385\n",
            "\n",
            "Epoch 17/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17: 100%|██████████| 2188/2188 [05:22<00:00,  6.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.3174, Accuracy: 99.25%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 469/469 [00:20<00:00, 23.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.1228, Validation Accuracy: 96.29%\n",
            "Balanced Accuracy: 0.9612\n",
            "\n",
            "Epoch 18/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18: 100%|██████████| 2188/2188 [05:22<00:00,  6.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.3049, Accuracy: 99.29%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 469/469 [00:20<00:00, 23.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.1206, Validation Accuracy: 96.53%\n",
            "Balanced Accuracy: 0.9636\n",
            "\n",
            "Epoch 19/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19: 100%|██████████| 2188/2188 [05:22<00:00,  6.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.3004, Accuracy: 99.35%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 469/469 [00:20<00:00, 23.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.0970, Validation Accuracy: 97.11%\n",
            "Balanced Accuracy: 0.9692\n",
            "\n",
            "Epoch 20/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20: 100%|██████████| 2188/2188 [05:24<00:00,  6.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.2914, Accuracy: 99.44%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 469/469 [00:20<00:00, 22.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.4040, Validation Accuracy: 89.65%\n",
            "Balanced Accuracy: 0.8915\n",
            "\n",
            "Epoch 21/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 21: 100%|██████████| 2188/2188 [05:23<00:00,  6.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.2892, Accuracy: 99.47%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 469/469 [00:20<00:00, 22.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.1546, Validation Accuracy: 95.84%\n",
            "Balanced Accuracy: 0.9620\n",
            "\n",
            "Epoch 22/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 22: 100%|██████████| 2188/2188 [05:23<00:00,  6.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.2883, Accuracy: 99.46%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 469/469 [00:20<00:00, 22.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.1792, Validation Accuracy: 95.36%\n",
            "Balanced Accuracy: 0.9499\n",
            "\n",
            "Epoch 23/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 23: 100%|██████████| 2188/2188 [05:23<00:00,  6.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.2791, Accuracy: 99.53%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 469/469 [00:20<00:00, 22.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.1749, Validation Accuracy: 95.81%\n",
            "Balanced Accuracy: 0.9535\n",
            "\n",
            "Epoch 24/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 24: 100%|██████████| 2188/2188 [05:22<00:00,  6.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.2798, Accuracy: 99.54%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 469/469 [00:20<00:00, 23.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.1735, Validation Accuracy: 95.77%\n",
            "Balanced Accuracy: 0.9539\n",
            "\n",
            "Epoch 25/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 25: 100%|██████████| 2188/2188 [05:25<00:00,  6.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.2764, Accuracy: 99.54%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 469/469 [00:20<00:00, 22.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.2577, Validation Accuracy: 94.05%\n",
            "Balanced Accuracy: 0.9349\n",
            "\n",
            "Epoch 26/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 26: 100%|██████████| 2188/2188 [05:27<00:00,  6.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.2748, Accuracy: 99.56%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 469/469 [00:20<00:00, 22.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.1592, Validation Accuracy: 95.69%\n",
            "Balanced Accuracy: 0.9576\n",
            "\n",
            "Epoch 27/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 27: 100%|██████████| 2188/2188 [05:21<00:00,  6.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.2731, Accuracy: 99.57%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 469/469 [00:20<00:00, 22.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.5640, Validation Accuracy: 88.78%\n",
            "Balanced Accuracy: 0.8791\n",
            "\n",
            "Epoch 28/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 28: 100%|██████████| 2188/2188 [05:28<00:00,  6.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.2675, Accuracy: 99.60%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 469/469 [00:20<00:00, 22.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.2671, Validation Accuracy: 93.84%\n",
            "Balanced Accuracy: 0.9353\n",
            "\n",
            "Epoch 29/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 29: 100%|██████████| 2188/2188 [05:27<00:00,  6.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.2686, Accuracy: 99.60%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 469/469 [00:20<00:00, 23.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.4625, Validation Accuracy: 88.76%\n",
            "Balanced Accuracy: 0.8750\n",
            "\n",
            "Epoch 30/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 30: 100%|██████████| 2188/2188 [05:28<00:00,  6.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.2621, Accuracy: 99.66%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 469/469 [00:20<00:00, 22.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.9093, Validation Accuracy: 83.15%\n",
            "Balanced Accuracy: 0.8166\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "bsda_layer = BSDALayer(feature_dim=512, bsda_lambda=0.8).to(device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "    train(model, train_loader, criterion, optimizer, device, epoch,\n",
        "          use_bsda=True,\n",
        "          bsda_layer=bsda_layer,\n",
        "          multi=10,\n",
        "          bsda_alpha=0.5,\n",
        "          kl_weight=8e-4,\n",
        "          recon_weight=1.0,\n",
        "          use_ori=True,\n",
        "          num_epochs=num_epochs)\n",
        "\n",
        "    evaluate(model, val_loader, criterion, device, epoch, phase=\"Validation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCGf9fvf2-qk",
        "outputId": "af9968d0-be7b-4bec-9598-d005c46b0c6f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "model.load_state_dict(torch.load(\"best_model.pth\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yq3Yxnau6V3e",
        "outputId": "643a3b77-879f-4763-9b64-f505fa9be25f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Test Evaluation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Test: 100%|██████████| 469/469 [00:21<00:00, 21.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.1001, Test Accuracy: 96.64%\n",
            "Balanced Accuracy: 0.9653\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nFinal Test Evaluation\")\n",
        "evaluate(model,test_loader, criterion, device, epoch, phase=\"Test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlazeL-a0WTt",
        "outputId": "dd7c25fe-e6e2-4464-eb3b-def906cc85c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference Time Measurement Results:\n",
            "Total Inferences: 469\n",
            "Average Time: 6.06 ms\n",
            "Standard Deviation: 0.19 ms\n",
            "Maximum Time: 7.34 ms\n",
            "Minimum Time: 5.72 ms\n"
          ]
        }
      ],
      "source": [
        "times = measure_inference_time(model, test_loader, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c77rmtOz0XuA",
        "outputId": "3533c204-1c5b-4957-ef9c-2ca6452e83a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                           aten::matmul         5.92%       1.038ms        42.22%       7.407ms     308.626us       0.000us         0.00%       2.529ms     105.371us            24  \n",
            "                                           aten::linear         0.50%      88.368us        32.61%       5.721ms     336.525us       0.000us         0.00%       1.811ms     106.518us            17  \n",
            "                                               aten::mm         3.58%     628.202us        30.32%       5.319ms     332.415us       1.799ms        38.28%       1.799ms     112.415us            16  \n",
            "                                           aten::conv2d         0.84%     147.686us        20.10%       3.525ms     587.576us       0.000us         0.00%     721.370us     120.228us             6  \n",
            "                                      aten::convolution         0.36%      62.700us        19.25%       3.378ms     562.962us       0.000us         0.00%     721.370us     120.228us             6  \n",
            "                                     aten::_convolution         1.06%     186.266us        18.90%       3.315ms     552.512us       0.000us         0.00%     721.370us     120.228us             6  \n",
            "                                aten::cudnn_convolution        16.10%       2.824ms        17.53%       3.075ms     512.429us     706.649us        15.04%     706.649us     117.775us             6  \n",
            "                                 ampere_sgemm_32x128_tn         0.00%       0.000us         0.00%       0.000us       0.000us     676.571us        14.40%     676.571us     169.143us             4  \n",
            "                                              aten::bmm         1.59%     279.595us         2.01%     353.148us      44.143us     567.677us        12.08%     567.677us      70.960us             8  \n",
            "                                       aten::batch_norm         0.79%     139.394us        22.08%       3.874ms     176.100us       0.000us         0.00%     534.747us      24.307us            22  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 17.543ms\n",
            "Self CUDA time total: 4.698ms\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from torch import profiler\n",
        "\n",
        "dummy_input = torch.randn(32, 3, 224, 224).cuda()\n",
        "\n",
        "# Profiling inference\n",
        "with profiler.profile(\n",
        "    activities=[\n",
        "       profiler.ProfilerActivity.CPU,\n",
        "        profiler.ProfilerActivity.CUDA,  # Include if using GPU\n",
        "    ],\n",
        "    on_trace_ready=profiler.tensorboard_trace_handler(\"./logs\"),  # Optional logging\n",
        "    record_shapes=True,\n",
        "    with_stack=True\n",
        ") as prof:\n",
        "    with torch.no_grad():\n",
        "        model(dummy_input)\n",
        "\n",
        "\n",
        "# Print results\n",
        "print(prof.key_averages().table(sort_by=\"cuda_time_total\" if torch.cuda.is_available() else \"cpu_time_total\", row_limit=10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "jXvKsupvZYjZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score, recall_score, precision_score, roc_curve, auc\n",
        "\n",
        "def score_evaluate(model, data_loader, criterion, device, phase=\"Validation\"):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(data_loader, desc=f\"{phase}\"):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "\n",
        "    overall_f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
        "    overall_recall = recall_score(all_labels, all_preds, average=\"macro\")\n",
        "    overall_precision = precision_score(all_labels, all_preds, average=\"macro\")\n",
        "\n",
        "    f1_per_class = f1_score(all_labels, all_preds, average=None)\n",
        "    recall_per_class = recall_score(all_labels, all_preds, average=None)\n",
        "    precision_per_class = precision_score(all_labels, all_preds, average=None)\n",
        "    class_labels = sorted(set(all_labels))\n",
        "\n",
        "    epoch_loss = running_loss / len(data_loader)\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"{phase} Loss: {epoch_loss:.4f}, {phase} Accuracy: {accuracy:.2f}%\")\n",
        "    val_losses.append(epoch_loss)\n",
        "    val_accuracies.append(accuracy)\n",
        "\n",
        "    print(f\"Overall - F1: {overall_f1:.4f}, Recall: {overall_recall:.4f}, Precision: {overall_precision:.4f}\")\n",
        "    print(\"Per-Class Metrics:\")\n",
        "    for i, label in enumerate(class_labels):\n",
        "        print(f\"Class {label} - F1: {f1_per_class[i]:.4f}, Recall: {recall_per_class[i]:.4f}, Precision: {precision_per_class[i]:.4f}\")\n",
        "\n",
        "    return overall_f1, overall_recall, overall_precision, f1_per_class, recall_per_class, precision_per_class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "oe9DO2tS-BpI"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "def roc_auc(model, data_loader, device, num_classes):\n",
        "\n",
        "    y = [\"F1\", \"Precision\", \"Recall\"]\n",
        "\n",
        "    model.eval()\n",
        "    all_labels = []\n",
        "    all_scores = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(data_loader, desc=\"ROC AUC\"):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_scores.extend(outputs.cpu().numpy())\n",
        "\n",
        "    all_labels = label_binarize(all_labels, classes=list(range(num_classes)))\n",
        "    print(f\"Binarized all_labels shape: {all_labels.shape}\")\n",
        "    print(f\"All_scores shape: {np.array(all_scores).shape}\")\n",
        "\n",
        "    fpr, tpr, _ = roc_curve(all_labels.ravel(), np.array(all_scores).ravel())\n",
        "    roc_auc_value = auc(fpr, tpr)\n",
        "\n",
        "    plt.figure(figsize=(9, 7))\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc_value:0.4f})')\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"Overall ROC AUC: {roc_auc_value:.4f}\")\n",
        "\n",
        "    return fpr, tpr, roc_auc_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubbBuFrU-GRd",
        "outputId": "1ebd59f6-94c5-47af-de09-94736e447acf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Test: 100%|██████████| 469/469 [00:21<00:00, 21.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.1001, Test Accuracy: 96.64%\n",
            "Overall - F1: 0.9658, Recall: 0.9653, Precision: 0.9673\n",
            "Per-Class Metrics:\n",
            "Class 0 - F1: 0.9921, Recall: 0.9994, Precision: 0.9848\n",
            "Class 1 - F1: 0.9956, Recall: 0.9981, Precision: 0.9931\n",
            "Class 2 - F1: 0.9464, Recall: 0.9768, Precision: 0.9178\n",
            "Class 3 - F1: 0.9951, Recall: 0.9942, Precision: 0.9960\n",
            "Class 4 - F1: 0.9643, Recall: 0.9708, Precision: 0.9578\n",
            "Class 5 - F1: 0.9591, Recall: 0.9650, Precision: 0.9533\n",
            "Class 6 - F1: 0.9629, Recall: 0.9475, Precision: 0.9788\n",
            "Class 7 - F1: 0.9045, Recall: 0.8583, Precision: 0.9559\n",
            "Class 8 - F1: 0.9726, Recall: 0.9772, Precision: 0.9682\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "overall_f1, overall_recall, overall_precision, f1_per_class, recall_per_class, precision_per_class = score_evaluate(model, test_loader, criterion, device, phase=\"Test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        },
        "id": "3qaLb5c--H0Q",
        "outputId": "2d2e5979-6553-4f76-d413-c135639be7a6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2000x1200 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1600x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "x = [\"ADI\", \"BACK\", \"DEB\", \"LYM\", \"MUC\", \"MUS\", \"NORM\", 'STR', \"TUM\"]\n",
        "y = [\"F1\", \"Precision\", \"Recall\"]\n",
        "\n",
        "f1_score_list = []\n",
        "precision_score_list = []\n",
        "recall_score_list = []\n",
        "\n",
        "for i, class_name in enumerate(x):\n",
        "    f1_score_list.append(f1_per_class[i])\n",
        "    precision_score_list.append(precision_per_class[i])\n",
        "    recall_score_list.append(recall_per_class[i])\n",
        "\n",
        "x.append(\"Overall\")\n",
        "f1_score_list.append(overall_f1)\n",
        "precision_score_list.append(overall_precision)\n",
        "recall_score_list.append(overall_recall)\n",
        "\n",
        "score_list = [f1_score_list, precision_score_list, recall_score_list]\n",
        "score_list = np.array(score_list)\n",
        "plt.figure(figsize=(20, 12))\n",
        "score_cm = plt.matshow(score_list, cmap=plt.cm.Blues, alpha=0.3)\n",
        "plt.xticks(range(len(x)), x)\n",
        "plt.yticks(range(len(y)), y)\n",
        "for i in range(len(y)):\n",
        "    for j in range(len(x)):\n",
        "        # 행렬의 각각의 수치를 각 칸의 중앙에 넣어준다\n",
        "        plt.text(x=j, y=i,\n",
        "                     s=\"{:.2f}\".format(score_list[i, j]),\n",
        "                     va='center',\n",
        "                     ha='center',\n",
        "                     )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "QCHdGKmbuw9W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12fceb14-0cbd-476c-e5cd-373d0a24360a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model weights saved to HoViT22_elastic_bsda.pth\n"
          ]
        }
      ],
      "source": [
        "save_path = \"HoViT22_elastic_bsda.pth\"\n",
        "torch.save(model.state_dict(), save_path)\n",
        "print(f\"Model weights saved to {save_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K2NMYNeV0Ly5"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}