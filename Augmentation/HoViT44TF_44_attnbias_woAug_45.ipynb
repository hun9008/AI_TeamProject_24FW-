{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 428,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYpt3QcVCl83",
        "outputId": "ae7e3287-9a30-462c-dce8-b4b9fe432550"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wget in /usr/local/lib/python3.11/dist-packages (3.2)\n",
            "Requirement already satisfied: torchinfo in /usr/local/lib/python3.11/dist-packages (1.8.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install wget torchinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnxTtKfkN46C",
        "outputId": "646b476a-5f5b-4d0a-b8c2-2c79611ae310"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-27 18:49:11--  https://zenodo.org/record/1214456/files/NCT-CRC-HE-100K.zip\n",
            "Resolving zenodo.org (zenodo.org)... 188.185.48.194, 188.185.43.25, 188.185.45.92, ...\n",
            "Connecting to zenodo.org (zenodo.org)|188.185.48.194|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 MOVED PERMANENTLY\n",
            "Location: /records/1214456/files/NCT-CRC-HE-100K.zip [following]\n",
            "--2025-02-27 18:49:12--  https://zenodo.org/records/1214456/files/NCT-CRC-HE-100K.zip\n",
            "Reusing existing connection to zenodo.org:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11690284003 (11G) [application/octet-stream]\n",
            "Saving to: ‘NCT-CRC-HE-100K.zip’\n",
            "\n",
            "NCT-CRC-HE-100K.zip   1%[                    ] 112.58M  16.1MB/s    eta 14m 3s "
          ]
        }
      ],
      "source": [
        "!wget -O NCT-CRC-HE-100K.zip https://zenodo.org/record/1214456/files/NCT-CRC-HE-100K.zip\n",
        "!unzip -qq NCT-CRC-HE-100K.zip -d train\n",
        "\n",
        "print(\"dataset setup complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install peft"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_9sIsiJ2ldL",
        "outputId": "10b4235e-69fb-4724-d39c-8dd9ef92fa2d"
      },
      "execution_count": 429,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.14.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from peft) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from peft) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from peft) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft) (2.5.1+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from peft) (4.48.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from peft) (4.67.1)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft) (1.3.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft) (0.5.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from peft) (0.28.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (2024.10.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (0.21.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchao\n",
        "!pip install torchtune"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPMfB8vT2qjy",
        "outputId": "ec78cc9f-ecf6-46b8-94d9-9cb3faee5903"
      },
      "execution_count": 430,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchao in /usr/local/lib/python3.11/dist-packages (0.8.0)\n",
            "Requirement already satisfied: torchtune in /usr/local/lib/python3.11/dist-packages (0.5.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (from torchtune) (3.3.2)\n",
            "Requirement already satisfied: huggingface_hub[hf_transfer] in /usr/local/lib/python3.11/dist-packages (from torchtune) (0.28.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from torchtune) (0.5.2)\n",
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.11/dist-packages (from torchtune) (0.3.9)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from torchtune) (0.2.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from torchtune) (0.9.0)\n",
            "Requirement already satisfied: blobfile>=2 in /usr/local/lib/python3.11/dist-packages (from torchtune) (3.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchtune) (1.26.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torchtune) (4.67.1)\n",
            "Requirement already satisfied: omegaconf in /usr/local/lib/python3.11/dist-packages (from torchtune) (2.3.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from torchtune) (5.9.5)\n",
            "Requirement already satisfied: Pillow>=9.4.0 in /usr/local/lib/python3.11/dist-packages (from torchtune) (11.1.0)\n",
            "Requirement already satisfied: pycryptodomex>=3.8 in /usr/local/lib/python3.11/dist-packages (from blobfile>=2->torchtune) (3.21.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.25.3 in /usr/local/lib/python3.11/dist-packages (from blobfile>=2->torchtune) (2.3.0)\n",
            "Requirement already satisfied: lxml>=4.9 in /usr/local/lib/python3.11/dist-packages (from blobfile>=2->torchtune) (5.3.1)\n",
            "Requirement already satisfied: filelock>=3.0 in /usr/local/lib/python3.11/dist-packages (from blobfile>=2->torchtune) (3.17.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets->torchtune) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets->torchtune) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets->torchtune) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets->torchtune) (2.32.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets->torchtune) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets->torchtune) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets->torchtune) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets->torchtune) (3.11.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets->torchtune) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets->torchtune) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_transfer]->torchtune) (4.12.2)\n",
            "Requirement already satisfied: hf-transfer>=0.1.4 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_transfer]->torchtune) (0.1.9)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.11/dist-packages (from omegaconf->torchtune) (4.9.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->torchtune) (2024.11.6)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->torchtune) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->torchtune) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->torchtune) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->torchtune) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->torchtune) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->torchtune) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->torchtune) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets->torchtune) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets->torchtune) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets->torchtune) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->torchtune) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->torchtune) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->torchtune) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->torchtune) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 431,
      "metadata": {
        "id": "Wa8mdzSwvzMe"
      },
      "outputs": [],
      "source": [
        "import timm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, ConcatDataset\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "import itertools\n",
        "from torchinfo import summary\n",
        "from sklearn.metrics import accuracy_score, balanced_accuracy_score, classification_report, confusion_matrix\n",
        "from PIL import Image\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import random\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Subset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtune.modules.peft import LoRALinear"
      ],
      "metadata": {
        "id": "7VFmN8Yv2vjz"
      },
      "execution_count": 432,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_lora_model(model, rank=8, alpha=16, exclude=[]):\n",
        "    for name, module in model.named_children():\n",
        "        if name in exclude:\n",
        "            continue\n",
        "        if isinstance(module, nn.Linear):\n",
        "            lora_linear = LoRALinear(\n",
        "                in_dim=module.in_features,\n",
        "                out_dim=module.out_features,\n",
        "                rank=rank,\n",
        "                alpha=alpha,\n",
        "                use_bias=module.bias is not None\n",
        "            )\n",
        "            lora_linear.weight.data = module.weight.data #모델이 처음부터 다시 학습할 필요 없게 하기 위해\n",
        "            if module.bias is not None:\n",
        "                lora_linear.bias.data = module.bias.data\n",
        "            lora_linear.to(module.weight.device)\n",
        "            # 기존 linear layer를 loRA linear로 교체체\n",
        "            setattr(model, name, lora_linear)\n",
        "        # 재귀함수 (하위모듈도 확인)\n",
        "        else:\n",
        "            convert_to_lora_model(module, rank, alpha, exclude)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "xM9fPVgw2xbJ"
      },
      "execution_count": 433,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 434,
      "metadata": {
        "id": "4FLtpbS_Vrd7"
      },
      "outputs": [],
      "source": [
        "class ConvNorm(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=2, padding=1):\n",
        "        super(ConvNorm, self).__init__()\n",
        "        self.linear = nn.Conv2d(\n",
        "            in_channels, out_channels, kernel_size=kernel_size,\n",
        "            stride=stride, padding=padding, bias=False\n",
        "        )\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear(x)\n",
        "        x = self.bn(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 435,
      "metadata": {
        "id": "zhaJu1oZyp0D"
      },
      "outputs": [],
      "source": [
        "class Stem16(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Stem16, self).__init__()\n",
        "        self.conv1 = ConvNorm(3, 32)\n",
        "        self.act1 = nn.Hardswish()\n",
        "        self.conv2 = ConvNorm(32, 64)\n",
        "        self.act2 = nn.Hardswish()\n",
        "        self.conv3 = ConvNorm(64, 128)\n",
        "        self.act3 = nn.Hardswish()\n",
        "        self.conv4 = ConvNorm(128, 256)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.act1(self.conv1(x))\n",
        "        x = self.act2(self.conv2(x))\n",
        "        x = self.act3(self.conv3(x))\n",
        "        x = self.conv4(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 436,
      "metadata": {
        "id": "W1Xh6-wOyskM"
      },
      "outputs": [],
      "source": [
        "class LinearNorm(nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super(LinearNorm, self).__init__()\n",
        "        self.linear = nn.Linear(in_features, out_features, bias=False)\n",
        "        self.bn = nn.BatchNorm1d(out_features)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        if x.dim() == 3:\n",
        "            B, N, C = x.shape\n",
        "            x = x.reshape(B * N, C)\n",
        "            x = self.bn(self.linear(x))\n",
        "            x = x.reshape(B, N, -1)\n",
        "        else:\n",
        "            x = self.bn(self.linear(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 437,
      "metadata": {
        "id": "wHeuFDCPyuLf"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, num_heads, attn_ratio=2):\n",
        "        super(Attention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = head_dim ** -0.5\n",
        "        inner_dim = head_dim * num_heads * 3\n",
        "        self.qkv = LinearNorm(dim, inner_dim)\n",
        "\n",
        "        self.proj = nn.Sequential(\n",
        "            nn.Hardswish(),\n",
        "            LinearNorm(dim, dim)\n",
        "        )\n",
        "\n",
        "        self.attention_biases = None\n",
        "        self.attention_bias_idxs = None\n",
        "\n",
        "    def compute_attention_bias(self, resolution):\n",
        "\n",
        "        points = list(itertools.product(range(resolution), range(resolution)))\n",
        "        N = len(points)\n",
        "\n",
        "        attention_offsets = {}\n",
        "        idxs = []\n",
        "\n",
        "        # if N = 196, then resolution = 14\n",
        "        for p1 in points:\n",
        "            for p2 in points:\n",
        "                offset = (abs(p1[0] - p2[0]), abs(p1[1] - p2[1]))\n",
        "                if offset not in attention_offsets:\n",
        "                    attention_offsets[offset] = len(attention_offsets)\n",
        "                idxs.append(attention_offsets[offset])\n",
        "\n",
        "        num_offsets = len(attention_offsets)\n",
        "\n",
        "        # 각 attention head에 대해 num_offsets 만큼의 학습 가능한 Bias를 생성\n",
        "        self.attention_biases = nn.Parameter(torch.zeros(self.num_heads, num_offsets).to(next(self.parameters()).device))\n",
        "        self.attention_bias_idxs = torch.LongTensor(idxs).view(N, N).to(next(self.parameters()).device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        resolution = int(N ** 0.5)\n",
        "\n",
        "        if self.attention_biases is None or self.attention_bias_idxs.shape[0] != N:\n",
        "            self.compute_attention_bias(resolution)\n",
        "\n",
        "        qkv = self.qkv(x)\n",
        "        qkv = qkv.view(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        # qkv: (3, B, num_heads, N, head_dim)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2] # q, k, v: (B, num_heads, N, head_dim)\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale # attn: (B, num_heads, N, N)\n",
        "        attn_bias = self.attention_biases[:, self.attention_bias_idxs].unsqueeze(0) # attn_bias: (1, num_heads, N, N)\n",
        "        attn = attn + attn_bias\n",
        "        attn = attn.softmax(dim=-1)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        return self.proj(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 438,
      "metadata": {
        "id": "xvyY-FeUywyR"
      },
      "outputs": [],
      "source": [
        "class LevitMlp(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features, out_features):\n",
        "        super(LevitMlp, self).__init__()\n",
        "        self.ln1 = LinearNorm(in_features, hidden_features)\n",
        "        self.act = nn.Hardswish()\n",
        "        self.drop = nn.Dropout(p=0.5, inplace=False)#dropout 적용\n",
        "        self.ln2 = LinearNorm(hidden_features, out_features)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.ln1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.ln2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 439,
      "metadata": {
        "id": "Ubl233osyyHQ"
      },
      "outputs": [],
      "source": [
        "class LevitBlock(nn.Module):\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=2):\n",
        "        super(LevitBlock, self).__init__()\n",
        "        self.attn = Attention(dim, num_heads)\n",
        "        self.drop_path1 = nn.Identity()\n",
        "        self.mlp = LevitMlp(dim, dim * mlp_ratio, dim)\n",
        "        self.drop_path2 = nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.drop_path1(self.attn(x))\n",
        "        x = x + self.drop_path2(self.mlp(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 440,
      "metadata": {
        "id": "cjBHkWJYyzfc"
      },
      "outputs": [],
      "source": [
        "class CNNDownsample(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(CNNDownsample, self).__init__()\n",
        "        self.out_channels = out_channels\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1)\n",
        "        self.act = nn.Hardswish()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # print(x.shape)\n",
        "        B, N, C = x.shape # (B, N, C)  N=H*W (16 * 16 = 196)\n",
        "        H = int(np.sqrt(N))\n",
        "        x = x.view(B, H, H, C).permute(0, 3, 1, 2)\n",
        "        x = self.conv(x)\n",
        "        x = self.act(x)\n",
        "        x = x.permute(0, 2, 3, 1).view(B, -1, self.out_channels)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 441,
      "metadata": {
        "id": "jNpOjwVVy1Im"
      },
      "outputs": [],
      "source": [
        "class LevitStage(nn.Module):\n",
        "    def __init__(self, dim, out_dim, num_heads, num_blocks, downsample=True):\n",
        "        super(LevitStage, self).__init__()\n",
        "        self.downsample = CNNDownsample(dim, out_dim) if downsample else nn.Identity()\n",
        "        self.blocks = nn.Sequential(*[LevitBlock(out_dim, num_heads) for _ in range(num_blocks)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.downsample(x)\n",
        "        x = self.blocks(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class LevitStage_TinyFusion(nn.Module):\n",
        "    def __init__(self, dim, out_dim, num_heads, num_blocks, num_select, downsample=True):\n",
        "        super(LevitStage_TinyFusion, self).__init__()\n",
        "        assert num_select <= num_blocks\n",
        "        self.downsample = CNNDownsample(dim, out_dim) if downsample else nn.Identity()\n",
        "        self.blocks = nn.Sequential(*[LevitBlock(out_dim, num_heads) for _ in range(num_blocks)])\n",
        "        self.num_blocks = num_blocks\n",
        "        self.num_select = num_select\n",
        "        init_probs = torch.ones(num_blocks) / num_blocks\n",
        "        self.gumbel_gate = nn.Parameter(torch.log(init_probs))\n",
        "\n",
        "\n",
        "    def forward(self, x, tau=1):\n",
        "        x = self.downsample(x)\n",
        "\n",
        "        if self.training:\n",
        "            gate_probs = F.gumbel_softmax(self.gumbel_gate, tau=tau, hard=False)\n",
        "        else:\n",
        "            gate_probs = F.gumbel_softmax(self.gumbel_gate, tau=tau, hard=True)\n",
        "\n",
        "        for i in range(self.num_blocks):\n",
        "            if gate_probs[i] > 0: # skip zero blocks\n",
        "              x = x + gate_probs[i] * self.blocks[i](x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "dMufn6p-3F_X"
      },
      "execution_count": 442,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 443,
      "metadata": {
        "id": "FyFtflRsy2QE"
      },
      "outputs": [],
      "source": [
        "class ConvLevitStage(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, num_blocks, kernel_size, stride, padding):\n",
        "        super(ConvLevitStage, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            *[nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size, stride, padding)\n",
        "              for i in range(num_blocks)],\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 444,
      "metadata": {
        "id": "yKzCWv-Ny3j9"
      },
      "outputs": [],
      "source": [
        "class NormLinear(nn.Module):\n",
        "    def __init__(self, in_features, out_features, dropout_prob=0.5):#drop_out_0.5 적용\n",
        "        super(NormLinear, self).__init__()\n",
        "        self.bn = nn.BatchNorm1d(in_features)\n",
        "        self.drop = nn.Dropout(p=dropout_prob, inplace=False)\n",
        "        self.linear = nn.Linear(in_features, out_features, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.bn(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.linear(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LevitDistilledTinyfusion(nn.Module):\n",
        "    def __init__(self, num_classes=9):\n",
        "        super(LevitDistilledTinyfusion, self).__init__()\n",
        "\n",
        "        self.stem = Stem16()\n",
        "\n",
        "        self.stage1 = LevitStage_TinyFusion(dim=256, out_dim=256, num_heads=4, num_blocks=4, num_select=2, downsample=False) # block 수 적용\n",
        "        self.stage2 = LevitStage_TinyFusion(dim=256, out_dim=384, num_heads=6, num_blocks=4, num_select=2, downsample=True)\n",
        "\n",
        "        self.conv1x1 = nn.Sequential(\n",
        "            nn.Conv2d(384, 512, kernel_size=1, stride=1, padding=0),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        self.head = NormLinear(in_features=512, out_features=num_classes, dropout_prob=0.0)\n",
        "        self.head_dist = NormLinear(in_features=512, out_features=num_classes, dropout_prob=0.0)\n",
        "\n",
        "    def forward(self, x, tau):\n",
        "        x = self.stem(x)\n",
        "\n",
        "        B, C, H, W = x.shape\n",
        "        x = x.view(B, C, -1).transpose(1, 2)\n",
        "        x = self.stage1(x, tau)\n",
        "        x = self.stage2(x, tau)\n",
        "\n",
        "        H = W = int(x.shape[1]**0.5)\n",
        "        x = x.transpose(1, 2).view(B, 384, H, W)\n",
        "\n",
        "        x = self.conv1x1(x)\n",
        "\n",
        "        x = torch.mean(x, dim=(2, 3))\n",
        "        out = self.head(x)\n",
        "        out_dist = self.head_dist(x)\n",
        "        return out"
      ],
      "metadata": {
        "id": "kIR4o8va3Im9"
      },
      "execution_count": 445,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 446,
      "metadata": {
        "id": "nS7iQ8-453Ru"
      },
      "outputs": [],
      "source": [
        "class LevitDistilled(nn.Module):\n",
        "    def __init__(self, num_classes=9):\n",
        "        super(LevitDistilled, self).__init__()\n",
        "\n",
        "        self.stem = Stem16()\n",
        "\n",
        "        self.stage1 = LevitStage(dim=256, out_dim=256, num_heads=4, num_blocks=4, downsample=False) # block 수 적용\n",
        "        self.stage2 = LevitStage(dim=256, out_dim=384, num_heads=6, num_blocks=4, downsample=True)\n",
        "\n",
        "        self.conv1x1 = nn.Sequential(\n",
        "            nn.Conv2d(384, 512, kernel_size=1, stride=1, padding=0),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        self.head = NormLinear(in_features=512, out_features=num_classes, dropout_prob=0.0)\n",
        "        self.head_dist = NormLinear(in_features=512, out_features=num_classes, dropout_prob=0.0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.stem(x)\n",
        "\n",
        "        B, C, H, W = x.shape\n",
        "        x = x.view(B, C, -1).transpose(1, 2)\n",
        "        x = self.stage1(x)\n",
        "        x = self.stage2(x)\n",
        "\n",
        "        H = W = int(x.shape[1]**0.5)\n",
        "        x = x.transpose(1, 2).view(B, 384, H, W)\n",
        "\n",
        "        x = self.conv1x1(x)\n",
        "\n",
        "        x = torch.mean(x, dim=(2, 3))\n",
        "        out = self.head(x)\n",
        "        out_dist = self.head_dist(x)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = LevitDistilledTinyfusion(num_classes=9)"
      ],
      "metadata": {
        "id": "B68yiGFF3S88"
      },
      "execution_count": 447,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 448,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "lbCq25bWzJSQ",
        "outputId": "7162459b-71b5-4bd7-fc8e-129ce8171acf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LevitDistilledTinyfusion(\n",
            "  (stem): Stem16(\n",
            "    (conv1): ConvNorm(\n",
            "      (linear): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (act1): Hardswish()\n",
            "    (conv2): ConvNorm(\n",
            "      (linear): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (act2): Hardswish()\n",
            "    (conv3): ConvNorm(\n",
            "      (linear): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (act3): Hardswish()\n",
            "    (conv4): ConvNorm(\n",
            "      (linear): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (stage1): LevitStage_TinyFusion(\n",
            "    (downsample): Identity()\n",
            "    (blocks): Sequential(\n",
            "      (0): LevitBlock(\n",
            "        (attn): Attention(\n",
            "          (qkv): LinearNorm(\n",
            "            (linear): Linear(in_features=256, out_features=768, bias=False)\n",
            "            (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (proj): Sequential(\n",
            "            (0): Hardswish()\n",
            "            (1): LinearNorm(\n",
            "              (linear): Linear(in_features=256, out_features=256, bias=False)\n",
            "              (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (drop_path1): Identity()\n",
            "        (mlp): LevitMlp(\n",
            "          (ln1): LinearNorm(\n",
            "            (linear): Linear(in_features=256, out_features=512, bias=False)\n",
            "            (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (act): Hardswish()\n",
            "          (drop): Dropout(p=0.5, inplace=False)\n",
            "          (ln2): LinearNorm(\n",
            "            (linear): Linear(in_features=512, out_features=256, bias=False)\n",
            "            (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (drop_path2): Identity()\n",
            "      )\n",
            "      (1): LevitBlock(\n",
            "        (attn): Attention(\n",
            "          (qkv): LinearNorm(\n",
            "            (linear): Linear(in_features=256, out_features=768, bias=False)\n",
            "            (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (proj): Sequential(\n",
            "            (0): Hardswish()\n",
            "            (1): LinearNorm(\n",
            "              (linear): Linear(in_features=256, out_features=256, bias=False)\n",
            "              (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (drop_path1): Identity()\n",
            "        (mlp): LevitMlp(\n",
            "          (ln1): LinearNorm(\n",
            "            (linear): Linear(in_features=256, out_features=512, bias=False)\n",
            "            (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (act): Hardswish()\n",
            "          (drop): Dropout(p=0.5, inplace=False)\n",
            "          (ln2): LinearNorm(\n",
            "            (linear): Linear(in_features=512, out_features=256, bias=False)\n",
            "            (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (drop_path2): Identity()\n",
            "      )\n",
            "      (2): LevitBlock(\n",
            "        (attn): Attention(\n",
            "          (qkv): LinearNorm(\n",
            "            (linear): Linear(in_features=256, out_features=768, bias=False)\n",
            "            (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (proj): Sequential(\n",
            "            (0): Hardswish()\n",
            "            (1): LinearNorm(\n",
            "              (linear): Linear(in_features=256, out_features=256, bias=False)\n",
            "              (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (drop_path1): Identity()\n",
            "        (mlp): LevitMlp(\n",
            "          (ln1): LinearNorm(\n",
            "            (linear): Linear(in_features=256, out_features=512, bias=False)\n",
            "            (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (act): Hardswish()\n",
            "          (drop): Dropout(p=0.5, inplace=False)\n",
            "          (ln2): LinearNorm(\n",
            "            (linear): Linear(in_features=512, out_features=256, bias=False)\n",
            "            (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (drop_path2): Identity()\n",
            "      )\n",
            "      (3): LevitBlock(\n",
            "        (attn): Attention(\n",
            "          (qkv): LinearNorm(\n",
            "            (linear): Linear(in_features=256, out_features=768, bias=False)\n",
            "            (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (proj): Sequential(\n",
            "            (0): Hardswish()\n",
            "            (1): LinearNorm(\n",
            "              (linear): Linear(in_features=256, out_features=256, bias=False)\n",
            "              (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (drop_path1): Identity()\n",
            "        (mlp): LevitMlp(\n",
            "          (ln1): LinearNorm(\n",
            "            (linear): Linear(in_features=256, out_features=512, bias=False)\n",
            "            (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (act): Hardswish()\n",
            "          (drop): Dropout(p=0.5, inplace=False)\n",
            "          (ln2): LinearNorm(\n",
            "            (linear): Linear(in_features=512, out_features=256, bias=False)\n",
            "            (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (drop_path2): Identity()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (stage2): LevitStage_TinyFusion(\n",
            "    (downsample): CNNDownsample(\n",
            "      (conv): Conv2d(256, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "      (act): Hardswish()\n",
            "    )\n",
            "    (blocks): Sequential(\n",
            "      (0): LevitBlock(\n",
            "        (attn): Attention(\n",
            "          (qkv): LinearNorm(\n",
            "            (linear): Linear(in_features=384, out_features=1152, bias=False)\n",
            "            (bn): BatchNorm1d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (proj): Sequential(\n",
            "            (0): Hardswish()\n",
            "            (1): LinearNorm(\n",
            "              (linear): Linear(in_features=384, out_features=384, bias=False)\n",
            "              (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (drop_path1): Identity()\n",
            "        (mlp): LevitMlp(\n",
            "          (ln1): LinearNorm(\n",
            "            (linear): Linear(in_features=384, out_features=768, bias=False)\n",
            "            (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (act): Hardswish()\n",
            "          (drop): Dropout(p=0.5, inplace=False)\n",
            "          (ln2): LinearNorm(\n",
            "            (linear): Linear(in_features=768, out_features=384, bias=False)\n",
            "            (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (drop_path2): Identity()\n",
            "      )\n",
            "      (1): LevitBlock(\n",
            "        (attn): Attention(\n",
            "          (qkv): LinearNorm(\n",
            "            (linear): Linear(in_features=384, out_features=1152, bias=False)\n",
            "            (bn): BatchNorm1d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (proj): Sequential(\n",
            "            (0): Hardswish()\n",
            "            (1): LinearNorm(\n",
            "              (linear): Linear(in_features=384, out_features=384, bias=False)\n",
            "              (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (drop_path1): Identity()\n",
            "        (mlp): LevitMlp(\n",
            "          (ln1): LinearNorm(\n",
            "            (linear): Linear(in_features=384, out_features=768, bias=False)\n",
            "            (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (act): Hardswish()\n",
            "          (drop): Dropout(p=0.5, inplace=False)\n",
            "          (ln2): LinearNorm(\n",
            "            (linear): Linear(in_features=768, out_features=384, bias=False)\n",
            "            (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (drop_path2): Identity()\n",
            "      )\n",
            "      (2): LevitBlock(\n",
            "        (attn): Attention(\n",
            "          (qkv): LinearNorm(\n",
            "            (linear): Linear(in_features=384, out_features=1152, bias=False)\n",
            "            (bn): BatchNorm1d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (proj): Sequential(\n",
            "            (0): Hardswish()\n",
            "            (1): LinearNorm(\n",
            "              (linear): Linear(in_features=384, out_features=384, bias=False)\n",
            "              (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (drop_path1): Identity()\n",
            "        (mlp): LevitMlp(\n",
            "          (ln1): LinearNorm(\n",
            "            (linear): Linear(in_features=384, out_features=768, bias=False)\n",
            "            (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (act): Hardswish()\n",
            "          (drop): Dropout(p=0.5, inplace=False)\n",
            "          (ln2): LinearNorm(\n",
            "            (linear): Linear(in_features=768, out_features=384, bias=False)\n",
            "            (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (drop_path2): Identity()\n",
            "      )\n",
            "      (3): LevitBlock(\n",
            "        (attn): Attention(\n",
            "          (qkv): LinearNorm(\n",
            "            (linear): Linear(in_features=384, out_features=1152, bias=False)\n",
            "            (bn): BatchNorm1d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (proj): Sequential(\n",
            "            (0): Hardswish()\n",
            "            (1): LinearNorm(\n",
            "              (linear): Linear(in_features=384, out_features=384, bias=False)\n",
            "              (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (drop_path1): Identity()\n",
            "        (mlp): LevitMlp(\n",
            "          (ln1): LinearNorm(\n",
            "            (linear): Linear(in_features=384, out_features=768, bias=False)\n",
            "            (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (act): Hardswish()\n",
            "          (drop): Dropout(p=0.5, inplace=False)\n",
            "          (ln2): LinearNorm(\n",
            "            (linear): Linear(in_features=768, out_features=384, bias=False)\n",
            "            (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (drop_path2): Identity()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (conv1x1): Sequential(\n",
            "    (0): Conv2d(384, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "  )\n",
            "  (head): NormLinear(\n",
            "    (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (drop): Dropout(p=0.0, inplace=False)\n",
            "    (linear): Linear(in_features=512, out_features=9, bias=True)\n",
            "  )\n",
            "  (head_dist): NormLinear(\n",
            "    (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (drop): Dropout(p=0.0, inplace=False)\n",
            "    (linear): Linear(in_features=512, out_features=9, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "#model = LevitDistilled(num_classes=9)\n",
        "print(model)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "batch_size = 32\n",
        "learning_rate = 5e-4\n",
        "num_epochs = 45\n",
        "weight_decay = 1e-6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 449,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNpMsNYAzLDF",
        "outputId": "148488e3-23be-4284-d430-8f94b13ebef2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=========================================================================================================\n",
            "Layer (type:depth-idx)                                  Output Shape              Param #\n",
            "=========================================================================================================\n",
            "LevitDistilledTinyfusion                                [32, 9]                   --\n",
            "├─Stem16: 1-1                                           [32, 256, 14, 14]         --\n",
            "│    └─ConvNorm: 2-1                                    [32, 32, 112, 112]        --\n",
            "│    │    └─Conv2d: 3-1                                 [32, 32, 112, 112]        864\n",
            "│    │    └─BatchNorm2d: 3-2                            [32, 32, 112, 112]        64\n",
            "│    └─Hardswish: 2-2                                   [32, 32, 112, 112]        --\n",
            "│    └─ConvNorm: 2-3                                    [32, 64, 56, 56]          --\n",
            "│    │    └─Conv2d: 3-3                                 [32, 64, 56, 56]          18,432\n",
            "│    │    └─BatchNorm2d: 3-4                            [32, 64, 56, 56]          128\n",
            "│    └─Hardswish: 2-4                                   [32, 64, 56, 56]          --\n",
            "│    └─ConvNorm: 2-5                                    [32, 128, 28, 28]         --\n",
            "│    │    └─Conv2d: 3-5                                 [32, 128, 28, 28]         73,728\n",
            "│    │    └─BatchNorm2d: 3-6                            [32, 128, 28, 28]         256\n",
            "│    └─Hardswish: 2-6                                   [32, 128, 28, 28]         --\n",
            "│    └─ConvNorm: 2-7                                    [32, 256, 14, 14]         --\n",
            "│    │    └─Conv2d: 3-7                                 [32, 256, 14, 14]         294,912\n",
            "│    │    └─BatchNorm2d: 3-8                            [32, 256, 14, 14]         512\n",
            "├─LevitStage_TinyFusion: 1-2                            [32, 196, 256]            --\n",
            "│    └─Identity: 2-8                                    [32, 196, 256]            --\n",
            "│    └─Sequential: 2-9                                  --                        1,584,400\n",
            "│    │    └─LevitBlock: 3-9                             [32, 196, 256]            527,872\n",
            "├─LevitStage_TinyFusion: 1-3                            [32, 49, 384]             --\n",
            "│    └─CNNDownsample: 2-10                              [32, 49, 384]             --\n",
            "│    │    └─Conv2d: 3-10                                [32, 384, 7, 7]           885,120\n",
            "│    │    └─Hardswish: 3-11                             [32, 384, 7, 7]           --\n",
            "│    └─Sequential: 2-11                                 --                        3,555,366\n",
            "│    │    └─LevitBlock: 3-12                            [32, 49, 384]             1,185,024\n",
            "├─Sequential: 1-4                                       [32, 512, 7, 7]           --\n",
            "│    └─Conv2d: 2-12                                     [32, 512, 7, 7]           197,120\n",
            "│    └─BatchNorm2d: 2-13                                [32, 512, 7, 7]           1,024\n",
            "│    └─ReLU: 2-14                                       [32, 512, 7, 7]           --\n",
            "├─NormLinear: 1-5                                       [32, 9]                   --\n",
            "│    └─BatchNorm1d: 2-15                                [32, 512]                 1,024\n",
            "│    └─Dropout: 2-16                                    [32, 512]                 --\n",
            "│    └─Linear: 2-17                                     [32, 9]                   4,617\n",
            "├─NormLinear: 1-6                                       [32, 9]                   --\n",
            "│    └─BatchNorm1d: 2-18                                [32, 512]                 1,024\n",
            "│    └─Dropout: 2-19                                    [32, 512]                 --\n",
            "│    └─Linear: 2-20                                     [32, 9]                   4,617\n",
            "=========================================================================================================\n",
            "Total params: 8,336,104\n",
            "Trainable params: 8,336,104\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (Units.GIGABYTES): 12.76\n",
            "=========================================================================================================\n",
            "Input size (MB): 19.27\n",
            "Forward/backward pass size (MB): 650.55\n",
            "Params size (MB): 12.79\n",
            "Estimated Total Size (MB): 682.60\n",
            "=========================================================================================================\n"
          ]
        }
      ],
      "source": [
        "print(summary(model, input_size=(32, 3, 224, 224), tau = 10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 450,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "qDELjYy2zP90",
        "outputId": "7667494f-3f49-42a3-9369-9c9e31c1b939"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=========================================================================================================\n",
            "Layer (type:depth-idx)                                  Output Shape              Param #\n",
            "=========================================================================================================\n",
            "LevitDistilledTinyfusion                                [32, 9]                   --\n",
            "├─Stem16: 1-1                                           [32, 256, 14, 14]         --\n",
            "│    └─conv1.linear.weight                                                        ├─864\n",
            "│    └─conv1.bn.weight                                                            ├─32\n",
            "│    └─conv1.bn.bias                                                              ├─32\n",
            "│    └─conv2.linear.weight                                                        ├─18,432\n",
            "│    └─conv2.bn.weight                                                            ├─64\n",
            "│    └─conv2.bn.bias                                                              ├─64\n",
            "│    └─conv3.linear.weight                                                        ├─73,728\n",
            "│    └─conv3.bn.weight                                                            ├─128\n",
            "│    └─conv3.bn.bias                                                              ├─128\n",
            "│    └─conv4.linear.weight                                                        ├─294,912\n",
            "│    └─conv4.bn.weight                                                            ├─256\n",
            "│    └─conv4.bn.bias                                                              └─256\n",
            "│    └─ConvNorm: 2-1                                    [32, 32, 112, 112]        --\n",
            "│    │    └─linear.weight                                                         ├─864\n",
            "│    │    └─bn.weight                                                             ├─32\n",
            "│    │    └─bn.bias                                                               └─32\n",
            "│    │    └─Conv2d: 3-1                                 [32, 32, 112, 112]        864\n",
            "│    │    │    └─weight                                                           └─864\n",
            "│    │    └─BatchNorm2d: 3-2                            [32, 32, 112, 112]        64\n",
            "│    │    │    └─weight                                                           ├─32\n",
            "│    │    │    └─bias                                                             └─32\n",
            "│    └─Hardswish: 2-2                                   [32, 32, 112, 112]        --\n",
            "│    └─ConvNorm: 2-3                                    [32, 64, 56, 56]          --\n",
            "│    │    └─linear.weight                                                         ├─18,432\n",
            "│    │    └─bn.weight                                                             ├─64\n",
            "│    │    └─bn.bias                                                               └─64\n",
            "│    │    └─Conv2d: 3-3                                 [32, 64, 56, 56]          18,432\n",
            "│    │    │    └─weight                                                           └─18,432\n",
            "│    │    └─BatchNorm2d: 3-4                            [32, 64, 56, 56]          128\n",
            "│    │    │    └─weight                                                           ├─64\n",
            "│    │    │    └─bias                                                             └─64\n",
            "│    └─Hardswish: 2-4                                   [32, 64, 56, 56]          --\n",
            "│    └─ConvNorm: 2-5                                    [32, 128, 28, 28]         --\n",
            "│    │    └─linear.weight                                                         ├─73,728\n",
            "│    │    └─bn.weight                                                             ├─128\n",
            "│    │    └─bn.bias                                                               └─128\n",
            "│    │    └─Conv2d: 3-5                                 [32, 128, 28, 28]         73,728\n",
            "│    │    │    └─weight                                                           └─73,728\n",
            "│    │    └─BatchNorm2d: 3-6                            [32, 128, 28, 28]         256\n",
            "│    │    │    └─weight                                                           ├─128\n",
            "│    │    │    └─bias                                                             └─128\n",
            "│    └─Hardswish: 2-6                                   [32, 128, 28, 28]         --\n",
            "│    └─ConvNorm: 2-7                                    [32, 256, 14, 14]         --\n",
            "│    │    └─linear.weight                                                         ├─294,912\n",
            "│    │    └─bn.weight                                                             ├─256\n",
            "│    │    └─bn.bias                                                               └─256\n",
            "│    │    └─Conv2d: 3-7                                 [32, 256, 14, 14]         294,912\n",
            "│    │    │    └─weight                                                           └─294,912\n",
            "│    │    └─BatchNorm2d: 3-8                            [32, 256, 14, 14]         512\n",
            "│    │    │    └─weight                                                           ├─256\n",
            "│    │    │    └─bias                                                             └─256\n",
            "├─LevitStage_TinyFusion: 1-2                            [32, 196, 256]            --\n",
            "│    └─gumbel_gate                                                                ├─4\n",
            "│    └─blocks.0.attn.qkv.linear.weight                                            ├─196,608\n",
            "│    └─blocks.0.attn.qkv.bn.weight                                                ├─768\n",
            "│    └─blocks.0.attn.qkv.bn.bias                                                  ├─768\n",
            "│    └─blocks.0.attn.proj.1.linear.weight                                         ├─65,536\n",
            "│    └─blocks.0.attn.proj.1.bn.weight                                             ├─256\n",
            "│    └─blocks.0.attn.proj.1.bn.bias                                               ├─256\n",
            "│    └─blocks.0.mlp.ln1.linear.weight                                             ├─131,072\n",
            "│    └─blocks.0.mlp.ln1.bn.weight                                                 ├─512\n",
            "│    └─blocks.0.mlp.ln1.bn.bias                                                   ├─512\n",
            "│    └─blocks.0.mlp.ln2.linear.weight                                             ├─131,072\n",
            "│    └─blocks.0.mlp.ln2.bn.weight                                                 ├─256\n",
            "│    └─blocks.0.mlp.ln2.bn.bias                                                   ├─256\n",
            "│    └─blocks.1.attn.qkv.linear.weight                                            ├─196,608\n",
            "│    └─blocks.1.attn.qkv.bn.weight                                                ├─768\n",
            "│    └─blocks.1.attn.qkv.bn.bias                                                  ├─768\n",
            "│    └─blocks.1.attn.proj.1.linear.weight                                         ├─65,536\n",
            "│    └─blocks.1.attn.proj.1.bn.weight                                             ├─256\n",
            "│    └─blocks.1.attn.proj.1.bn.bias                                               ├─256\n",
            "│    └─blocks.1.mlp.ln1.linear.weight                                             ├─131,072\n",
            "│    └─blocks.1.mlp.ln1.bn.weight                                                 ├─512\n",
            "│    └─blocks.1.mlp.ln1.bn.bias                                                   ├─512\n",
            "│    └─blocks.1.mlp.ln2.linear.weight                                             ├─131,072\n",
            "│    └─blocks.1.mlp.ln2.bn.weight                                                 ├─256\n",
            "│    └─blocks.1.mlp.ln2.bn.bias                                                   ├─256\n",
            "│    └─blocks.2.attn.attention_biases                                             ├─784\n",
            "│    └─blocks.2.attn.qkv.linear.weight                                            ├─196,608\n",
            "│    └─blocks.2.attn.qkv.bn.weight                                                ├─768\n",
            "│    └─blocks.2.attn.qkv.bn.bias                                                  ├─768\n",
            "│    └─blocks.2.attn.proj.1.linear.weight                                         ├─65,536\n",
            "│    └─blocks.2.attn.proj.1.bn.weight                                             ├─256\n",
            "│    └─blocks.2.attn.proj.1.bn.bias                                               ├─256\n",
            "│    └─blocks.2.mlp.ln1.linear.weight                                             ├─131,072\n",
            "│    └─blocks.2.mlp.ln1.bn.weight                                                 ├─512\n",
            "│    └─blocks.2.mlp.ln1.bn.bias                                                   ├─512\n",
            "│    └─blocks.2.mlp.ln2.linear.weight                                             ├─131,072\n",
            "│    └─blocks.2.mlp.ln2.bn.weight                                                 ├─256\n",
            "│    └─blocks.2.mlp.ln2.bn.bias                                                   ├─256\n",
            "│    └─blocks.3.attn.qkv.linear.weight                                            ├─196,608\n",
            "│    └─blocks.3.attn.qkv.bn.weight                                                ├─768\n",
            "│    └─blocks.3.attn.qkv.bn.bias                                                  ├─768\n",
            "│    └─blocks.3.attn.proj.1.linear.weight                                         ├─65,536\n",
            "│    └─blocks.3.attn.proj.1.bn.weight                                             ├─256\n",
            "│    └─blocks.3.attn.proj.1.bn.bias                                               ├─256\n",
            "│    └─blocks.3.mlp.ln1.linear.weight                                             ├─131,072\n",
            "│    └─blocks.3.mlp.ln1.bn.weight                                                 ├─512\n",
            "│    └─blocks.3.mlp.ln1.bn.bias                                                   ├─512\n",
            "│    └─blocks.3.mlp.ln2.linear.weight                                             ├─131,072\n",
            "│    └─blocks.3.mlp.ln2.bn.weight                                                 ├─256\n",
            "│    └─blocks.3.mlp.ln2.bn.bias                                                   └─256\n",
            "│    └─Identity: 2-8                                    [32, 196, 256]            --\n",
            "│    └─Sequential: 2-9                                  --                        1,585,184\n",
            "│    │    └─0.attn.attention_biases                                               ├─784\n",
            "│    │    └─0.attn.qkv.linear.weight                                              ├─196,608\n",
            "│    │    └─0.attn.qkv.bn.weight                                                  ├─768\n",
            "│    │    └─0.attn.qkv.bn.bias                                                    ├─768\n",
            "│    │    └─0.attn.proj.1.linear.weight                                           ├─65,536\n",
            "│    │    └─0.attn.proj.1.bn.weight                                               ├─256\n",
            "│    │    └─0.attn.proj.1.bn.bias                                                 ├─256\n",
            "│    │    └─0.mlp.ln1.linear.weight                                               ├─131,072\n",
            "│    │    └─0.mlp.ln1.bn.weight                                                   ├─512\n",
            "│    │    └─0.mlp.ln1.bn.bias                                                     ├─512\n",
            "│    │    └─0.mlp.ln2.linear.weight                                               ├─131,072\n",
            "│    │    └─0.mlp.ln2.bn.weight                                                   ├─256\n",
            "│    │    └─0.mlp.ln2.bn.bias                                                     ├─256\n",
            "│    │    └─1.attn.qkv.linear.weight                                              ├─196,608\n",
            "│    │    └─1.attn.qkv.bn.weight                                                  ├─768\n",
            "│    │    └─1.attn.qkv.bn.bias                                                    ├─768\n",
            "│    │    └─1.attn.proj.1.linear.weight                                           ├─65,536\n",
            "│    │    └─1.attn.proj.1.bn.weight                                               ├─256\n",
            "│    │    └─1.attn.proj.1.bn.bias                                                 ├─256\n",
            "│    │    └─1.mlp.ln1.linear.weight                                               ├─131,072\n",
            "│    │    └─1.mlp.ln1.bn.weight                                                   ├─512\n",
            "│    │    └─1.mlp.ln1.bn.bias                                                     ├─512\n",
            "│    │    └─1.mlp.ln2.linear.weight                                               ├─131,072\n",
            "│    │    └─1.mlp.ln2.bn.weight                                                   ├─256\n",
            "│    │    └─1.mlp.ln2.bn.bias                                                     ├─256\n",
            "│    │    └─2.attn.attention_biases                                               ├─784\n",
            "│    │    └─2.attn.qkv.linear.weight                                              ├─196,608\n",
            "│    │    └─2.attn.qkv.bn.weight                                                  ├─768\n",
            "│    │    └─2.attn.qkv.bn.bias                                                    ├─768\n",
            "│    │    └─2.attn.proj.1.linear.weight                                           ├─65,536\n",
            "│    │    └─2.attn.proj.1.bn.weight                                               ├─256\n",
            "│    │    └─2.attn.proj.1.bn.bias                                                 ├─256\n",
            "│    │    └─2.mlp.ln1.linear.weight                                               ├─131,072\n",
            "│    │    └─2.mlp.ln1.bn.weight                                                   ├─512\n",
            "│    │    └─2.mlp.ln1.bn.bias                                                     ├─512\n",
            "│    │    └─2.mlp.ln2.linear.weight                                               ├─131,072\n",
            "│    │    └─2.mlp.ln2.bn.weight                                                   ├─256\n",
            "│    │    └─2.mlp.ln2.bn.bias                                                     ├─256\n",
            "│    │    └─3.attn.qkv.linear.weight                                              ├─196,608\n",
            "│    │    └─3.attn.qkv.bn.weight                                                  ├─768\n",
            "│    │    └─3.attn.qkv.bn.bias                                                    ├─768\n",
            "│    │    └─3.attn.proj.1.linear.weight                                           ├─65,536\n",
            "│    │    └─3.attn.proj.1.bn.weight                                               ├─256\n",
            "│    │    └─3.attn.proj.1.bn.bias                                                 ├─256\n",
            "│    │    └─3.mlp.ln1.linear.weight                                               ├─131,072\n",
            "│    │    └─3.mlp.ln1.bn.weight                                                   ├─512\n",
            "│    │    └─3.mlp.ln1.bn.bias                                                     ├─512\n",
            "│    │    └─3.mlp.ln2.linear.weight                                               ├─131,072\n",
            "│    │    └─3.mlp.ln2.bn.weight                                                   ├─256\n",
            "│    │    └─3.mlp.ln2.bn.bias                                                     └─256\n",
            "│    │    └─LevitBlock: 3-9                             [32, 196, 256]            527,872\n",
            "│    │    │    └─attn.qkv.linear.weight                                           ├─196,608\n",
            "│    │    │    └─attn.qkv.bn.weight                                               ├─768\n",
            "│    │    │    └─attn.qkv.bn.bias                                                 ├─768\n",
            "│    │    │    └─attn.proj.1.linear.weight                                        ├─65,536\n",
            "│    │    │    └─attn.proj.1.bn.weight                                            ├─256\n",
            "│    │    │    └─attn.proj.1.bn.bias                                              ├─256\n",
            "│    │    │    └─mlp.ln1.linear.weight                                            ├─131,072\n",
            "│    │    │    └─mlp.ln1.bn.weight                                                ├─512\n",
            "│    │    │    └─mlp.ln1.bn.bias                                                  ├─512\n",
            "│    │    │    └─mlp.ln2.linear.weight                                            ├─131,072\n",
            "│    │    │    └─mlp.ln2.bn.weight                                                ├─256\n",
            "│    │    │    └─mlp.ln2.bn.bias                                                  └─256\n",
            "├─LevitStage_TinyFusion: 1-3                            [32, 49, 384]             --\n",
            "│    └─gumbel_gate                                                                ├─4\n",
            "│    └─downsample.conv.weight                                                     ├─884,736\n",
            "│    └─downsample.conv.bias                                                       ├─384\n",
            "│    └─blocks.0.attn.qkv.linear.weight                                            ├─442,368\n",
            "│    └─blocks.0.attn.qkv.bn.weight                                                ├─1,152\n",
            "│    └─blocks.0.attn.qkv.bn.bias                                                  ├─1,152\n",
            "│    └─blocks.0.attn.proj.1.linear.weight                                         ├─147,456\n",
            "│    └─blocks.0.attn.proj.1.bn.weight                                             ├─384\n",
            "│    └─blocks.0.attn.proj.1.bn.bias                                               ├─384\n",
            "│    └─blocks.0.mlp.ln1.linear.weight                                             ├─294,912\n",
            "│    └─blocks.0.mlp.ln1.bn.weight                                                 ├─768\n",
            "│    └─blocks.0.mlp.ln1.bn.bias                                                   ├─768\n",
            "│    └─blocks.0.mlp.ln2.linear.weight                                             ├─294,912\n",
            "│    └─blocks.0.mlp.ln2.bn.weight                                                 ├─384\n",
            "│    └─blocks.0.mlp.ln2.bn.bias                                                   ├─384\n",
            "│    └─blocks.1.attn.attention_biases                                             ├─294\n",
            "│    └─blocks.1.attn.qkv.linear.weight                                            ├─442,368\n",
            "│    └─blocks.1.attn.qkv.bn.weight                                                ├─1,152\n",
            "│    └─blocks.1.attn.qkv.bn.bias                                                  ├─1,152\n",
            "│    └─blocks.1.attn.proj.1.linear.weight                                         ├─147,456\n",
            "│    └─blocks.1.attn.proj.1.bn.weight                                             ├─384\n",
            "│    └─blocks.1.attn.proj.1.bn.bias                                               ├─384\n",
            "│    └─blocks.1.mlp.ln1.linear.weight                                             ├─294,912\n",
            "│    └─blocks.1.mlp.ln1.bn.weight                                                 ├─768\n",
            "│    └─blocks.1.mlp.ln1.bn.bias                                                   ├─768\n",
            "│    └─blocks.1.mlp.ln2.linear.weight                                             ├─294,912\n",
            "│    └─blocks.1.mlp.ln2.bn.weight                                                 ├─384\n",
            "│    └─blocks.1.mlp.ln2.bn.bias                                                   ├─384\n",
            "│    └─blocks.2.attn.qkv.linear.weight                                            ├─442,368\n",
            "│    └─blocks.2.attn.qkv.bn.weight                                                ├─1,152\n",
            "│    └─blocks.2.attn.qkv.bn.bias                                                  ├─1,152\n",
            "│    └─blocks.2.attn.proj.1.linear.weight                                         ├─147,456\n",
            "│    └─blocks.2.attn.proj.1.bn.weight                                             ├─384\n",
            "│    └─blocks.2.attn.proj.1.bn.bias                                               ├─384\n",
            "│    └─blocks.2.mlp.ln1.linear.weight                                             ├─294,912\n",
            "│    └─blocks.2.mlp.ln1.bn.weight                                                 ├─768\n",
            "│    └─blocks.2.mlp.ln1.bn.bias                                                   ├─768\n",
            "│    └─blocks.2.mlp.ln2.linear.weight                                             ├─294,912\n",
            "│    └─blocks.2.mlp.ln2.bn.weight                                                 ├─384\n",
            "│    └─blocks.2.mlp.ln2.bn.bias                                                   ├─384\n",
            "│    └─blocks.3.attn.qkv.linear.weight                                            ├─442,368\n",
            "│    └─blocks.3.attn.qkv.bn.weight                                                ├─1,152\n",
            "│    └─blocks.3.attn.qkv.bn.bias                                                  ├─1,152\n",
            "│    └─blocks.3.attn.proj.1.linear.weight                                         ├─147,456\n",
            "│    └─blocks.3.attn.proj.1.bn.weight                                             ├─384\n",
            "│    └─blocks.3.attn.proj.1.bn.bias                                               ├─384\n",
            "│    └─blocks.3.mlp.ln1.linear.weight                                             ├─294,912\n",
            "│    └─blocks.3.mlp.ln1.bn.weight                                                 ├─768\n",
            "│    └─blocks.3.mlp.ln1.bn.bias                                                   ├─768\n",
            "│    └─blocks.3.mlp.ln2.linear.weight                                             ├─294,912\n",
            "│    └─blocks.3.mlp.ln2.bn.weight                                                 ├─384\n",
            "│    └─blocks.3.mlp.ln2.bn.bias                                                   └─384\n",
            "│    └─CNNDownsample: 2-10                              [32, 49, 384]             --\n",
            "│    │    └─conv.weight                                                           ├─884,736\n",
            "│    │    └─conv.bias                                                             └─384\n",
            "│    │    └─Conv2d: 3-10                                [32, 384, 7, 7]           885,120\n",
            "│    │    │    └─weight                                                           ├─884,736\n",
            "│    │    │    └─bias                                                             └─384\n",
            "│    │    └─Hardswish: 3-11                             [32, 384, 7, 7]           --\n",
            "│    └─Sequential: 2-11                                 --                        3,555,660\n",
            "│    │    └─0.attn.attention_biases                                               ├─294\n",
            "│    │    └─0.attn.qkv.linear.weight                                              ├─442,368\n",
            "│    │    └─0.attn.qkv.bn.weight                                                  ├─1,152\n",
            "│    │    └─0.attn.qkv.bn.bias                                                    ├─1,152\n",
            "│    │    └─0.attn.proj.1.linear.weight                                           ├─147,456\n",
            "│    │    └─0.attn.proj.1.bn.weight                                               ├─384\n",
            "│    │    └─0.attn.proj.1.bn.bias                                                 ├─384\n",
            "│    │    └─0.mlp.ln1.linear.weight                                               ├─294,912\n",
            "│    │    └─0.mlp.ln1.bn.weight                                                   ├─768\n",
            "│    │    └─0.mlp.ln1.bn.bias                                                     ├─768\n",
            "│    │    └─0.mlp.ln2.linear.weight                                               ├─294,912\n",
            "│    │    └─0.mlp.ln2.bn.weight                                                   ├─384\n",
            "│    │    └─0.mlp.ln2.bn.bias                                                     ├─384\n",
            "│    │    └─1.attn.attention_biases                                               ├─294\n",
            "│    │    └─1.attn.qkv.linear.weight                                              ├─442,368\n",
            "│    │    └─1.attn.qkv.bn.weight                                                  ├─1,152\n",
            "│    │    └─1.attn.qkv.bn.bias                                                    ├─1,152\n",
            "│    │    └─1.attn.proj.1.linear.weight                                           ├─147,456\n",
            "│    │    └─1.attn.proj.1.bn.weight                                               ├─384\n",
            "│    │    └─1.attn.proj.1.bn.bias                                                 ├─384\n",
            "│    │    └─1.mlp.ln1.linear.weight                                               ├─294,912\n",
            "│    │    └─1.mlp.ln1.bn.weight                                                   ├─768\n",
            "│    │    └─1.mlp.ln1.bn.bias                                                     ├─768\n",
            "│    │    └─1.mlp.ln2.linear.weight                                               ├─294,912\n",
            "│    │    └─1.mlp.ln2.bn.weight                                                   ├─384\n",
            "│    │    └─1.mlp.ln2.bn.bias                                                     ├─384\n",
            "│    │    └─2.attn.qkv.linear.weight                                              ├─442,368\n",
            "│    │    └─2.attn.qkv.bn.weight                                                  ├─1,152\n",
            "│    │    └─2.attn.qkv.bn.bias                                                    ├─1,152\n",
            "│    │    └─2.attn.proj.1.linear.weight                                           ├─147,456\n",
            "│    │    └─2.attn.proj.1.bn.weight                                               ├─384\n",
            "│    │    └─2.attn.proj.1.bn.bias                                                 ├─384\n",
            "│    │    └─2.mlp.ln1.linear.weight                                               ├─294,912\n",
            "│    │    └─2.mlp.ln1.bn.weight                                                   ├─768\n",
            "│    │    └─2.mlp.ln1.bn.bias                                                     ├─768\n",
            "│    │    └─2.mlp.ln2.linear.weight                                               ├─294,912\n",
            "│    │    └─2.mlp.ln2.bn.weight                                                   ├─384\n",
            "│    │    └─2.mlp.ln2.bn.bias                                                     ├─384\n",
            "│    │    └─3.attn.qkv.linear.weight                                              ├─442,368\n",
            "│    │    └─3.attn.qkv.bn.weight                                                  ├─1,152\n",
            "│    │    └─3.attn.qkv.bn.bias                                                    ├─1,152\n",
            "│    │    └─3.attn.proj.1.linear.weight                                           ├─147,456\n",
            "│    │    └─3.attn.proj.1.bn.weight                                               ├─384\n",
            "│    │    └─3.attn.proj.1.bn.bias                                                 ├─384\n",
            "│    │    └─3.mlp.ln1.linear.weight                                               ├─294,912\n",
            "│    │    └─3.mlp.ln1.bn.weight                                                   ├─768\n",
            "│    │    └─3.mlp.ln1.bn.bias                                                     ├─768\n",
            "│    │    └─3.mlp.ln2.linear.weight                                               ├─294,912\n",
            "│    │    └─3.mlp.ln2.bn.weight                                                   ├─384\n",
            "│    │    └─3.mlp.ln2.bn.bias                                                     └─384\n",
            "│    │    └─LevitBlock: 3-12                            [32, 49, 384]             1,185,024\n",
            "│    │    │    └─attn.qkv.linear.weight                                           ├─442,368\n",
            "│    │    │    └─attn.qkv.bn.weight                                               ├─1,152\n",
            "│    │    │    └─attn.qkv.bn.bias                                                 ├─1,152\n",
            "│    │    │    └─attn.proj.1.linear.weight                                        ├─147,456\n",
            "│    │    │    └─attn.proj.1.bn.weight                                            ├─384\n",
            "│    │    │    └─attn.proj.1.bn.bias                                              ├─384\n",
            "│    │    │    └─mlp.ln1.linear.weight                                            ├─294,912\n",
            "│    │    │    └─mlp.ln1.bn.weight                                                ├─768\n",
            "│    │    │    └─mlp.ln1.bn.bias                                                  ├─768\n",
            "│    │    │    └─mlp.ln2.linear.weight                                            ├─294,912\n",
            "│    │    │    └─mlp.ln2.bn.weight                                                ├─384\n",
            "│    │    │    └─mlp.ln2.bn.bias                                                  └─384\n",
            "├─Sequential: 1-4                                       [32, 512, 7, 7]           --\n",
            "│    └─0.weight                                                                   ├─196,608\n",
            "│    └─0.bias                                                                     ├─512\n",
            "│    └─1.weight                                                                   ├─512\n",
            "│    └─1.bias                                                                     └─512\n",
            "│    └─Conv2d: 2-12                                     [32, 512, 7, 7]           197,120\n",
            "│    │    └─weight                                                                ├─196,608\n",
            "│    │    └─bias                                                                  └─512\n",
            "│    └─BatchNorm2d: 2-13                                [32, 512, 7, 7]           1,024\n",
            "│    │    └─weight                                                                ├─512\n",
            "│    │    └─bias                                                                  └─512\n",
            "│    └─ReLU: 2-14                                       [32, 512, 7, 7]           --\n",
            "├─NormLinear: 1-5                                       [32, 9]                   --\n",
            "│    └─bn.weight                                                                  ├─512\n",
            "│    └─bn.bias                                                                    ├─512\n",
            "│    └─linear.weight                                                              ├─4,608\n",
            "│    └─linear.bias                                                                └─9\n",
            "│    └─BatchNorm1d: 2-15                                [32, 512]                 1,024\n",
            "│    │    └─weight                                                                ├─512\n",
            "│    │    └─bias                                                                  └─512\n",
            "│    └─Dropout: 2-16                                    [32, 512]                 --\n",
            "│    └─Linear: 2-17                                     [32, 9]                   4,617\n",
            "│    │    └─weight                                                                ├─4,608\n",
            "│    │    └─bias                                                                  └─9\n",
            "├─NormLinear: 1-6                                       [32, 9]                   --\n",
            "│    └─bn.weight                                                                  ├─512\n",
            "│    └─bn.bias                                                                    ├─512\n",
            "│    └─linear.weight                                                              ├─4,608\n",
            "│    └─linear.bias                                                                └─9\n",
            "│    └─BatchNorm1d: 2-18                                [32, 512]                 1,024\n",
            "│    │    └─weight                                                                ├─512\n",
            "│    │    └─bias                                                                  └─512\n",
            "│    └─Dropout: 2-19                                    [32, 512]                 --\n",
            "│    └─Linear: 2-20                                     [32, 9]                   4,617\n",
            "│    │    └─weight                                                                ├─4,608\n",
            "│    │    └─bias                                                                  └─9\n",
            "=========================================================================================================\n",
            "Total params: 8,337,182\n",
            "Trainable params: 8,337,182\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (Units.GIGABYTES): 12.76\n",
            "=========================================================================================================\n",
            "Input size (MB): 19.27\n",
            "Forward/backward pass size (MB): 650.55\n",
            "Params size (MB): 12.79\n",
            "Estimated Total Size (MB): 682.60\n",
            "=========================================================================================================\n",
            "=========================================================================================================\n",
            "Layer (type:depth-idx)                                  Output Shape              Param #\n",
            "=========================================================================================================\n",
            "LevitDistilledTinyfusion                                [32, 9]                   --\n",
            "├─Stem16: 1-1                                           [32, 256, 14, 14]         --\n",
            "│    └─conv1.linear.weight                                                        ├─864\n",
            "│    └─conv1.bn.weight                                                            ├─32\n",
            "│    └─conv1.bn.bias                                                              ├─32\n",
            "│    └─conv2.linear.weight                                                        ├─18,432\n",
            "│    └─conv2.bn.weight                                                            ├─64\n",
            "│    └─conv2.bn.bias                                                              ├─64\n",
            "│    └─conv3.linear.weight                                                        ├─73,728\n",
            "│    └─conv3.bn.weight                                                            ├─128\n",
            "│    └─conv3.bn.bias                                                              ├─128\n",
            "│    └─conv4.linear.weight                                                        ├─294,912\n",
            "│    └─conv4.bn.weight                                                            ├─256\n",
            "│    └─conv4.bn.bias                                                              └─256\n",
            "│    └─ConvNorm: 2-1                                    [32, 32, 112, 112]        --\n",
            "│    │    └─linear.weight                                                         ├─864\n",
            "│    │    └─bn.weight                                                             ├─32\n",
            "│    │    └─bn.bias                                                               └─32\n",
            "│    │    └─Conv2d: 3-1                                 [32, 32, 112, 112]        864\n",
            "│    │    │    └─weight                                                           └─864\n",
            "│    │    └─BatchNorm2d: 3-2                            [32, 32, 112, 112]        64\n",
            "│    │    │    └─weight                                                           ├─32\n",
            "│    │    │    └─bias                                                             └─32\n",
            "│    └─Hardswish: 2-2                                   [32, 32, 112, 112]        --\n",
            "│    └─ConvNorm: 2-3                                    [32, 64, 56, 56]          --\n",
            "│    │    └─linear.weight                                                         ├─18,432\n",
            "│    │    └─bn.weight                                                             ├─64\n",
            "│    │    └─bn.bias                                                               └─64\n",
            "│    │    └─Conv2d: 3-3                                 [32, 64, 56, 56]          18,432\n",
            "│    │    │    └─weight                                                           └─18,432\n",
            "│    │    └─BatchNorm2d: 3-4                            [32, 64, 56, 56]          128\n",
            "│    │    │    └─weight                                                           ├─64\n",
            "│    │    │    └─bias                                                             └─64\n",
            "│    └─Hardswish: 2-4                                   [32, 64, 56, 56]          --\n",
            "│    └─ConvNorm: 2-5                                    [32, 128, 28, 28]         --\n",
            "│    │    └─linear.weight                                                         ├─73,728\n",
            "│    │    └─bn.weight                                                             ├─128\n",
            "│    │    └─bn.bias                                                               └─128\n",
            "│    │    └─Conv2d: 3-5                                 [32, 128, 28, 28]         73,728\n",
            "│    │    │    └─weight                                                           └─73,728\n",
            "│    │    └─BatchNorm2d: 3-6                            [32, 128, 28, 28]         256\n",
            "│    │    │    └─weight                                                           ├─128\n",
            "│    │    │    └─bias                                                             └─128\n",
            "│    └─Hardswish: 2-6                                   [32, 128, 28, 28]         --\n",
            "│    └─ConvNorm: 2-7                                    [32, 256, 14, 14]         --\n",
            "│    │    └─linear.weight                                                         ├─294,912\n",
            "│    │    └─bn.weight                                                             ├─256\n",
            "│    │    └─bn.bias                                                               └─256\n",
            "│    │    └─Conv2d: 3-7                                 [32, 256, 14, 14]         294,912\n",
            "│    │    │    └─weight                                                           └─294,912\n",
            "│    │    └─BatchNorm2d: 3-8                            [32, 256, 14, 14]         512\n",
            "│    │    │    └─weight                                                           ├─256\n",
            "│    │    │    └─bias                                                             └─256\n",
            "├─LevitStage_TinyFusion: 1-2                            [32, 196, 256]            --\n",
            "│    └─gumbel_gate                                                                ├─4\n",
            "│    └─blocks.0.attn.qkv.linear.weight                                            ├─196,608\n",
            "│    └─blocks.0.attn.qkv.bn.weight                                                ├─768\n",
            "│    └─blocks.0.attn.qkv.bn.bias                                                  ├─768\n",
            "│    └─blocks.0.attn.proj.1.linear.weight                                         ├─65,536\n",
            "│    └─blocks.0.attn.proj.1.bn.weight                                             ├─256\n",
            "│    └─blocks.0.attn.proj.1.bn.bias                                               ├─256\n",
            "│    └─blocks.0.mlp.ln1.linear.weight                                             ├─131,072\n",
            "│    └─blocks.0.mlp.ln1.bn.weight                                                 ├─512\n",
            "│    └─blocks.0.mlp.ln1.bn.bias                                                   ├─512\n",
            "│    └─blocks.0.mlp.ln2.linear.weight                                             ├─131,072\n",
            "│    └─blocks.0.mlp.ln2.bn.weight                                                 ├─256\n",
            "│    └─blocks.0.mlp.ln2.bn.bias                                                   ├─256\n",
            "│    └─blocks.1.attn.qkv.linear.weight                                            ├─196,608\n",
            "│    └─blocks.1.attn.qkv.bn.weight                                                ├─768\n",
            "│    └─blocks.1.attn.qkv.bn.bias                                                  ├─768\n",
            "│    └─blocks.1.attn.proj.1.linear.weight                                         ├─65,536\n",
            "│    └─blocks.1.attn.proj.1.bn.weight                                             ├─256\n",
            "│    └─blocks.1.attn.proj.1.bn.bias                                               ├─256\n",
            "│    └─blocks.1.mlp.ln1.linear.weight                                             ├─131,072\n",
            "│    └─blocks.1.mlp.ln1.bn.weight                                                 ├─512\n",
            "│    └─blocks.1.mlp.ln1.bn.bias                                                   ├─512\n",
            "│    └─blocks.1.mlp.ln2.linear.weight                                             ├─131,072\n",
            "│    └─blocks.1.mlp.ln2.bn.weight                                                 ├─256\n",
            "│    └─blocks.1.mlp.ln2.bn.bias                                                   ├─256\n",
            "│    └─blocks.2.attn.attention_biases                                             ├─784\n",
            "│    └─blocks.2.attn.qkv.linear.weight                                            ├─196,608\n",
            "│    └─blocks.2.attn.qkv.bn.weight                                                ├─768\n",
            "│    └─blocks.2.attn.qkv.bn.bias                                                  ├─768\n",
            "│    └─blocks.2.attn.proj.1.linear.weight                                         ├─65,536\n",
            "│    └─blocks.2.attn.proj.1.bn.weight                                             ├─256\n",
            "│    └─blocks.2.attn.proj.1.bn.bias                                               ├─256\n",
            "│    └─blocks.2.mlp.ln1.linear.weight                                             ├─131,072\n",
            "│    └─blocks.2.mlp.ln1.bn.weight                                                 ├─512\n",
            "│    └─blocks.2.mlp.ln1.bn.bias                                                   ├─512\n",
            "│    └─blocks.2.mlp.ln2.linear.weight                                             ├─131,072\n",
            "│    └─blocks.2.mlp.ln2.bn.weight                                                 ├─256\n",
            "│    └─blocks.2.mlp.ln2.bn.bias                                                   ├─256\n",
            "│    └─blocks.3.attn.qkv.linear.weight                                            ├─196,608\n",
            "│    └─blocks.3.attn.qkv.bn.weight                                                ├─768\n",
            "│    └─blocks.3.attn.qkv.bn.bias                                                  ├─768\n",
            "│    └─blocks.3.attn.proj.1.linear.weight                                         ├─65,536\n",
            "│    └─blocks.3.attn.proj.1.bn.weight                                             ├─256\n",
            "│    └─blocks.3.attn.proj.1.bn.bias                                               ├─256\n",
            "│    └─blocks.3.mlp.ln1.linear.weight                                             ├─131,072\n",
            "│    └─blocks.3.mlp.ln1.bn.weight                                                 ├─512\n",
            "│    └─blocks.3.mlp.ln1.bn.bias                                                   ├─512\n",
            "│    └─blocks.3.mlp.ln2.linear.weight                                             ├─131,072\n",
            "│    └─blocks.3.mlp.ln2.bn.weight                                                 ├─256\n",
            "│    └─blocks.3.mlp.ln2.bn.bias                                                   └─256\n",
            "│    └─Identity: 2-8                                    [32, 196, 256]            --\n",
            "│    └─Sequential: 2-9                                  --                        1,585,184\n",
            "│    │    └─0.attn.attention_biases                                               ├─784\n",
            "│    │    └─0.attn.qkv.linear.weight                                              ├─196,608\n",
            "│    │    └─0.attn.qkv.bn.weight                                                  ├─768\n",
            "│    │    └─0.attn.qkv.bn.bias                                                    ├─768\n",
            "│    │    └─0.attn.proj.1.linear.weight                                           ├─65,536\n",
            "│    │    └─0.attn.proj.1.bn.weight                                               ├─256\n",
            "│    │    └─0.attn.proj.1.bn.bias                                                 ├─256\n",
            "│    │    └─0.mlp.ln1.linear.weight                                               ├─131,072\n",
            "│    │    └─0.mlp.ln1.bn.weight                                                   ├─512\n",
            "│    │    └─0.mlp.ln1.bn.bias                                                     ├─512\n",
            "│    │    └─0.mlp.ln2.linear.weight                                               ├─131,072\n",
            "│    │    └─0.mlp.ln2.bn.weight                                                   ├─256\n",
            "│    │    └─0.mlp.ln2.bn.bias                                                     ├─256\n",
            "│    │    └─1.attn.qkv.linear.weight                                              ├─196,608\n",
            "│    │    └─1.attn.qkv.bn.weight                                                  ├─768\n",
            "│    │    └─1.attn.qkv.bn.bias                                                    ├─768\n",
            "│    │    └─1.attn.proj.1.linear.weight                                           ├─65,536\n",
            "│    │    └─1.attn.proj.1.bn.weight                                               ├─256\n",
            "│    │    └─1.attn.proj.1.bn.bias                                                 ├─256\n",
            "│    │    └─1.mlp.ln1.linear.weight                                               ├─131,072\n",
            "│    │    └─1.mlp.ln1.bn.weight                                                   ├─512\n",
            "│    │    └─1.mlp.ln1.bn.bias                                                     ├─512\n",
            "│    │    └─1.mlp.ln2.linear.weight                                               ├─131,072\n",
            "│    │    └─1.mlp.ln2.bn.weight                                                   ├─256\n",
            "│    │    └─1.mlp.ln2.bn.bias                                                     ├─256\n",
            "│    │    └─2.attn.attention_biases                                               ├─784\n",
            "│    │    └─2.attn.qkv.linear.weight                                              ├─196,608\n",
            "│    │    └─2.attn.qkv.bn.weight                                                  ├─768\n",
            "│    │    └─2.attn.qkv.bn.bias                                                    ├─768\n",
            "│    │    └─2.attn.proj.1.linear.weight                                           ├─65,536\n",
            "│    │    └─2.attn.proj.1.bn.weight                                               ├─256\n",
            "│    │    └─2.attn.proj.1.bn.bias                                                 ├─256\n",
            "│    │    └─2.mlp.ln1.linear.weight                                               ├─131,072\n",
            "│    │    └─2.mlp.ln1.bn.weight                                                   ├─512\n",
            "│    │    └─2.mlp.ln1.bn.bias                                                     ├─512\n",
            "│    │    └─2.mlp.ln2.linear.weight                                               ├─131,072\n",
            "│    │    └─2.mlp.ln2.bn.weight                                                   ├─256\n",
            "│    │    └─2.mlp.ln2.bn.bias                                                     ├─256\n",
            "│    │    └─3.attn.qkv.linear.weight                                              ├─196,608\n",
            "│    │    └─3.attn.qkv.bn.weight                                                  ├─768\n",
            "│    │    └─3.attn.qkv.bn.bias                                                    ├─768\n",
            "│    │    └─3.attn.proj.1.linear.weight                                           ├─65,536\n",
            "│    │    └─3.attn.proj.1.bn.weight                                               ├─256\n",
            "│    │    └─3.attn.proj.1.bn.bias                                                 ├─256\n",
            "│    │    └─3.mlp.ln1.linear.weight                                               ├─131,072\n",
            "│    │    └─3.mlp.ln1.bn.weight                                                   ├─512\n",
            "│    │    └─3.mlp.ln1.bn.bias                                                     ├─512\n",
            "│    │    └─3.mlp.ln2.linear.weight                                               ├─131,072\n",
            "│    │    └─3.mlp.ln2.bn.weight                                                   ├─256\n",
            "│    │    └─3.mlp.ln2.bn.bias                                                     └─256\n",
            "│    │    └─LevitBlock: 3-9                             [32, 196, 256]            527,872\n",
            "│    │    │    └─attn.qkv.linear.weight                                           ├─196,608\n",
            "│    │    │    └─attn.qkv.bn.weight                                               ├─768\n",
            "│    │    │    └─attn.qkv.bn.bias                                                 ├─768\n",
            "│    │    │    └─attn.proj.1.linear.weight                                        ├─65,536\n",
            "│    │    │    └─attn.proj.1.bn.weight                                            ├─256\n",
            "│    │    │    └─attn.proj.1.bn.bias                                              ├─256\n",
            "│    │    │    └─mlp.ln1.linear.weight                                            ├─131,072\n",
            "│    │    │    └─mlp.ln1.bn.weight                                                ├─512\n",
            "│    │    │    └─mlp.ln1.bn.bias                                                  ├─512\n",
            "│    │    │    └─mlp.ln2.linear.weight                                            ├─131,072\n",
            "│    │    │    └─mlp.ln2.bn.weight                                                ├─256\n",
            "│    │    │    └─mlp.ln2.bn.bias                                                  └─256\n",
            "├─LevitStage_TinyFusion: 1-3                            [32, 49, 384]             --\n",
            "│    └─gumbel_gate                                                                ├─4\n",
            "│    └─downsample.conv.weight                                                     ├─884,736\n",
            "│    └─downsample.conv.bias                                                       ├─384\n",
            "│    └─blocks.0.attn.qkv.linear.weight                                            ├─442,368\n",
            "│    └─blocks.0.attn.qkv.bn.weight                                                ├─1,152\n",
            "│    └─blocks.0.attn.qkv.bn.bias                                                  ├─1,152\n",
            "│    └─blocks.0.attn.proj.1.linear.weight                                         ├─147,456\n",
            "│    └─blocks.0.attn.proj.1.bn.weight                                             ├─384\n",
            "│    └─blocks.0.attn.proj.1.bn.bias                                               ├─384\n",
            "│    └─blocks.0.mlp.ln1.linear.weight                                             ├─294,912\n",
            "│    └─blocks.0.mlp.ln1.bn.weight                                                 ├─768\n",
            "│    └─blocks.0.mlp.ln1.bn.bias                                                   ├─768\n",
            "│    └─blocks.0.mlp.ln2.linear.weight                                             ├─294,912\n",
            "│    └─blocks.0.mlp.ln2.bn.weight                                                 ├─384\n",
            "│    └─blocks.0.mlp.ln2.bn.bias                                                   ├─384\n",
            "│    └─blocks.1.attn.attention_biases                                             ├─294\n",
            "│    └─blocks.1.attn.qkv.linear.weight                                            ├─442,368\n",
            "│    └─blocks.1.attn.qkv.bn.weight                                                ├─1,152\n",
            "│    └─blocks.1.attn.qkv.bn.bias                                                  ├─1,152\n",
            "│    └─blocks.1.attn.proj.1.linear.weight                                         ├─147,456\n",
            "│    └─blocks.1.attn.proj.1.bn.weight                                             ├─384\n",
            "│    └─blocks.1.attn.proj.1.bn.bias                                               ├─384\n",
            "│    └─blocks.1.mlp.ln1.linear.weight                                             ├─294,912\n",
            "│    └─blocks.1.mlp.ln1.bn.weight                                                 ├─768\n",
            "│    └─blocks.1.mlp.ln1.bn.bias                                                   ├─768\n",
            "│    └─blocks.1.mlp.ln2.linear.weight                                             ├─294,912\n",
            "│    └─blocks.1.mlp.ln2.bn.weight                                                 ├─384\n",
            "│    └─blocks.1.mlp.ln2.bn.bias                                                   ├─384\n",
            "│    └─blocks.2.attn.qkv.linear.weight                                            ├─442,368\n",
            "│    └─blocks.2.attn.qkv.bn.weight                                                ├─1,152\n",
            "│    └─blocks.2.attn.qkv.bn.bias                                                  ├─1,152\n",
            "│    └─blocks.2.attn.proj.1.linear.weight                                         ├─147,456\n",
            "│    └─blocks.2.attn.proj.1.bn.weight                                             ├─384\n",
            "│    └─blocks.2.attn.proj.1.bn.bias                                               ├─384\n",
            "│    └─blocks.2.mlp.ln1.linear.weight                                             ├─294,912\n",
            "│    └─blocks.2.mlp.ln1.bn.weight                                                 ├─768\n",
            "│    └─blocks.2.mlp.ln1.bn.bias                                                   ├─768\n",
            "│    └─blocks.2.mlp.ln2.linear.weight                                             ├─294,912\n",
            "│    └─blocks.2.mlp.ln2.bn.weight                                                 ├─384\n",
            "│    └─blocks.2.mlp.ln2.bn.bias                                                   ├─384\n",
            "│    └─blocks.3.attn.qkv.linear.weight                                            ├─442,368\n",
            "│    └─blocks.3.attn.qkv.bn.weight                                                ├─1,152\n",
            "│    └─blocks.3.attn.qkv.bn.bias                                                  ├─1,152\n",
            "│    └─blocks.3.attn.proj.1.linear.weight                                         ├─147,456\n",
            "│    └─blocks.3.attn.proj.1.bn.weight                                             ├─384\n",
            "│    └─blocks.3.attn.proj.1.bn.bias                                               ├─384\n",
            "│    └─blocks.3.mlp.ln1.linear.weight                                             ├─294,912\n",
            "│    └─blocks.3.mlp.ln1.bn.weight                                                 ├─768\n",
            "│    └─blocks.3.mlp.ln1.bn.bias                                                   ├─768\n",
            "│    └─blocks.3.mlp.ln2.linear.weight                                             ├─294,912\n",
            "│    └─blocks.3.mlp.ln2.bn.weight                                                 ├─384\n",
            "│    └─blocks.3.mlp.ln2.bn.bias                                                   └─384\n",
            "│    └─CNNDownsample: 2-10                              [32, 49, 384]             --\n",
            "│    │    └─conv.weight                                                           ├─884,736\n",
            "│    │    └─conv.bias                                                             └─384\n",
            "│    │    └─Conv2d: 3-10                                [32, 384, 7, 7]           885,120\n",
            "│    │    │    └─weight                                                           ├─884,736\n",
            "│    │    │    └─bias                                                             └─384\n",
            "│    │    └─Hardswish: 3-11                             [32, 384, 7, 7]           --\n",
            "│    └─Sequential: 2-11                                 --                        3,555,660\n",
            "│    │    └─0.attn.attention_biases                                               ├─294\n",
            "│    │    └─0.attn.qkv.linear.weight                                              ├─442,368\n",
            "│    │    └─0.attn.qkv.bn.weight                                                  ├─1,152\n",
            "│    │    └─0.attn.qkv.bn.bias                                                    ├─1,152\n",
            "│    │    └─0.attn.proj.1.linear.weight                                           ├─147,456\n",
            "│    │    └─0.attn.proj.1.bn.weight                                               ├─384\n",
            "│    │    └─0.attn.proj.1.bn.bias                                                 ├─384\n",
            "│    │    └─0.mlp.ln1.linear.weight                                               ├─294,912\n",
            "│    │    └─0.mlp.ln1.bn.weight                                                   ├─768\n",
            "│    │    └─0.mlp.ln1.bn.bias                                                     ├─768\n",
            "│    │    └─0.mlp.ln2.linear.weight                                               ├─294,912\n",
            "│    │    └─0.mlp.ln2.bn.weight                                                   ├─384\n",
            "│    │    └─0.mlp.ln2.bn.bias                                                     ├─384\n",
            "│    │    └─1.attn.attention_biases                                               ├─294\n",
            "│    │    └─1.attn.qkv.linear.weight                                              ├─442,368\n",
            "│    │    └─1.attn.qkv.bn.weight                                                  ├─1,152\n",
            "│    │    └─1.attn.qkv.bn.bias                                                    ├─1,152\n",
            "│    │    └─1.attn.proj.1.linear.weight                                           ├─147,456\n",
            "│    │    └─1.attn.proj.1.bn.weight                                               ├─384\n",
            "│    │    └─1.attn.proj.1.bn.bias                                                 ├─384\n",
            "│    │    └─1.mlp.ln1.linear.weight                                               ├─294,912\n",
            "│    │    └─1.mlp.ln1.bn.weight                                                   ├─768\n",
            "│    │    └─1.mlp.ln1.bn.bias                                                     ├─768\n",
            "│    │    └─1.mlp.ln2.linear.weight                                               ├─294,912\n",
            "│    │    └─1.mlp.ln2.bn.weight                                                   ├─384\n",
            "│    │    └─1.mlp.ln2.bn.bias                                                     ├─384\n",
            "│    │    └─2.attn.qkv.linear.weight                                              ├─442,368\n",
            "│    │    └─2.attn.qkv.bn.weight                                                  ├─1,152\n",
            "│    │    └─2.attn.qkv.bn.bias                                                    ├─1,152\n",
            "│    │    └─2.attn.proj.1.linear.weight                                           ├─147,456\n",
            "│    │    └─2.attn.proj.1.bn.weight                                               ├─384\n",
            "│    │    └─2.attn.proj.1.bn.bias                                                 ├─384\n",
            "│    │    └─2.mlp.ln1.linear.weight                                               ├─294,912\n",
            "│    │    └─2.mlp.ln1.bn.weight                                                   ├─768\n",
            "│    │    └─2.mlp.ln1.bn.bias                                                     ├─768\n",
            "│    │    └─2.mlp.ln2.linear.weight                                               ├─294,912\n",
            "│    │    └─2.mlp.ln2.bn.weight                                                   ├─384\n",
            "│    │    └─2.mlp.ln2.bn.bias                                                     ├─384\n",
            "│    │    └─3.attn.qkv.linear.weight                                              ├─442,368\n",
            "│    │    └─3.attn.qkv.bn.weight                                                  ├─1,152\n",
            "│    │    └─3.attn.qkv.bn.bias                                                    ├─1,152\n",
            "│    │    └─3.attn.proj.1.linear.weight                                           ├─147,456\n",
            "│    │    └─3.attn.proj.1.bn.weight                                               ├─384\n",
            "│    │    └─3.attn.proj.1.bn.bias                                                 ├─384\n",
            "│    │    └─3.mlp.ln1.linear.weight                                               ├─294,912\n",
            "│    │    └─3.mlp.ln1.bn.weight                                                   ├─768\n",
            "│    │    └─3.mlp.ln1.bn.bias                                                     ├─768\n",
            "│    │    └─3.mlp.ln2.linear.weight                                               ├─294,912\n",
            "│    │    └─3.mlp.ln2.bn.weight                                                   ├─384\n",
            "│    │    └─3.mlp.ln2.bn.bias                                                     └─384\n",
            "│    │    └─LevitBlock: 3-12                            [32, 49, 384]             1,185,024\n",
            "│    │    │    └─attn.qkv.linear.weight                                           ├─442,368\n",
            "│    │    │    └─attn.qkv.bn.weight                                               ├─1,152\n",
            "│    │    │    └─attn.qkv.bn.bias                                                 ├─1,152\n",
            "│    │    │    └─attn.proj.1.linear.weight                                        ├─147,456\n",
            "│    │    │    └─attn.proj.1.bn.weight                                            ├─384\n",
            "│    │    │    └─attn.proj.1.bn.bias                                              ├─384\n",
            "│    │    │    └─mlp.ln1.linear.weight                                            ├─294,912\n",
            "│    │    │    └─mlp.ln1.bn.weight                                                ├─768\n",
            "│    │    │    └─mlp.ln1.bn.bias                                                  ├─768\n",
            "│    │    │    └─mlp.ln2.linear.weight                                            ├─294,912\n",
            "│    │    │    └─mlp.ln2.bn.weight                                                ├─384\n",
            "│    │    │    └─mlp.ln2.bn.bias                                                  └─384\n",
            "├─Sequential: 1-4                                       [32, 512, 7, 7]           --\n",
            "│    └─0.weight                                                                   ├─196,608\n",
            "│    └─0.bias                                                                     ├─512\n",
            "│    └─1.weight                                                                   ├─512\n",
            "│    └─1.bias                                                                     └─512\n",
            "│    └─Conv2d: 2-12                                     [32, 512, 7, 7]           197,120\n",
            "│    │    └─weight                                                                ├─196,608\n",
            "│    │    └─bias                                                                  └─512\n",
            "│    └─BatchNorm2d: 2-13                                [32, 512, 7, 7]           1,024\n",
            "│    │    └─weight                                                                ├─512\n",
            "│    │    └─bias                                                                  └─512\n",
            "│    └─ReLU: 2-14                                       [32, 512, 7, 7]           --\n",
            "├─NormLinear: 1-5                                       [32, 9]                   --\n",
            "│    └─bn.weight                                                                  ├─512\n",
            "│    └─bn.bias                                                                    ├─512\n",
            "│    └─linear.weight                                                              ├─4,608\n",
            "│    └─linear.bias                                                                └─9\n",
            "│    └─BatchNorm1d: 2-15                                [32, 512]                 1,024\n",
            "│    │    └─weight                                                                ├─512\n",
            "│    │    └─bias                                                                  └─512\n",
            "│    └─Dropout: 2-16                                    [32, 512]                 --\n",
            "│    └─Linear: 2-17                                     [32, 9]                   4,617\n",
            "│    │    └─weight                                                                ├─4,608\n",
            "│    │    └─bias                                                                  └─9\n",
            "├─NormLinear: 1-6                                       [32, 9]                   --\n",
            "│    └─bn.weight                                                                  ├─512\n",
            "│    └─bn.bias                                                                    ├─512\n",
            "│    └─linear.weight                                                              ├─4,608\n",
            "│    └─linear.bias                                                                └─9\n",
            "│    └─BatchNorm1d: 2-18                                [32, 512]                 1,024\n",
            "│    │    └─weight                                                                ├─512\n",
            "│    │    └─bias                                                                  └─512\n",
            "│    └─Dropout: 2-19                                    [32, 512]                 --\n",
            "│    └─Linear: 2-20                                     [32, 9]                   4,617\n",
            "│    │    └─weight                                                                ├─4,608\n",
            "│    │    └─bias                                                                  └─9\n",
            "=========================================================================================================\n",
            "Total params: 8,337,182\n",
            "Trainable params: 8,337,182\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (Units.GIGABYTES): 12.76\n",
            "=========================================================================================================\n",
            "Input size (MB): 19.27\n",
            "Forward/backward pass size (MB): 650.55\n",
            "Params size (MB): 12.79\n",
            "Estimated Total Size (MB): 682.60\n",
            "=========================================================================================================\n"
          ]
        }
      ],
      "source": [
        "print(summary(model, input_size=(32, 3, 224, 224), verbose=2, tau = 10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 451,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2XJWJO0JTIMT",
        "outputId": "d7c9840e-c8bd-4981-dfee-3c42c724f518"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LevitDistilledTinyfusion(\n",
              "  (stem): Stem16(\n",
              "    (conv1): ConvNorm(\n",
              "      (linear): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (act1): Hardswish()\n",
              "    (conv2): ConvNorm(\n",
              "      (linear): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (act2): Hardswish()\n",
              "    (conv3): ConvNorm(\n",
              "      (linear): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (act3): Hardswish()\n",
              "    (conv4): ConvNorm(\n",
              "      (linear): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (stage1): LevitStage_TinyFusion(\n",
              "    (downsample): Identity()\n",
              "    (blocks): Sequential(\n",
              "      (0): LevitBlock(\n",
              "        (attn): Attention(\n",
              "          (qkv): LinearNorm(\n",
              "            (linear): Linear(in_features=256, out_features=768, bias=False)\n",
              "            (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "          (proj): Sequential(\n",
              "            (0): Hardswish()\n",
              "            (1): LinearNorm(\n",
              "              (linear): Linear(in_features=256, out_features=256, bias=False)\n",
              "              (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (drop_path1): Identity()\n",
              "        (mlp): LevitMlp(\n",
              "          (ln1): LinearNorm(\n",
              "            (linear): Linear(in_features=256, out_features=512, bias=False)\n",
              "            (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "          (act): Hardswish()\n",
              "          (drop): Dropout(p=0.5, inplace=False)\n",
              "          (ln2): LinearNorm(\n",
              "            (linear): Linear(in_features=512, out_features=256, bias=False)\n",
              "            (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "      (1): LevitBlock(\n",
              "        (attn): Attention(\n",
              "          (qkv): LinearNorm(\n",
              "            (linear): Linear(in_features=256, out_features=768, bias=False)\n",
              "            (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "          (proj): Sequential(\n",
              "            (0): Hardswish()\n",
              "            (1): LinearNorm(\n",
              "              (linear): Linear(in_features=256, out_features=256, bias=False)\n",
              "              (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (drop_path1): Identity()\n",
              "        (mlp): LevitMlp(\n",
              "          (ln1): LinearNorm(\n",
              "            (linear): Linear(in_features=256, out_features=512, bias=False)\n",
              "            (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "          (act): Hardswish()\n",
              "          (drop): Dropout(p=0.5, inplace=False)\n",
              "          (ln2): LinearNorm(\n",
              "            (linear): Linear(in_features=512, out_features=256, bias=False)\n",
              "            (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "      (2): LevitBlock(\n",
              "        (attn): Attention(\n",
              "          (qkv): LinearNorm(\n",
              "            (linear): Linear(in_features=256, out_features=768, bias=False)\n",
              "            (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "          (proj): Sequential(\n",
              "            (0): Hardswish()\n",
              "            (1): LinearNorm(\n",
              "              (linear): Linear(in_features=256, out_features=256, bias=False)\n",
              "              (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (drop_path1): Identity()\n",
              "        (mlp): LevitMlp(\n",
              "          (ln1): LinearNorm(\n",
              "            (linear): Linear(in_features=256, out_features=512, bias=False)\n",
              "            (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "          (act): Hardswish()\n",
              "          (drop): Dropout(p=0.5, inplace=False)\n",
              "          (ln2): LinearNorm(\n",
              "            (linear): Linear(in_features=512, out_features=256, bias=False)\n",
              "            (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "      (3): LevitBlock(\n",
              "        (attn): Attention(\n",
              "          (qkv): LinearNorm(\n",
              "            (linear): Linear(in_features=256, out_features=768, bias=False)\n",
              "            (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "          (proj): Sequential(\n",
              "            (0): Hardswish()\n",
              "            (1): LinearNorm(\n",
              "              (linear): Linear(in_features=256, out_features=256, bias=False)\n",
              "              (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (drop_path1): Identity()\n",
              "        (mlp): LevitMlp(\n",
              "          (ln1): LinearNorm(\n",
              "            (linear): Linear(in_features=256, out_features=512, bias=False)\n",
              "            (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "          (act): Hardswish()\n",
              "          (drop): Dropout(p=0.5, inplace=False)\n",
              "          (ln2): LinearNorm(\n",
              "            (linear): Linear(in_features=512, out_features=256, bias=False)\n",
              "            (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (stage2): LevitStage_TinyFusion(\n",
              "    (downsample): CNNDownsample(\n",
              "      (conv): Conv2d(256, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "      (act): Hardswish()\n",
              "    )\n",
              "    (blocks): Sequential(\n",
              "      (0): LevitBlock(\n",
              "        (attn): Attention(\n",
              "          (qkv): LinearNorm(\n",
              "            (linear): Linear(in_features=384, out_features=1152, bias=False)\n",
              "            (bn): BatchNorm1d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "          (proj): Sequential(\n",
              "            (0): Hardswish()\n",
              "            (1): LinearNorm(\n",
              "              (linear): Linear(in_features=384, out_features=384, bias=False)\n",
              "              (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (drop_path1): Identity()\n",
              "        (mlp): LevitMlp(\n",
              "          (ln1): LinearNorm(\n",
              "            (linear): Linear(in_features=384, out_features=768, bias=False)\n",
              "            (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "          (act): Hardswish()\n",
              "          (drop): Dropout(p=0.5, inplace=False)\n",
              "          (ln2): LinearNorm(\n",
              "            (linear): Linear(in_features=768, out_features=384, bias=False)\n",
              "            (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "      (1): LevitBlock(\n",
              "        (attn): Attention(\n",
              "          (qkv): LinearNorm(\n",
              "            (linear): Linear(in_features=384, out_features=1152, bias=False)\n",
              "            (bn): BatchNorm1d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "          (proj): Sequential(\n",
              "            (0): Hardswish()\n",
              "            (1): LinearNorm(\n",
              "              (linear): Linear(in_features=384, out_features=384, bias=False)\n",
              "              (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (drop_path1): Identity()\n",
              "        (mlp): LevitMlp(\n",
              "          (ln1): LinearNorm(\n",
              "            (linear): Linear(in_features=384, out_features=768, bias=False)\n",
              "            (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "          (act): Hardswish()\n",
              "          (drop): Dropout(p=0.5, inplace=False)\n",
              "          (ln2): LinearNorm(\n",
              "            (linear): Linear(in_features=768, out_features=384, bias=False)\n",
              "            (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "      (2): LevitBlock(\n",
              "        (attn): Attention(\n",
              "          (qkv): LinearNorm(\n",
              "            (linear): Linear(in_features=384, out_features=1152, bias=False)\n",
              "            (bn): BatchNorm1d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "          (proj): Sequential(\n",
              "            (0): Hardswish()\n",
              "            (1): LinearNorm(\n",
              "              (linear): Linear(in_features=384, out_features=384, bias=False)\n",
              "              (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (drop_path1): Identity()\n",
              "        (mlp): LevitMlp(\n",
              "          (ln1): LinearNorm(\n",
              "            (linear): Linear(in_features=384, out_features=768, bias=False)\n",
              "            (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "          (act): Hardswish()\n",
              "          (drop): Dropout(p=0.5, inplace=False)\n",
              "          (ln2): LinearNorm(\n",
              "            (linear): Linear(in_features=768, out_features=384, bias=False)\n",
              "            (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "      (3): LevitBlock(\n",
              "        (attn): Attention(\n",
              "          (qkv): LinearNorm(\n",
              "            (linear): Linear(in_features=384, out_features=1152, bias=False)\n",
              "            (bn): BatchNorm1d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "          (proj): Sequential(\n",
              "            (0): Hardswish()\n",
              "            (1): LinearNorm(\n",
              "              (linear): Linear(in_features=384, out_features=384, bias=False)\n",
              "              (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (drop_path1): Identity()\n",
              "        (mlp): LevitMlp(\n",
              "          (ln1): LinearNorm(\n",
              "            (linear): Linear(in_features=384, out_features=768, bias=False)\n",
              "            (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "          (act): Hardswish()\n",
              "          (drop): Dropout(p=0.5, inplace=False)\n",
              "          (ln2): LinearNorm(\n",
              "            (linear): Linear(in_features=768, out_features=384, bias=False)\n",
              "            (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (conv1x1): Sequential(\n",
              "    (0): Conv2d(384, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "  )\n",
              "  (head): NormLinear(\n",
              "    (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (drop): Dropout(p=0.0, inplace=False)\n",
              "    (linear): Linear(in_features=512, out_features=9, bias=True)\n",
              "  )\n",
              "  (head_dist): NormLinear(\n",
              "    (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (drop): Dropout(p=0.0, inplace=False)\n",
              "    (linear): Linear(in_features=512, out_features=9, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 451
        }
      ],
      "source": [
        "import torch.nn.init as init\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)  # CUDA 연산 시 동일한 결과 보장\n",
        "    torch.cuda.manual_seed_all(seed)  # 멀티-GPU 환경에서 동일한 결과 보장\n",
        "    torch.backends.cudnn.deterministic = True  # CuDNN 연산을 deterministic하게 설정\n",
        "    torch.backends.cudnn.benchmark = False  # 연산 속도를 희생하고 일관된 연산을 수행\n",
        "\n",
        "def initialize_weights(m):\n",
        "    if isinstance(m, nn.Conv2d):  # Conv 레이어 초기화\n",
        "        init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "        if m.bias is not None:\n",
        "            init.constant_(m.bias, 0)\n",
        "    elif isinstance(m, nn.Linear):  # Linear 레이어 초기화\n",
        "        init.xavier_uniform_(m.weight)\n",
        "        if m.bias is not None:\n",
        "            init.constant_(m.bias, 0)\n",
        "    elif isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):  # BatchNorm 초기화\n",
        "        init.constant_(m.weight, 1)\n",
        "        init.constant_(m.bias, 0)\n",
        "\n",
        "set_seed(42)  # 랜덤 시드 고정\n",
        "model.apply(initialize_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 452,
      "metadata": {
        "id": "p5GDxFHzR-y7"
      },
      "outputs": [],
      "source": [
        "train_dir     = './train/NCT-CRC-HE-100K'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])"
      ],
      "metadata": {
        "id": "NcNx9kgbdulf"
      },
      "execution_count": 453,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = datasets.ImageFolder(root=train_dir, transform=transform)"
      ],
      "metadata": {
        "id": "y_1SUIuidwbE"
      },
      "execution_count": 454,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 458,
      "metadata": {
        "id": "N0BkxEAYTMvj"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with open(\"default_index_dict.json\", \"r\") as f:\n",
        "    index_dict = json.load(f)\n",
        "load_train_idx = index_dict[\"train_idx\"]\n",
        "load_val_idx = index_dict[\"val_idx\"]\n",
        "load_test_idx = index_dict[\"test_idx\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 459,
      "metadata": {
        "id": "rEIH5EZxTN6n"
      },
      "outputs": [],
      "source": [
        "train_data = Subset(dataset, load_train_idx)\n",
        "val_data = Subset(dataset, load_val_idx)\n",
        "test_data = Subset(dataset, load_test_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 460,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8inM-bBnztEN",
        "outputId": "402c989a-f597-43c6-eb1d-f74814d507e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set size: 70000\n",
            "Validation set size: 15000\n",
            "Test set size: 15000\n"
          ]
        }
      ],
      "source": [
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=False)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(f\"Train set size: {len(train_data)}\")\n",
        "print(f\"Validation set size: {len(val_data)}\")\n",
        "print(f\"Test set size: {len(test_data)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gumbel_params = []\n",
        "other_params = []\n",
        "exclude = [\"head\", \"head_dist\"]\n",
        "\n",
        "for name, module in model.named_children():\n",
        "    if name in exclude:\n",
        "        print(exclude)\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    if \"gumbel_gate\" in name:\n",
        "        gumbel_params.append(param)\n",
        "        print(name)\n",
        "    else:\n",
        "        other_params.append(param)\n",
        "\n",
        "#model = convert_to_lora_model(model, exclude=exclude)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UrBLmUdJ4IZD",
        "outputId": "27f8d464-f8cb-4d1c-a787-fb725f53aaa6"
      },
      "execution_count": 461,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['head', 'head_dist']\n",
            "['head', 'head_dist']\n",
            "stage1.gumbel_gate\n",
            "stage2.gumbel_gate\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 462,
      "metadata": {
        "id": "1No-7jzGgzrV"
      },
      "outputs": [],
      "source": [
        "train_losses = []\n",
        "train_accuracies = []\n",
        "val_losses = []\n",
        "val_accuracies = []"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in model.named_parameters():\n",
        "    print(f\"Parameter name: {name}, requires_grad: {param.requires_grad}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CV-Fkvff1JIO",
        "outputId": "0e3f4a91-f513-424d-dc75-181af88b1318"
      },
      "execution_count": 463,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter name: stem.conv1.linear.weight, requires_grad: True\n",
            "Parameter name: stem.conv1.bn.weight, requires_grad: True\n",
            "Parameter name: stem.conv1.bn.bias, requires_grad: True\n",
            "Parameter name: stem.conv2.linear.weight, requires_grad: True\n",
            "Parameter name: stem.conv2.bn.weight, requires_grad: True\n",
            "Parameter name: stem.conv2.bn.bias, requires_grad: True\n",
            "Parameter name: stem.conv3.linear.weight, requires_grad: True\n",
            "Parameter name: stem.conv3.bn.weight, requires_grad: True\n",
            "Parameter name: stem.conv3.bn.bias, requires_grad: True\n",
            "Parameter name: stem.conv4.linear.weight, requires_grad: True\n",
            "Parameter name: stem.conv4.bn.weight, requires_grad: True\n",
            "Parameter name: stem.conv4.bn.bias, requires_grad: True\n",
            "Parameter name: stage1.gumbel_gate, requires_grad: True\n",
            "Parameter name: stage1.blocks.0.attn.attention_biases, requires_grad: True\n",
            "Parameter name: stage1.blocks.0.attn.qkv.linear.weight, requires_grad: True\n",
            "Parameter name: stage1.blocks.0.attn.qkv.bn.weight, requires_grad: True\n",
            "Parameter name: stage1.blocks.0.attn.qkv.bn.bias, requires_grad: True\n",
            "Parameter name: stage1.blocks.0.attn.proj.1.linear.weight, requires_grad: True\n",
            "Parameter name: stage1.blocks.0.attn.proj.1.bn.weight, requires_grad: True\n",
            "Parameter name: stage1.blocks.0.attn.proj.1.bn.bias, requires_grad: True\n",
            "Parameter name: stage1.blocks.0.mlp.ln1.linear.weight, requires_grad: True\n",
            "Parameter name: stage1.blocks.0.mlp.ln1.bn.weight, requires_grad: True\n",
            "Parameter name: stage1.blocks.0.mlp.ln1.bn.bias, requires_grad: True\n",
            "Parameter name: stage1.blocks.0.mlp.ln2.linear.weight, requires_grad: True\n",
            "Parameter name: stage1.blocks.0.mlp.ln2.bn.weight, requires_grad: True\n",
            "Parameter name: stage1.blocks.0.mlp.ln2.bn.bias, requires_grad: True\n",
            "Parameter name: stage1.blocks.1.attn.qkv.linear.weight, requires_grad: True\n",
            "Parameter name: stage1.blocks.1.attn.qkv.bn.weight, requires_grad: True\n",
            "Parameter name: stage1.blocks.1.attn.qkv.bn.bias, requires_grad: True\n",
            "Parameter name: stage1.blocks.1.attn.proj.1.linear.weight, requires_grad: True\n",
            "Parameter name: stage1.blocks.1.attn.proj.1.bn.weight, requires_grad: True\n",
            "Parameter name: stage1.blocks.1.attn.proj.1.bn.bias, requires_grad: True\n",
            "Parameter name: stage1.blocks.1.mlp.ln1.linear.weight, requires_grad: True\n",
            "Parameter name: stage1.blocks.1.mlp.ln1.bn.weight, requires_grad: True\n",
            "Parameter name: stage1.blocks.1.mlp.ln1.bn.bias, requires_grad: True\n",
            "Parameter name: stage1.blocks.1.mlp.ln2.linear.weight, requires_grad: True\n",
            "Parameter name: stage1.blocks.1.mlp.ln2.bn.weight, requires_grad: True\n",
            "Parameter name: stage1.blocks.1.mlp.ln2.bn.bias, requires_grad: True\n",
            "Parameter name: stage1.blocks.2.attn.attention_biases, requires_grad: True\n",
            "Parameter name: stage1.blocks.2.attn.qkv.linear.weight, requires_grad: True\n",
            "Parameter name: stage1.blocks.2.attn.qkv.bn.weight, requires_grad: True\n",
            "Parameter name: stage1.blocks.2.attn.qkv.bn.bias, requires_grad: True\n",
            "Parameter name: stage1.blocks.2.attn.proj.1.linear.weight, requires_grad: True\n",
            "Parameter name: stage1.blocks.2.attn.proj.1.bn.weight, requires_grad: True\n",
            "Parameter name: stage1.blocks.2.attn.proj.1.bn.bias, requires_grad: True\n",
            "Parameter name: stage1.blocks.2.mlp.ln1.linear.weight, requires_grad: True\n",
            "Parameter name: stage1.blocks.2.mlp.ln1.bn.weight, requires_grad: True\n",
            "Parameter name: stage1.blocks.2.mlp.ln1.bn.bias, requires_grad: True\n",
            "Parameter name: stage1.blocks.2.mlp.ln2.linear.weight, requires_grad: True\n",
            "Parameter name: stage1.blocks.2.mlp.ln2.bn.weight, requires_grad: True\n",
            "Parameter name: stage1.blocks.2.mlp.ln2.bn.bias, requires_grad: True\n",
            "Parameter name: stage1.blocks.3.attn.qkv.linear.weight, requires_grad: True\n",
            "Parameter name: stage1.blocks.3.attn.qkv.bn.weight, requires_grad: True\n",
            "Parameter name: stage1.blocks.3.attn.qkv.bn.bias, requires_grad: True\n",
            "Parameter name: stage1.blocks.3.attn.proj.1.linear.weight, requires_grad: True\n",
            "Parameter name: stage1.blocks.3.attn.proj.1.bn.weight, requires_grad: True\n",
            "Parameter name: stage1.blocks.3.attn.proj.1.bn.bias, requires_grad: True\n",
            "Parameter name: stage1.blocks.3.mlp.ln1.linear.weight, requires_grad: True\n",
            "Parameter name: stage1.blocks.3.mlp.ln1.bn.weight, requires_grad: True\n",
            "Parameter name: stage1.blocks.3.mlp.ln1.bn.bias, requires_grad: True\n",
            "Parameter name: stage1.blocks.3.mlp.ln2.linear.weight, requires_grad: True\n",
            "Parameter name: stage1.blocks.3.mlp.ln2.bn.weight, requires_grad: True\n",
            "Parameter name: stage1.blocks.3.mlp.ln2.bn.bias, requires_grad: True\n",
            "Parameter name: stage2.gumbel_gate, requires_grad: True\n",
            "Parameter name: stage2.downsample.conv.weight, requires_grad: True\n",
            "Parameter name: stage2.downsample.conv.bias, requires_grad: True\n",
            "Parameter name: stage2.blocks.0.attn.attention_biases, requires_grad: True\n",
            "Parameter name: stage2.blocks.0.attn.qkv.linear.weight, requires_grad: True\n",
            "Parameter name: stage2.blocks.0.attn.qkv.bn.weight, requires_grad: True\n",
            "Parameter name: stage2.blocks.0.attn.qkv.bn.bias, requires_grad: True\n",
            "Parameter name: stage2.blocks.0.attn.proj.1.linear.weight, requires_grad: True\n",
            "Parameter name: stage2.blocks.0.attn.proj.1.bn.weight, requires_grad: True\n",
            "Parameter name: stage2.blocks.0.attn.proj.1.bn.bias, requires_grad: True\n",
            "Parameter name: stage2.blocks.0.mlp.ln1.linear.weight, requires_grad: True\n",
            "Parameter name: stage2.blocks.0.mlp.ln1.bn.weight, requires_grad: True\n",
            "Parameter name: stage2.blocks.0.mlp.ln1.bn.bias, requires_grad: True\n",
            "Parameter name: stage2.blocks.0.mlp.ln2.linear.weight, requires_grad: True\n",
            "Parameter name: stage2.blocks.0.mlp.ln2.bn.weight, requires_grad: True\n",
            "Parameter name: stage2.blocks.0.mlp.ln2.bn.bias, requires_grad: True\n",
            "Parameter name: stage2.blocks.1.attn.attention_biases, requires_grad: True\n",
            "Parameter name: stage2.blocks.1.attn.qkv.linear.weight, requires_grad: True\n",
            "Parameter name: stage2.blocks.1.attn.qkv.bn.weight, requires_grad: True\n",
            "Parameter name: stage2.blocks.1.attn.qkv.bn.bias, requires_grad: True\n",
            "Parameter name: stage2.blocks.1.attn.proj.1.linear.weight, requires_grad: True\n",
            "Parameter name: stage2.blocks.1.attn.proj.1.bn.weight, requires_grad: True\n",
            "Parameter name: stage2.blocks.1.attn.proj.1.bn.bias, requires_grad: True\n",
            "Parameter name: stage2.blocks.1.mlp.ln1.linear.weight, requires_grad: True\n",
            "Parameter name: stage2.blocks.1.mlp.ln1.bn.weight, requires_grad: True\n",
            "Parameter name: stage2.blocks.1.mlp.ln1.bn.bias, requires_grad: True\n",
            "Parameter name: stage2.blocks.1.mlp.ln2.linear.weight, requires_grad: True\n",
            "Parameter name: stage2.blocks.1.mlp.ln2.bn.weight, requires_grad: True\n",
            "Parameter name: stage2.blocks.1.mlp.ln2.bn.bias, requires_grad: True\n",
            "Parameter name: stage2.blocks.2.attn.qkv.linear.weight, requires_grad: True\n",
            "Parameter name: stage2.blocks.2.attn.qkv.bn.weight, requires_grad: True\n",
            "Parameter name: stage2.blocks.2.attn.qkv.bn.bias, requires_grad: True\n",
            "Parameter name: stage2.blocks.2.attn.proj.1.linear.weight, requires_grad: True\n",
            "Parameter name: stage2.blocks.2.attn.proj.1.bn.weight, requires_grad: True\n",
            "Parameter name: stage2.blocks.2.attn.proj.1.bn.bias, requires_grad: True\n",
            "Parameter name: stage2.blocks.2.mlp.ln1.linear.weight, requires_grad: True\n",
            "Parameter name: stage2.blocks.2.mlp.ln1.bn.weight, requires_grad: True\n",
            "Parameter name: stage2.blocks.2.mlp.ln1.bn.bias, requires_grad: True\n",
            "Parameter name: stage2.blocks.2.mlp.ln2.linear.weight, requires_grad: True\n",
            "Parameter name: stage2.blocks.2.mlp.ln2.bn.weight, requires_grad: True\n",
            "Parameter name: stage2.blocks.2.mlp.ln2.bn.bias, requires_grad: True\n",
            "Parameter name: stage2.blocks.3.attn.qkv.linear.weight, requires_grad: True\n",
            "Parameter name: stage2.blocks.3.attn.qkv.bn.weight, requires_grad: True\n",
            "Parameter name: stage2.blocks.3.attn.qkv.bn.bias, requires_grad: True\n",
            "Parameter name: stage2.blocks.3.attn.proj.1.linear.weight, requires_grad: True\n",
            "Parameter name: stage2.blocks.3.attn.proj.1.bn.weight, requires_grad: True\n",
            "Parameter name: stage2.blocks.3.attn.proj.1.bn.bias, requires_grad: True\n",
            "Parameter name: stage2.blocks.3.mlp.ln1.linear.weight, requires_grad: True\n",
            "Parameter name: stage2.blocks.3.mlp.ln1.bn.weight, requires_grad: True\n",
            "Parameter name: stage2.blocks.3.mlp.ln1.bn.bias, requires_grad: True\n",
            "Parameter name: stage2.blocks.3.mlp.ln2.linear.weight, requires_grad: True\n",
            "Parameter name: stage2.blocks.3.mlp.ln2.bn.weight, requires_grad: True\n",
            "Parameter name: stage2.blocks.3.mlp.ln2.bn.bias, requires_grad: True\n",
            "Parameter name: conv1x1.0.weight, requires_grad: True\n",
            "Parameter name: conv1x1.0.bias, requires_grad: True\n",
            "Parameter name: conv1x1.1.weight, requires_grad: True\n",
            "Parameter name: conv1x1.1.bias, requires_grad: True\n",
            "Parameter name: head.bn.weight, requires_grad: True\n",
            "Parameter name: head.bn.bias, requires_grad: True\n",
            "Parameter name: head.linear.weight, requires_grad: True\n",
            "Parameter name: head.linear.bias, requires_grad: True\n",
            "Parameter name: head_dist.bn.weight, requires_grad: True\n",
            "Parameter name: head_dist.bn.bias, requires_grad: True\n",
            "Parameter name: head_dist.linear.weight, requires_grad: True\n",
            "Parameter name: head_dist.linear.bias, requires_grad: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def transfer_weights(pretrained_model: LevitDistilled, tinyfusion_model: LevitDistilledTinyfusion, num_blocks):\n",
        "\n",
        "    pretrained_dict = pretrained_model.state_dict()\n",
        "    tinyfusion_dict = tinyfusion_model.state_dict()\n",
        "\n",
        "    new_state_dict = {}\n",
        "\n",
        "    # 1️⃣ 공통된 가중치 복사 (stem, conv1x1, head, head_dist)\n",
        "    for key in tinyfusion_dict.keys():\n",
        "        if key in pretrained_dict and not key.startswith(\"stage\"):\n",
        "            new_state_dict[key] = pretrained_dict[key]\n",
        "\n",
        "    stage = []\n",
        "    for i in range(num_blocks):\n",
        "        stage.append(f\"stage{i}\")\n",
        "    # 2️⃣ stage1, stage2의 가중치 변환 적용\n",
        "    for stage_name in stage:\n",
        "        for i in range(num_blocks):  # num_blocks=4\n",
        "            old_key = f\"{stage_name}.blocks.{i}\"  # 원래 모델의 key\n",
        "            new_key = f\"{stage_name}.blocks.{i}\"  # TinyFusion 모델의 key\n",
        "\n",
        "            if old_key in pretrained_dict and new_key in tinyfusion_dict:\n",
        "                new_state_dict[new_key] = pretrained_dict[old_key]\n",
        "\n",
        "    # 3️⃣ `gumble_gate`는 원래 모델에 없으므로, 초기화된 값 유지 (로드 안함)\n",
        "    print(\"✅ 가중치 변환 완료! TinyFusion 모델에 적용합니다.\")\n",
        "\n",
        "    # 4️⃣ 변환된 가중치를 TinyFusion 모델에 로드 (strict=False)\n",
        "    tinyfusion_model.load_state_dict(new_state_dict, strict=False)"
      ],
      "metadata": {
        "id": "cPE55RV91MU7"
      },
      "execution_count": 464,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained_model = LevitDistilled(num_classes=9)\n",
        "pretrained_model.stage1.blocks[0].attn.compute_attention_bias(14)\n",
        "pretrained_model.stage1.blocks[1].attn.compute_attention_bias(14)\n",
        "pretrained_model.stage1.blocks[2].attn.compute_attention_bias(14)\n",
        "pretrained_model.stage1.blocks[3].attn.compute_attention_bias(14)\n",
        "pretrained_model.stage2.blocks[0].attn.compute_attention_bias(7)\n",
        "pretrained_model.stage2.blocks[1].attn.compute_attention_bias(7)\n",
        "pretrained_model.stage2.blocks[2].attn.compute_attention_bias(7)\n",
        "pretrained_model.stage2.blocks[3].attn.compute_attention_bias(7)\n",
        "model.stage1.blocks[0].attn.compute_attention_bias(14)\n",
        "model.stage1.blocks[1].attn.compute_attention_bias(14)\n",
        "model.stage1.blocks[2].attn.compute_attention_bias(14)\n",
        "model.stage1.blocks[3].attn.compute_attention_bias(14)\n",
        "model.stage2.blocks[0].attn.compute_attention_bias(7)\n",
        "model.stage2.blocks[1].attn.compute_attention_bias(7)\n",
        "model.stage2.blocks[2].attn.compute_attention_bias(7)\n",
        "model.stage2.blocks[3].attn.compute_attention_bias(7)\n",
        "pretrained_model.load_state_dict(torch.load(\"BaseLine_HoViT_44.pth\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9IyfpjVPq-L",
        "outputId": "a3e18a04-f3ee-4f10-bad9-2d7369f13f08"
      },
      "execution_count": 465,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-465-50b800dbf540>:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  pretrained_model.load_state_dict(torch.load(\"BaseLine_HoViT_44.pth\"))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 465
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transfer_weights(pretrained_model, model, num_blocks=4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3tRiWIi1P2W",
        "outputId": "45af4c43-cc84-423b-bb8b-4c9abeab968d"
      },
      "execution_count": 466,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 가중치 변환 완료! TinyFusion 모델에 적용합니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = convert_to_lora_model(model, exclude=exclude)"
      ],
      "metadata": {
        "id": "f5gT1SRD1Qtm"
      },
      "execution_count": 467,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in model.named_parameters():\n",
        "    if \"lora_a\" in name or \"lora_b\" in name:\n",
        "        param.requires_grad = True\n",
        "        print(f\"✅ LoRA Trainable: {name}\")\n",
        "    if \"gumbel_gate\" in name:\n",
        "        param.requires_grad = True\n",
        "        print(f\"✅ TinyFusion Trainable: {name}\")\n",
        "    elif \"head\" in name or \"head_dist\" in name:\n",
        "        param.requires_grad = True\n",
        "        print(f\"✅ Head Trainable: {name}\")\n",
        "    elif \"conv1x1\" in name:\n",
        "        param.requires_grad = True\n",
        "        print(f\"✅ Conv1x1 Trainable: {name}\")\n",
        "    elif \"attention_biases\" in name:\n",
        "        param.requires_grad = True\n",
        "        print(f\"✅ Attention Biases Trainable: {name}\")\n",
        "    else:\n",
        "        param.requires_grad = False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WW-W9UFA1TKZ",
        "outputId": "da5e6b74-ff29-4ccd-f69c-539a09255b26"
      },
      "execution_count": 468,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ TinyFusion Trainable: stage1.gumbel_gate\n",
            "✅ Attention Biases Trainable: stage1.blocks.0.attn.attention_biases\n",
            "✅ LoRA Trainable: stage1.blocks.0.attn.qkv.linear.lora_a.weight\n",
            "✅ LoRA Trainable: stage1.blocks.0.attn.qkv.linear.lora_b.weight\n",
            "✅ LoRA Trainable: stage1.blocks.0.attn.proj.1.linear.lora_a.weight\n",
            "✅ LoRA Trainable: stage1.blocks.0.attn.proj.1.linear.lora_b.weight\n",
            "✅ LoRA Trainable: stage1.blocks.0.mlp.ln1.linear.lora_a.weight\n",
            "✅ LoRA Trainable: stage1.blocks.0.mlp.ln1.linear.lora_b.weight\n",
            "✅ LoRA Trainable: stage1.blocks.0.mlp.ln2.linear.lora_a.weight\n",
            "✅ LoRA Trainable: stage1.blocks.0.mlp.ln2.linear.lora_b.weight\n",
            "✅ Attention Biases Trainable: stage1.blocks.1.attn.attention_biases\n",
            "✅ LoRA Trainable: stage1.blocks.1.attn.qkv.linear.lora_a.weight\n",
            "✅ LoRA Trainable: stage1.blocks.1.attn.qkv.linear.lora_b.weight\n",
            "✅ LoRA Trainable: stage1.blocks.1.attn.proj.1.linear.lora_a.weight\n",
            "✅ LoRA Trainable: stage1.blocks.1.attn.proj.1.linear.lora_b.weight\n",
            "✅ LoRA Trainable: stage1.blocks.1.mlp.ln1.linear.lora_a.weight\n",
            "✅ LoRA Trainable: stage1.blocks.1.mlp.ln1.linear.lora_b.weight\n",
            "✅ LoRA Trainable: stage1.blocks.1.mlp.ln2.linear.lora_a.weight\n",
            "✅ LoRA Trainable: stage1.blocks.1.mlp.ln2.linear.lora_b.weight\n",
            "✅ Attention Biases Trainable: stage1.blocks.2.attn.attention_biases\n",
            "✅ LoRA Trainable: stage1.blocks.2.attn.qkv.linear.lora_a.weight\n",
            "✅ LoRA Trainable: stage1.blocks.2.attn.qkv.linear.lora_b.weight\n",
            "✅ LoRA Trainable: stage1.blocks.2.attn.proj.1.linear.lora_a.weight\n",
            "✅ LoRA Trainable: stage1.blocks.2.attn.proj.1.linear.lora_b.weight\n",
            "✅ LoRA Trainable: stage1.blocks.2.mlp.ln1.linear.lora_a.weight\n",
            "✅ LoRA Trainable: stage1.blocks.2.mlp.ln1.linear.lora_b.weight\n",
            "✅ LoRA Trainable: stage1.blocks.2.mlp.ln2.linear.lora_a.weight\n",
            "✅ LoRA Trainable: stage1.blocks.2.mlp.ln2.linear.lora_b.weight\n",
            "✅ Attention Biases Trainable: stage1.blocks.3.attn.attention_biases\n",
            "✅ LoRA Trainable: stage1.blocks.3.attn.qkv.linear.lora_a.weight\n",
            "✅ LoRA Trainable: stage1.blocks.3.attn.qkv.linear.lora_b.weight\n",
            "✅ LoRA Trainable: stage1.blocks.3.attn.proj.1.linear.lora_a.weight\n",
            "✅ LoRA Trainable: stage1.blocks.3.attn.proj.1.linear.lora_b.weight\n",
            "✅ LoRA Trainable: stage1.blocks.3.mlp.ln1.linear.lora_a.weight\n",
            "✅ LoRA Trainable: stage1.blocks.3.mlp.ln1.linear.lora_b.weight\n",
            "✅ LoRA Trainable: stage1.blocks.3.mlp.ln2.linear.lora_a.weight\n",
            "✅ LoRA Trainable: stage1.blocks.3.mlp.ln2.linear.lora_b.weight\n",
            "✅ TinyFusion Trainable: stage2.gumbel_gate\n",
            "✅ Attention Biases Trainable: stage2.blocks.0.attn.attention_biases\n",
            "✅ LoRA Trainable: stage2.blocks.0.attn.qkv.linear.lora_a.weight\n",
            "✅ LoRA Trainable: stage2.blocks.0.attn.qkv.linear.lora_b.weight\n",
            "✅ LoRA Trainable: stage2.blocks.0.attn.proj.1.linear.lora_a.weight\n",
            "✅ LoRA Trainable: stage2.blocks.0.attn.proj.1.linear.lora_b.weight\n",
            "✅ LoRA Trainable: stage2.blocks.0.mlp.ln1.linear.lora_a.weight\n",
            "✅ LoRA Trainable: stage2.blocks.0.mlp.ln1.linear.lora_b.weight\n",
            "✅ LoRA Trainable: stage2.blocks.0.mlp.ln2.linear.lora_a.weight\n",
            "✅ LoRA Trainable: stage2.blocks.0.mlp.ln2.linear.lora_b.weight\n",
            "✅ Attention Biases Trainable: stage2.blocks.1.attn.attention_biases\n",
            "✅ LoRA Trainable: stage2.blocks.1.attn.qkv.linear.lora_a.weight\n",
            "✅ LoRA Trainable: stage2.blocks.1.attn.qkv.linear.lora_b.weight\n",
            "✅ LoRA Trainable: stage2.blocks.1.attn.proj.1.linear.lora_a.weight\n",
            "✅ LoRA Trainable: stage2.blocks.1.attn.proj.1.linear.lora_b.weight\n",
            "✅ LoRA Trainable: stage2.blocks.1.mlp.ln1.linear.lora_a.weight\n",
            "✅ LoRA Trainable: stage2.blocks.1.mlp.ln1.linear.lora_b.weight\n",
            "✅ LoRA Trainable: stage2.blocks.1.mlp.ln2.linear.lora_a.weight\n",
            "✅ LoRA Trainable: stage2.blocks.1.mlp.ln2.linear.lora_b.weight\n",
            "✅ Attention Biases Trainable: stage2.blocks.2.attn.attention_biases\n",
            "✅ LoRA Trainable: stage2.blocks.2.attn.qkv.linear.lora_a.weight\n",
            "✅ LoRA Trainable: stage2.blocks.2.attn.qkv.linear.lora_b.weight\n",
            "✅ LoRA Trainable: stage2.blocks.2.attn.proj.1.linear.lora_a.weight\n",
            "✅ LoRA Trainable: stage2.blocks.2.attn.proj.1.linear.lora_b.weight\n",
            "✅ LoRA Trainable: stage2.blocks.2.mlp.ln1.linear.lora_a.weight\n",
            "✅ LoRA Trainable: stage2.blocks.2.mlp.ln1.linear.lora_b.weight\n",
            "✅ LoRA Trainable: stage2.blocks.2.mlp.ln2.linear.lora_a.weight\n",
            "✅ LoRA Trainable: stage2.blocks.2.mlp.ln2.linear.lora_b.weight\n",
            "✅ Attention Biases Trainable: stage2.blocks.3.attn.attention_biases\n",
            "✅ LoRA Trainable: stage2.blocks.3.attn.qkv.linear.lora_a.weight\n",
            "✅ LoRA Trainable: stage2.blocks.3.attn.qkv.linear.lora_b.weight\n",
            "✅ LoRA Trainable: stage2.blocks.3.attn.proj.1.linear.lora_a.weight\n",
            "✅ LoRA Trainable: stage2.blocks.3.attn.proj.1.linear.lora_b.weight\n",
            "✅ LoRA Trainable: stage2.blocks.3.mlp.ln1.linear.lora_a.weight\n",
            "✅ LoRA Trainable: stage2.blocks.3.mlp.ln1.linear.lora_b.weight\n",
            "✅ LoRA Trainable: stage2.blocks.3.mlp.ln2.linear.lora_a.weight\n",
            "✅ LoRA Trainable: stage2.blocks.3.mlp.ln2.linear.lora_b.weight\n",
            "✅ Conv1x1 Trainable: conv1x1.0.weight\n",
            "✅ Conv1x1 Trainable: conv1x1.0.bias\n",
            "✅ Conv1x1 Trainable: conv1x1.1.weight\n",
            "✅ Conv1x1 Trainable: conv1x1.1.bias\n",
            "✅ Head Trainable: head.bn.weight\n",
            "✅ Head Trainable: head.bn.bias\n",
            "✅ Head Trainable: head.linear.weight\n",
            "✅ Head Trainable: head.linear.bias\n",
            "✅ Head Trainable: head_dist.bn.weight\n",
            "✅ Head Trainable: head_dist.bn.bias\n",
            "✅ Head Trainable: head_dist.linear.weight\n",
            "✅ Head Trainable: head_dist.linear.bias\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# gumbel_gate 파라미터가 포함되어 있는지 확인\n",
        "for name, param in model.named_parameters():\n",
        "    print(f\"Parameter name: {name}, requires_grad: {param.requires_grad}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wqFKGLI21R2I",
        "outputId": "5600e119-0003-4330-acab-76c956479ccf"
      },
      "execution_count": 469,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter name: stem.conv1.linear.weight, requires_grad: False\n",
            "Parameter name: stem.conv1.bn.weight, requires_grad: False\n",
            "Parameter name: stem.conv1.bn.bias, requires_grad: False\n",
            "Parameter name: stem.conv2.linear.weight, requires_grad: False\n",
            "Parameter name: stem.conv2.bn.weight, requires_grad: False\n",
            "Parameter name: stem.conv2.bn.bias, requires_grad: False\n",
            "Parameter name: stem.conv3.linear.weight, requires_grad: False\n",
            "Parameter name: stem.conv3.bn.weight, requires_grad: False\n",
            "Parameter name: stem.conv3.bn.bias, requires_grad: False\n",
            "Parameter name: stem.conv4.linear.weight, requires_grad: False\n",
            "Parameter name: stem.conv4.bn.weight, requires_grad: False\n",
            "Parameter name: stem.conv4.bn.bias, requires_grad: False\n",
            "Parameter name: stage1.gumbel_gate, requires_grad: True\n",
            "Parameter name: stage1.blocks.0.attn.attention_biases, requires_grad: True\n",
            "Parameter name: stage1.blocks.0.attn.qkv.linear.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.0.attn.qkv.linear.lora_a.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.0.attn.qkv.linear.lora_b.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.0.attn.qkv.bn.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.0.attn.qkv.bn.bias, requires_grad: False\n",
            "Parameter name: stage1.blocks.0.attn.proj.1.linear.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.0.attn.proj.1.linear.lora_a.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.0.attn.proj.1.linear.lora_b.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.0.attn.proj.1.bn.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.0.attn.proj.1.bn.bias, requires_grad: False\n",
            "Parameter name: stage1.blocks.0.mlp.ln1.linear.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.0.mlp.ln1.linear.lora_a.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.0.mlp.ln1.linear.lora_b.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.0.mlp.ln1.bn.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.0.mlp.ln1.bn.bias, requires_grad: False\n",
            "Parameter name: stage1.blocks.0.mlp.ln2.linear.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.0.mlp.ln2.linear.lora_a.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.0.mlp.ln2.linear.lora_b.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.0.mlp.ln2.bn.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.0.mlp.ln2.bn.bias, requires_grad: False\n",
            "Parameter name: stage1.blocks.1.attn.attention_biases, requires_grad: True\n",
            "Parameter name: stage1.blocks.1.attn.qkv.linear.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.1.attn.qkv.linear.lora_a.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.1.attn.qkv.linear.lora_b.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.1.attn.qkv.bn.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.1.attn.qkv.bn.bias, requires_grad: False\n",
            "Parameter name: stage1.blocks.1.attn.proj.1.linear.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.1.attn.proj.1.linear.lora_a.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.1.attn.proj.1.linear.lora_b.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.1.attn.proj.1.bn.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.1.attn.proj.1.bn.bias, requires_grad: False\n",
            "Parameter name: stage1.blocks.1.mlp.ln1.linear.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.1.mlp.ln1.linear.lora_a.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.1.mlp.ln1.linear.lora_b.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.1.mlp.ln1.bn.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.1.mlp.ln1.bn.bias, requires_grad: False\n",
            "Parameter name: stage1.blocks.1.mlp.ln2.linear.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.1.mlp.ln2.linear.lora_a.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.1.mlp.ln2.linear.lora_b.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.1.mlp.ln2.bn.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.1.mlp.ln2.bn.bias, requires_grad: False\n",
            "Parameter name: stage1.blocks.2.attn.attention_biases, requires_grad: True\n",
            "Parameter name: stage1.blocks.2.attn.qkv.linear.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.2.attn.qkv.linear.lora_a.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.2.attn.qkv.linear.lora_b.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.2.attn.qkv.bn.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.2.attn.qkv.bn.bias, requires_grad: False\n",
            "Parameter name: stage1.blocks.2.attn.proj.1.linear.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.2.attn.proj.1.linear.lora_a.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.2.attn.proj.1.linear.lora_b.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.2.attn.proj.1.bn.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.2.attn.proj.1.bn.bias, requires_grad: False\n",
            "Parameter name: stage1.blocks.2.mlp.ln1.linear.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.2.mlp.ln1.linear.lora_a.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.2.mlp.ln1.linear.lora_b.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.2.mlp.ln1.bn.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.2.mlp.ln1.bn.bias, requires_grad: False\n",
            "Parameter name: stage1.blocks.2.mlp.ln2.linear.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.2.mlp.ln2.linear.lora_a.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.2.mlp.ln2.linear.lora_b.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.2.mlp.ln2.bn.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.2.mlp.ln2.bn.bias, requires_grad: False\n",
            "Parameter name: stage1.blocks.3.attn.attention_biases, requires_grad: True\n",
            "Parameter name: stage1.blocks.3.attn.qkv.linear.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.3.attn.qkv.linear.lora_a.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.3.attn.qkv.linear.lora_b.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.3.attn.qkv.bn.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.3.attn.qkv.bn.bias, requires_grad: False\n",
            "Parameter name: stage1.blocks.3.attn.proj.1.linear.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.3.attn.proj.1.linear.lora_a.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.3.attn.proj.1.linear.lora_b.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.3.attn.proj.1.bn.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.3.attn.proj.1.bn.bias, requires_grad: False\n",
            "Parameter name: stage1.blocks.3.mlp.ln1.linear.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.3.mlp.ln1.linear.lora_a.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.3.mlp.ln1.linear.lora_b.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.3.mlp.ln1.bn.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.3.mlp.ln1.bn.bias, requires_grad: False\n",
            "Parameter name: stage1.blocks.3.mlp.ln2.linear.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.3.mlp.ln2.linear.lora_a.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.3.mlp.ln2.linear.lora_b.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.3.mlp.ln2.bn.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.3.mlp.ln2.bn.bias, requires_grad: False\n",
            "Parameter name: stage2.gumbel_gate, requires_grad: True\n",
            "Parameter name: stage2.downsample.conv.weight, requires_grad: False\n",
            "Parameter name: stage2.downsample.conv.bias, requires_grad: False\n",
            "Parameter name: stage2.blocks.0.attn.attention_biases, requires_grad: True\n",
            "Parameter name: stage2.blocks.0.attn.qkv.linear.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.0.attn.qkv.linear.lora_a.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.0.attn.qkv.linear.lora_b.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.0.attn.qkv.bn.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.0.attn.qkv.bn.bias, requires_grad: False\n",
            "Parameter name: stage2.blocks.0.attn.proj.1.linear.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.0.attn.proj.1.linear.lora_a.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.0.attn.proj.1.linear.lora_b.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.0.attn.proj.1.bn.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.0.attn.proj.1.bn.bias, requires_grad: False\n",
            "Parameter name: stage2.blocks.0.mlp.ln1.linear.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.0.mlp.ln1.linear.lora_a.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.0.mlp.ln1.linear.lora_b.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.0.mlp.ln1.bn.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.0.mlp.ln1.bn.bias, requires_grad: False\n",
            "Parameter name: stage2.blocks.0.mlp.ln2.linear.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.0.mlp.ln2.linear.lora_a.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.0.mlp.ln2.linear.lora_b.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.0.mlp.ln2.bn.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.0.mlp.ln2.bn.bias, requires_grad: False\n",
            "Parameter name: stage2.blocks.1.attn.attention_biases, requires_grad: True\n",
            "Parameter name: stage2.blocks.1.attn.qkv.linear.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.1.attn.qkv.linear.lora_a.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.1.attn.qkv.linear.lora_b.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.1.attn.qkv.bn.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.1.attn.qkv.bn.bias, requires_grad: False\n",
            "Parameter name: stage2.blocks.1.attn.proj.1.linear.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.1.attn.proj.1.linear.lora_a.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.1.attn.proj.1.linear.lora_b.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.1.attn.proj.1.bn.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.1.attn.proj.1.bn.bias, requires_grad: False\n",
            "Parameter name: stage2.blocks.1.mlp.ln1.linear.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.1.mlp.ln1.linear.lora_a.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.1.mlp.ln1.linear.lora_b.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.1.mlp.ln1.bn.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.1.mlp.ln1.bn.bias, requires_grad: False\n",
            "Parameter name: stage2.blocks.1.mlp.ln2.linear.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.1.mlp.ln2.linear.lora_a.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.1.mlp.ln2.linear.lora_b.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.1.mlp.ln2.bn.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.1.mlp.ln2.bn.bias, requires_grad: False\n",
            "Parameter name: stage2.blocks.2.attn.attention_biases, requires_grad: True\n",
            "Parameter name: stage2.blocks.2.attn.qkv.linear.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.2.attn.qkv.linear.lora_a.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.2.attn.qkv.linear.lora_b.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.2.attn.qkv.bn.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.2.attn.qkv.bn.bias, requires_grad: False\n",
            "Parameter name: stage2.blocks.2.attn.proj.1.linear.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.2.attn.proj.1.linear.lora_a.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.2.attn.proj.1.linear.lora_b.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.2.attn.proj.1.bn.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.2.attn.proj.1.bn.bias, requires_grad: False\n",
            "Parameter name: stage2.blocks.2.mlp.ln1.linear.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.2.mlp.ln1.linear.lora_a.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.2.mlp.ln1.linear.lora_b.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.2.mlp.ln1.bn.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.2.mlp.ln1.bn.bias, requires_grad: False\n",
            "Parameter name: stage2.blocks.2.mlp.ln2.linear.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.2.mlp.ln2.linear.lora_a.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.2.mlp.ln2.linear.lora_b.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.2.mlp.ln2.bn.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.2.mlp.ln2.bn.bias, requires_grad: False\n",
            "Parameter name: stage2.blocks.3.attn.attention_biases, requires_grad: True\n",
            "Parameter name: stage2.blocks.3.attn.qkv.linear.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.3.attn.qkv.linear.lora_a.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.3.attn.qkv.linear.lora_b.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.3.attn.qkv.bn.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.3.attn.qkv.bn.bias, requires_grad: False\n",
            "Parameter name: stage2.blocks.3.attn.proj.1.linear.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.3.attn.proj.1.linear.lora_a.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.3.attn.proj.1.linear.lora_b.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.3.attn.proj.1.bn.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.3.attn.proj.1.bn.bias, requires_grad: False\n",
            "Parameter name: stage2.blocks.3.mlp.ln1.linear.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.3.mlp.ln1.linear.lora_a.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.3.mlp.ln1.linear.lora_b.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.3.mlp.ln1.bn.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.3.mlp.ln1.bn.bias, requires_grad: False\n",
            "Parameter name: stage2.blocks.3.mlp.ln2.linear.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.3.mlp.ln2.linear.lora_a.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.3.mlp.ln2.linear.lora_b.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.3.mlp.ln2.bn.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.3.mlp.ln2.bn.bias, requires_grad: False\n",
            "Parameter name: conv1x1.0.weight, requires_grad: True\n",
            "Parameter name: conv1x1.0.bias, requires_grad: True\n",
            "Parameter name: conv1x1.1.weight, requires_grad: True\n",
            "Parameter name: conv1x1.1.bias, requires_grad: True\n",
            "Parameter name: head.bn.weight, requires_grad: True\n",
            "Parameter name: head.bn.bias, requires_grad: True\n",
            "Parameter name: head.linear.weight, requires_grad: True\n",
            "Parameter name: head.linear.bias, requires_grad: True\n",
            "Parameter name: head_dist.bn.weight, requires_grad: True\n",
            "Parameter name: head_dist.bn.bias, requires_grad: True\n",
            "Parameter name: head_dist.linear.weight, requires_grad: True\n",
            "Parameter name: head_dist.linear.bias, requires_grad: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gumbel_params = []\n",
        "other_params = []\n",
        "exclude = [\"head\", \"head_dist\"]\n",
        "\n",
        "for name, module in model.named_children():\n",
        "    if name in exclude:\n",
        "        print(exclude)\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    if \"gumbel_gate\" in name:\n",
        "        gumbel_params.append(param)\n",
        "        print(name)\n",
        "    else:\n",
        "        other_params.append(param)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xuz4HhpzJWGo",
        "outputId": "e47ce80f-ce03-4b19-dedf-b332632e90c8"
      },
      "execution_count": 470,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['head', 'head_dist']\n",
            "['head', 'head_dist']\n",
            "stage1.gumbel_gate\n",
            "stage2.gumbel_gate\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam([\n",
        "    {'params' : gumbel_params, 'lr': learning_rate * 10},\n",
        "    {'params' : other_params, 'lr': learning_rate}\n",
        "])"
      ],
      "metadata": {
        "id": "G6iAZdW91cB1"
      },
      "execution_count": 471,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 472,
      "metadata": {
        "id": "Yf0ZPGLtz_gB"
      },
      "outputs": [],
      "source": [
        "def train(model, train_loader, criterion, optimizer, device, epoch, total_epcohs=100):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    tau_max = 1\n",
        "    tau_min = 0.1\n",
        "    total_steps = total_epcohs * len(train_loader)\n",
        "    train_steps = epoch * len(train_loader)\n",
        "\n",
        "    for i, (inputs, labels) in enumerate(tqdm(train_loader, desc=\"Training\")):\n",
        "        tau = tau_max - (tau_max - tau_min) * min(1.0, (train_steps + i) / total_steps)\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs, tau)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Train Loss: {epoch_loss:.4f}, Train Accuracy: {accuracy:.2f}%\")\n",
        "    print(f\"Tau: {tau:.4f}\")\n",
        "    train_losses.append(epoch_loss)\n",
        "    train_accuracies.append(accuracy)\n",
        "\n",
        "    print(\"Each stage of block probabilities:\")\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, LevitStage_TinyFusion):\n",
        "            gate_probs = torch.softmax(module.gumbel_gate, dim=0) # 각 block의 확률\n",
        "            topk_indx = torch.topk(gate_probs, module.num_select).indices.tolist() # 상위 k개의 index\n",
        "            print(f\"{name}\")\n",
        "            for i, prob in enumerate(gate_probs):\n",
        "                mask = \" \" if i in topk_indx else \"*\" # mask\n",
        "                print(f\"Block {i}: {prob:.4f} {mask}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 473,
      "metadata": {
        "id": "rbdjXJ520Btp"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, data_loader, criterion, device, phase=\"Validation\", epoch=0, total_epcohs=100):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "    tau_max = 1\n",
        "    tau_min = 0.1\n",
        "    total_steps = total_epcohs * len(data_loader)\n",
        "    train_steps = epoch * len(data_loader)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, labels) in enumerate(tqdm(data_loader, desc=f\"{phase}\")):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            if phase == \"Validation\":\n",
        "                tau = tau_max - (tau_max - tau_min) * min(1.0, (train_steps + i) / total_steps)\n",
        "            elif phase == \"Test\":\n",
        "                tau = 1e-5\n",
        "            outputs = model(inputs, tau)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            # Save all labels and predictions for balanced accuracy\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "    epoch_loss = running_loss / len(data_loader)\n",
        "    accuracy = 100 * correct / total\n",
        "    balanced_acc = balanced_accuracy_score(all_labels, all_predictions)\n",
        "\n",
        "    print(f\"{phase} Loss: {epoch_loss:.4f}, {phase} Accuracy: {accuracy:.2f}%\")\n",
        "    print(f\"Balanced Accuracy: {balanced_acc:.4f}\")\n",
        "    print(f\"Tau: {tau:.4f}\")\n",
        "\n",
        "    val_losses.append(epoch_loss)\n",
        "    val_accuracies.append(accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 474,
      "metadata": {
        "id": "GfBjePI-0DkY"
      },
      "outputs": [],
      "source": [
        "def measure_inference_time(model, data_loader, device):\n",
        "    model.eval()\n",
        "    times = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, _ in data_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            start_time = torch.cuda.Event(enable_timing=True)\n",
        "            end_time = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "            start_time.record()\n",
        "            _ = model(inputs, tau=0.1)  # inference 수행\n",
        "            end_time.record()\n",
        "\n",
        "            # 시간 측정\n",
        "            torch.cuda.synchronize()  # CUDA에서 모든 커널이 완료될 때까지 대기\n",
        "            elapsed_time = start_time.elapsed_time(end_time)  # 밀리초 단위로 반환\n",
        "            times.append(elapsed_time)\n",
        "\n",
        "    # 통계량 계산\n",
        "    times_np = np.array(times)\n",
        "    total_inferences = len(times_np)\n",
        "    avg_time = np.mean(times_np)\n",
        "    std_dev = np.std(times_np)\n",
        "    max_time = np.max(times_np)\n",
        "    min_time = np.min(times_np)\n",
        "\n",
        "    # 결과 출력\n",
        "    print(f\"Inference Time Measurement Results:\")\n",
        "    print(f\"Total Inferences: {total_inferences}\")\n",
        "    print(f\"Average Time: {avg_time:.2f} ms\")\n",
        "    print(f\"Standard Deviation: {std_dev:.2f} ms\")\n",
        "    print(f\"Maximum Time: {max_time:.2f} ms\")\n",
        "    print(f\"Minimum Time: {min_time:.2f} ms\")\n",
        "\n",
        "    return times"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in model.named_parameters():\n",
        "    print(f\"Parameter name: {name}, requires_grad: {param.requires_grad}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2E_O1ooo4c1A",
        "outputId": "610f59eb-4699-482c-c3db-6019e471fb1f"
      },
      "execution_count": 475,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter name: stem.conv1.linear.weight, requires_grad: False\n",
            "Parameter name: stem.conv1.bn.weight, requires_grad: False\n",
            "Parameter name: stem.conv1.bn.bias, requires_grad: False\n",
            "Parameter name: stem.conv2.linear.weight, requires_grad: False\n",
            "Parameter name: stem.conv2.bn.weight, requires_grad: False\n",
            "Parameter name: stem.conv2.bn.bias, requires_grad: False\n",
            "Parameter name: stem.conv3.linear.weight, requires_grad: False\n",
            "Parameter name: stem.conv3.bn.weight, requires_grad: False\n",
            "Parameter name: stem.conv3.bn.bias, requires_grad: False\n",
            "Parameter name: stem.conv4.linear.weight, requires_grad: False\n",
            "Parameter name: stem.conv4.bn.weight, requires_grad: False\n",
            "Parameter name: stem.conv4.bn.bias, requires_grad: False\n",
            "Parameter name: stage1.gumbel_gate, requires_grad: True\n",
            "Parameter name: stage1.blocks.0.attn.attention_biases, requires_grad: True\n",
            "Parameter name: stage1.blocks.0.attn.qkv.linear.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.0.attn.qkv.linear.lora_a.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.0.attn.qkv.linear.lora_b.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.0.attn.qkv.bn.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.0.attn.qkv.bn.bias, requires_grad: False\n",
            "Parameter name: stage1.blocks.0.attn.proj.1.linear.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.0.attn.proj.1.linear.lora_a.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.0.attn.proj.1.linear.lora_b.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.0.attn.proj.1.bn.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.0.attn.proj.1.bn.bias, requires_grad: False\n",
            "Parameter name: stage1.blocks.0.mlp.ln1.linear.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.0.mlp.ln1.linear.lora_a.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.0.mlp.ln1.linear.lora_b.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.0.mlp.ln1.bn.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.0.mlp.ln1.bn.bias, requires_grad: False\n",
            "Parameter name: stage1.blocks.0.mlp.ln2.linear.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.0.mlp.ln2.linear.lora_a.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.0.mlp.ln2.linear.lora_b.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.0.mlp.ln2.bn.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.0.mlp.ln2.bn.bias, requires_grad: False\n",
            "Parameter name: stage1.blocks.1.attn.attention_biases, requires_grad: True\n",
            "Parameter name: stage1.blocks.1.attn.qkv.linear.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.1.attn.qkv.linear.lora_a.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.1.attn.qkv.linear.lora_b.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.1.attn.qkv.bn.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.1.attn.qkv.bn.bias, requires_grad: False\n",
            "Parameter name: stage1.blocks.1.attn.proj.1.linear.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.1.attn.proj.1.linear.lora_a.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.1.attn.proj.1.linear.lora_b.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.1.attn.proj.1.bn.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.1.attn.proj.1.bn.bias, requires_grad: False\n",
            "Parameter name: stage1.blocks.1.mlp.ln1.linear.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.1.mlp.ln1.linear.lora_a.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.1.mlp.ln1.linear.lora_b.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.1.mlp.ln1.bn.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.1.mlp.ln1.bn.bias, requires_grad: False\n",
            "Parameter name: stage1.blocks.1.mlp.ln2.linear.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.1.mlp.ln2.linear.lora_a.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.1.mlp.ln2.linear.lora_b.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.1.mlp.ln2.bn.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.1.mlp.ln2.bn.bias, requires_grad: False\n",
            "Parameter name: stage1.blocks.2.attn.attention_biases, requires_grad: True\n",
            "Parameter name: stage1.blocks.2.attn.qkv.linear.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.2.attn.qkv.linear.lora_a.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.2.attn.qkv.linear.lora_b.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.2.attn.qkv.bn.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.2.attn.qkv.bn.bias, requires_grad: False\n",
            "Parameter name: stage1.blocks.2.attn.proj.1.linear.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.2.attn.proj.1.linear.lora_a.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.2.attn.proj.1.linear.lora_b.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.2.attn.proj.1.bn.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.2.attn.proj.1.bn.bias, requires_grad: False\n",
            "Parameter name: stage1.blocks.2.mlp.ln1.linear.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.2.mlp.ln1.linear.lora_a.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.2.mlp.ln1.linear.lora_b.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.2.mlp.ln1.bn.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.2.mlp.ln1.bn.bias, requires_grad: False\n",
            "Parameter name: stage1.blocks.2.mlp.ln2.linear.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.2.mlp.ln2.linear.lora_a.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.2.mlp.ln2.linear.lora_b.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.2.mlp.ln2.bn.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.2.mlp.ln2.bn.bias, requires_grad: False\n",
            "Parameter name: stage1.blocks.3.attn.attention_biases, requires_grad: True\n",
            "Parameter name: stage1.blocks.3.attn.qkv.linear.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.3.attn.qkv.linear.lora_a.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.3.attn.qkv.linear.lora_b.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.3.attn.qkv.bn.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.3.attn.qkv.bn.bias, requires_grad: False\n",
            "Parameter name: stage1.blocks.3.attn.proj.1.linear.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.3.attn.proj.1.linear.lora_a.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.3.attn.proj.1.linear.lora_b.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.3.attn.proj.1.bn.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.3.attn.proj.1.bn.bias, requires_grad: False\n",
            "Parameter name: stage1.blocks.3.mlp.ln1.linear.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.3.mlp.ln1.linear.lora_a.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.3.mlp.ln1.linear.lora_b.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.3.mlp.ln1.bn.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.3.mlp.ln1.bn.bias, requires_grad: False\n",
            "Parameter name: stage1.blocks.3.mlp.ln2.linear.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.3.mlp.ln2.linear.lora_a.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.3.mlp.ln2.linear.lora_b.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.3.mlp.ln2.bn.weight, requires_grad: False\n",
            "Parameter name: stage1.blocks.3.mlp.ln2.bn.bias, requires_grad: False\n",
            "Parameter name: stage2.gumbel_gate, requires_grad: True\n",
            "Parameter name: stage2.downsample.conv.weight, requires_grad: False\n",
            "Parameter name: stage2.downsample.conv.bias, requires_grad: False\n",
            "Parameter name: stage2.blocks.0.attn.attention_biases, requires_grad: True\n",
            "Parameter name: stage2.blocks.0.attn.qkv.linear.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.0.attn.qkv.linear.lora_a.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.0.attn.qkv.linear.lora_b.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.0.attn.qkv.bn.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.0.attn.qkv.bn.bias, requires_grad: False\n",
            "Parameter name: stage2.blocks.0.attn.proj.1.linear.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.0.attn.proj.1.linear.lora_a.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.0.attn.proj.1.linear.lora_b.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.0.attn.proj.1.bn.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.0.attn.proj.1.bn.bias, requires_grad: False\n",
            "Parameter name: stage2.blocks.0.mlp.ln1.linear.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.0.mlp.ln1.linear.lora_a.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.0.mlp.ln1.linear.lora_b.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.0.mlp.ln1.bn.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.0.mlp.ln1.bn.bias, requires_grad: False\n",
            "Parameter name: stage2.blocks.0.mlp.ln2.linear.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.0.mlp.ln2.linear.lora_a.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.0.mlp.ln2.linear.lora_b.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.0.mlp.ln2.bn.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.0.mlp.ln2.bn.bias, requires_grad: False\n",
            "Parameter name: stage2.blocks.1.attn.attention_biases, requires_grad: True\n",
            "Parameter name: stage2.blocks.1.attn.qkv.linear.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.1.attn.qkv.linear.lora_a.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.1.attn.qkv.linear.lora_b.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.1.attn.qkv.bn.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.1.attn.qkv.bn.bias, requires_grad: False\n",
            "Parameter name: stage2.blocks.1.attn.proj.1.linear.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.1.attn.proj.1.linear.lora_a.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.1.attn.proj.1.linear.lora_b.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.1.attn.proj.1.bn.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.1.attn.proj.1.bn.bias, requires_grad: False\n",
            "Parameter name: stage2.blocks.1.mlp.ln1.linear.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.1.mlp.ln1.linear.lora_a.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.1.mlp.ln1.linear.lora_b.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.1.mlp.ln1.bn.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.1.mlp.ln1.bn.bias, requires_grad: False\n",
            "Parameter name: stage2.blocks.1.mlp.ln2.linear.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.1.mlp.ln2.linear.lora_a.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.1.mlp.ln2.linear.lora_b.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.1.mlp.ln2.bn.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.1.mlp.ln2.bn.bias, requires_grad: False\n",
            "Parameter name: stage2.blocks.2.attn.attention_biases, requires_grad: True\n",
            "Parameter name: stage2.blocks.2.attn.qkv.linear.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.2.attn.qkv.linear.lora_a.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.2.attn.qkv.linear.lora_b.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.2.attn.qkv.bn.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.2.attn.qkv.bn.bias, requires_grad: False\n",
            "Parameter name: stage2.blocks.2.attn.proj.1.linear.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.2.attn.proj.1.linear.lora_a.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.2.attn.proj.1.linear.lora_b.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.2.attn.proj.1.bn.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.2.attn.proj.1.bn.bias, requires_grad: False\n",
            "Parameter name: stage2.blocks.2.mlp.ln1.linear.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.2.mlp.ln1.linear.lora_a.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.2.mlp.ln1.linear.lora_b.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.2.mlp.ln1.bn.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.2.mlp.ln1.bn.bias, requires_grad: False\n",
            "Parameter name: stage2.blocks.2.mlp.ln2.linear.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.2.mlp.ln2.linear.lora_a.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.2.mlp.ln2.linear.lora_b.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.2.mlp.ln2.bn.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.2.mlp.ln2.bn.bias, requires_grad: False\n",
            "Parameter name: stage2.blocks.3.attn.attention_biases, requires_grad: True\n",
            "Parameter name: stage2.blocks.3.attn.qkv.linear.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.3.attn.qkv.linear.lora_a.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.3.attn.qkv.linear.lora_b.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.3.attn.qkv.bn.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.3.attn.qkv.bn.bias, requires_grad: False\n",
            "Parameter name: stage2.blocks.3.attn.proj.1.linear.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.3.attn.proj.1.linear.lora_a.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.3.attn.proj.1.linear.lora_b.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.3.attn.proj.1.bn.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.3.attn.proj.1.bn.bias, requires_grad: False\n",
            "Parameter name: stage2.blocks.3.mlp.ln1.linear.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.3.mlp.ln1.linear.lora_a.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.3.mlp.ln1.linear.lora_b.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.3.mlp.ln1.bn.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.3.mlp.ln1.bn.bias, requires_grad: False\n",
            "Parameter name: stage2.blocks.3.mlp.ln2.linear.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.3.mlp.ln2.linear.lora_a.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.3.mlp.ln2.linear.lora_b.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.3.mlp.ln2.bn.weight, requires_grad: False\n",
            "Parameter name: stage2.blocks.3.mlp.ln2.bn.bias, requires_grad: False\n",
            "Parameter name: conv1x1.0.weight, requires_grad: True\n",
            "Parameter name: conv1x1.0.bias, requires_grad: True\n",
            "Parameter name: conv1x1.1.weight, requires_grad: True\n",
            "Parameter name: conv1x1.1.bias, requires_grad: True\n",
            "Parameter name: head.bn.weight, requires_grad: True\n",
            "Parameter name: head.bn.bias, requires_grad: True\n",
            "Parameter name: head.linear.weight, requires_grad: True\n",
            "Parameter name: head.linear.bias, requires_grad: True\n",
            "Parameter name: head_dist.bn.weight, requires_grad: True\n",
            "Parameter name: head_dist.bn.bias, requires_grad: True\n",
            "Parameter name: head_dist.linear.weight, requires_grad: True\n",
            "Parameter name: head_dist.linear.bias, requires_grad: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "    train(model, train_loader, criterion, optimizer, device, epoch)\n",
        "    evaluate(model, val_loader, criterion, device, phase=\"Validation\", epoch=epoch)\n",
        "    if((epoch+1) % 10 == 0 and epoch > 30):\n",
        "        save_path = f\"HoViT_44_tinyfusion_base_r8a16_{epoch}.pth\"\n",
        "        torch.save(model.state_dict(), save_path)\n",
        "        print(f\"Model weights saved to {save_path}\")\n",
        "        print(f\"{epoch+1} model test result:\")\n",
        "        evaluate(model, test_loader, criterion, device, phase=\"Test\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4vwrhWT-J7V",
        "outputId": "85d248fe-9f66-4039-d0ab-c12568f83c18"
      },
      "execution_count": 476,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/45\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 2188/2188 [03:14<00:00, 11.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.3165, Train Accuracy: 90.60%\n",
            "Tau: 0.9910\n",
            "Each stage of block probabilities:\n",
            "stage1\n",
            "Block 0: 0.7945  \n",
            "Block 1: 0.1068  \n",
            "Block 2: 0.0585 *\n",
            "Block 3: 0.0402 *\n",
            "stage2\n",
            "Block 0: 0.2018 *\n",
            "Block 1: 0.1850 *\n",
            "Block 2: 0.3359  \n",
            "Block 3: 0.2773  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 469/469 [00:22<00:00, 21.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.2649, Validation Accuracy: 91.19%\n",
            "Balanced Accuracy: 0.9112\n",
            "Tau: 0.9910\n",
            "\n",
            "Epoch 2/45\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 2188/2188 [03:13<00:00, 11.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1288, Train Accuracy: 95.69%\n",
            "Tau: 0.9820\n",
            "Each stage of block probabilities:\n",
            "stage1\n",
            "Block 0: 0.9553  \n",
            "Block 1: 0.0249  \n",
            "Block 2: 0.0106 *\n",
            "Block 3: 0.0092 *\n",
            "stage2\n",
            "Block 0: 0.2102 *\n",
            "Block 1: 0.1858 *\n",
            "Block 2: 0.3472  \n",
            "Block 3: 0.2567  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 469/469 [00:22<00:00, 21.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.1227, Validation Accuracy: 96.00%\n",
            "Balanced Accuracy: 0.9593\n",
            "Tau: 0.9820\n",
            "\n",
            "Epoch 3/45\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 2188/2188 [03:14<00:00, 11.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0896, Train Accuracy: 97.04%\n",
            "Tau: 0.9730\n",
            "Each stage of block probabilities:\n",
            "stage1\n",
            "Block 0: 0.9868  \n",
            "Block 1: 0.0065  \n",
            "Block 2: 0.0041 *\n",
            "Block 3: 0.0027 *\n",
            "stage2\n",
            "Block 0: 0.1985 *\n",
            "Block 1: 0.1774 *\n",
            "Block 2: 0.3472  \n",
            "Block 3: 0.2769  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 469/469 [00:22<00:00, 20.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.0878, Validation Accuracy: 97.11%\n",
            "Balanced Accuracy: 0.9699\n",
            "Tau: 0.9730\n",
            "\n",
            "Epoch 4/45\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 2188/2188 [03:14<00:00, 11.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0691, Train Accuracy: 97.72%\n",
            "Tau: 0.9640\n",
            "Each stage of block probabilities:\n",
            "stage1\n",
            "Block 0: 0.9960  \n",
            "Block 1: 0.0021  \n",
            "Block 2: 0.0012 *\n",
            "Block 3: 0.0007 *\n",
            "stage2\n",
            "Block 0: 0.2142 *\n",
            "Block 1: 0.1516 *\n",
            "Block 2: 0.3972  \n",
            "Block 3: 0.2370  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 469/469 [00:22<00:00, 20.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.0714, Validation Accuracy: 97.72%\n",
            "Balanced Accuracy: 0.9765\n",
            "Tau: 0.9640\n",
            "\n",
            "Epoch 5/45\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 2188/2188 [03:14<00:00, 11.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0555, Train Accuracy: 98.16%\n",
            "Tau: 0.9550\n",
            "Each stage of block probabilities:\n",
            "stage1\n",
            "Block 0: 0.9978  \n",
            "Block 1: 0.0010  \n",
            "Block 2: 0.0007 *\n",
            "Block 3: 0.0005 *\n",
            "stage2\n",
            "Block 0: 0.2263  \n",
            "Block 1: 0.1148 *\n",
            "Block 2: 0.4745  \n",
            "Block 3: 0.1845 *\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 469/469 [00:22<00:00, 21.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.0783, Validation Accuracy: 97.61%\n",
            "Balanced Accuracy: 0.9757\n",
            "Tau: 0.9550\n",
            "\n",
            "Epoch 6/45\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 2188/2188 [03:13<00:00, 11.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0444, Train Accuracy: 98.55%\n",
            "Tau: 0.9460\n",
            "Each stage of block probabilities:\n",
            "stage1\n",
            "Block 0: 0.9986  \n",
            "Block 1: 0.0007  \n",
            "Block 2: 0.0004 *\n",
            "Block 3: 0.0003 *\n",
            "stage2\n",
            "Block 0: 0.2270  \n",
            "Block 1: 0.1165 *\n",
            "Block 2: 0.4606  \n",
            "Block 3: 0.1959 *\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 469/469 [00:22<00:00, 20.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.0749, Validation Accuracy: 97.83%\n",
            "Balanced Accuracy: 0.9779\n",
            "Tau: 0.9460\n",
            "\n",
            "Epoch 7/45\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 2188/2188 [03:13<00:00, 11.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0361, Train Accuracy: 98.86%\n",
            "Tau: 0.9370\n",
            "Each stage of block probabilities:\n",
            "stage1\n",
            "Block 0: 0.9990  \n",
            "Block 1: 0.0005  \n",
            "Block 2: 0.0003 *\n",
            "Block 3: 0.0002 *\n",
            "stage2\n",
            "Block 0: 0.2107  \n",
            "Block 1: 0.0909 *\n",
            "Block 2: 0.5294  \n",
            "Block 3: 0.1690 *\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 469/469 [00:22<00:00, 20.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.0902, Validation Accuracy: 97.75%\n",
            "Balanced Accuracy: 0.9773\n",
            "Tau: 0.9370\n",
            "\n",
            "Epoch 8/45\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 2188/2188 [03:13<00:00, 11.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0280, Train Accuracy: 99.12%\n",
            "Tau: 0.9280\n",
            "Each stage of block probabilities:\n",
            "stage1\n",
            "Block 0: 0.9995  \n",
            "Block 1: 0.0002  \n",
            "Block 2: 0.0002 *\n",
            "Block 3: 0.0001 *\n",
            "stage2\n",
            "Block 0: 0.1989  \n",
            "Block 1: 0.0736 *\n",
            "Block 2: 0.6063  \n",
            "Block 3: 0.1212 *\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 469/469 [00:22<00:00, 21.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.0594, Validation Accuracy: 98.06%\n",
            "Balanced Accuracy: 0.9803\n",
            "Tau: 0.9280\n",
            "\n",
            "Epoch 9/45\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 2188/2188 [03:14<00:00, 11.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0249, Train Accuracy: 99.21%\n",
            "Tau: 0.9190\n",
            "Each stage of block probabilities:\n",
            "stage1\n",
            "Block 0: 0.9997  \n",
            "Block 1: 0.0001 *\n",
            "Block 2: 0.0001  \n",
            "Block 3: 0.0001 *\n",
            "stage2\n",
            "Block 0: 0.2073  \n",
            "Block 1: 0.0690 *\n",
            "Block 2: 0.6244  \n",
            "Block 3: 0.0993 *\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 469/469 [00:22<00:00, 21.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.0782, Validation Accuracy: 97.75%\n",
            "Balanced Accuracy: 0.9774\n",
            "Tau: 0.9190\n",
            "\n",
            "Epoch 10/45\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 2188/2188 [03:14<00:00, 11.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0195, Train Accuracy: 99.42%\n",
            "Tau: 0.9100\n",
            "Each stage of block probabilities:\n",
            "stage1\n",
            "Block 0: 0.9997  \n",
            "Block 1: 0.0001 *\n",
            "Block 2: 0.0001  \n",
            "Block 3: 0.0001 *\n",
            "stage2\n",
            "Block 0: 0.2004  \n",
            "Block 1: 0.0663 *\n",
            "Block 2: 0.6575  \n",
            "Block 3: 0.0759 *\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 469/469 [00:22<00:00, 20.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.0696, Validation Accuracy: 98.09%\n",
            "Balanced Accuracy: 0.9807\n",
            "Tau: 0.9100\n",
            "\n",
            "Epoch 11/45\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 2188/2188 [03:14<00:00, 11.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0162, Train Accuracy: 99.49%\n",
            "Tau: 0.9010\n",
            "Each stage of block probabilities:\n",
            "stage1\n",
            "Block 0: 0.9998  \n",
            "Block 1: 0.0001 *\n",
            "Block 2: 0.0001  \n",
            "Block 3: 0.0000 *\n",
            "stage2\n",
            "Block 0: 0.1872  \n",
            "Block 1: 0.0478 *\n",
            "Block 2: 0.6906  \n",
            "Block 3: 0.0744 *\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 469/469 [00:22<00:00, 20.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.0689, Validation Accuracy: 97.99%\n",
            "Balanced Accuracy: 0.9797\n",
            "Tau: 0.9010\n",
            "\n",
            "Epoch 12/45\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 2188/2188 [03:13<00:00, 11.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0136, Train Accuracy: 99.62%\n",
            "Tau: 0.8920\n",
            "Each stage of block probabilities:\n",
            "stage1\n",
            "Block 0: 0.9998  \n",
            "Block 1: 0.0001  \n",
            "Block 2: 0.0001 *\n",
            "Block 3: 0.0000 *\n",
            "stage2\n",
            "Block 0: 0.1433  \n",
            "Block 1: 0.0405 *\n",
            "Block 2: 0.7682  \n",
            "Block 3: 0.0481 *\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 469/469 [00:22<00:00, 20.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.0582, Validation Accuracy: 98.26%\n",
            "Balanced Accuracy: 0.9821\n",
            "Tau: 0.8920\n",
            "\n",
            "Epoch 13/45\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 2188/2188 [03:14<00:00, 11.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0121, Train Accuracy: 99.63%\n",
            "Tau: 0.8830\n",
            "Each stage of block probabilities:\n",
            "stage1\n",
            "Block 0: 0.9998  \n",
            "Block 1: 0.0001 *\n",
            "Block 2: 0.0001  \n",
            "Block 3: 0.0000 *\n",
            "stage2\n",
            "Block 0: 0.1858  \n",
            "Block 1: 0.0405 *\n",
            "Block 2: 0.7284  \n",
            "Block 3: 0.0452 *\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 469/469 [00:22<00:00, 20.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.0593, Validation Accuracy: 98.12%\n",
            "Balanced Accuracy: 0.9814\n",
            "Tau: 0.8830\n",
            "\n",
            "Epoch 14/45\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 2188/2188 [03:15<00:00, 11.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0113, Train Accuracy: 99.63%\n",
            "Tau: 0.8740\n",
            "Each stage of block probabilities:\n",
            "stage1\n",
            "Block 0: 0.9999  \n",
            "Block 1: 0.0001  \n",
            "Block 2: 0.0000 *\n",
            "Block 3: 0.0000 *\n",
            "stage2\n",
            "Block 0: 0.1704  \n",
            "Block 1: 0.0282 *\n",
            "Block 2: 0.7757  \n",
            "Block 3: 0.0257 *\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 469/469 [00:22<00:00, 21.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.0664, Validation Accuracy: 98.41%\n",
            "Balanced Accuracy: 0.9839\n",
            "Tau: 0.8740\n",
            "\n",
            "Epoch 15/45\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 2188/2188 [03:13<00:00, 11.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0086, Train Accuracy: 99.75%\n",
            "Tau: 0.8650\n",
            "Each stage of block probabilities:\n",
            "stage1\n",
            "Block 0: 0.9999  \n",
            "Block 1: 0.0001  \n",
            "Block 2: 0.0000 *\n",
            "Block 3: 0.0000 *\n",
            "stage2\n",
            "Block 0: 0.1475  \n",
            "Block 1: 0.0257 *\n",
            "Block 2: 0.8011  \n",
            "Block 3: 0.0256 *\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 469/469 [00:22<00:00, 21.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.0633, Validation Accuracy: 98.36%\n",
            "Balanced Accuracy: 0.9835\n",
            "Tau: 0.8650\n",
            "\n",
            "Epoch 16/45\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 2188/2188 [03:13<00:00, 11.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0090, Train Accuracy: 99.72%\n",
            "Tau: 0.8560\n",
            "Each stage of block probabilities:\n",
            "stage1\n",
            "Block 0: 0.9999  \n",
            "Block 1: 0.0001  \n",
            "Block 2: 0.0000 *\n",
            "Block 3: 0.0000 *\n",
            "stage2\n",
            "Block 0: 0.2502  \n",
            "Block 1: 0.0307 *\n",
            "Block 2: 0.6963  \n",
            "Block 3: 0.0228 *\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 469/469 [00:22<00:00, 21.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.0609, Validation Accuracy: 98.35%\n",
            "Balanced Accuracy: 0.9834\n",
            "Tau: 0.8560\n",
            "\n",
            "Epoch 17/45\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 2188/2188 [03:13<00:00, 11.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0082, Train Accuracy: 99.74%\n",
            "Tau: 0.8470\n",
            "Each stage of block probabilities:\n",
            "stage1\n",
            "Block 0: 0.9999  \n",
            "Block 1: 0.0001  \n",
            "Block 2: 0.0000 *\n",
            "Block 3: 0.0000 *\n",
            "stage2\n",
            "Block 0: 0.1951  \n",
            "Block 1: 0.0189 *\n",
            "Block 2: 0.7740  \n",
            "Block 3: 0.0120 *\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 469/469 [00:22<00:00, 21.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.0567, Validation Accuracy: 98.49%\n",
            "Balanced Accuracy: 0.9852\n",
            "Tau: 0.8470\n",
            "\n",
            "Epoch 18/45\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 2188/2188 [03:13<00:00, 11.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0089, Train Accuracy: 99.70%\n",
            "Tau: 0.8380\n",
            "Each stage of block probabilities:\n",
            "stage1\n",
            "Block 0: 0.9999  \n",
            "Block 1: 0.0001  \n",
            "Block 2: 0.0000 *\n",
            "Block 3: 0.0000 *\n",
            "stage2\n",
            "Block 0: 0.1740  \n",
            "Block 1: 0.0177 *\n",
            "Block 2: 0.7970  \n",
            "Block 3: 0.0113 *\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 469/469 [00:22<00:00, 20.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.0548, Validation Accuracy: 98.60%\n",
            "Balanced Accuracy: 0.9864\n",
            "Tau: 0.8380\n",
            "\n",
            "Epoch 19/45\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 2188/2188 [03:13<00:00, 11.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0071, Train Accuracy: 99.77%\n",
            "Tau: 0.8290\n",
            "Each stage of block probabilities:\n",
            "stage1\n",
            "Block 0: 0.9999  \n",
            "Block 1: 0.0000  \n",
            "Block 2: 0.0000 *\n",
            "Block 3: 0.0000 *\n",
            "stage2\n",
            "Block 0: 0.1151  \n",
            "Block 1: 0.0152 *\n",
            "Block 2: 0.8605  \n",
            "Block 3: 0.0091 *\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 469/469 [00:22<00:00, 20.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.0716, Validation Accuracy: 98.12%\n",
            "Balanced Accuracy: 0.9812\n",
            "Tau: 0.8290\n",
            "\n",
            "Epoch 20/45\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 2188/2188 [03:13<00:00, 11.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0063, Train Accuracy: 99.81%\n",
            "Tau: 0.8200\n",
            "Each stage of block probabilities:\n",
            "stage1\n",
            "Block 0: 0.9999  \n",
            "Block 1: 0.0000  \n",
            "Block 2: 0.0000 *\n",
            "Block 3: 0.0000 *\n",
            "stage2\n",
            "Block 0: 0.1698  \n",
            "Block 1: 0.0225 *\n",
            "Block 2: 0.7983  \n",
            "Block 3: 0.0094 *\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 469/469 [00:22<00:00, 20.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.0557, Validation Accuracy: 98.62%\n",
            "Balanced Accuracy: 0.9864\n",
            "Tau: 0.8200\n",
            "\n",
            "Epoch 21/45\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 2188/2188 [03:13<00:00, 11.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0066, Train Accuracy: 99.79%\n",
            "Tau: 0.8110\n",
            "Each stage of block probabilities:\n",
            "stage1\n",
            "Block 0: 0.9999  \n",
            "Block 1: 0.0000  \n",
            "Block 2: 0.0000 *\n",
            "Block 3: 0.0000 *\n",
            "stage2\n",
            "Block 0: 0.1241  \n",
            "Block 1: 0.0132 *\n",
            "Block 2: 0.8563  \n",
            "Block 3: 0.0063 *\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 469/469 [00:22<00:00, 20.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.0543, Validation Accuracy: 98.53%\n",
            "Balanced Accuracy: 0.9851\n",
            "Tau: 0.8110\n",
            "\n",
            "Epoch 22/45\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 2188/2188 [03:13<00:00, 11.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0052, Train Accuracy: 99.84%\n",
            "Tau: 0.8020\n",
            "Each stage of block probabilities:\n",
            "stage1\n",
            "Block 0: 0.9999  \n",
            "Block 1: 0.0000  \n",
            "Block 2: 0.0000 *\n",
            "Block 3: 0.0000 *\n",
            "stage2\n",
            "Block 0: 0.1050  \n",
            "Block 1: 0.0104 *\n",
            "Block 2: 0.8789  \n",
            "Block 3: 0.0057 *\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 469/469 [00:22<00:00, 20.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.0584, Validation Accuracy: 98.49%\n",
            "Balanced Accuracy: 0.9849\n",
            "Tau: 0.8020\n",
            "\n",
            "Epoch 23/45\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 2188/2188 [03:13<00:00, 11.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0063, Train Accuracy: 99.80%\n",
            "Tau: 0.7930\n",
            "Each stage of block probabilities:\n",
            "stage1\n",
            "Block 0: 0.9999  \n",
            "Block 1: 0.0000  \n",
            "Block 2: 0.0000 *\n",
            "Block 3: 0.0000 *\n",
            "stage2\n",
            "Block 0: 0.1324  \n",
            "Block 1: 0.0120 *\n",
            "Block 2: 0.8512  \n",
            "Block 3: 0.0044 *\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 469/469 [00:22<00:00, 20.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.0570, Validation Accuracy: 98.58%\n",
            "Balanced Accuracy: 0.9861\n",
            "Tau: 0.7930\n",
            "\n",
            "Epoch 24/45\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 2188/2188 [03:13<00:00, 11.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0049, Train Accuracy: 99.84%\n",
            "Tau: 0.7840\n",
            "Each stage of block probabilities:\n",
            "stage1\n",
            "Block 0: 0.9999  \n",
            "Block 1: 0.0000  \n",
            "Block 2: 0.0000 *\n",
            "Block 3: 0.0000 *\n",
            "stage2\n",
            "Block 0: 0.1096  \n",
            "Block 1: 0.0113 *\n",
            "Block 2: 0.8752  \n",
            "Block 3: 0.0038 *\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 469/469 [00:22<00:00, 21.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.0555, Validation Accuracy: 98.67%\n",
            "Balanced Accuracy: 0.9866\n",
            "Tau: 0.7840\n",
            "\n",
            "Epoch 25/45\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 2188/2188 [03:13<00:00, 11.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0056, Train Accuracy: 99.83%\n",
            "Tau: 0.7750\n",
            "Each stage of block probabilities:\n",
            "stage1\n",
            "Block 0: 0.9999  \n",
            "Block 1: 0.0001  \n",
            "Block 2: 0.0000 *\n",
            "Block 3: 0.0000 *\n",
            "stage2\n",
            "Block 0: 0.1536  \n",
            "Block 1: 0.0130 *\n",
            "Block 2: 0.8295  \n",
            "Block 3: 0.0039 *\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 469/469 [00:22<00:00, 21.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.0694, Validation Accuracy: 98.31%\n",
            "Balanced Accuracy: 0.9836\n",
            "Tau: 0.7750\n",
            "\n",
            "Epoch 26/45\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 2188/2188 [03:14<00:00, 11.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0041, Train Accuracy: 99.88%\n",
            "Tau: 0.7660\n",
            "Each stage of block probabilities:\n",
            "stage1\n",
            "Block 0: 0.9999  \n",
            "Block 1: 0.0000  \n",
            "Block 2: 0.0000 *\n",
            "Block 3: 0.0000 *\n",
            "stage2\n",
            "Block 0: 0.0964  \n",
            "Block 1: 0.0077 *\n",
            "Block 2: 0.8937  \n",
            "Block 3: 0.0023 *\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 469/469 [00:22<00:00, 20.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.0557, Validation Accuracy: 98.75%\n",
            "Balanced Accuracy: 0.9876\n",
            "Tau: 0.7660\n",
            "\n",
            "Epoch 27/45\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 2188/2188 [03:13<00:00, 11.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0049, Train Accuracy: 99.84%\n",
            "Tau: 0.7570\n",
            "Each stage of block probabilities:\n",
            "stage1\n",
            "Block 0: 0.9999  \n",
            "Block 1: 0.0000  \n",
            "Block 2: 0.0000 *\n",
            "Block 3: 0.0000 *\n",
            "stage2\n",
            "Block 0: 0.1123  \n",
            "Block 1: 0.0069 *\n",
            "Block 2: 0.8785  \n",
            "Block 3: 0.0022 *\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 469/469 [00:22<00:00, 21.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.0610, Validation Accuracy: 98.49%\n",
            "Balanced Accuracy: 0.9851\n",
            "Tau: 0.7570\n",
            "\n",
            "Epoch 28/45\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 2188/2188 [03:13<00:00, 11.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0053, Train Accuracy: 99.85%\n",
            "Tau: 0.7480\n",
            "Each stage of block probabilities:\n",
            "stage1\n",
            "Block 0: 0.9999  \n",
            "Block 1: 0.0000  \n",
            "Block 2: 0.0000 *\n",
            "Block 3: 0.0000 *\n",
            "stage2\n",
            "Block 0: 0.1343  \n",
            "Block 1: 0.0088 *\n",
            "Block 2: 0.8542  \n",
            "Block 3: 0.0027 *\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 469/469 [00:22<00:00, 20.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.0563, Validation Accuracy: 98.62%\n",
            "Balanced Accuracy: 0.9857\n",
            "Tau: 0.7480\n",
            "\n",
            "Epoch 29/45\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 2188/2188 [03:13<00:00, 11.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0049, Train Accuracy: 99.84%\n",
            "Tau: 0.7390\n",
            "Each stage of block probabilities:\n",
            "stage1\n",
            "Block 0: 0.9999  \n",
            "Block 1: 0.0000  \n",
            "Block 2: 0.0000 *\n",
            "Block 3: 0.0000 *\n",
            "stage2\n",
            "Block 0: 0.1216  \n",
            "Block 1: 0.0049 *\n",
            "Block 2: 0.8716  \n",
            "Block 3: 0.0019 *\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 469/469 [00:22<00:00, 20.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.0608, Validation Accuracy: 98.72%\n",
            "Balanced Accuracy: 0.9868\n",
            "Tau: 0.7390\n",
            "\n",
            "Epoch 30/45\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 2188/2188 [03:13<00:00, 11.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0032, Train Accuracy: 99.91%\n",
            "Tau: 0.7300\n",
            "Each stage of block probabilities:\n",
            "stage1\n",
            "Block 0: 0.9999  \n",
            "Block 1: 0.0000  \n",
            "Block 2: 0.0000 *\n",
            "Block 3: 0.0000 *\n",
            "stage2\n",
            "Block 0: 0.0878  \n",
            "Block 1: 0.0045 *\n",
            "Block 2: 0.9064  \n",
            "Block 3: 0.0013 *\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 469/469 [00:22<00:00, 21.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.0606, Validation Accuracy: 98.58%\n",
            "Balanced Accuracy: 0.9856\n",
            "Tau: 0.7300\n",
            "\n",
            "Epoch 31/45\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 2188/2188 [03:14<00:00, 11.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0046, Train Accuracy: 99.85%\n",
            "Tau: 0.7210\n",
            "Each stage of block probabilities:\n",
            "stage1\n",
            "Block 0: 0.9999  \n",
            "Block 1: 0.0000  \n",
            "Block 2: 0.0000 *\n",
            "Block 3: 0.0000 *\n",
            "stage2\n",
            "Block 0: 0.0702  \n",
            "Block 1: 0.0039 *\n",
            "Block 2: 0.9247  \n",
            "Block 3: 0.0012 *\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 469/469 [00:22<00:00, 21.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.0604, Validation Accuracy: 98.70%\n",
            "Balanced Accuracy: 0.9871\n",
            "Tau: 0.7210\n",
            "\n",
            "Epoch 32/45\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 2188/2188 [03:13<00:00, 11.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0038, Train Accuracy: 99.89%\n",
            "Tau: 0.7120\n",
            "Each stage of block probabilities:\n",
            "stage1\n",
            "Block 0: 0.9999  \n",
            "Block 1: 0.0000  \n",
            "Block 2: 0.0000 *\n",
            "Block 3: 0.0000 *\n",
            "stage2\n",
            "Block 0: 0.0589  \n",
            "Block 1: 0.0033 *\n",
            "Block 2: 0.9367  \n",
            "Block 3: 0.0011 *\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 469/469 [00:22<00:00, 20.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.0569, Validation Accuracy: 98.71%\n",
            "Balanced Accuracy: 0.9869\n",
            "Tau: 0.7120\n",
            "\n",
            "Epoch 33/45\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 2188/2188 [03:14<00:00, 11.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0042, Train Accuracy: 99.88%\n",
            "Tau: 0.7030\n",
            "Each stage of block probabilities:\n",
            "stage1\n",
            "Block 0: 0.9999  \n",
            "Block 1: 0.0000  \n",
            "Block 2: 0.0000 *\n",
            "Block 3: 0.0000 *\n",
            "stage2\n",
            "Block 0: 0.1081  \n",
            "Block 1: 0.0043 *\n",
            "Block 2: 0.8861  \n",
            "Block 3: 0.0014 *\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 469/469 [00:22<00:00, 20.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.0550, Validation Accuracy: 98.81%\n",
            "Balanced Accuracy: 0.9877\n",
            "Tau: 0.7030\n",
            "\n",
            "Epoch 34/45\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 2188/2188 [03:13<00:00, 11.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0044, Train Accuracy: 99.87%\n",
            "Tau: 0.6940\n",
            "Each stage of block probabilities:\n",
            "stage1\n",
            "Block 0: 0.9999  \n",
            "Block 1: 0.0000  \n",
            "Block 2: 0.0000 *\n",
            "Block 3: 0.0000 *\n",
            "stage2\n",
            "Block 0: 0.0683  \n",
            "Block 1: 0.0038 *\n",
            "Block 2: 0.9264  \n",
            "Block 3: 0.0014 *\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 469/469 [00:22<00:00, 21.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.0548, Validation Accuracy: 98.85%\n",
            "Balanced Accuracy: 0.9883\n",
            "Tau: 0.6940\n",
            "\n",
            "Epoch 35/45\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 2188/2188 [03:13<00:00, 11.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0028, Train Accuracy: 99.92%\n",
            "Tau: 0.6850\n",
            "Each stage of block probabilities:\n",
            "stage1\n",
            "Block 0: 0.9999  \n",
            "Block 1: 0.0000  \n",
            "Block 2: 0.0000 *\n",
            "Block 3: 0.0000 *\n",
            "stage2\n",
            "Block 0: 0.0487  \n",
            "Block 1: 0.0025 *\n",
            "Block 2: 0.9477  \n",
            "Block 3: 0.0011 *\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 469/469 [00:22<00:00, 21.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.0594, Validation Accuracy: 98.80%\n",
            "Balanced Accuracy: 0.9880\n",
            "Tau: 0.6850\n",
            "\n",
            "Epoch 36/45\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 2188/2188 [03:13<00:00, 11.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0041, Train Accuracy: 99.87%\n",
            "Tau: 0.6760\n",
            "Each stage of block probabilities:\n",
            "stage1\n",
            "Block 0: 0.9999  \n",
            "Block 1: 0.0000  \n",
            "Block 2: 0.0000 *\n",
            "Block 3: 0.0000 *\n",
            "stage2\n",
            "Block 0: 0.0791  \n",
            "Block 1: 0.0033 *\n",
            "Block 2: 0.9165  \n",
            "Block 3: 0.0010 *\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 469/469 [00:22<00:00, 20.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.0575, Validation Accuracy: 98.83%\n",
            "Balanced Accuracy: 0.9883\n",
            "Tau: 0.6760\n",
            "\n",
            "Epoch 37/45\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 2188/2188 [03:14<00:00, 11.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0027, Train Accuracy: 99.91%\n",
            "Tau: 0.6670\n",
            "Each stage of block probabilities:\n",
            "stage1\n",
            "Block 0: 0.9999  \n",
            "Block 1: 0.0000  \n",
            "Block 2: 0.0000 *\n",
            "Block 3: 0.0000 *\n",
            "stage2\n",
            "Block 0: 0.0545  \n",
            "Block 1: 0.0021 *\n",
            "Block 2: 0.9425  \n",
            "Block 3: 0.0009 *\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 469/469 [00:22<00:00, 20.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.0668, Validation Accuracy: 98.76%\n",
            "Balanced Accuracy: 0.9876\n",
            "Tau: 0.6670\n",
            "\n",
            "Epoch 38/45\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 2188/2188 [03:14<00:00, 11.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0034, Train Accuracy: 99.90%\n",
            "Tau: 0.6580\n",
            "Each stage of block probabilities:\n",
            "stage1\n",
            "Block 0: 0.9999  \n",
            "Block 1: 0.0000  \n",
            "Block 2: 0.0000 *\n",
            "Block 3: 0.0000 *\n",
            "stage2\n",
            "Block 0: 0.0333  \n",
            "Block 1: 0.0015 *\n",
            "Block 2: 0.9645  \n",
            "Block 3: 0.0007 *\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 469/469 [00:22<00:00, 20.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.0695, Validation Accuracy: 98.54%\n",
            "Balanced Accuracy: 0.9862\n",
            "Tau: 0.6580\n",
            "\n",
            "Epoch 39/45\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 2188/2188 [03:14<00:00, 11.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0044, Train Accuracy: 99.86%\n",
            "Tau: 0.6490\n",
            "Each stage of block probabilities:\n",
            "stage1\n",
            "Block 0: 0.9999  \n",
            "Block 1: 0.0000  \n",
            "Block 2: 0.0000 *\n",
            "Block 3: 0.0000 *\n",
            "stage2\n",
            "Block 0: 0.0150  \n",
            "Block 1: 0.0009 *\n",
            "Block 2: 0.9837  \n",
            "Block 3: 0.0004 *\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 469/469 [00:22<00:00, 20.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.0624, Validation Accuracy: 98.80%\n",
            "Balanced Accuracy: 0.9880\n",
            "Tau: 0.6490\n",
            "\n",
            "Epoch 40/45\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 2188/2188 [03:13<00:00, 11.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0032, Train Accuracy: 99.90%\n",
            "Tau: 0.6400\n",
            "Each stage of block probabilities:\n",
            "stage1\n",
            "Block 0: 0.9999  \n",
            "Block 1: 0.0000  \n",
            "Block 2: 0.0000 *\n",
            "Block 3: 0.0000 *\n",
            "stage2\n",
            "Block 0: 0.0145  \n",
            "Block 1: 0.0008 *\n",
            "Block 2: 0.9843  \n",
            "Block 3: 0.0004 *\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 469/469 [00:22<00:00, 20.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.0664, Validation Accuracy: 98.77%\n",
            "Balanced Accuracy: 0.9878\n",
            "Tau: 0.6400\n",
            "Model weights saved to HoViT_44_tinyfusion_base_r8a16_39.pth\n",
            "40 model test result:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Test: 100%|██████████| 469/469 [00:22<00:00, 20.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.0732, Test Accuracy: 98.68%\n",
            "Balanced Accuracy: 0.9869\n",
            "Tau: 0.0000\n",
            "\n",
            "Epoch 41/45\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 2188/2188 [03:14<00:00, 11.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0023, Train Accuracy: 99.92%\n",
            "Tau: 0.6310\n",
            "Each stage of block probabilities:\n",
            "stage1\n",
            "Block 0: 0.9999  \n",
            "Block 1: 0.0000  \n",
            "Block 2: 0.0000 *\n",
            "Block 3: 0.0000 *\n",
            "stage2\n",
            "Block 0: 0.0096  \n",
            "Block 1: 0.0008 *\n",
            "Block 2: 0.9894  \n",
            "Block 3: 0.0002 *\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 469/469 [00:22<00:00, 20.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.0580, Validation Accuracy: 98.85%\n",
            "Balanced Accuracy: 0.9889\n",
            "Tau: 0.6310\n",
            "\n",
            "Epoch 42/45\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 2188/2188 [03:14<00:00, 11.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0037, Train Accuracy: 99.87%\n",
            "Tau: 0.6220\n",
            "Each stage of block probabilities:\n",
            "stage1\n",
            "Block 0: 0.9999  \n",
            "Block 1: 0.0000  \n",
            "Block 2: 0.0000 *\n",
            "Block 3: 0.0000 *\n",
            "stage2\n",
            "Block 0: 0.0097  \n",
            "Block 1: 0.0009 *\n",
            "Block 2: 0.9892  \n",
            "Block 3: 0.0003 *\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 469/469 [00:22<00:00, 20.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.0593, Validation Accuracy: 98.93%\n",
            "Balanced Accuracy: 0.9893\n",
            "Tau: 0.6220\n",
            "\n",
            "Epoch 43/45\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 2188/2188 [03:14<00:00, 11.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0035, Train Accuracy: 99.86%\n",
            "Tau: 0.6130\n",
            "Each stage of block probabilities:\n",
            "stage1\n",
            "Block 0: 0.9999  \n",
            "Block 1: 0.0000  \n",
            "Block 2: 0.0000 *\n",
            "Block 3: 0.0000 *\n",
            "stage2\n",
            "Block 0: 0.0332  \n",
            "Block 1: 0.0015 *\n",
            "Block 2: 0.9646  \n",
            "Block 3: 0.0007 *\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 469/469 [00:22<00:00, 20.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.0721, Validation Accuracy: 98.77%\n",
            "Balanced Accuracy: 0.9878\n",
            "Tau: 0.6130\n",
            "\n",
            "Epoch 44/45\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 2188/2188 [03:14<00:00, 11.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0029, Train Accuracy: 99.90%\n",
            "Tau: 0.6040\n",
            "Each stage of block probabilities:\n",
            "stage1\n",
            "Block 0: 0.9999  \n",
            "Block 1: 0.0000  \n",
            "Block 2: 0.0000 *\n",
            "Block 3: 0.0000 *\n",
            "stage2\n",
            "Block 0: 0.0369  \n",
            "Block 1: 0.0012 *\n",
            "Block 2: 0.9611  \n",
            "Block 3: 0.0008 *\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 469/469 [00:22<00:00, 20.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.0580, Validation Accuracy: 98.90%\n",
            "Balanced Accuracy: 0.9888\n",
            "Tau: 0.6040\n",
            "\n",
            "Epoch 45/45\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 2188/2188 [03:14<00:00, 11.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0025, Train Accuracy: 99.93%\n",
            "Tau: 0.5950\n",
            "Each stage of block probabilities:\n",
            "stage1\n",
            "Block 0: 0.9999  \n",
            "Block 1: 0.0000  \n",
            "Block 2: 0.0000 *\n",
            "Block 3: 0.0000 *\n",
            "stage2\n",
            "Block 0: 0.0214  \n",
            "Block 1: 0.0009 *\n",
            "Block 2: 0.9770  \n",
            "Block 3: 0.0006 *\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 469/469 [00:22<00:00, 20.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.0551, Validation Accuracy: 99.03%\n",
            "Balanced Accuracy: 0.9902\n",
            "Tau: 0.5950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 477,
      "metadata": {
        "id": "yq3Yxnau6V3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcf19e9f-9951-4ecc-e560-114cd158581b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Test Evaluation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Test: 100%|██████████| 469/469 [00:22<00:00, 21.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.0639, Test Accuracy: 98.83%\n",
            "Balanced Accuracy: 0.9884\n",
            "Tau: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nFinal Test Evaluation\")\n",
        "evaluate(model,test_loader, criterion, device, phase=\"Test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 478,
      "metadata": {
        "id": "dlazeL-a0WTt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24f1567a-6fbd-4f4a-f151-23c912406a8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference Time Measurement Results:\n",
            "Total Inferences: 469\n",
            "Average Time: 7.81 ms\n",
            "Standard Deviation: 0.43 ms\n",
            "Maximum Time: 13.19 ms\n",
            "Minimum Time: 7.36 ms\n"
          ]
        }
      ],
      "source": [
        "times = measure_inference_time(model, test_loader, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 479,
      "metadata": {
        "id": "c77rmtOz0XuA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc84f645-6aee-4325-de6c-313ae4d56902"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                 ampere_sgemm_32x128_tn         0.00%       0.000us         0.00%       0.000us       0.000us     409.565us        12.07%     409.565us      45.507us             9  \n",
            "                                ampere_sgemm_128x128_nn         0.00%       0.000us         0.00%       0.000us       0.000us     246.494us         7.27%     246.494us     123.247us             2  \n",
            "void cudnn::engines_precompiled::nchwToNhwcKernel<fl...         0.00%       0.000us         0.00%       0.000us       0.000us     205.503us         6.06%     205.503us      20.550us            10  \n",
            "void cudnn::bn_fw_inf_1C11_kernel_NCHW<float, float,...         0.00%       0.000us         0.00%       0.000us       0.000us     200.957us         5.92%     200.957us      40.191us             5  \n",
            "                                 ampere_sgemm_128x64_tn         0.00%       0.000us         0.00%       0.000us       0.000us     194.751us         5.74%     194.751us      97.375us             2  \n",
            "void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     176.288us         5.20%     176.288us      14.691us            12  \n",
            "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     156.671us         4.62%     156.671us      19.584us             8  \n",
            "                        ampere_sgemm_32x32_sliced1x4_tn         0.00%       0.000us         0.00%       0.000us       0.000us     152.894us         4.51%     152.894us      15.289us            10  \n",
            "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     147.777us         4.36%     147.777us       9.236us            16  \n",
            "                        ampere_sgemm_64x32_sliced1x4_tn         0.00%       0.000us         0.00%       0.000us       0.000us     145.343us         4.28%     145.343us      72.671us             2  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 12.294ms\n",
            "Self CUDA time total: 3.392ms\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from torch import profiler\n",
        "\n",
        "dummy_input = torch.randn(32, 3, 224, 224).cuda()\n",
        "\n",
        "# Profiling inference\n",
        "with profiler.profile(\n",
        "    activities=[\n",
        "       profiler.ProfilerActivity.CPU,\n",
        "        profiler.ProfilerActivity.CUDA,  # Include if using GPU\n",
        "    ],\n",
        "    on_trace_ready=profiler.tensorboard_trace_handler(\"./logs\"),  # Optional logging\n",
        "    record_shapes=True,\n",
        "    with_stack=True\n",
        ") as prof:\n",
        "    with torch.no_grad():\n",
        "        model(dummy_input, tau=0.1)\n",
        "\n",
        "\n",
        "# Print results\n",
        "print(prof.key_averages().table(sort_by=\"cuda_time_total\" if torch.cuda.is_available() else \"cpu_time_total\", row_limit=10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 480,
      "metadata": {
        "id": "jXvKsupvZYjZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score, recall_score, precision_score, roc_curve, auc\n",
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def score_evaluate(model, data_loader, criterion, device, phase=\"Validation\"):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(data_loader, desc=f\"{phase}\"):\n",
        "            inputs, labels = inputs.to(device), labels.to(device) # move tensors to device\n",
        "            outputs = model(inputs, tau=0.1)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "\n",
        "    overall_f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
        "    overall_recall = recall_score(all_labels, all_preds, average=\"macro\")\n",
        "    overall_precision = precision_score(all_labels, all_preds, average=\"macro\")\n",
        "\n",
        "    f1_per_class = f1_score(all_labels, all_preds, average=None)\n",
        "    recall_per_class = recall_score(all_labels, all_preds, average=None)\n",
        "    precision_per_class = precision_score(all_labels, all_preds, average=None)\n",
        "    class_labels = sorted(set(all_labels))\n",
        "\n",
        "    epoch_loss = running_loss / len(data_loader)\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"{phase} Loss: {epoch_loss:.4f}, {phase} Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "\n",
        "    print(f\"Overall - F1: {overall_f1:.4f}, Recall: {overall_recall:.4f}, Precision: {overall_precision:.4f}\")\n",
        "    print(\"Per-Class Metrics:\")\n",
        "    for i, label in enumerate(class_labels):\n",
        "        print(f\"Class {label} - F1: {f1_per_class[i]:.4f}, Recall: {recall_per_class[i]:.4f}, Precision: {precision_per_class[i]:.4f}\")\n",
        "\n",
        "    return overall_f1, overall_recall, overall_precision, f1_per_class, recall_per_class, precision_per_class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 481,
      "metadata": {
        "id": "oe9DO2tS-BpI"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "def roc_auc(model, data_loader, device, num_classes):\n",
        "\n",
        "    y = [\"F1\", \"Precision\", \"Recall\"]\n",
        "\n",
        "    model.eval()\n",
        "    all_labels = []\n",
        "    all_scores = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(data_loader, desc=\"ROC AUC\"):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_scores.extend(outputs.cpu().numpy())\n",
        "\n",
        "    all_labels = label_binarize(all_labels, classes=list(range(num_classes)))\n",
        "    print(f\"Binarized all_labels shape: {all_labels.shape}\")\n",
        "    print(f\"All_scores shape: {np.array(all_scores).shape}\")\n",
        "\n",
        "    fpr, tpr, _ = roc_curve(all_labels.ravel(), np.array(all_scores).ravel())\n",
        "    roc_auc_value = auc(fpr, tpr)\n",
        "\n",
        "    plt.figure(figsize=(9, 7))\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc_value:0.4f})')\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"Overall ROC AUC: {roc_auc_value:.4f}\")\n",
        "\n",
        "    return fpr, tpr, roc_auc_value"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "overall_f1, overall_recall, overall_precision, f1_per_class, recall_per_class, precision_per_class = score_evaluate(model, test_loader, criterion, device, phase=\"Test\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_moDYMWp3KH",
        "outputId": "b76edc24-b848-4321-efb4-1e785b553099"
      },
      "execution_count": 482,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Test: 100%|██████████| 469/469 [00:22<00:00, 20.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.0743, Test Accuracy: 98.83%\n",
            "Overall - F1: 0.9881, Recall: 0.9884, Precision: 0.9878\n",
            "Per-Class Metrics:\n",
            "Class 0 - F1: 0.9968, Recall: 0.9968, Precision: 0.9968\n",
            "Class 1 - F1: 0.9975, Recall: 0.9968, Precision: 0.9981\n",
            "Class 2 - F1: 0.9843, Recall: 0.9832, Precision: 0.9855\n",
            "Class 3 - F1: 0.9963, Recall: 0.9983, Precision: 0.9943\n",
            "Class 4 - F1: 0.9825, Recall: 0.9895, Precision: 0.9756\n",
            "Class 5 - F1: 0.9918, Recall: 0.9887, Precision: 0.9950\n",
            "Class 6 - F1: 0.9836, Recall: 0.9802, Precision: 0.9870\n",
            "Class 7 - F1: 0.9740, Recall: 0.9796, Precision: 0.9685\n",
            "Class 8 - F1: 0.9860, Recall: 0.9823, Precision: 0.9897\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xbSpMMWGk3bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02cf03f4-12de-46db-a819-f672e15980cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-27 09:06:41--  https://zenodo.org/records/1214456/files/CRC-VAL-HE-7K.zip\n",
            "Resolving zenodo.org (zenodo.org)... 188.185.43.25, 188.185.45.92, 188.185.48.194, ...\n",
            "Connecting to zenodo.org (zenodo.org)|188.185.43.25|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 800276929 (763M) [application/octet-stream]\n",
            "Saving to: ‘CRC-VAL-HE-7K.zip’\n",
            "\n",
            "CRC-VAL-HE-7K.zip   100%[===================>] 763.20M  24.8MB/s    in 5m 44s  \n",
            "\n",
            "2025-02-27 09:12:26 (2.22 MB/s) - ‘CRC-VAL-HE-7K.zip’ saved [800276929/800276929]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -O CRC-VAL-HE-7K.zip https://zenodo.org/records/1214456/files/CRC-VAL-HE-7K.zip\n",
        "!unzip -qq CRC-VAL-HE-7K.zip -d val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rldaXFNXpT4r"
      },
      "outputs": [],
      "source": [
        "test_7k_dir = './val/CRC-VAL-HE-7K'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = datasets.ImageFolder(root=test_7k_dir, transform=transform)"
      ],
      "metadata": {
        "id": "p0bL6aK1sQOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Belqw8QytnZL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "#test7k_dataset = Dataset(dir=test_7k_dir, aug=False)\n",
        "#test7k_dataloader = DataLoader(test7k_dataset, batch_size=32, shuffle=False, num_workers=8, pin_memory=False, drop_last=False)\n",
        "test7k_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2y3cywDpjUQ",
        "outputId": "3f86e961-ef80-4c3a-c6f9-59485a921c23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Test: 100%|██████████| 225/225 [00:10<00:00, 21.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.7122, Test Accuracy: 89.87%\n",
            "Overall - F1: 0.8661, Recall: 0.8747, Precision: 0.8645\n",
            "Per-Class Metrics:\n",
            "Class 0 - F1: 0.9648, Recall: 0.9619, Precision: 0.9677\n",
            "Class 1 - F1: 0.9941, Recall: 1.0000, Precision: 0.9883\n",
            "Class 2 - F1: 0.8112, Recall: 0.9440, Precision: 0.7111\n",
            "Class 3 - F1: 0.8569, Recall: 0.7792, Precision: 0.9518\n",
            "Class 4 - F1: 0.9239, Recall: 0.8850, Precision: 0.9662\n",
            "Class 5 - F1: 0.7946, Recall: 0.7973, Precision: 0.7919\n",
            "Class 6 - F1: 0.8764, Recall: 0.9474, Precision: 0.8153\n",
            "Class 7 - F1: 0.6260, Recall: 0.6223, Precision: 0.6298\n",
            "Class 8 - F1: 0.9466, Recall: 0.9351, Precision: 0.9584\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "overall_f1, overall_recall, overall_precision, f1_per_class, recall_per_class, precision_per_class = score_evaluate(model, test7k_dataloader, criterion, device, phase=\"Test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "id": "p4pYjMPUpy2m",
        "outputId": "b5b3e8f3-e03c-45b5-f8b4-c515df1bc51c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2000x1200 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1600x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "x = [\"ADI\", \"BACK\", \"DEB\", \"LYM\", \"MUC\", \"MUS\", \"NORM\", 'STR', \"TUM\"]\n",
        "y = [\"F1\", \"Precision\", \"Recall\"]\n",
        "\n",
        "f1_score_list = []\n",
        "precision_score_list = []\n",
        "recall_score_list = []\n",
        "\n",
        "for i, class_name in enumerate(x):\n",
        "    f1_score_list.append(f1_per_class[i])\n",
        "    precision_score_list.append(precision_per_class[i])\n",
        "    recall_score_list.append(recall_per_class[i])\n",
        "\n",
        "x.append(\"Overall\")\n",
        "f1_score_list.append(overall_f1)\n",
        "precision_score_list.append(overall_precision)\n",
        "recall_score_list.append(overall_recall)\n",
        "\n",
        "score_list = [f1_score_list, precision_score_list, recall_score_list]\n",
        "score_list = np.array(score_list)\n",
        "plt.figure(figsize=(20, 12))\n",
        "score_cm = plt.matshow(score_list, cmap=plt.cm.Blues, alpha=0.3)\n",
        "plt.xticks(range(len(x)), x)\n",
        "plt.yticks(range(len(y)), y)\n",
        "for i in range(len(y)):\n",
        "    for j in range(len(x)):\n",
        "        # 행렬의 각각의 수치를 각 칸의 중앙에 넣어준다\n",
        "        plt.text(x=j, y=i,\n",
        "                     s=\"{:.2f}\".format(score_list[i, j]),\n",
        "                     va='center',\n",
        "                     ha='center',\n",
        "                     )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5EnagUDtpzDU",
        "outputId": "fd07624b-db27-4dd9-f795-f9953829edb2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference Time Measurement Results:\n",
            "Total Inferences: 225\n",
            "Average Time: 7.50 ms\n",
            "Standard Deviation: 0.41 ms\n",
            "Maximum Time: 9.23 ms\n",
            "Minimum Time: 7.06 ms\n"
          ]
        }
      ],
      "source": [
        "times = measure_inference_time(model, test7k_dataloader, device)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}