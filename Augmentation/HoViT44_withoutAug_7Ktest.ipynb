{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYpt3QcVCl83",
        "outputId": "bc13e201-128f-465f-89ca-d4a285168d64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9656 sha256=94ccf96895bd8f467e884c11d261a351e71b41bc02b7c9300284e20c18bdf43e\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/b3/0f/a40dbd1c6861731779f62cc4babcb234387e11d697df70ee97\n",
            "Successfully built wget\n",
            "Installing collected packages: wget, torchinfo\n",
            "Successfully installed torchinfo-1.8.0 wget-3.2\n"
          ]
        }
      ],
      "source": [
        "!pip install wget torchinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Wa8mdzSwvzMe"
      },
      "outputs": [],
      "source": [
        "import timm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, ConcatDataset\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "import itertools\n",
        "from torchinfo import summary\n",
        "from sklearn.metrics import accuracy_score, balanced_accuracy_score, classification_report, confusion_matrix\n",
        "from PIL import Image\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import random\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Subset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "4FLtpbS_Vrd7"
      },
      "outputs": [],
      "source": [
        "class ConvNorm(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=2, padding=1):\n",
        "        super(ConvNorm, self).__init__()\n",
        "        self.linear = nn.Conv2d(\n",
        "            in_channels, out_channels, kernel_size=kernel_size,\n",
        "            stride=stride, padding=padding, bias=False\n",
        "        )\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear(x)\n",
        "        x = self.bn(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "zhaJu1oZyp0D"
      },
      "outputs": [],
      "source": [
        "class Stem16(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Stem16, self).__init__()\n",
        "        self.conv1 = ConvNorm(3, 32)\n",
        "        self.act1 = nn.Hardswish()\n",
        "        self.conv2 = ConvNorm(32, 64)\n",
        "        self.act2 = nn.Hardswish()\n",
        "        self.conv3 = ConvNorm(64, 128)\n",
        "        self.act3 = nn.Hardswish()\n",
        "        self.conv4 = ConvNorm(128, 256)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.act1(self.conv1(x))\n",
        "        x = self.act2(self.conv2(x))\n",
        "        x = self.act3(self.conv3(x))\n",
        "        x = self.conv4(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "W1Xh6-wOyskM"
      },
      "outputs": [],
      "source": [
        "class LinearNorm(nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super(LinearNorm, self).__init__()\n",
        "        self.linear = nn.Linear(in_features, out_features, bias=False)\n",
        "        self.bn = nn.BatchNorm1d(out_features)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        if x.dim() == 3:\n",
        "            B, N, C = x.shape\n",
        "            x = x.reshape(B * N, C)\n",
        "            x = self.bn(self.linear(x))\n",
        "            x = x.reshape(B, N, -1)\n",
        "        else:\n",
        "            x = self.bn(self.linear(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "wHeuFDCPyuLf"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, num_heads, attn_ratio=2):\n",
        "        super(Attention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = head_dim ** -0.5\n",
        "        inner_dim = head_dim * num_heads * 3\n",
        "        self.qkv = LinearNorm(dim, inner_dim)\n",
        "\n",
        "        self.proj = nn.Sequential(\n",
        "            nn.Hardswish(),\n",
        "            LinearNorm(dim, dim)\n",
        "        )\n",
        "\n",
        "        self.attention_biases = None\n",
        "        self.attention_bias_idxs = None\n",
        "\n",
        "    def compute_attention_bias(self, resolution):\n",
        "\n",
        "        points = list(itertools.product(range(resolution), range(resolution)))\n",
        "        N = len(points)\n",
        "\n",
        "        attention_offsets = {}\n",
        "        idxs = []\n",
        "\n",
        "        # if N = 196, then resolution = 14\n",
        "        for p1 in points:\n",
        "            for p2 in points:\n",
        "                offset = (abs(p1[0] - p2[0]), abs(p1[1] - p2[1]))\n",
        "                if offset not in attention_offsets:\n",
        "                    attention_offsets[offset] = len(attention_offsets)\n",
        "                idxs.append(attention_offsets[offset])\n",
        "\n",
        "        num_offsets = len(attention_offsets)\n",
        "\n",
        "        # 각 attention head에 대해 num_offsets 만큼의 학습 가능한 Bias를 생성\n",
        "        self.attention_biases = nn.Parameter(torch.zeros(self.num_heads, num_offsets).to(next(self.parameters()).device))\n",
        "        self.attention_bias_idxs = torch.LongTensor(idxs).view(N, N).to(next(self.parameters()).device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        resolution = int(N ** 0.5)\n",
        "\n",
        "        if self.attention_biases is None or self.attention_bias_idxs.shape[0] != N:\n",
        "            self.compute_attention_bias(resolution)\n",
        "\n",
        "        qkv = self.qkv(x)\n",
        "        qkv = qkv.view(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        # qkv: (3, B, num_heads, N, head_dim)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2] # q, k, v: (B, num_heads, N, head_dim)\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale # attn: (B, num_heads, N, N)\n",
        "        attn_bias = self.attention_biases[:, self.attention_bias_idxs].unsqueeze(0) # attn_bias: (1, num_heads, N, N)\n",
        "        attn = attn + attn_bias\n",
        "        attn = attn.softmax(dim=-1)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        return self.proj(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "xvyY-FeUywyR"
      },
      "outputs": [],
      "source": [
        "class LevitMlp(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features, out_features):\n",
        "        super(LevitMlp, self).__init__()\n",
        "        self.ln1 = LinearNorm(in_features, hidden_features)\n",
        "        self.act = nn.Hardswish()\n",
        "        self.drop = nn.Dropout(p=0.5, inplace=False)#dropout 적용\n",
        "        self.ln2 = LinearNorm(hidden_features, out_features)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.ln1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.ln2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "Ubl233osyyHQ"
      },
      "outputs": [],
      "source": [
        "class LevitBlock(nn.Module):\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=2):\n",
        "        super(LevitBlock, self).__init__()\n",
        "        self.attn = Attention(dim, num_heads)\n",
        "        self.drop_path1 = nn.Identity()\n",
        "        self.mlp = LevitMlp(dim, dim * mlp_ratio, dim)\n",
        "        self.drop_path2 = nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.drop_path1(self.attn(x))\n",
        "        x = x + self.drop_path2(self.mlp(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "cjBHkWJYyzfc"
      },
      "outputs": [],
      "source": [
        "class CNNDownsample(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(CNNDownsample, self).__init__()\n",
        "        self.out_channels = out_channels\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1)\n",
        "        self.act = nn.Hardswish()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # print(x.shape)\n",
        "        B, N, C = x.shape # (B, N, C)  N=H*W (16 * 16 = 196)\n",
        "        H = int(np.sqrt(N))\n",
        "        x = x.view(B, H, H, C).permute(0, 3, 1, 2)\n",
        "        x = self.conv(x)\n",
        "        x = self.act(x)\n",
        "        x = x.permute(0, 2, 3, 1).view(B, -1, self.out_channels)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "jNpOjwVVy1Im"
      },
      "outputs": [],
      "source": [
        "class LevitStage(nn.Module):\n",
        "    def __init__(self, dim, out_dim, num_heads, num_blocks, downsample=True):\n",
        "        super(LevitStage, self).__init__()\n",
        "        self.downsample = CNNDownsample(dim, out_dim) if downsample else nn.Identity()\n",
        "        self.blocks = nn.Sequential(*[LevitBlock(out_dim, num_heads) for _ in range(num_blocks)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.downsample(x)\n",
        "        x = self.blocks(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "FyFtflRsy2QE"
      },
      "outputs": [],
      "source": [
        "class ConvLevitStage(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, num_blocks, kernel_size, stride, padding):\n",
        "        super(ConvLevitStage, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            *[nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size, stride, padding)\n",
        "              for i in range(num_blocks)],\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "yKzCWv-Ny3j9"
      },
      "outputs": [],
      "source": [
        "class NormLinear(nn.Module):\n",
        "    def __init__(self, in_features, out_features, dropout_prob=0.5):#drop_out_0.5 적용\n",
        "        super(NormLinear, self).__init__()\n",
        "        self.bn = nn.BatchNorm1d(in_features)\n",
        "        self.drop = nn.Dropout(p=dropout_prob, inplace=False)\n",
        "        self.linear = nn.Linear(in_features, out_features, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.bn(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.linear(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "nS7iQ8-453Ru"
      },
      "outputs": [],
      "source": [
        "class LevitDistilled(nn.Module):\n",
        "    def __init__(self, num_classes=9):\n",
        "        super(LevitDistilled, self).__init__()\n",
        "\n",
        "        self.stem = Stem16()\n",
        "\n",
        "        self.stage1 = LevitStage(dim=256, out_dim=256, num_heads=4, num_blocks=4, downsample=False) # block 수 적용\n",
        "        self.stage2 = LevitStage(dim=256, out_dim=384, num_heads=6, num_blocks=4, downsample=True)\n",
        "\n",
        "        self.conv1x1 = nn.Sequential(\n",
        "            nn.Conv2d(384, 512, kernel_size=1, stride=1, padding=0),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        self.head = NormLinear(in_features=512, out_features=num_classes, dropout_prob=0.0)\n",
        "        self.head_dist = NormLinear(in_features=512, out_features=num_classes, dropout_prob=0.0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.stem(x)\n",
        "\n",
        "        B, C, H, W = x.shape\n",
        "        x = x.view(B, C, -1).transpose(1, 2)\n",
        "        x = self.stage1(x)\n",
        "        x = self.stage2(x)\n",
        "\n",
        "        H = W = int(x.shape[1]**0.5)\n",
        "        x = x.transpose(1, 2).view(B, 384, H, W)\n",
        "\n",
        "        x = self.conv1x1(x)\n",
        "\n",
        "        x = torch.mean(x, dim=(2, 3))\n",
        "        out = self.head(x)\n",
        "        out_dist = self.head_dist(x)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "lbCq25bWzJSQ",
        "outputId": "c0d10813-afaf-4aab-cfe1-3f6469c87d78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LevitDistilled(\n",
            "  (stem): Stem16(\n",
            "    (conv1): ConvNorm(\n",
            "      (linear): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (act1): Hardswish()\n",
            "    (conv2): ConvNorm(\n",
            "      (linear): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (act2): Hardswish()\n",
            "    (conv3): ConvNorm(\n",
            "      (linear): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (act3): Hardswish()\n",
            "    (conv4): ConvNorm(\n",
            "      (linear): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (stage1): LevitStage(\n",
            "    (downsample): Identity()\n",
            "    (blocks): Sequential(\n",
            "      (0): LevitBlock(\n",
            "        (attn): Attention(\n",
            "          (qkv): LinearNorm(\n",
            "            (linear): Linear(in_features=256, out_features=768, bias=False)\n",
            "            (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (proj): Sequential(\n",
            "            (0): Hardswish()\n",
            "            (1): LinearNorm(\n",
            "              (linear): Linear(in_features=256, out_features=256, bias=False)\n",
            "              (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (drop_path1): Identity()\n",
            "        (mlp): LevitMlp(\n",
            "          (ln1): LinearNorm(\n",
            "            (linear): Linear(in_features=256, out_features=512, bias=False)\n",
            "            (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (act): Hardswish()\n",
            "          (drop): Dropout(p=0.5, inplace=False)\n",
            "          (ln2): LinearNorm(\n",
            "            (linear): Linear(in_features=512, out_features=256, bias=False)\n",
            "            (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (drop_path2): Identity()\n",
            "      )\n",
            "      (1): LevitBlock(\n",
            "        (attn): Attention(\n",
            "          (qkv): LinearNorm(\n",
            "            (linear): Linear(in_features=256, out_features=768, bias=False)\n",
            "            (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (proj): Sequential(\n",
            "            (0): Hardswish()\n",
            "            (1): LinearNorm(\n",
            "              (linear): Linear(in_features=256, out_features=256, bias=False)\n",
            "              (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (drop_path1): Identity()\n",
            "        (mlp): LevitMlp(\n",
            "          (ln1): LinearNorm(\n",
            "            (linear): Linear(in_features=256, out_features=512, bias=False)\n",
            "            (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (act): Hardswish()\n",
            "          (drop): Dropout(p=0.5, inplace=False)\n",
            "          (ln2): LinearNorm(\n",
            "            (linear): Linear(in_features=512, out_features=256, bias=False)\n",
            "            (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (drop_path2): Identity()\n",
            "      )\n",
            "      (2): LevitBlock(\n",
            "        (attn): Attention(\n",
            "          (qkv): LinearNorm(\n",
            "            (linear): Linear(in_features=256, out_features=768, bias=False)\n",
            "            (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (proj): Sequential(\n",
            "            (0): Hardswish()\n",
            "            (1): LinearNorm(\n",
            "              (linear): Linear(in_features=256, out_features=256, bias=False)\n",
            "              (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (drop_path1): Identity()\n",
            "        (mlp): LevitMlp(\n",
            "          (ln1): LinearNorm(\n",
            "            (linear): Linear(in_features=256, out_features=512, bias=False)\n",
            "            (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (act): Hardswish()\n",
            "          (drop): Dropout(p=0.5, inplace=False)\n",
            "          (ln2): LinearNorm(\n",
            "            (linear): Linear(in_features=512, out_features=256, bias=False)\n",
            "            (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (drop_path2): Identity()\n",
            "      )\n",
            "      (3): LevitBlock(\n",
            "        (attn): Attention(\n",
            "          (qkv): LinearNorm(\n",
            "            (linear): Linear(in_features=256, out_features=768, bias=False)\n",
            "            (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (proj): Sequential(\n",
            "            (0): Hardswish()\n",
            "            (1): LinearNorm(\n",
            "              (linear): Linear(in_features=256, out_features=256, bias=False)\n",
            "              (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (drop_path1): Identity()\n",
            "        (mlp): LevitMlp(\n",
            "          (ln1): LinearNorm(\n",
            "            (linear): Linear(in_features=256, out_features=512, bias=False)\n",
            "            (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (act): Hardswish()\n",
            "          (drop): Dropout(p=0.5, inplace=False)\n",
            "          (ln2): LinearNorm(\n",
            "            (linear): Linear(in_features=512, out_features=256, bias=False)\n",
            "            (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (drop_path2): Identity()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (stage2): LevitStage(\n",
            "    (downsample): CNNDownsample(\n",
            "      (conv): Conv2d(256, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "      (act): Hardswish()\n",
            "    )\n",
            "    (blocks): Sequential(\n",
            "      (0): LevitBlock(\n",
            "        (attn): Attention(\n",
            "          (qkv): LinearNorm(\n",
            "            (linear): Linear(in_features=384, out_features=1152, bias=False)\n",
            "            (bn): BatchNorm1d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (proj): Sequential(\n",
            "            (0): Hardswish()\n",
            "            (1): LinearNorm(\n",
            "              (linear): Linear(in_features=384, out_features=384, bias=False)\n",
            "              (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (drop_path1): Identity()\n",
            "        (mlp): LevitMlp(\n",
            "          (ln1): LinearNorm(\n",
            "            (linear): Linear(in_features=384, out_features=768, bias=False)\n",
            "            (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (act): Hardswish()\n",
            "          (drop): Dropout(p=0.5, inplace=False)\n",
            "          (ln2): LinearNorm(\n",
            "            (linear): Linear(in_features=768, out_features=384, bias=False)\n",
            "            (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (drop_path2): Identity()\n",
            "      )\n",
            "      (1): LevitBlock(\n",
            "        (attn): Attention(\n",
            "          (qkv): LinearNorm(\n",
            "            (linear): Linear(in_features=384, out_features=1152, bias=False)\n",
            "            (bn): BatchNorm1d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (proj): Sequential(\n",
            "            (0): Hardswish()\n",
            "            (1): LinearNorm(\n",
            "              (linear): Linear(in_features=384, out_features=384, bias=False)\n",
            "              (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (drop_path1): Identity()\n",
            "        (mlp): LevitMlp(\n",
            "          (ln1): LinearNorm(\n",
            "            (linear): Linear(in_features=384, out_features=768, bias=False)\n",
            "            (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (act): Hardswish()\n",
            "          (drop): Dropout(p=0.5, inplace=False)\n",
            "          (ln2): LinearNorm(\n",
            "            (linear): Linear(in_features=768, out_features=384, bias=False)\n",
            "            (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (drop_path2): Identity()\n",
            "      )\n",
            "      (2): LevitBlock(\n",
            "        (attn): Attention(\n",
            "          (qkv): LinearNorm(\n",
            "            (linear): Linear(in_features=384, out_features=1152, bias=False)\n",
            "            (bn): BatchNorm1d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (proj): Sequential(\n",
            "            (0): Hardswish()\n",
            "            (1): LinearNorm(\n",
            "              (linear): Linear(in_features=384, out_features=384, bias=False)\n",
            "              (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (drop_path1): Identity()\n",
            "        (mlp): LevitMlp(\n",
            "          (ln1): LinearNorm(\n",
            "            (linear): Linear(in_features=384, out_features=768, bias=False)\n",
            "            (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (act): Hardswish()\n",
            "          (drop): Dropout(p=0.5, inplace=False)\n",
            "          (ln2): LinearNorm(\n",
            "            (linear): Linear(in_features=768, out_features=384, bias=False)\n",
            "            (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (drop_path2): Identity()\n",
            "      )\n",
            "      (3): LevitBlock(\n",
            "        (attn): Attention(\n",
            "          (qkv): LinearNorm(\n",
            "            (linear): Linear(in_features=384, out_features=1152, bias=False)\n",
            "            (bn): BatchNorm1d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (proj): Sequential(\n",
            "            (0): Hardswish()\n",
            "            (1): LinearNorm(\n",
            "              (linear): Linear(in_features=384, out_features=384, bias=False)\n",
            "              (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (drop_path1): Identity()\n",
            "        (mlp): LevitMlp(\n",
            "          (ln1): LinearNorm(\n",
            "            (linear): Linear(in_features=384, out_features=768, bias=False)\n",
            "            (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (act): Hardswish()\n",
            "          (drop): Dropout(p=0.5, inplace=False)\n",
            "          (ln2): LinearNorm(\n",
            "            (linear): Linear(in_features=768, out_features=384, bias=False)\n",
            "            (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (drop_path2): Identity()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (conv1x1): Sequential(\n",
            "    (0): Conv2d(384, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "  )\n",
            "  (head): NormLinear(\n",
            "    (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (drop): Dropout(p=0.0, inplace=False)\n",
            "    (linear): Linear(in_features=512, out_features=9, bias=True)\n",
            "  )\n",
            "  (head_dist): NormLinear(\n",
            "    (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (drop): Dropout(p=0.0, inplace=False)\n",
            "    (linear): Linear(in_features=512, out_features=9, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model = LevitDistilled(num_classes=9)\n",
        "print(model)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "batch_size = 32\n",
        "learning_rate = 5e-4\n",
        "num_epochs = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNpMsNYAzLDF",
        "outputId": "fbc33d9e-451b-4173-e01f-982055a21fa5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=========================================================================================================\n",
            "Layer (type:depth-idx)                                  Output Shape              Param #\n",
            "=========================================================================================================\n",
            "LevitDistilled                                          [32, 9]                   --\n",
            "├─Stem16: 1-1                                           [32, 256, 14, 14]         --\n",
            "│    └─ConvNorm: 2-1                                    [32, 32, 112, 112]        --\n",
            "│    │    └─Conv2d: 3-1                                 [32, 32, 112, 112]        864\n",
            "│    │    └─BatchNorm2d: 3-2                            [32, 32, 112, 112]        64\n",
            "│    └─Hardswish: 2-2                                   [32, 32, 112, 112]        --\n",
            "│    └─ConvNorm: 2-3                                    [32, 64, 56, 56]          --\n",
            "│    │    └─Conv2d: 3-3                                 [32, 64, 56, 56]          18,432\n",
            "│    │    └─BatchNorm2d: 3-4                            [32, 64, 56, 56]          128\n",
            "│    └─Hardswish: 2-4                                   [32, 64, 56, 56]          --\n",
            "│    └─ConvNorm: 2-5                                    [32, 128, 28, 28]         --\n",
            "│    │    └─Conv2d: 3-5                                 [32, 128, 28, 28]         73,728\n",
            "│    │    └─BatchNorm2d: 3-6                            [32, 128, 28, 28]         256\n",
            "│    └─Hardswish: 2-6                                   [32, 128, 28, 28]         --\n",
            "│    └─ConvNorm: 2-7                                    [32, 256, 14, 14]         --\n",
            "│    │    └─Conv2d: 3-7                                 [32, 256, 14, 14]         294,912\n",
            "│    │    └─BatchNorm2d: 3-8                            [32, 256, 14, 14]         512\n",
            "├─LevitStage: 1-2                                       [32, 196, 256]            --\n",
            "│    └─Identity: 2-8                                    [32, 196, 256]            --\n",
            "│    └─Sequential: 2-9                                  [32, 196, 256]            --\n",
            "│    │    └─LevitBlock: 3-9                             [32, 196, 256]            527,872\n",
            "│    │    └─LevitBlock: 3-10                            [32, 196, 256]            527,872\n",
            "│    │    └─LevitBlock: 3-11                            [32, 196, 256]            527,872\n",
            "│    │    └─LevitBlock: 3-12                            [32, 196, 256]            527,872\n",
            "├─LevitStage: 1-3                                       [32, 49, 384]             --\n",
            "│    └─CNNDownsample: 2-10                              [32, 49, 384]             --\n",
            "│    │    └─Conv2d: 3-13                                [32, 384, 7, 7]           885,120\n",
            "│    │    └─Hardswish: 3-14                             [32, 384, 7, 7]           --\n",
            "│    └─Sequential: 2-11                                 [32, 49, 384]             --\n",
            "│    │    └─LevitBlock: 3-15                            [32, 49, 384]             1,185,024\n",
            "│    │    └─LevitBlock: 3-16                            [32, 49, 384]             1,185,024\n",
            "│    │    └─LevitBlock: 3-17                            [32, 49, 384]             1,185,024\n",
            "│    │    └─LevitBlock: 3-18                            [32, 49, 384]             1,185,024\n",
            "├─Sequential: 1-4                                       [32, 512, 7, 7]           --\n",
            "│    └─Conv2d: 2-12                                     [32, 512, 7, 7]           197,120\n",
            "│    └─BatchNorm2d: 2-13                                [32, 512, 7, 7]           1,024\n",
            "│    └─ReLU: 2-14                                       [32, 512, 7, 7]           --\n",
            "├─NormLinear: 1-5                                       [32, 9]                   --\n",
            "│    └─BatchNorm1d: 2-15                                [32, 512]                 1,024\n",
            "│    └─Dropout: 2-16                                    [32, 512]                 --\n",
            "│    └─Linear: 2-17                                     [32, 9]                   4,617\n",
            "├─NormLinear: 1-6                                       [32, 9]                   --\n",
            "│    └─BatchNorm1d: 2-18                                [32, 512]                 1,024\n",
            "│    └─Dropout: 2-19                                    [32, 512]                 --\n",
            "│    └─Linear: 2-20                                     [32, 9]                   4,617\n",
            "=========================================================================================================\n",
            "Total params: 8,335,026\n",
            "Trainable params: 8,335,026\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (Units.GIGABYTES): 28.27\n",
            "=========================================================================================================\n",
            "Input size (MB): 19.27\n",
            "Forward/backward pass size (MB): 1392.35\n",
            "Params size (MB): 33.34\n",
            "Estimated Total Size (MB): 1444.96\n",
            "=========================================================================================================\n"
          ]
        }
      ],
      "source": [
        "print(summary(model, input_size=(32, 3, 224, 224)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "qDELjYy2zP90",
        "outputId": "803f6008-fb8a-4e20-f174-b95f0d272125"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=========================================================================================================\n",
            "Layer (type:depth-idx)                                  Output Shape              Param #\n",
            "=========================================================================================================\n",
            "LevitDistilled                                          [32, 9]                   --\n",
            "├─Stem16: 1-1                                           [32, 256, 14, 14]         --\n",
            "│    └─conv1.linear.weight                                                        ├─864\n",
            "│    └─conv1.bn.weight                                                            ├─32\n",
            "│    └─conv1.bn.bias                                                              ├─32\n",
            "│    └─conv2.linear.weight                                                        ├─18,432\n",
            "│    └─conv2.bn.weight                                                            ├─64\n",
            "│    └─conv2.bn.bias                                                              ├─64\n",
            "│    └─conv3.linear.weight                                                        ├─73,728\n",
            "│    └─conv3.bn.weight                                                            ├─128\n",
            "│    └─conv3.bn.bias                                                              ├─128\n",
            "│    └─conv4.linear.weight                                                        ├─294,912\n",
            "│    └─conv4.bn.weight                                                            ├─256\n",
            "│    └─conv4.bn.bias                                                              └─256\n",
            "│    └─ConvNorm: 2-1                                    [32, 32, 112, 112]        --\n",
            "│    │    └─linear.weight                                                         ├─864\n",
            "│    │    └─bn.weight                                                             ├─32\n",
            "│    │    └─bn.bias                                                               └─32\n",
            "│    │    └─Conv2d: 3-1                                 [32, 32, 112, 112]        864\n",
            "│    │    │    └─weight                                                           └─864\n",
            "│    │    └─BatchNorm2d: 3-2                            [32, 32, 112, 112]        64\n",
            "│    │    │    └─weight                                                           ├─32\n",
            "│    │    │    └─bias                                                             └─32\n",
            "│    └─Hardswish: 2-2                                   [32, 32, 112, 112]        --\n",
            "│    └─ConvNorm: 2-3                                    [32, 64, 56, 56]          --\n",
            "│    │    └─linear.weight                                                         ├─18,432\n",
            "│    │    └─bn.weight                                                             ├─64\n",
            "│    │    └─bn.bias                                                               └─64\n",
            "│    │    └─Conv2d: 3-3                                 [32, 64, 56, 56]          18,432\n",
            "│    │    │    └─weight                                                           └─18,432\n",
            "│    │    └─BatchNorm2d: 3-4                            [32, 64, 56, 56]          128\n",
            "│    │    │    └─weight                                                           ├─64\n",
            "│    │    │    └─bias                                                             └─64\n",
            "│    └─Hardswish: 2-4                                   [32, 64, 56, 56]          --\n",
            "│    └─ConvNorm: 2-5                                    [32, 128, 28, 28]         --\n",
            "│    │    └─linear.weight                                                         ├─73,728\n",
            "│    │    └─bn.weight                                                             ├─128\n",
            "│    │    └─bn.bias                                                               └─128\n",
            "│    │    └─Conv2d: 3-5                                 [32, 128, 28, 28]         73,728\n",
            "│    │    │    └─weight                                                           └─73,728\n",
            "│    │    └─BatchNorm2d: 3-6                            [32, 128, 28, 28]         256\n",
            "│    │    │    └─weight                                                           ├─128\n",
            "│    │    │    └─bias                                                             └─128\n",
            "│    └─Hardswish: 2-6                                   [32, 128, 28, 28]         --\n",
            "│    └─ConvNorm: 2-7                                    [32, 256, 14, 14]         --\n",
            "│    │    └─linear.weight                                                         ├─294,912\n",
            "│    │    └─bn.weight                                                             ├─256\n",
            "│    │    └─bn.bias                                                               └─256\n",
            "│    │    └─Conv2d: 3-7                                 [32, 256, 14, 14]         294,912\n",
            "│    │    │    └─weight                                                           └─294,912\n",
            "│    │    └─BatchNorm2d: 3-8                            [32, 256, 14, 14]         512\n",
            "│    │    │    └─weight                                                           ├─256\n",
            "│    │    │    └─bias                                                             └─256\n",
            "├─LevitStage: 1-2                                       [32, 196, 256]            --\n",
            "│    └─blocks.0.attn.attention_biases                                             ├─784\n",
            "│    └─blocks.0.attn.qkv.linear.weight                                            ├─196,608\n",
            "│    └─blocks.0.attn.qkv.bn.weight                                                ├─768\n",
            "│    └─blocks.0.attn.qkv.bn.bias                                                  ├─768\n",
            "│    └─blocks.0.attn.proj.1.linear.weight                                         ├─65,536\n",
            "│    └─blocks.0.attn.proj.1.bn.weight                                             ├─256\n",
            "│    └─blocks.0.attn.proj.1.bn.bias                                               ├─256\n",
            "│    └─blocks.0.mlp.ln1.linear.weight                                             ├─131,072\n",
            "│    └─blocks.0.mlp.ln1.bn.weight                                                 ├─512\n",
            "│    └─blocks.0.mlp.ln1.bn.bias                                                   ├─512\n",
            "│    └─blocks.0.mlp.ln2.linear.weight                                             ├─131,072\n",
            "│    └─blocks.0.mlp.ln2.bn.weight                                                 ├─256\n",
            "│    └─blocks.0.mlp.ln2.bn.bias                                                   ├─256\n",
            "│    └─blocks.1.attn.attention_biases                                             ├─784\n",
            "│    └─blocks.1.attn.qkv.linear.weight                                            ├─196,608\n",
            "│    └─blocks.1.attn.qkv.bn.weight                                                ├─768\n",
            "│    └─blocks.1.attn.qkv.bn.bias                                                  ├─768\n",
            "│    └─blocks.1.attn.proj.1.linear.weight                                         ├─65,536\n",
            "│    └─blocks.1.attn.proj.1.bn.weight                                             ├─256\n",
            "│    └─blocks.1.attn.proj.1.bn.bias                                               ├─256\n",
            "│    └─blocks.1.mlp.ln1.linear.weight                                             ├─131,072\n",
            "│    └─blocks.1.mlp.ln1.bn.weight                                                 ├─512\n",
            "│    └─blocks.1.mlp.ln1.bn.bias                                                   ├─512\n",
            "│    └─blocks.1.mlp.ln2.linear.weight                                             ├─131,072\n",
            "│    └─blocks.1.mlp.ln2.bn.weight                                                 ├─256\n",
            "│    └─blocks.1.mlp.ln2.bn.bias                                                   ├─256\n",
            "│    └─blocks.2.attn.attention_biases                                             ├─784\n",
            "│    └─blocks.2.attn.qkv.linear.weight                                            ├─196,608\n",
            "│    └─blocks.2.attn.qkv.bn.weight                                                ├─768\n",
            "│    └─blocks.2.attn.qkv.bn.bias                                                  ├─768\n",
            "│    └─blocks.2.attn.proj.1.linear.weight                                         ├─65,536\n",
            "│    └─blocks.2.attn.proj.1.bn.weight                                             ├─256\n",
            "│    └─blocks.2.attn.proj.1.bn.bias                                               ├─256\n",
            "│    └─blocks.2.mlp.ln1.linear.weight                                             ├─131,072\n",
            "│    └─blocks.2.mlp.ln1.bn.weight                                                 ├─512\n",
            "│    └─blocks.2.mlp.ln1.bn.bias                                                   ├─512\n",
            "│    └─blocks.2.mlp.ln2.linear.weight                                             ├─131,072\n",
            "│    └─blocks.2.mlp.ln2.bn.weight                                                 ├─256\n",
            "│    └─blocks.2.mlp.ln2.bn.bias                                                   ├─256\n",
            "│    └─blocks.3.attn.attention_biases                                             ├─784\n",
            "│    └─blocks.3.attn.qkv.linear.weight                                            ├─196,608\n",
            "│    └─blocks.3.attn.qkv.bn.weight                                                ├─768\n",
            "│    └─blocks.3.attn.qkv.bn.bias                                                  ├─768\n",
            "│    └─blocks.3.attn.proj.1.linear.weight                                         ├─65,536\n",
            "│    └─blocks.3.attn.proj.1.bn.weight                                             ├─256\n",
            "│    └─blocks.3.attn.proj.1.bn.bias                                               ├─256\n",
            "│    └─blocks.3.mlp.ln1.linear.weight                                             ├─131,072\n",
            "│    └─blocks.3.mlp.ln1.bn.weight                                                 ├─512\n",
            "│    └─blocks.3.mlp.ln1.bn.bias                                                   ├─512\n",
            "│    └─blocks.3.mlp.ln2.linear.weight                                             ├─131,072\n",
            "│    └─blocks.3.mlp.ln2.bn.weight                                                 ├─256\n",
            "│    └─blocks.3.mlp.ln2.bn.bias                                                   └─256\n",
            "│    └─Identity: 2-8                                    [32, 196, 256]            --\n",
            "│    └─Sequential: 2-9                                  [32, 196, 256]            --\n",
            "│    │    └─0.attn.attention_biases                                               ├─784\n",
            "│    │    └─0.attn.qkv.linear.weight                                              ├─196,608\n",
            "│    │    └─0.attn.qkv.bn.weight                                                  ├─768\n",
            "│    │    └─0.attn.qkv.bn.bias                                                    ├─768\n",
            "│    │    └─0.attn.proj.1.linear.weight                                           ├─65,536\n",
            "│    │    └─0.attn.proj.1.bn.weight                                               ├─256\n",
            "│    │    └─0.attn.proj.1.bn.bias                                                 ├─256\n",
            "│    │    └─0.mlp.ln1.linear.weight                                               ├─131,072\n",
            "│    │    └─0.mlp.ln1.bn.weight                                                   ├─512\n",
            "│    │    └─0.mlp.ln1.bn.bias                                                     ├─512\n",
            "│    │    └─0.mlp.ln2.linear.weight                                               ├─131,072\n",
            "│    │    └─0.mlp.ln2.bn.weight                                                   ├─256\n",
            "│    │    └─0.mlp.ln2.bn.bias                                                     ├─256\n",
            "│    │    └─1.attn.attention_biases                                               ├─784\n",
            "│    │    └─1.attn.qkv.linear.weight                                              ├─196,608\n",
            "│    │    └─1.attn.qkv.bn.weight                                                  ├─768\n",
            "│    │    └─1.attn.qkv.bn.bias                                                    ├─768\n",
            "│    │    └─1.attn.proj.1.linear.weight                                           ├─65,536\n",
            "│    │    └─1.attn.proj.1.bn.weight                                               ├─256\n",
            "│    │    └─1.attn.proj.1.bn.bias                                                 ├─256\n",
            "│    │    └─1.mlp.ln1.linear.weight                                               ├─131,072\n",
            "│    │    └─1.mlp.ln1.bn.weight                                                   ├─512\n",
            "│    │    └─1.mlp.ln1.bn.bias                                                     ├─512\n",
            "│    │    └─1.mlp.ln2.linear.weight                                               ├─131,072\n",
            "│    │    └─1.mlp.ln2.bn.weight                                                   ├─256\n",
            "│    │    └─1.mlp.ln2.bn.bias                                                     ├─256\n",
            "│    │    └─2.attn.attention_biases                                               ├─784\n",
            "│    │    └─2.attn.qkv.linear.weight                                              ├─196,608\n",
            "│    │    └─2.attn.qkv.bn.weight                                                  ├─768\n",
            "│    │    └─2.attn.qkv.bn.bias                                                    ├─768\n",
            "│    │    └─2.attn.proj.1.linear.weight                                           ├─65,536\n",
            "│    │    └─2.attn.proj.1.bn.weight                                               ├─256\n",
            "│    │    └─2.attn.proj.1.bn.bias                                                 ├─256\n",
            "│    │    └─2.mlp.ln1.linear.weight                                               ├─131,072\n",
            "│    │    └─2.mlp.ln1.bn.weight                                                   ├─512\n",
            "│    │    └─2.mlp.ln1.bn.bias                                                     ├─512\n",
            "│    │    └─2.mlp.ln2.linear.weight                                               ├─131,072\n",
            "│    │    └─2.mlp.ln2.bn.weight                                                   ├─256\n",
            "│    │    └─2.mlp.ln2.bn.bias                                                     ├─256\n",
            "│    │    └─3.attn.attention_biases                                               ├─784\n",
            "│    │    └─3.attn.qkv.linear.weight                                              ├─196,608\n",
            "│    │    └─3.attn.qkv.bn.weight                                                  ├─768\n",
            "│    │    └─3.attn.qkv.bn.bias                                                    ├─768\n",
            "│    │    └─3.attn.proj.1.linear.weight                                           ├─65,536\n",
            "│    │    └─3.attn.proj.1.bn.weight                                               ├─256\n",
            "│    │    └─3.attn.proj.1.bn.bias                                                 ├─256\n",
            "│    │    └─3.mlp.ln1.linear.weight                                               ├─131,072\n",
            "│    │    └─3.mlp.ln1.bn.weight                                                   ├─512\n",
            "│    │    └─3.mlp.ln1.bn.bias                                                     ├─512\n",
            "│    │    └─3.mlp.ln2.linear.weight                                               ├─131,072\n",
            "│    │    └─3.mlp.ln2.bn.weight                                                   ├─256\n",
            "│    │    └─3.mlp.ln2.bn.bias                                                     └─256\n",
            "│    │    └─LevitBlock: 3-9                             [32, 196, 256]            528,656\n",
            "│    │    │    └─attn.attention_biases                                            ├─784\n",
            "│    │    │    └─attn.qkv.linear.weight                                           ├─196,608\n",
            "│    │    │    └─attn.qkv.bn.weight                                               ├─768\n",
            "│    │    │    └─attn.qkv.bn.bias                                                 ├─768\n",
            "│    │    │    └─attn.proj.1.linear.weight                                        ├─65,536\n",
            "│    │    │    └─attn.proj.1.bn.weight                                            ├─256\n",
            "│    │    │    └─attn.proj.1.bn.bias                                              ├─256\n",
            "│    │    │    └─mlp.ln1.linear.weight                                            ├─131,072\n",
            "│    │    │    └─mlp.ln1.bn.weight                                                ├─512\n",
            "│    │    │    └─mlp.ln1.bn.bias                                                  ├─512\n",
            "│    │    │    └─mlp.ln2.linear.weight                                            ├─131,072\n",
            "│    │    │    └─mlp.ln2.bn.weight                                                ├─256\n",
            "│    │    │    └─mlp.ln2.bn.bias                                                  └─256\n",
            "│    │    └─LevitBlock: 3-10                            [32, 196, 256]            528,656\n",
            "│    │    │    └─attn.attention_biases                                            ├─784\n",
            "│    │    │    └─attn.qkv.linear.weight                                           ├─196,608\n",
            "│    │    │    └─attn.qkv.bn.weight                                               ├─768\n",
            "│    │    │    └─attn.qkv.bn.bias                                                 ├─768\n",
            "│    │    │    └─attn.proj.1.linear.weight                                        ├─65,536\n",
            "│    │    │    └─attn.proj.1.bn.weight                                            ├─256\n",
            "│    │    │    └─attn.proj.1.bn.bias                                              ├─256\n",
            "│    │    │    └─mlp.ln1.linear.weight                                            ├─131,072\n",
            "│    │    │    └─mlp.ln1.bn.weight                                                ├─512\n",
            "│    │    │    └─mlp.ln1.bn.bias                                                  ├─512\n",
            "│    │    │    └─mlp.ln2.linear.weight                                            ├─131,072\n",
            "│    │    │    └─mlp.ln2.bn.weight                                                ├─256\n",
            "│    │    │    └─mlp.ln2.bn.bias                                                  └─256\n",
            "│    │    └─LevitBlock: 3-11                            [32, 196, 256]            528,656\n",
            "│    │    │    └─attn.attention_biases                                            ├─784\n",
            "│    │    │    └─attn.qkv.linear.weight                                           ├─196,608\n",
            "│    │    │    └─attn.qkv.bn.weight                                               ├─768\n",
            "│    │    │    └─attn.qkv.bn.bias                                                 ├─768\n",
            "│    │    │    └─attn.proj.1.linear.weight                                        ├─65,536\n",
            "│    │    │    └─attn.proj.1.bn.weight                                            ├─256\n",
            "│    │    │    └─attn.proj.1.bn.bias                                              ├─256\n",
            "│    │    │    └─mlp.ln1.linear.weight                                            ├─131,072\n",
            "│    │    │    └─mlp.ln1.bn.weight                                                ├─512\n",
            "│    │    │    └─mlp.ln1.bn.bias                                                  ├─512\n",
            "│    │    │    └─mlp.ln2.linear.weight                                            ├─131,072\n",
            "│    │    │    └─mlp.ln2.bn.weight                                                ├─256\n",
            "│    │    │    └─mlp.ln2.bn.bias                                                  └─256\n",
            "│    │    └─LevitBlock: 3-12                            [32, 196, 256]            528,656\n",
            "│    │    │    └─attn.attention_biases                                            ├─784\n",
            "│    │    │    └─attn.qkv.linear.weight                                           ├─196,608\n",
            "│    │    │    └─attn.qkv.bn.weight                                               ├─768\n",
            "│    │    │    └─attn.qkv.bn.bias                                                 ├─768\n",
            "│    │    │    └─attn.proj.1.linear.weight                                        ├─65,536\n",
            "│    │    │    └─attn.proj.1.bn.weight                                            ├─256\n",
            "│    │    │    └─attn.proj.1.bn.bias                                              ├─256\n",
            "│    │    │    └─mlp.ln1.linear.weight                                            ├─131,072\n",
            "│    │    │    └─mlp.ln1.bn.weight                                                ├─512\n",
            "│    │    │    └─mlp.ln1.bn.bias                                                  ├─512\n",
            "│    │    │    └─mlp.ln2.linear.weight                                            ├─131,072\n",
            "│    │    │    └─mlp.ln2.bn.weight                                                ├─256\n",
            "│    │    │    └─mlp.ln2.bn.bias                                                  └─256\n",
            "├─LevitStage: 1-3                                       [32, 49, 384]             --\n",
            "│    └─downsample.conv.weight                                                     ├─884,736\n",
            "│    └─downsample.conv.bias                                                       ├─384\n",
            "│    └─blocks.0.attn.attention_biases                                             ├─294\n",
            "│    └─blocks.0.attn.qkv.linear.weight                                            ├─442,368\n",
            "│    └─blocks.0.attn.qkv.bn.weight                                                ├─1,152\n",
            "│    └─blocks.0.attn.qkv.bn.bias                                                  ├─1,152\n",
            "│    └─blocks.0.attn.proj.1.linear.weight                                         ├─147,456\n",
            "│    └─blocks.0.attn.proj.1.bn.weight                                             ├─384\n",
            "│    └─blocks.0.attn.proj.1.bn.bias                                               ├─384\n",
            "│    └─blocks.0.mlp.ln1.linear.weight                                             ├─294,912\n",
            "│    └─blocks.0.mlp.ln1.bn.weight                                                 ├─768\n",
            "│    └─blocks.0.mlp.ln1.bn.bias                                                   ├─768\n",
            "│    └─blocks.0.mlp.ln2.linear.weight                                             ├─294,912\n",
            "│    └─blocks.0.mlp.ln2.bn.weight                                                 ├─384\n",
            "│    └─blocks.0.mlp.ln2.bn.bias                                                   ├─384\n",
            "│    └─blocks.1.attn.attention_biases                                             ├─294\n",
            "│    └─blocks.1.attn.qkv.linear.weight                                            ├─442,368\n",
            "│    └─blocks.1.attn.qkv.bn.weight                                                ├─1,152\n",
            "│    └─blocks.1.attn.qkv.bn.bias                                                  ├─1,152\n",
            "│    └─blocks.1.attn.proj.1.linear.weight                                         ├─147,456\n",
            "│    └─blocks.1.attn.proj.1.bn.weight                                             ├─384\n",
            "│    └─blocks.1.attn.proj.1.bn.bias                                               ├─384\n",
            "│    └─blocks.1.mlp.ln1.linear.weight                                             ├─294,912\n",
            "│    └─blocks.1.mlp.ln1.bn.weight                                                 ├─768\n",
            "│    └─blocks.1.mlp.ln1.bn.bias                                                   ├─768\n",
            "│    └─blocks.1.mlp.ln2.linear.weight                                             ├─294,912\n",
            "│    └─blocks.1.mlp.ln2.bn.weight                                                 ├─384\n",
            "│    └─blocks.1.mlp.ln2.bn.bias                                                   ├─384\n",
            "│    └─blocks.2.attn.attention_biases                                             ├─294\n",
            "│    └─blocks.2.attn.qkv.linear.weight                                            ├─442,368\n",
            "│    └─blocks.2.attn.qkv.bn.weight                                                ├─1,152\n",
            "│    └─blocks.2.attn.qkv.bn.bias                                                  ├─1,152\n",
            "│    └─blocks.2.attn.proj.1.linear.weight                                         ├─147,456\n",
            "│    └─blocks.2.attn.proj.1.bn.weight                                             ├─384\n",
            "│    └─blocks.2.attn.proj.1.bn.bias                                               ├─384\n",
            "│    └─blocks.2.mlp.ln1.linear.weight                                             ├─294,912\n",
            "│    └─blocks.2.mlp.ln1.bn.weight                                                 ├─768\n",
            "│    └─blocks.2.mlp.ln1.bn.bias                                                   ├─768\n",
            "│    └─blocks.2.mlp.ln2.linear.weight                                             ├─294,912\n",
            "│    └─blocks.2.mlp.ln2.bn.weight                                                 ├─384\n",
            "│    └─blocks.2.mlp.ln2.bn.bias                                                   ├─384\n",
            "│    └─blocks.3.attn.attention_biases                                             ├─294\n",
            "│    └─blocks.3.attn.qkv.linear.weight                                            ├─442,368\n",
            "│    └─blocks.3.attn.qkv.bn.weight                                                ├─1,152\n",
            "│    └─blocks.3.attn.qkv.bn.bias                                                  ├─1,152\n",
            "│    └─blocks.3.attn.proj.1.linear.weight                                         ├─147,456\n",
            "│    └─blocks.3.attn.proj.1.bn.weight                                             ├─384\n",
            "│    └─blocks.3.attn.proj.1.bn.bias                                               ├─384\n",
            "│    └─blocks.3.mlp.ln1.linear.weight                                             ├─294,912\n",
            "│    └─blocks.3.mlp.ln1.bn.weight                                                 ├─768\n",
            "│    └─blocks.3.mlp.ln1.bn.bias                                                   ├─768\n",
            "│    └─blocks.3.mlp.ln2.linear.weight                                             ├─294,912\n",
            "│    └─blocks.3.mlp.ln2.bn.weight                                                 ├─384\n",
            "│    └─blocks.3.mlp.ln2.bn.bias                                                   └─384\n",
            "│    └─CNNDownsample: 2-10                              [32, 49, 384]             --\n",
            "│    │    └─conv.weight                                                           ├─884,736\n",
            "│    │    └─conv.bias                                                             └─384\n",
            "│    │    └─Conv2d: 3-13                                [32, 384, 7, 7]           885,120\n",
            "│    │    │    └─weight                                                           ├─884,736\n",
            "│    │    │    └─bias                                                             └─384\n",
            "│    │    └─Hardswish: 3-14                             [32, 384, 7, 7]           --\n",
            "│    └─Sequential: 2-11                                 [32, 49, 384]             --\n",
            "│    │    └─0.attn.attention_biases                                               ├─294\n",
            "│    │    └─0.attn.qkv.linear.weight                                              ├─442,368\n",
            "│    │    └─0.attn.qkv.bn.weight                                                  ├─1,152\n",
            "│    │    └─0.attn.qkv.bn.bias                                                    ├─1,152\n",
            "│    │    └─0.attn.proj.1.linear.weight                                           ├─147,456\n",
            "│    │    └─0.attn.proj.1.bn.weight                                               ├─384\n",
            "│    │    └─0.attn.proj.1.bn.bias                                                 ├─384\n",
            "│    │    └─0.mlp.ln1.linear.weight                                               ├─294,912\n",
            "│    │    └─0.mlp.ln1.bn.weight                                                   ├─768\n",
            "│    │    └─0.mlp.ln1.bn.bias                                                     ├─768\n",
            "│    │    └─0.mlp.ln2.linear.weight                                               ├─294,912\n",
            "│    │    └─0.mlp.ln2.bn.weight                                                   ├─384\n",
            "│    │    └─0.mlp.ln2.bn.bias                                                     ├─384\n",
            "│    │    └─1.attn.attention_biases                                               ├─294\n",
            "│    │    └─1.attn.qkv.linear.weight                                              ├─442,368\n",
            "│    │    └─1.attn.qkv.bn.weight                                                  ├─1,152\n",
            "│    │    └─1.attn.qkv.bn.bias                                                    ├─1,152\n",
            "│    │    └─1.attn.proj.1.linear.weight                                           ├─147,456\n",
            "│    │    └─1.attn.proj.1.bn.weight                                               ├─384\n",
            "│    │    └─1.attn.proj.1.bn.bias                                                 ├─384\n",
            "│    │    └─1.mlp.ln1.linear.weight                                               ├─294,912\n",
            "│    │    └─1.mlp.ln1.bn.weight                                                   ├─768\n",
            "│    │    └─1.mlp.ln1.bn.bias                                                     ├─768\n",
            "│    │    └─1.mlp.ln2.linear.weight                                               ├─294,912\n",
            "│    │    └─1.mlp.ln2.bn.weight                                                   ├─384\n",
            "│    │    └─1.mlp.ln2.bn.bias                                                     ├─384\n",
            "│    │    └─2.attn.attention_biases                                               ├─294\n",
            "│    │    └─2.attn.qkv.linear.weight                                              ├─442,368\n",
            "│    │    └─2.attn.qkv.bn.weight                                                  ├─1,152\n",
            "│    │    └─2.attn.qkv.bn.bias                                                    ├─1,152\n",
            "│    │    └─2.attn.proj.1.linear.weight                                           ├─147,456\n",
            "│    │    └─2.attn.proj.1.bn.weight                                               ├─384\n",
            "│    │    └─2.attn.proj.1.bn.bias                                                 ├─384\n",
            "│    │    └─2.mlp.ln1.linear.weight                                               ├─294,912\n",
            "│    │    └─2.mlp.ln1.bn.weight                                                   ├─768\n",
            "│    │    └─2.mlp.ln1.bn.bias                                                     ├─768\n",
            "│    │    └─2.mlp.ln2.linear.weight                                               ├─294,912\n",
            "│    │    └─2.mlp.ln2.bn.weight                                                   ├─384\n",
            "│    │    └─2.mlp.ln2.bn.bias                                                     ├─384\n",
            "│    │    └─3.attn.attention_biases                                               ├─294\n",
            "│    │    └─3.attn.qkv.linear.weight                                              ├─442,368\n",
            "│    │    └─3.attn.qkv.bn.weight                                                  ├─1,152\n",
            "│    │    └─3.attn.qkv.bn.bias                                                    ├─1,152\n",
            "│    │    └─3.attn.proj.1.linear.weight                                           ├─147,456\n",
            "│    │    └─3.attn.proj.1.bn.weight                                               ├─384\n",
            "│    │    └─3.attn.proj.1.bn.bias                                                 ├─384\n",
            "│    │    └─3.mlp.ln1.linear.weight                                               ├─294,912\n",
            "│    │    └─3.mlp.ln1.bn.weight                                                   ├─768\n",
            "│    │    └─3.mlp.ln1.bn.bias                                                     ├─768\n",
            "│    │    └─3.mlp.ln2.linear.weight                                               ├─294,912\n",
            "│    │    └─3.mlp.ln2.bn.weight                                                   ├─384\n",
            "│    │    └─3.mlp.ln2.bn.bias                                                     └─384\n",
            "│    │    └─LevitBlock: 3-15                            [32, 49, 384]             1,185,318\n",
            "│    │    │    └─attn.attention_biases                                            ├─294\n",
            "│    │    │    └─attn.qkv.linear.weight                                           ├─442,368\n",
            "│    │    │    └─attn.qkv.bn.weight                                               ├─1,152\n",
            "│    │    │    └─attn.qkv.bn.bias                                                 ├─1,152\n",
            "│    │    │    └─attn.proj.1.linear.weight                                        ├─147,456\n",
            "│    │    │    └─attn.proj.1.bn.weight                                            ├─384\n",
            "│    │    │    └─attn.proj.1.bn.bias                                              ├─384\n",
            "│    │    │    └─mlp.ln1.linear.weight                                            ├─294,912\n",
            "│    │    │    └─mlp.ln1.bn.weight                                                ├─768\n",
            "│    │    │    └─mlp.ln1.bn.bias                                                  ├─768\n",
            "│    │    │    └─mlp.ln2.linear.weight                                            ├─294,912\n",
            "│    │    │    └─mlp.ln2.bn.weight                                                ├─384\n",
            "│    │    │    └─mlp.ln2.bn.bias                                                  └─384\n",
            "│    │    └─LevitBlock: 3-16                            [32, 49, 384]             1,185,318\n",
            "│    │    │    └─attn.attention_biases                                            ├─294\n",
            "│    │    │    └─attn.qkv.linear.weight                                           ├─442,368\n",
            "│    │    │    └─attn.qkv.bn.weight                                               ├─1,152\n",
            "│    │    │    └─attn.qkv.bn.bias                                                 ├─1,152\n",
            "│    │    │    └─attn.proj.1.linear.weight                                        ├─147,456\n",
            "│    │    │    └─attn.proj.1.bn.weight                                            ├─384\n",
            "│    │    │    └─attn.proj.1.bn.bias                                              ├─384\n",
            "│    │    │    └─mlp.ln1.linear.weight                                            ├─294,912\n",
            "│    │    │    └─mlp.ln1.bn.weight                                                ├─768\n",
            "│    │    │    └─mlp.ln1.bn.bias                                                  ├─768\n",
            "│    │    │    └─mlp.ln2.linear.weight                                            ├─294,912\n",
            "│    │    │    └─mlp.ln2.bn.weight                                                ├─384\n",
            "│    │    │    └─mlp.ln2.bn.bias                                                  └─384\n",
            "│    │    └─LevitBlock: 3-17                            [32, 49, 384]             1,185,318\n",
            "│    │    │    └─attn.attention_biases                                            ├─294\n",
            "│    │    │    └─attn.qkv.linear.weight                                           ├─442,368\n",
            "│    │    │    └─attn.qkv.bn.weight                                               ├─1,152\n",
            "│    │    │    └─attn.qkv.bn.bias                                                 ├─1,152\n",
            "│    │    │    └─attn.proj.1.linear.weight                                        ├─147,456\n",
            "│    │    │    └─attn.proj.1.bn.weight                                            ├─384\n",
            "│    │    │    └─attn.proj.1.bn.bias                                              ├─384\n",
            "│    │    │    └─mlp.ln1.linear.weight                                            ├─294,912\n",
            "│    │    │    └─mlp.ln1.bn.weight                                                ├─768\n",
            "│    │    │    └─mlp.ln1.bn.bias                                                  ├─768\n",
            "│    │    │    └─mlp.ln2.linear.weight                                            ├─294,912\n",
            "│    │    │    └─mlp.ln2.bn.weight                                                ├─384\n",
            "│    │    │    └─mlp.ln2.bn.bias                                                  └─384\n",
            "│    │    └─LevitBlock: 3-18                            [32, 49, 384]             1,185,318\n",
            "│    │    │    └─attn.attention_biases                                            ├─294\n",
            "│    │    │    └─attn.qkv.linear.weight                                           ├─442,368\n",
            "│    │    │    └─attn.qkv.bn.weight                                               ├─1,152\n",
            "│    │    │    └─attn.qkv.bn.bias                                                 ├─1,152\n",
            "│    │    │    └─attn.proj.1.linear.weight                                        ├─147,456\n",
            "│    │    │    └─attn.proj.1.bn.weight                                            ├─384\n",
            "│    │    │    └─attn.proj.1.bn.bias                                              ├─384\n",
            "│    │    │    └─mlp.ln1.linear.weight                                            ├─294,912\n",
            "│    │    │    └─mlp.ln1.bn.weight                                                ├─768\n",
            "│    │    │    └─mlp.ln1.bn.bias                                                  ├─768\n",
            "│    │    │    └─mlp.ln2.linear.weight                                            ├─294,912\n",
            "│    │    │    └─mlp.ln2.bn.weight                                                ├─384\n",
            "│    │    │    └─mlp.ln2.bn.bias                                                  └─384\n",
            "├─Sequential: 1-4                                       [32, 512, 7, 7]           --\n",
            "│    └─0.weight                                                                   ├─196,608\n",
            "│    └─0.bias                                                                     ├─512\n",
            "│    └─1.weight                                                                   ├─512\n",
            "│    └─1.bias                                                                     └─512\n",
            "│    └─Conv2d: 2-12                                     [32, 512, 7, 7]           197,120\n",
            "│    │    └─weight                                                                ├─196,608\n",
            "│    │    └─bias                                                                  └─512\n",
            "│    └─BatchNorm2d: 2-13                                [32, 512, 7, 7]           1,024\n",
            "│    │    └─weight                                                                ├─512\n",
            "│    │    └─bias                                                                  └─512\n",
            "│    └─ReLU: 2-14                                       [32, 512, 7, 7]           --\n",
            "├─NormLinear: 1-5                                       [32, 9]                   --\n",
            "│    └─bn.weight                                                                  ├─512\n",
            "│    └─bn.bias                                                                    ├─512\n",
            "│    └─linear.weight                                                              ├─4,608\n",
            "│    └─linear.bias                                                                └─9\n",
            "│    └─BatchNorm1d: 2-15                                [32, 512]                 1,024\n",
            "│    │    └─weight                                                                ├─512\n",
            "│    │    └─bias                                                                  └─512\n",
            "│    └─Dropout: 2-16                                    [32, 512]                 --\n",
            "│    └─Linear: 2-17                                     [32, 9]                   4,617\n",
            "│    │    └─weight                                                                ├─4,608\n",
            "│    │    └─bias                                                                  └─9\n",
            "├─NormLinear: 1-6                                       [32, 9]                   --\n",
            "│    └─bn.weight                                                                  ├─512\n",
            "│    └─bn.bias                                                                    ├─512\n",
            "│    └─linear.weight                                                              ├─4,608\n",
            "│    └─linear.bias                                                                └─9\n",
            "│    └─BatchNorm1d: 2-18                                [32, 512]                 1,024\n",
            "│    │    └─weight                                                                ├─512\n",
            "│    │    └─bias                                                                  └─512\n",
            "│    └─Dropout: 2-19                                    [32, 512]                 --\n",
            "│    └─Linear: 2-20                                     [32, 9]                   4,617\n",
            "│    │    └─weight                                                                ├─4,608\n",
            "│    │    └─bias                                                                  └─9\n",
            "=========================================================================================================\n",
            "Total params: 8,339,338\n",
            "Trainable params: 8,339,338\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (Units.GIGABYTES): 28.27\n",
            "=========================================================================================================\n",
            "Input size (MB): 19.27\n",
            "Forward/backward pass size (MB): 1392.35\n",
            "Params size (MB): 33.34\n",
            "Estimated Total Size (MB): 1444.96\n",
            "=========================================================================================================\n",
            "=========================================================================================================\n",
            "Layer (type:depth-idx)                                  Output Shape              Param #\n",
            "=========================================================================================================\n",
            "LevitDistilled                                          [32, 9]                   --\n",
            "├─Stem16: 1-1                                           [32, 256, 14, 14]         --\n",
            "│    └─conv1.linear.weight                                                        ├─864\n",
            "│    └─conv1.bn.weight                                                            ├─32\n",
            "│    └─conv1.bn.bias                                                              ├─32\n",
            "│    └─conv2.linear.weight                                                        ├─18,432\n",
            "│    └─conv2.bn.weight                                                            ├─64\n",
            "│    └─conv2.bn.bias                                                              ├─64\n",
            "│    └─conv3.linear.weight                                                        ├─73,728\n",
            "│    └─conv3.bn.weight                                                            ├─128\n",
            "│    └─conv3.bn.bias                                                              ├─128\n",
            "│    └─conv4.linear.weight                                                        ├─294,912\n",
            "│    └─conv4.bn.weight                                                            ├─256\n",
            "│    └─conv4.bn.bias                                                              └─256\n",
            "│    └─ConvNorm: 2-1                                    [32, 32, 112, 112]        --\n",
            "│    │    └─linear.weight                                                         ├─864\n",
            "│    │    └─bn.weight                                                             ├─32\n",
            "│    │    └─bn.bias                                                               └─32\n",
            "│    │    └─Conv2d: 3-1                                 [32, 32, 112, 112]        864\n",
            "│    │    │    └─weight                                                           └─864\n",
            "│    │    └─BatchNorm2d: 3-2                            [32, 32, 112, 112]        64\n",
            "│    │    │    └─weight                                                           ├─32\n",
            "│    │    │    └─bias                                                             └─32\n",
            "│    └─Hardswish: 2-2                                   [32, 32, 112, 112]        --\n",
            "│    └─ConvNorm: 2-3                                    [32, 64, 56, 56]          --\n",
            "│    │    └─linear.weight                                                         ├─18,432\n",
            "│    │    └─bn.weight                                                             ├─64\n",
            "│    │    └─bn.bias                                                               └─64\n",
            "│    │    └─Conv2d: 3-3                                 [32, 64, 56, 56]          18,432\n",
            "│    │    │    └─weight                                                           └─18,432\n",
            "│    │    └─BatchNorm2d: 3-4                            [32, 64, 56, 56]          128\n",
            "│    │    │    └─weight                                                           ├─64\n",
            "│    │    │    └─bias                                                             └─64\n",
            "│    └─Hardswish: 2-4                                   [32, 64, 56, 56]          --\n",
            "│    └─ConvNorm: 2-5                                    [32, 128, 28, 28]         --\n",
            "│    │    └─linear.weight                                                         ├─73,728\n",
            "│    │    └─bn.weight                                                             ├─128\n",
            "│    │    └─bn.bias                                                               └─128\n",
            "│    │    └─Conv2d: 3-5                                 [32, 128, 28, 28]         73,728\n",
            "│    │    │    └─weight                                                           └─73,728\n",
            "│    │    └─BatchNorm2d: 3-6                            [32, 128, 28, 28]         256\n",
            "│    │    │    └─weight                                                           ├─128\n",
            "│    │    │    └─bias                                                             └─128\n",
            "│    └─Hardswish: 2-6                                   [32, 128, 28, 28]         --\n",
            "│    └─ConvNorm: 2-7                                    [32, 256, 14, 14]         --\n",
            "│    │    └─linear.weight                                                         ├─294,912\n",
            "│    │    └─bn.weight                                                             ├─256\n",
            "│    │    └─bn.bias                                                               └─256\n",
            "│    │    └─Conv2d: 3-7                                 [32, 256, 14, 14]         294,912\n",
            "│    │    │    └─weight                                                           └─294,912\n",
            "│    │    └─BatchNorm2d: 3-8                            [32, 256, 14, 14]         512\n",
            "│    │    │    └─weight                                                           ├─256\n",
            "│    │    │    └─bias                                                             └─256\n",
            "├─LevitStage: 1-2                                       [32, 196, 256]            --\n",
            "│    └─blocks.0.attn.attention_biases                                             ├─784\n",
            "│    └─blocks.0.attn.qkv.linear.weight                                            ├─196,608\n",
            "│    └─blocks.0.attn.qkv.bn.weight                                                ├─768\n",
            "│    └─blocks.0.attn.qkv.bn.bias                                                  ├─768\n",
            "│    └─blocks.0.attn.proj.1.linear.weight                                         ├─65,536\n",
            "│    └─blocks.0.attn.proj.1.bn.weight                                             ├─256\n",
            "│    └─blocks.0.attn.proj.1.bn.bias                                               ├─256\n",
            "│    └─blocks.0.mlp.ln1.linear.weight                                             ├─131,072\n",
            "│    └─blocks.0.mlp.ln1.bn.weight                                                 ├─512\n",
            "│    └─blocks.0.mlp.ln1.bn.bias                                                   ├─512\n",
            "│    └─blocks.0.mlp.ln2.linear.weight                                             ├─131,072\n",
            "│    └─blocks.0.mlp.ln2.bn.weight                                                 ├─256\n",
            "│    └─blocks.0.mlp.ln2.bn.bias                                                   ├─256\n",
            "│    └─blocks.1.attn.attention_biases                                             ├─784\n",
            "│    └─blocks.1.attn.qkv.linear.weight                                            ├─196,608\n",
            "│    └─blocks.1.attn.qkv.bn.weight                                                ├─768\n",
            "│    └─blocks.1.attn.qkv.bn.bias                                                  ├─768\n",
            "│    └─blocks.1.attn.proj.1.linear.weight                                         ├─65,536\n",
            "│    └─blocks.1.attn.proj.1.bn.weight                                             ├─256\n",
            "│    └─blocks.1.attn.proj.1.bn.bias                                               ├─256\n",
            "│    └─blocks.1.mlp.ln1.linear.weight                                             ├─131,072\n",
            "│    └─blocks.1.mlp.ln1.bn.weight                                                 ├─512\n",
            "│    └─blocks.1.mlp.ln1.bn.bias                                                   ├─512\n",
            "│    └─blocks.1.mlp.ln2.linear.weight                                             ├─131,072\n",
            "│    └─blocks.1.mlp.ln2.bn.weight                                                 ├─256\n",
            "│    └─blocks.1.mlp.ln2.bn.bias                                                   ├─256\n",
            "│    └─blocks.2.attn.attention_biases                                             ├─784\n",
            "│    └─blocks.2.attn.qkv.linear.weight                                            ├─196,608\n",
            "│    └─blocks.2.attn.qkv.bn.weight                                                ├─768\n",
            "│    └─blocks.2.attn.qkv.bn.bias                                                  ├─768\n",
            "│    └─blocks.2.attn.proj.1.linear.weight                                         ├─65,536\n",
            "│    └─blocks.2.attn.proj.1.bn.weight                                             ├─256\n",
            "│    └─blocks.2.attn.proj.1.bn.bias                                               ├─256\n",
            "│    └─blocks.2.mlp.ln1.linear.weight                                             ├─131,072\n",
            "│    └─blocks.2.mlp.ln1.bn.weight                                                 ├─512\n",
            "│    └─blocks.2.mlp.ln1.bn.bias                                                   ├─512\n",
            "│    └─blocks.2.mlp.ln2.linear.weight                                             ├─131,072\n",
            "│    └─blocks.2.mlp.ln2.bn.weight                                                 ├─256\n",
            "│    └─blocks.2.mlp.ln2.bn.bias                                                   ├─256\n",
            "│    └─blocks.3.attn.attention_biases                                             ├─784\n",
            "│    └─blocks.3.attn.qkv.linear.weight                                            ├─196,608\n",
            "│    └─blocks.3.attn.qkv.bn.weight                                                ├─768\n",
            "│    └─blocks.3.attn.qkv.bn.bias                                                  ├─768\n",
            "│    └─blocks.3.attn.proj.1.linear.weight                                         ├─65,536\n",
            "│    └─blocks.3.attn.proj.1.bn.weight                                             ├─256\n",
            "│    └─blocks.3.attn.proj.1.bn.bias                                               ├─256\n",
            "│    └─blocks.3.mlp.ln1.linear.weight                                             ├─131,072\n",
            "│    └─blocks.3.mlp.ln1.bn.weight                                                 ├─512\n",
            "│    └─blocks.3.mlp.ln1.bn.bias                                                   ├─512\n",
            "│    └─blocks.3.mlp.ln2.linear.weight                                             ├─131,072\n",
            "│    └─blocks.3.mlp.ln2.bn.weight                                                 ├─256\n",
            "│    └─blocks.3.mlp.ln2.bn.bias                                                   └─256\n",
            "│    └─Identity: 2-8                                    [32, 196, 256]            --\n",
            "│    └─Sequential: 2-9                                  [32, 196, 256]            --\n",
            "│    │    └─0.attn.attention_biases                                               ├─784\n",
            "│    │    └─0.attn.qkv.linear.weight                                              ├─196,608\n",
            "│    │    └─0.attn.qkv.bn.weight                                                  ├─768\n",
            "│    │    └─0.attn.qkv.bn.bias                                                    ├─768\n",
            "│    │    └─0.attn.proj.1.linear.weight                                           ├─65,536\n",
            "│    │    └─0.attn.proj.1.bn.weight                                               ├─256\n",
            "│    │    └─0.attn.proj.1.bn.bias                                                 ├─256\n",
            "│    │    └─0.mlp.ln1.linear.weight                                               ├─131,072\n",
            "│    │    └─0.mlp.ln1.bn.weight                                                   ├─512\n",
            "│    │    └─0.mlp.ln1.bn.bias                                                     ├─512\n",
            "│    │    └─0.mlp.ln2.linear.weight                                               ├─131,072\n",
            "│    │    └─0.mlp.ln2.bn.weight                                                   ├─256\n",
            "│    │    └─0.mlp.ln2.bn.bias                                                     ├─256\n",
            "│    │    └─1.attn.attention_biases                                               ├─784\n",
            "│    │    └─1.attn.qkv.linear.weight                                              ├─196,608\n",
            "│    │    └─1.attn.qkv.bn.weight                                                  ├─768\n",
            "│    │    └─1.attn.qkv.bn.bias                                                    ├─768\n",
            "│    │    └─1.attn.proj.1.linear.weight                                           ├─65,536\n",
            "│    │    └─1.attn.proj.1.bn.weight                                               ├─256\n",
            "│    │    └─1.attn.proj.1.bn.bias                                                 ├─256\n",
            "│    │    └─1.mlp.ln1.linear.weight                                               ├─131,072\n",
            "│    │    └─1.mlp.ln1.bn.weight                                                   ├─512\n",
            "│    │    └─1.mlp.ln1.bn.bias                                                     ├─512\n",
            "│    │    └─1.mlp.ln2.linear.weight                                               ├─131,072\n",
            "│    │    └─1.mlp.ln2.bn.weight                                                   ├─256\n",
            "│    │    └─1.mlp.ln2.bn.bias                                                     ├─256\n",
            "│    │    └─2.attn.attention_biases                                               ├─784\n",
            "│    │    └─2.attn.qkv.linear.weight                                              ├─196,608\n",
            "│    │    └─2.attn.qkv.bn.weight                                                  ├─768\n",
            "│    │    └─2.attn.qkv.bn.bias                                                    ├─768\n",
            "│    │    └─2.attn.proj.1.linear.weight                                           ├─65,536\n",
            "│    │    └─2.attn.proj.1.bn.weight                                               ├─256\n",
            "│    │    └─2.attn.proj.1.bn.bias                                                 ├─256\n",
            "│    │    └─2.mlp.ln1.linear.weight                                               ├─131,072\n",
            "│    │    └─2.mlp.ln1.bn.weight                                                   ├─512\n",
            "│    │    └─2.mlp.ln1.bn.bias                                                     ├─512\n",
            "│    │    └─2.mlp.ln2.linear.weight                                               ├─131,072\n",
            "│    │    └─2.mlp.ln2.bn.weight                                                   ├─256\n",
            "│    │    └─2.mlp.ln2.bn.bias                                                     ├─256\n",
            "│    │    └─3.attn.attention_biases                                               ├─784\n",
            "│    │    └─3.attn.qkv.linear.weight                                              ├─196,608\n",
            "│    │    └─3.attn.qkv.bn.weight                                                  ├─768\n",
            "│    │    └─3.attn.qkv.bn.bias                                                    ├─768\n",
            "│    │    └─3.attn.proj.1.linear.weight                                           ├─65,536\n",
            "│    │    └─3.attn.proj.1.bn.weight                                               ├─256\n",
            "│    │    └─3.attn.proj.1.bn.bias                                                 ├─256\n",
            "│    │    └─3.mlp.ln1.linear.weight                                               ├─131,072\n",
            "│    │    └─3.mlp.ln1.bn.weight                                                   ├─512\n",
            "│    │    └─3.mlp.ln1.bn.bias                                                     ├─512\n",
            "│    │    └─3.mlp.ln2.linear.weight                                               ├─131,072\n",
            "│    │    └─3.mlp.ln2.bn.weight                                                   ├─256\n",
            "│    │    └─3.mlp.ln2.bn.bias                                                     └─256\n",
            "│    │    └─LevitBlock: 3-9                             [32, 196, 256]            528,656\n",
            "│    │    │    └─attn.attention_biases                                            ├─784\n",
            "│    │    │    └─attn.qkv.linear.weight                                           ├─196,608\n",
            "│    │    │    └─attn.qkv.bn.weight                                               ├─768\n",
            "│    │    │    └─attn.qkv.bn.bias                                                 ├─768\n",
            "│    │    │    └─attn.proj.1.linear.weight                                        ├─65,536\n",
            "│    │    │    └─attn.proj.1.bn.weight                                            ├─256\n",
            "│    │    │    └─attn.proj.1.bn.bias                                              ├─256\n",
            "│    │    │    └─mlp.ln1.linear.weight                                            ├─131,072\n",
            "│    │    │    └─mlp.ln1.bn.weight                                                ├─512\n",
            "│    │    │    └─mlp.ln1.bn.bias                                                  ├─512\n",
            "│    │    │    └─mlp.ln2.linear.weight                                            ├─131,072\n",
            "│    │    │    └─mlp.ln2.bn.weight                                                ├─256\n",
            "│    │    │    └─mlp.ln2.bn.bias                                                  └─256\n",
            "│    │    └─LevitBlock: 3-10                            [32, 196, 256]            528,656\n",
            "│    │    │    └─attn.attention_biases                                            ├─784\n",
            "│    │    │    └─attn.qkv.linear.weight                                           ├─196,608\n",
            "│    │    │    └─attn.qkv.bn.weight                                               ├─768\n",
            "│    │    │    └─attn.qkv.bn.bias                                                 ├─768\n",
            "│    │    │    └─attn.proj.1.linear.weight                                        ├─65,536\n",
            "│    │    │    └─attn.proj.1.bn.weight                                            ├─256\n",
            "│    │    │    └─attn.proj.1.bn.bias                                              ├─256\n",
            "│    │    │    └─mlp.ln1.linear.weight                                            ├─131,072\n",
            "│    │    │    └─mlp.ln1.bn.weight                                                ├─512\n",
            "│    │    │    └─mlp.ln1.bn.bias                                                  ├─512\n",
            "│    │    │    └─mlp.ln2.linear.weight                                            ├─131,072\n",
            "│    │    │    └─mlp.ln2.bn.weight                                                ├─256\n",
            "│    │    │    └─mlp.ln2.bn.bias                                                  └─256\n",
            "│    │    └─LevitBlock: 3-11                            [32, 196, 256]            528,656\n",
            "│    │    │    └─attn.attention_biases                                            ├─784\n",
            "│    │    │    └─attn.qkv.linear.weight                                           ├─196,608\n",
            "│    │    │    └─attn.qkv.bn.weight                                               ├─768\n",
            "│    │    │    └─attn.qkv.bn.bias                                                 ├─768\n",
            "│    │    │    └─attn.proj.1.linear.weight                                        ├─65,536\n",
            "│    │    │    └─attn.proj.1.bn.weight                                            ├─256\n",
            "│    │    │    └─attn.proj.1.bn.bias                                              ├─256\n",
            "│    │    │    └─mlp.ln1.linear.weight                                            ├─131,072\n",
            "│    │    │    └─mlp.ln1.bn.weight                                                ├─512\n",
            "│    │    │    └─mlp.ln1.bn.bias                                                  ├─512\n",
            "│    │    │    └─mlp.ln2.linear.weight                                            ├─131,072\n",
            "│    │    │    └─mlp.ln2.bn.weight                                                ├─256\n",
            "│    │    │    └─mlp.ln2.bn.bias                                                  └─256\n",
            "│    │    └─LevitBlock: 3-12                            [32, 196, 256]            528,656\n",
            "│    │    │    └─attn.attention_biases                                            ├─784\n",
            "│    │    │    └─attn.qkv.linear.weight                                           ├─196,608\n",
            "│    │    │    └─attn.qkv.bn.weight                                               ├─768\n",
            "│    │    │    └─attn.qkv.bn.bias                                                 ├─768\n",
            "│    │    │    └─attn.proj.1.linear.weight                                        ├─65,536\n",
            "│    │    │    └─attn.proj.1.bn.weight                                            ├─256\n",
            "│    │    │    └─attn.proj.1.bn.bias                                              ├─256\n",
            "│    │    │    └─mlp.ln1.linear.weight                                            ├─131,072\n",
            "│    │    │    └─mlp.ln1.bn.weight                                                ├─512\n",
            "│    │    │    └─mlp.ln1.bn.bias                                                  ├─512\n",
            "│    │    │    └─mlp.ln2.linear.weight                                            ├─131,072\n",
            "│    │    │    └─mlp.ln2.bn.weight                                                ├─256\n",
            "│    │    │    └─mlp.ln2.bn.bias                                                  └─256\n",
            "├─LevitStage: 1-3                                       [32, 49, 384]             --\n",
            "│    └─downsample.conv.weight                                                     ├─884,736\n",
            "│    └─downsample.conv.bias                                                       ├─384\n",
            "│    └─blocks.0.attn.attention_biases                                             ├─294\n",
            "│    └─blocks.0.attn.qkv.linear.weight                                            ├─442,368\n",
            "│    └─blocks.0.attn.qkv.bn.weight                                                ├─1,152\n",
            "│    └─blocks.0.attn.qkv.bn.bias                                                  ├─1,152\n",
            "│    └─blocks.0.attn.proj.1.linear.weight                                         ├─147,456\n",
            "│    └─blocks.0.attn.proj.1.bn.weight                                             ├─384\n",
            "│    └─blocks.0.attn.proj.1.bn.bias                                               ├─384\n",
            "│    └─blocks.0.mlp.ln1.linear.weight                                             ├─294,912\n",
            "│    └─blocks.0.mlp.ln1.bn.weight                                                 ├─768\n",
            "│    └─blocks.0.mlp.ln1.bn.bias                                                   ├─768\n",
            "│    └─blocks.0.mlp.ln2.linear.weight                                             ├─294,912\n",
            "│    └─blocks.0.mlp.ln2.bn.weight                                                 ├─384\n",
            "│    └─blocks.0.mlp.ln2.bn.bias                                                   ├─384\n",
            "│    └─blocks.1.attn.attention_biases                                             ├─294\n",
            "│    └─blocks.1.attn.qkv.linear.weight                                            ├─442,368\n",
            "│    └─blocks.1.attn.qkv.bn.weight                                                ├─1,152\n",
            "│    └─blocks.1.attn.qkv.bn.bias                                                  ├─1,152\n",
            "│    └─blocks.1.attn.proj.1.linear.weight                                         ├─147,456\n",
            "│    └─blocks.1.attn.proj.1.bn.weight                                             ├─384\n",
            "│    └─blocks.1.attn.proj.1.bn.bias                                               ├─384\n",
            "│    └─blocks.1.mlp.ln1.linear.weight                                             ├─294,912\n",
            "│    └─blocks.1.mlp.ln1.bn.weight                                                 ├─768\n",
            "│    └─blocks.1.mlp.ln1.bn.bias                                                   ├─768\n",
            "│    └─blocks.1.mlp.ln2.linear.weight                                             ├─294,912\n",
            "│    └─blocks.1.mlp.ln2.bn.weight                                                 ├─384\n",
            "│    └─blocks.1.mlp.ln2.bn.bias                                                   ├─384\n",
            "│    └─blocks.2.attn.attention_biases                                             ├─294\n",
            "│    └─blocks.2.attn.qkv.linear.weight                                            ├─442,368\n",
            "│    └─blocks.2.attn.qkv.bn.weight                                                ├─1,152\n",
            "│    └─blocks.2.attn.qkv.bn.bias                                                  ├─1,152\n",
            "│    └─blocks.2.attn.proj.1.linear.weight                                         ├─147,456\n",
            "│    └─blocks.2.attn.proj.1.bn.weight                                             ├─384\n",
            "│    └─blocks.2.attn.proj.1.bn.bias                                               ├─384\n",
            "│    └─blocks.2.mlp.ln1.linear.weight                                             ├─294,912\n",
            "│    └─blocks.2.mlp.ln1.bn.weight                                                 ├─768\n",
            "│    └─blocks.2.mlp.ln1.bn.bias                                                   ├─768\n",
            "│    └─blocks.2.mlp.ln2.linear.weight                                             ├─294,912\n",
            "│    └─blocks.2.mlp.ln2.bn.weight                                                 ├─384\n",
            "│    └─blocks.2.mlp.ln2.bn.bias                                                   ├─384\n",
            "│    └─blocks.3.attn.attention_biases                                             ├─294\n",
            "│    └─blocks.3.attn.qkv.linear.weight                                            ├─442,368\n",
            "│    └─blocks.3.attn.qkv.bn.weight                                                ├─1,152\n",
            "│    └─blocks.3.attn.qkv.bn.bias                                                  ├─1,152\n",
            "│    └─blocks.3.attn.proj.1.linear.weight                                         ├─147,456\n",
            "│    └─blocks.3.attn.proj.1.bn.weight                                             ├─384\n",
            "│    └─blocks.3.attn.proj.1.bn.bias                                               ├─384\n",
            "│    └─blocks.3.mlp.ln1.linear.weight                                             ├─294,912\n",
            "│    └─blocks.3.mlp.ln1.bn.weight                                                 ├─768\n",
            "│    └─blocks.3.mlp.ln1.bn.bias                                                   ├─768\n",
            "│    └─blocks.3.mlp.ln2.linear.weight                                             ├─294,912\n",
            "│    └─blocks.3.mlp.ln2.bn.weight                                                 ├─384\n",
            "│    └─blocks.3.mlp.ln2.bn.bias                                                   └─384\n",
            "│    └─CNNDownsample: 2-10                              [32, 49, 384]             --\n",
            "│    │    └─conv.weight                                                           ├─884,736\n",
            "│    │    └─conv.bias                                                             └─384\n",
            "│    │    └─Conv2d: 3-13                                [32, 384, 7, 7]           885,120\n",
            "│    │    │    └─weight                                                           ├─884,736\n",
            "│    │    │    └─bias                                                             └─384\n",
            "│    │    └─Hardswish: 3-14                             [32, 384, 7, 7]           --\n",
            "│    └─Sequential: 2-11                                 [32, 49, 384]             --\n",
            "│    │    └─0.attn.attention_biases                                               ├─294\n",
            "│    │    └─0.attn.qkv.linear.weight                                              ├─442,368\n",
            "│    │    └─0.attn.qkv.bn.weight                                                  ├─1,152\n",
            "│    │    └─0.attn.qkv.bn.bias                                                    ├─1,152\n",
            "│    │    └─0.attn.proj.1.linear.weight                                           ├─147,456\n",
            "│    │    └─0.attn.proj.1.bn.weight                                               ├─384\n",
            "│    │    └─0.attn.proj.1.bn.bias                                                 ├─384\n",
            "│    │    └─0.mlp.ln1.linear.weight                                               ├─294,912\n",
            "│    │    └─0.mlp.ln1.bn.weight                                                   ├─768\n",
            "│    │    └─0.mlp.ln1.bn.bias                                                     ├─768\n",
            "│    │    └─0.mlp.ln2.linear.weight                                               ├─294,912\n",
            "│    │    └─0.mlp.ln2.bn.weight                                                   ├─384\n",
            "│    │    └─0.mlp.ln2.bn.bias                                                     ├─384\n",
            "│    │    └─1.attn.attention_biases                                               ├─294\n",
            "│    │    └─1.attn.qkv.linear.weight                                              ├─442,368\n",
            "│    │    └─1.attn.qkv.bn.weight                                                  ├─1,152\n",
            "│    │    └─1.attn.qkv.bn.bias                                                    ├─1,152\n",
            "│    │    └─1.attn.proj.1.linear.weight                                           ├─147,456\n",
            "│    │    └─1.attn.proj.1.bn.weight                                               ├─384\n",
            "│    │    └─1.attn.proj.1.bn.bias                                                 ├─384\n",
            "│    │    └─1.mlp.ln1.linear.weight                                               ├─294,912\n",
            "│    │    └─1.mlp.ln1.bn.weight                                                   ├─768\n",
            "│    │    └─1.mlp.ln1.bn.bias                                                     ├─768\n",
            "│    │    └─1.mlp.ln2.linear.weight                                               ├─294,912\n",
            "│    │    └─1.mlp.ln2.bn.weight                                                   ├─384\n",
            "│    │    └─1.mlp.ln2.bn.bias                                                     ├─384\n",
            "│    │    └─2.attn.attention_biases                                               ├─294\n",
            "│    │    └─2.attn.qkv.linear.weight                                              ├─442,368\n",
            "│    │    └─2.attn.qkv.bn.weight                                                  ├─1,152\n",
            "│    │    └─2.attn.qkv.bn.bias                                                    ├─1,152\n",
            "│    │    └─2.attn.proj.1.linear.weight                                           ├─147,456\n",
            "│    │    └─2.attn.proj.1.bn.weight                                               ├─384\n",
            "│    │    └─2.attn.proj.1.bn.bias                                                 ├─384\n",
            "│    │    └─2.mlp.ln1.linear.weight                                               ├─294,912\n",
            "│    │    └─2.mlp.ln1.bn.weight                                                   ├─768\n",
            "│    │    └─2.mlp.ln1.bn.bias                                                     ├─768\n",
            "│    │    └─2.mlp.ln2.linear.weight                                               ├─294,912\n",
            "│    │    └─2.mlp.ln2.bn.weight                                                   ├─384\n",
            "│    │    └─2.mlp.ln2.bn.bias                                                     ├─384\n",
            "│    │    └─3.attn.attention_biases                                               ├─294\n",
            "│    │    └─3.attn.qkv.linear.weight                                              ├─442,368\n",
            "│    │    └─3.attn.qkv.bn.weight                                                  ├─1,152\n",
            "│    │    └─3.attn.qkv.bn.bias                                                    ├─1,152\n",
            "│    │    └─3.attn.proj.1.linear.weight                                           ├─147,456\n",
            "│    │    └─3.attn.proj.1.bn.weight                                               ├─384\n",
            "│    │    └─3.attn.proj.1.bn.bias                                                 ├─384\n",
            "│    │    └─3.mlp.ln1.linear.weight                                               ├─294,912\n",
            "│    │    └─3.mlp.ln1.bn.weight                                                   ├─768\n",
            "│    │    └─3.mlp.ln1.bn.bias                                                     ├─768\n",
            "│    │    └─3.mlp.ln2.linear.weight                                               ├─294,912\n",
            "│    │    └─3.mlp.ln2.bn.weight                                                   ├─384\n",
            "│    │    └─3.mlp.ln2.bn.bias                                                     └─384\n",
            "│    │    └─LevitBlock: 3-15                            [32, 49, 384]             1,185,318\n",
            "│    │    │    └─attn.attention_biases                                            ├─294\n",
            "│    │    │    └─attn.qkv.linear.weight                                           ├─442,368\n",
            "│    │    │    └─attn.qkv.bn.weight                                               ├─1,152\n",
            "│    │    │    └─attn.qkv.bn.bias                                                 ├─1,152\n",
            "│    │    │    └─attn.proj.1.linear.weight                                        ├─147,456\n",
            "│    │    │    └─attn.proj.1.bn.weight                                            ├─384\n",
            "│    │    │    └─attn.proj.1.bn.bias                                              ├─384\n",
            "│    │    │    └─mlp.ln1.linear.weight                                            ├─294,912\n",
            "│    │    │    └─mlp.ln1.bn.weight                                                ├─768\n",
            "│    │    │    └─mlp.ln1.bn.bias                                                  ├─768\n",
            "│    │    │    └─mlp.ln2.linear.weight                                            ├─294,912\n",
            "│    │    │    └─mlp.ln2.bn.weight                                                ├─384\n",
            "│    │    │    └─mlp.ln2.bn.bias                                                  └─384\n",
            "│    │    └─LevitBlock: 3-16                            [32, 49, 384]             1,185,318\n",
            "│    │    │    └─attn.attention_biases                                            ├─294\n",
            "│    │    │    └─attn.qkv.linear.weight                                           ├─442,368\n",
            "│    │    │    └─attn.qkv.bn.weight                                               ├─1,152\n",
            "│    │    │    └─attn.qkv.bn.bias                                                 ├─1,152\n",
            "│    │    │    └─attn.proj.1.linear.weight                                        ├─147,456\n",
            "│    │    │    └─attn.proj.1.bn.weight                                            ├─384\n",
            "│    │    │    └─attn.proj.1.bn.bias                                              ├─384\n",
            "│    │    │    └─mlp.ln1.linear.weight                                            ├─294,912\n",
            "│    │    │    └─mlp.ln1.bn.weight                                                ├─768\n",
            "│    │    │    └─mlp.ln1.bn.bias                                                  ├─768\n",
            "│    │    │    └─mlp.ln2.linear.weight                                            ├─294,912\n",
            "│    │    │    └─mlp.ln2.bn.weight                                                ├─384\n",
            "│    │    │    └─mlp.ln2.bn.bias                                                  └─384\n",
            "│    │    └─LevitBlock: 3-17                            [32, 49, 384]             1,185,318\n",
            "│    │    │    └─attn.attention_biases                                            ├─294\n",
            "│    │    │    └─attn.qkv.linear.weight                                           ├─442,368\n",
            "│    │    │    └─attn.qkv.bn.weight                                               ├─1,152\n",
            "│    │    │    └─attn.qkv.bn.bias                                                 ├─1,152\n",
            "│    │    │    └─attn.proj.1.linear.weight                                        ├─147,456\n",
            "│    │    │    └─attn.proj.1.bn.weight                                            ├─384\n",
            "│    │    │    └─attn.proj.1.bn.bias                                              ├─384\n",
            "│    │    │    └─mlp.ln1.linear.weight                                            ├─294,912\n",
            "│    │    │    └─mlp.ln1.bn.weight                                                ├─768\n",
            "│    │    │    └─mlp.ln1.bn.bias                                                  ├─768\n",
            "│    │    │    └─mlp.ln2.linear.weight                                            ├─294,912\n",
            "│    │    │    └─mlp.ln2.bn.weight                                                ├─384\n",
            "│    │    │    └─mlp.ln2.bn.bias                                                  └─384\n",
            "│    │    └─LevitBlock: 3-18                            [32, 49, 384]             1,185,318\n",
            "│    │    │    └─attn.attention_biases                                            ├─294\n",
            "│    │    │    └─attn.qkv.linear.weight                                           ├─442,368\n",
            "│    │    │    └─attn.qkv.bn.weight                                               ├─1,152\n",
            "│    │    │    └─attn.qkv.bn.bias                                                 ├─1,152\n",
            "│    │    │    └─attn.proj.1.linear.weight                                        ├─147,456\n",
            "│    │    │    └─attn.proj.1.bn.weight                                            ├─384\n",
            "│    │    │    └─attn.proj.1.bn.bias                                              ├─384\n",
            "│    │    │    └─mlp.ln1.linear.weight                                            ├─294,912\n",
            "│    │    │    └─mlp.ln1.bn.weight                                                ├─768\n",
            "│    │    │    └─mlp.ln1.bn.bias                                                  ├─768\n",
            "│    │    │    └─mlp.ln2.linear.weight                                            ├─294,912\n",
            "│    │    │    └─mlp.ln2.bn.weight                                                ├─384\n",
            "│    │    │    └─mlp.ln2.bn.bias                                                  └─384\n",
            "├─Sequential: 1-4                                       [32, 512, 7, 7]           --\n",
            "│    └─0.weight                                                                   ├─196,608\n",
            "│    └─0.bias                                                                     ├─512\n",
            "│    └─1.weight                                                                   ├─512\n",
            "│    └─1.bias                                                                     └─512\n",
            "│    └─Conv2d: 2-12                                     [32, 512, 7, 7]           197,120\n",
            "│    │    └─weight                                                                ├─196,608\n",
            "│    │    └─bias                                                                  └─512\n",
            "│    └─BatchNorm2d: 2-13                                [32, 512, 7, 7]           1,024\n",
            "│    │    └─weight                                                                ├─512\n",
            "│    │    └─bias                                                                  └─512\n",
            "│    └─ReLU: 2-14                                       [32, 512, 7, 7]           --\n",
            "├─NormLinear: 1-5                                       [32, 9]                   --\n",
            "│    └─bn.weight                                                                  ├─512\n",
            "│    └─bn.bias                                                                    ├─512\n",
            "│    └─linear.weight                                                              ├─4,608\n",
            "│    └─linear.bias                                                                └─9\n",
            "│    └─BatchNorm1d: 2-15                                [32, 512]                 1,024\n",
            "│    │    └─weight                                                                ├─512\n",
            "│    │    └─bias                                                                  └─512\n",
            "│    └─Dropout: 2-16                                    [32, 512]                 --\n",
            "│    └─Linear: 2-17                                     [32, 9]                   4,617\n",
            "│    │    └─weight                                                                ├─4,608\n",
            "│    │    └─bias                                                                  └─9\n",
            "├─NormLinear: 1-6                                       [32, 9]                   --\n",
            "│    └─bn.weight                                                                  ├─512\n",
            "│    └─bn.bias                                                                    ├─512\n",
            "│    └─linear.weight                                                              ├─4,608\n",
            "│    └─linear.bias                                                                └─9\n",
            "│    └─BatchNorm1d: 2-18                                [32, 512]                 1,024\n",
            "│    │    └─weight                                                                ├─512\n",
            "│    │    └─bias                                                                  └─512\n",
            "│    └─Dropout: 2-19                                    [32, 512]                 --\n",
            "│    └─Linear: 2-20                                     [32, 9]                   4,617\n",
            "│    │    └─weight                                                                ├─4,608\n",
            "│    │    └─bias                                                                  └─9\n",
            "=========================================================================================================\n",
            "Total params: 8,339,338\n",
            "Trainable params: 8,339,338\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (Units.GIGABYTES): 28.27\n",
            "=========================================================================================================\n",
            "Input size (MB): 19.27\n",
            "Forward/backward pass size (MB): 1392.35\n",
            "Params size (MB): 33.34\n",
            "Estimated Total Size (MB): 1444.96\n",
            "=========================================================================================================\n"
          ]
        }
      ],
      "source": [
        "print(summary(model, input_size=(32, 3, 224, 224), verbose=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2XJWJO0JTIMT",
        "outputId": "d84e0e1d-bf2f-4488-bd0f-c57081860e4f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LevitDistilled(\n",
              "  (stem): Stem16(\n",
              "    (conv1): ConvNorm(\n",
              "      (linear): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (act1): Hardswish()\n",
              "    (conv2): ConvNorm(\n",
              "      (linear): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (act2): Hardswish()\n",
              "    (conv3): ConvNorm(\n",
              "      (linear): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (act3): Hardswish()\n",
              "    (conv4): ConvNorm(\n",
              "      (linear): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (stage1): LevitStage(\n",
              "    (downsample): Identity()\n",
              "    (blocks): Sequential(\n",
              "      (0): LevitBlock(\n",
              "        (attn): Attention(\n",
              "          (qkv): LinearNorm(\n",
              "            (linear): Linear(in_features=256, out_features=768, bias=False)\n",
              "            (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "          (proj): Sequential(\n",
              "            (0): Hardswish()\n",
              "            (1): LinearNorm(\n",
              "              (linear): Linear(in_features=256, out_features=256, bias=False)\n",
              "              (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (drop_path1): Identity()\n",
              "        (mlp): LevitMlp(\n",
              "          (ln1): LinearNorm(\n",
              "            (linear): Linear(in_features=256, out_features=512, bias=False)\n",
              "            (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "          (act): Hardswish()\n",
              "          (drop): Dropout(p=0.5, inplace=False)\n",
              "          (ln2): LinearNorm(\n",
              "            (linear): Linear(in_features=512, out_features=256, bias=False)\n",
              "            (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "      (1): LevitBlock(\n",
              "        (attn): Attention(\n",
              "          (qkv): LinearNorm(\n",
              "            (linear): Linear(in_features=256, out_features=768, bias=False)\n",
              "            (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "          (proj): Sequential(\n",
              "            (0): Hardswish()\n",
              "            (1): LinearNorm(\n",
              "              (linear): Linear(in_features=256, out_features=256, bias=False)\n",
              "              (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (drop_path1): Identity()\n",
              "        (mlp): LevitMlp(\n",
              "          (ln1): LinearNorm(\n",
              "            (linear): Linear(in_features=256, out_features=512, bias=False)\n",
              "            (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "          (act): Hardswish()\n",
              "          (drop): Dropout(p=0.5, inplace=False)\n",
              "          (ln2): LinearNorm(\n",
              "            (linear): Linear(in_features=512, out_features=256, bias=False)\n",
              "            (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "      (2): LevitBlock(\n",
              "        (attn): Attention(\n",
              "          (qkv): LinearNorm(\n",
              "            (linear): Linear(in_features=256, out_features=768, bias=False)\n",
              "            (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "          (proj): Sequential(\n",
              "            (0): Hardswish()\n",
              "            (1): LinearNorm(\n",
              "              (linear): Linear(in_features=256, out_features=256, bias=False)\n",
              "              (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (drop_path1): Identity()\n",
              "        (mlp): LevitMlp(\n",
              "          (ln1): LinearNorm(\n",
              "            (linear): Linear(in_features=256, out_features=512, bias=False)\n",
              "            (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "          (act): Hardswish()\n",
              "          (drop): Dropout(p=0.5, inplace=False)\n",
              "          (ln2): LinearNorm(\n",
              "            (linear): Linear(in_features=512, out_features=256, bias=False)\n",
              "            (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "      (3): LevitBlock(\n",
              "        (attn): Attention(\n",
              "          (qkv): LinearNorm(\n",
              "            (linear): Linear(in_features=256, out_features=768, bias=False)\n",
              "            (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "          (proj): Sequential(\n",
              "            (0): Hardswish()\n",
              "            (1): LinearNorm(\n",
              "              (linear): Linear(in_features=256, out_features=256, bias=False)\n",
              "              (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (drop_path1): Identity()\n",
              "        (mlp): LevitMlp(\n",
              "          (ln1): LinearNorm(\n",
              "            (linear): Linear(in_features=256, out_features=512, bias=False)\n",
              "            (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "          (act): Hardswish()\n",
              "          (drop): Dropout(p=0.5, inplace=False)\n",
              "          (ln2): LinearNorm(\n",
              "            (linear): Linear(in_features=512, out_features=256, bias=False)\n",
              "            (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (stage2): LevitStage(\n",
              "    (downsample): CNNDownsample(\n",
              "      (conv): Conv2d(256, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "      (act): Hardswish()\n",
              "    )\n",
              "    (blocks): Sequential(\n",
              "      (0): LevitBlock(\n",
              "        (attn): Attention(\n",
              "          (qkv): LinearNorm(\n",
              "            (linear): Linear(in_features=384, out_features=1152, bias=False)\n",
              "            (bn): BatchNorm1d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "          (proj): Sequential(\n",
              "            (0): Hardswish()\n",
              "            (1): LinearNorm(\n",
              "              (linear): Linear(in_features=384, out_features=384, bias=False)\n",
              "              (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (drop_path1): Identity()\n",
              "        (mlp): LevitMlp(\n",
              "          (ln1): LinearNorm(\n",
              "            (linear): Linear(in_features=384, out_features=768, bias=False)\n",
              "            (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "          (act): Hardswish()\n",
              "          (drop): Dropout(p=0.5, inplace=False)\n",
              "          (ln2): LinearNorm(\n",
              "            (linear): Linear(in_features=768, out_features=384, bias=False)\n",
              "            (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "      (1): LevitBlock(\n",
              "        (attn): Attention(\n",
              "          (qkv): LinearNorm(\n",
              "            (linear): Linear(in_features=384, out_features=1152, bias=False)\n",
              "            (bn): BatchNorm1d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "          (proj): Sequential(\n",
              "            (0): Hardswish()\n",
              "            (1): LinearNorm(\n",
              "              (linear): Linear(in_features=384, out_features=384, bias=False)\n",
              "              (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (drop_path1): Identity()\n",
              "        (mlp): LevitMlp(\n",
              "          (ln1): LinearNorm(\n",
              "            (linear): Linear(in_features=384, out_features=768, bias=False)\n",
              "            (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "          (act): Hardswish()\n",
              "          (drop): Dropout(p=0.5, inplace=False)\n",
              "          (ln2): LinearNorm(\n",
              "            (linear): Linear(in_features=768, out_features=384, bias=False)\n",
              "            (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "      (2): LevitBlock(\n",
              "        (attn): Attention(\n",
              "          (qkv): LinearNorm(\n",
              "            (linear): Linear(in_features=384, out_features=1152, bias=False)\n",
              "            (bn): BatchNorm1d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "          (proj): Sequential(\n",
              "            (0): Hardswish()\n",
              "            (1): LinearNorm(\n",
              "              (linear): Linear(in_features=384, out_features=384, bias=False)\n",
              "              (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (drop_path1): Identity()\n",
              "        (mlp): LevitMlp(\n",
              "          (ln1): LinearNorm(\n",
              "            (linear): Linear(in_features=384, out_features=768, bias=False)\n",
              "            (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "          (act): Hardswish()\n",
              "          (drop): Dropout(p=0.5, inplace=False)\n",
              "          (ln2): LinearNorm(\n",
              "            (linear): Linear(in_features=768, out_features=384, bias=False)\n",
              "            (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "      (3): LevitBlock(\n",
              "        (attn): Attention(\n",
              "          (qkv): LinearNorm(\n",
              "            (linear): Linear(in_features=384, out_features=1152, bias=False)\n",
              "            (bn): BatchNorm1d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "          (proj): Sequential(\n",
              "            (0): Hardswish()\n",
              "            (1): LinearNorm(\n",
              "              (linear): Linear(in_features=384, out_features=384, bias=False)\n",
              "              (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (drop_path1): Identity()\n",
              "        (mlp): LevitMlp(\n",
              "          (ln1): LinearNorm(\n",
              "            (linear): Linear(in_features=384, out_features=768, bias=False)\n",
              "            (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "          (act): Hardswish()\n",
              "          (drop): Dropout(p=0.5, inplace=False)\n",
              "          (ln2): LinearNorm(\n",
              "            (linear): Linear(in_features=768, out_features=384, bias=False)\n",
              "            (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (conv1x1): Sequential(\n",
              "    (0): Conv2d(384, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "  )\n",
              "  (head): NormLinear(\n",
              "    (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (drop): Dropout(p=0.0, inplace=False)\n",
              "    (linear): Linear(in_features=512, out_features=9, bias=True)\n",
              "  )\n",
              "  (head_dist): NormLinear(\n",
              "    (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (drop): Dropout(p=0.0, inplace=False)\n",
              "    (linear): Linear(in_features=512, out_features=9, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ],
      "source": [
        "import torch.nn.init as init\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)  # CUDA 연산 시 동일한 결과 보장\n",
        "    torch.cuda.manual_seed_all(seed)  # 멀티-GPU 환경에서 동일한 결과 보장\n",
        "    torch.backends.cudnn.deterministic = True  # CuDNN 연산을 deterministic하게 설정\n",
        "    torch.backends.cudnn.benchmark = False  # 연산 속도를 희생하고 일관된 연산을 수행\n",
        "\n",
        "def initialize_weights(m):\n",
        "    if isinstance(m, nn.Conv2d):  # Conv 레이어 초기화\n",
        "        init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "        if m.bias is not None:\n",
        "            init.constant_(m.bias, 0)\n",
        "    elif isinstance(m, nn.Linear):  # Linear 레이어 초기화\n",
        "        init.xavier_uniform_(m.weight)\n",
        "        if m.bias is not None:\n",
        "            init.constant_(m.bias, 0)\n",
        "    elif isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):  # BatchNorm 초기화\n",
        "        init.constant_(m.weight, 1)\n",
        "        init.constant_(m.bias, 0)\n",
        "\n",
        "set_seed(42)  # 랜덤 시드 고정\n",
        "model.apply(initialize_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "4TSObUGwcYxd"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(lr=learning_rate, params=model.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "1No-7jzGgzrV"
      },
      "outputs": [],
      "source": [
        "train_losses = []\n",
        "train_accuracies = []\n",
        "val_losses = []\n",
        "val_accuracies = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "Yf0ZPGLtz_gB"
      },
      "outputs": [],
      "source": [
        "def train(model, train_loader, criterion, optimizer, device, epoch):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for i, (inputs, labels) in enumerate(tqdm(train_loader, desc=\"Training\")):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Train Loss: {epoch_loss:.4f}, Train Accuracy: {accuracy:.2f}%\")\n",
        "    train_losses.append(epoch_loss)\n",
        "    train_accuracies.append(accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "rbdjXJ520Btp"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, data_loader, criterion, device, epoch, phase=\"Validation\"):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, labels) in enumerate(tqdm(data_loader, desc=f\"{phase}\")):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            # Save all labels and predictions for balanced accuracy\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "    epoch_loss = running_loss / len(data_loader)\n",
        "    accuracy = 100 * correct / total\n",
        "    balanced_acc = balanced_accuracy_score(all_labels, all_predictions)\n",
        "\n",
        "    print(f\"{phase} Loss: {epoch_loss:.4f}, {phase} Accuracy: {accuracy:.2f}%\")\n",
        "    print(f\"Balanced Accuracy: {balanced_acc:.4f}\")\n",
        "\n",
        "    val_losses.append(epoch_loss)\n",
        "    val_accuracies.append(accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "GfBjePI-0DkY"
      },
      "outputs": [],
      "source": [
        "def measure_inference_time(model, data_loader, device):\n",
        "    model.eval()\n",
        "    times = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, _ in data_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            start_time = torch.cuda.Event(enable_timing=True)\n",
        "            end_time = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "            start_time.record()\n",
        "            _ = model(inputs)  # inference 수행\n",
        "            end_time.record()\n",
        "\n",
        "            # 시간 측정\n",
        "            torch.cuda.synchronize()  # CUDA에서 모든 커널이 완료될 때까지 대기\n",
        "            elapsed_time = start_time.elapsed_time(end_time)  # 밀리초 단위로 반환\n",
        "            times.append(elapsed_time)\n",
        "\n",
        "    # 통계량 계산\n",
        "    times_np = np.array(times)\n",
        "    total_inferences = len(times_np)\n",
        "    avg_time = np.mean(times_np)\n",
        "    std_dev = np.std(times_np)\n",
        "    max_time = np.max(times_np)\n",
        "    min_time = np.min(times_np)\n",
        "\n",
        "    # 결과 출력\n",
        "    print(f\"Inference Time Measurement Results:\")\n",
        "    print(f\"Total Inferences: {total_inferences}\")\n",
        "    print(f\"Average Time: {avg_time:.2f} ms\")\n",
        "    print(f\"Standard Deviation: {std_dev:.2f} ms\")\n",
        "    print(f\"Maximum Time: {max_time:.2f} ms\")\n",
        "    print(f\"Minimum Time: {min_time:.2f} ms\")\n",
        "\n",
        "    return times"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c77rmtOz0XuA",
        "outputId": "d36e02a6-f605-4fd5-bb8f-369e3326880b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                           aten::matmul         1.99%     366.347us        22.20%       4.089ms      85.190us       0.000us         0.00%       5.047ms     105.144us            48  \n",
            "                                           aten::linear         0.83%     153.619us        13.72%       2.528ms      74.345us       0.000us         0.00%       3.611ms     106.215us            34  \n",
            "                                               aten::mm         7.13%       1.313ms         9.84%       1.812ms      56.636us       3.587ms        37.60%       3.587ms     112.103us            32  \n",
            "                                       aten::batch_norm         1.10%     202.192us        25.32%       4.664ms     119.586us       0.000us         0.00%       2.039ms      52.287us            39  \n",
            "                           aten::_batch_norm_impl_index         1.77%     326.989us        24.22%       4.462ms     114.402us       0.000us         0.00%       2.039ms      52.287us            39  \n",
            "                                 ampere_sgemm_32x128_tn         0.00%       0.000us         0.00%       0.000us       0.000us       1.350ms        14.15%       1.350ms     168.804us             8  \n",
            "                                aten::native_batch_norm         6.08%       1.121ms        17.65%       3.252ms      95.655us       1.312ms        13.76%       1.341ms      39.439us            34  \n",
            "                                              aten::bmm         2.72%     501.523us         3.45%     634.803us      39.675us       1.131ms        11.85%       1.131ms      70.672us            16  \n",
            "                                ampere_sgemm_128x128_nn         0.00%       0.000us         0.00%       0.000us       0.000us     986.113us        10.34%     986.113us     123.264us             8  \n",
            "void at::native::batch_norm_collect_statistics_chann...         0.00%       0.000us         0.00%       0.000us       0.000us     864.129us         9.06%     864.129us      25.416us            34  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 18.422ms\n",
            "Self CUDA time total: 9.540ms\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from torch import profiler\n",
        "\n",
        "dummy_input = torch.randn(32, 3, 224, 224).cuda()\n",
        "\n",
        "# Profiling inference\n",
        "with profiler.profile(\n",
        "    activities=[\n",
        "       profiler.ProfilerActivity.CPU,\n",
        "        profiler.ProfilerActivity.CUDA,  # Include if using GPU\n",
        "    ],\n",
        "    on_trace_ready=profiler.tensorboard_trace_handler(\"./logs\"),  # Optional logging\n",
        "    record_shapes=True,\n",
        "    with_stack=True\n",
        ") as prof:\n",
        "    with torch.no_grad():\n",
        "        model(dummy_input)\n",
        "\n",
        "\n",
        "# Print results\n",
        "print(prof.key_averages().table(sort_by=\"cuda_time_total\" if torch.cuda.is_available() else \"cpu_time_total\", row_limit=10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "jXvKsupvZYjZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score, recall_score, precision_score, roc_curve, auc\n",
        "\n",
        "def score_evaluate(model, data_loader, criterion, device, phase=\"Validation\"):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(data_loader, desc=f\"{phase}\"):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "\n",
        "    overall_f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
        "    overall_recall = recall_score(all_labels, all_preds, average=\"macro\")\n",
        "    overall_precision = precision_score(all_labels, all_preds, average=\"macro\")\n",
        "\n",
        "    f1_per_class = f1_score(all_labels, all_preds, average=None)\n",
        "    recall_per_class = recall_score(all_labels, all_preds, average=None)\n",
        "    precision_per_class = precision_score(all_labels, all_preds, average=None)\n",
        "    class_labels = sorted(set(all_labels))\n",
        "\n",
        "    epoch_loss = running_loss / len(data_loader)\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"{phase} Loss: {epoch_loss:.4f}, {phase} Accuracy: {accuracy:.2f}%\")\n",
        "    val_losses.append(epoch_loss)\n",
        "    val_accuracies.append(accuracy)\n",
        "\n",
        "    print(f\"Overall - F1: {overall_f1:.4f}, Recall: {overall_recall:.4f}, Precision: {overall_precision:.4f}\")\n",
        "    print(\"Per-Class Metrics:\")\n",
        "    for i, label in enumerate(class_labels):\n",
        "        print(f\"Class {label} - F1: {f1_per_class[i]:.4f}, Recall: {recall_per_class[i]:.4f}, Precision: {precision_per_class[i]:.4f}\")\n",
        "\n",
        "    return overall_f1, overall_recall, overall_precision, f1_per_class, recall_per_class, precision_per_class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "oe9DO2tS-BpI"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "def roc_auc(model, data_loader, device, num_classes):\n",
        "\n",
        "    y = [\"F1\", \"Precision\", \"Recall\"]\n",
        "\n",
        "    model.eval()\n",
        "    all_labels = []\n",
        "    all_scores = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(data_loader, desc=\"ROC AUC\"):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_scores.extend(outputs.cpu().numpy())\n",
        "\n",
        "    all_labels = label_binarize(all_labels, classes=list(range(num_classes)))\n",
        "    print(f\"Binarized all_labels shape: {all_labels.shape}\")\n",
        "    print(f\"All_scores shape: {np.array(all_scores).shape}\")\n",
        "\n",
        "    fpr, tpr, _ = roc_curve(all_labels.ravel(), np.array(all_scores).ravel())\n",
        "    roc_auc_value = auc(fpr, tpr)\n",
        "\n",
        "    plt.figure(figsize=(9, 7))\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc_value:0.4f})')\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"Overall ROC AUC: {roc_auc_value:.4f}\")\n",
        "\n",
        "    return fpr, tpr, roc_auc_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29ACr21nOjBW",
        "outputId": "ebe154b7-e850-4f77-db05-4905743b4c51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-72-a84bcb28da6b>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('BaseLine_HoViT_44.pth'))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ],
      "source": [
        "model.load_state_dict(torch.load('BaseLine_HoViT_44.pth'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O CRC-VAL-HE-7K.zip https://zenodo.org/records/1214456/files/CRC-VAL-HE-7K.zip\n",
        "!unzip -qq CRC-VAL-HE-7K.zip -d val"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbSpMMWGk3bf",
        "outputId": "3a6e187e-ec98-4332-eb11-5f8fb939f5c1"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-26 15:21:56--  https://zenodo.org/records/1214456/files/CRC-VAL-HE-7K.zip\n",
            "Resolving zenodo.org (zenodo.org)... 188.185.43.25, 188.185.48.194, 188.185.45.92, ...\n",
            "Connecting to zenodo.org (zenodo.org)|188.185.43.25|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 800276929 (763M) [application/octet-stream]\n",
            "Saving to: ‘CRC-VAL-HE-7K.zip’\n",
            "\n",
            "CRC-VAL-HE-7K.zip   100%[===================>] 763.20M  4.62MB/s    in 7m 38s  \n",
            "\n",
            "2025-02-26 15:29:35 (1.66 MB/s) - ‘CRC-VAL-HE-7K.zip’ saved [800276929/800276929]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])"
      ],
      "metadata": {
        "id": "KheuiMZL8GDW"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dir = './val/CRC-VAL-HE-7K'\n",
        "test_dataset = datasets.ImageFolder(root=test_dir, transform=transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "rldaXFNXpT4r"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(model, test_loader, criterion, device, epoch=0, phase=\"Test\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMwSo6Nc6hoM",
        "outputId": "c016c9f5-9fcc-4864-8d0c-4310b653f61b"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Test: 100%|██████████| 225/225 [00:11<00:00, 19.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 1.1187, Test Accuracy: 85.19%\n",
            "Balanced Accuracy: 0.8489\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "overall_f1, overall_recall, overall_precision, f1_per_class, recall_per_class, precision_per_class = score_evaluate(model, test_loader, criterion, device, phase=\"Test\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2y3cywDpjUQ",
        "outputId": "749d2218-60ab-49bc-d1e9-f4a77af89387"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Test: 100%|██████████| 225/225 [00:11<00:00, 19.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 1.1187, Test Accuracy: 85.19%\n",
            "Overall - F1: 0.8305, Recall: 0.8489, Precision: 0.8318\n",
            "Per-Class Metrics:\n",
            "Class 0 - F1: 0.7963, Recall: 0.6704, Precision: 0.9803\n",
            "Class 1 - F1: 0.9385, Recall: 1.0000, Precision: 0.8841\n",
            "Class 2 - F1: 0.8210, Recall: 0.9676, Precision: 0.7130\n",
            "Class 3 - F1: 0.9798, Recall: 0.9921, Precision: 0.9677\n",
            "Class 4 - F1: 0.9253, Recall: 0.8734, Precision: 0.9837\n",
            "Class 5 - F1: 0.5295, Recall: 0.6672, Precision: 0.4389\n",
            "Class 6 - F1: 0.9163, Recall: 0.9676, Precision: 0.8701\n",
            "Class 7 - F1: 0.6150, Recall: 0.5558, Precision: 0.6882\n",
            "Class 8 - F1: 0.9530, Recall: 0.9457, Precision: 0.9605\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlazeL-a0WTt",
        "outputId": "daee00ff-fd6d-43f5-e0bf-37ce0061f5db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference Time Measurement Results:\n",
            "Total Inferences: 225\n",
            "Average Time: 12.35 ms\n",
            "Standard Deviation: 0.56 ms\n",
            "Maximum Time: 15.53 ms\n",
            "Minimum Time: 11.85 ms\n"
          ]
        }
      ],
      "source": [
        "times = measure_inference_time(model, test_loader, device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = [\"ADI\", \"BACK\", \"DEB\", \"LYM\", \"MUC\", \"MUS\", \"NORM\", 'STR', \"TUM\"]\n",
        "y = [\"F1\", \"Precision\", \"Recall\"]\n",
        "\n",
        "f1_score_list = []\n",
        "precision_score_list = []\n",
        "recall_score_list = []\n",
        "\n",
        "for i, class_name in enumerate(x):\n",
        "    f1_score_list.append(f1_per_class[i])\n",
        "    precision_score_list.append(precision_per_class[i])\n",
        "    recall_score_list.append(recall_per_class[i])\n",
        "\n",
        "x.append(\"Overall\")\n",
        "f1_score_list.append(overall_f1)\n",
        "precision_score_list.append(overall_precision)\n",
        "recall_score_list.append(overall_recall)\n",
        "\n",
        "score_list = [f1_score_list, precision_score_list, recall_score_list]\n",
        "score_list = np.array(score_list)\n",
        "plt.figure(figsize=(20, 12))\n",
        "score_cm = plt.matshow(score_list, cmap=plt.cm.Blues, alpha=0.3)\n",
        "plt.xticks(range(len(x)), x)\n",
        "plt.yticks(range(len(y)), y)\n",
        "for i in range(len(y)):\n",
        "    for j in range(len(x)):\n",
        "        # 행렬의 각각의 수치를 각 칸의 중앙에 넣어준다\n",
        "        plt.text(x=j, y=i,\n",
        "                     s=\"{:.2f}\".format(score_list[i, j]),\n",
        "                     va='center',\n",
        "                     ha='center',\n",
        "                     )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "id": "p4pYjMPUpy2m",
        "outputId": "308af3d6-33e5-42cf-e784-5d4fda28df94"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2000x1200 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1600x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5EnagUDtpzDU"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}